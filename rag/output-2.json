[
  {
    "title": "Going into production — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/admin/going-into-production.html#going-into-production",
    "html": "Going into production\n\nRunning CrateDB in different environments requires different approaches. This document outlines the basics you need to consider when going into production.\n\nTable of contents\n\nConfigure bootstrapping\n\nNaming\n\nConfigure a logical cluster name\n\nBind nodes to logical hostnames\n\nLogical node labels\n\nMulti-purpose nodes\n\nRequest handling and query execution nodes\n\nCluster management nodes\n\nRequest handling nodes\n\nConfigure persistent data paths\n\nTune the JVM\n\nHeap\n\nGarbage collection\n\nConfigure wire encryption\n\nConfigure bootstrapping\n\nThe process of forming a cluster is known as bootstrapping. Consult the how-to guide on CrateDB multi-node setups for an overview of the two different ways to bootstrap a cluster.\n\nIf you have been using CrateDB for development on your local machine, there is a good chance you have been using single host auto-bootstrapping.\n\nFor improved performance and resiliency, you should run production CrateDB clusters with three or more nodes and one node per host machine. To do this, you must manually configure the bootstrapping process by telling nodes how to:\n\nDiscover other nodes, more details at node discovery.\n\nElect a master node, more details at master node election.\n\nThis process is known as manual bootstrapping. See the how-to guide for more information about how to bootstrap a cluster manually.\n\nSwitching to a manual bootstrapping configuration is the first step towards going into production.\n\nNaming\nConfigure a logical cluster name\n\nThe cluster.name setting allows you to override the default cluster name of crate. You should use this setting to give a logical name to your cluster.\n\nFor example, add this to your configuration file:\n\ncluster.name: acme-prod\n\n\nThe acme-prod name suggests that this cluster is the production cluster for the Acme organization. If Acme has a cluster running in a staging environment, you might want to name it acme-staging. This way, you can differentiate your clusters by name (visible in the Admin UI).\n\nTip\n\nA node will refuse to join a cluster if the respective cluster names do not match\n\nSee Also\n\nCluster names for multi-node setups\n\nBind nodes to logical hostnames\n\nBy default, CrateDB binds to the loopback address (i.e., localhost). It listens on port 4200-4299 for HTTP traffic and port 4300-4399 for node-to-node communication. Because CrateDB uses a port range, if one port is busy, it will automatically try the next port.\n\nWhen using multiple hosts, nodes must bind to a non-loopback address.\n\nCaution\n\nNever expose an unprotected CrateDB node to the public internet\n\nYou can bind to a non-loopback address with the network.host setting in your configuration file, like so:\n\nnetwork.host: node-01-md.acme-prod.internal.example.com\n\n\nYou must configure the node-01-md.acme-prod.internal.example.com hostname using DNS. You must then set network.host to match the DNS name.\n\nYou should use the hostname to describe each node logically. To this end, the example hostname (above) has four components:\n\nexample.com – The root domain name\n\ninternal – The internal private network\n\nacme-prod – The cluster name\n\nnode-01-md – The node label\n\nWhen CrateDB is bound to a non-loopback address, CrateDB will enforce the bootstrap checks. These checks may require changes to your operating system configuration.\n\nSee Also\n\nHost settings\n\nLogical node labels\n\nCrateDB supports multiple types of node, determined by the node.master and node.data settings. You can use this information to give a logical DNS label to each of your nodes.\n\nTip\n\nCrateDB sets node names automatically. If you are happy with automatic node names, there is no need to set node.name and hence you can use the same configuration on every node.\n\nWhen configuring cluster bootstrapping, you can specify the list of master-eligible nodes using hostnames. This allows you to configure logical hostnames with DNS node labels that differ from the node name set by CrateDB.\n\nIf you would prefer your node names to match your DNS node labels, you will have to configure node.name manually on each host.\n\nSee Also\n\nNode names for multi-node setups\n\nMulti-purpose nodes\n\nYou can configure a master-eligible node that also handles query execution loads like this:\n\nnode.master: true\nnode.data: true\n\n\nA good DNS label for this node might be node-01-md.\n\nHere, node is used as base label with a sequence number of 01. Every node in the cluster should have a unique sequence number, independent of the node type. The letters md indicate that this node has node.master and node.data set to true.\n\nIf you optionally want your node name to match (see above), configure the node.name setting in your configuration file, like so:\n\nnode.name: node-01-md\n\n\nAlternatively, you can configure this setting at startup with a command-line option:\n\nsh$ bin/crate \\\n        -Cnode.name=node-01-md\n\nRequest handling and query execution nodes\n\nYou can configure a node that only handles client requests and query execution (i.e., is not master-eligible) like this:\n\nnode.master: false\nnode.data: true\n\n\nA good DNS label for this node might be node-02-d.\n\nHere, node is used as base label with a sequence number of 02. Every node in the cluster should have a unique sequence number, independent of the node type. The letter d indicates that this node has node.data set to true.\n\nIf you optionally want your node name to match (see above), configure the node.name setting in your configuration file, like so:\n\nnode.name: node-02-d\n\n\nAlternatively, you can configure this setting at startup with a command-line option:\n\nsh$ bin/crate \\\n        -Cnode.name=node-02-d\n\nCluster management nodes\n\nYou can configure a node that handles cluster management (i.e., is master-eligible) but does not handle query execution loads like this:\n\nnode.master: true\nnode.data: false\n\n\nA good DNS label for this node might be node-03-m.\n\nHere, node is used as base label with a sequence number of 03. Every node in the cluster should have a unique sequence number, independent of the node type. The letter m indicates that this node has node.master set to true.\n\nIf you optionally want your node name to match (see above), configure the node.name setting in your configuration file, like so:\n\nnode.name: node-03-m\n\n\nAlternatively, you can configure this setting at startup with a command-line option:\n\nsh$ bin/crate \\\n        -Cnode.name=node-03-m\n\nRequest handling nodes\n\nYou can configure a node that handles client requests but does not handle query execution loads or cluster management (i.e., is not master-eligible) like this:\n\nnode.master: false\nnode.data: false\n\n\nA good DNS label for this node might be node-04.\n\nHere, node is used as base label with a sequence number of 04. Every node in the cluster should have a unique sequence number, independent of the node type. The absence of any additional letters indicates that node.master and node.data are false.\n\nIf you optionally want your node name to match (see above), configure the node.name setting in your configuration file, like so:\n\nnode.name: node-04\n\n\nAlternatively, you can configure this setting at startup with a command-line option:\n\nsh$ bin/crate \\\n        -Cnode.name=node-04\n\nConfigure persistent data paths\n\nBy default, CrateDB keeps data under the CRATE_HOME directory (which defaults to the installation directory). When you upgrade CrateDB, you will have to switch to a new installation directory.\n\nInstead of migrating data by hand each time, you should move the data directories off to a persistent location. You can do this using the CRATE_HOME environment variable and the path settings in your configuration file.\n\nSee Also\n\nPath settings\n\nIf you are following the shared-nothing approach to deployment, the best way to handle persistent data is to keep it on an external volume. This allows you to persist data beyond the lifespan of an individual virtual machine or container.\n\nCaution\n\nThis is required if you are using Docker, which is stateless by design. Failing to persist data to a mounted volume will result in data loss when the container is stopped.\n\nTip\n\nUsing an external volume for persistence also allows you to optimize the underlying storage mechanism for performance.\n\nYou should take care to size your data storage volumes according to your needs. You should also use storage with high IOPS when possible to improve CrateDB performance.\n\nOn a Unix-like system, you might mount an external volume to a path like /opt/cratedb. If you are installing CrateDB by hand, you can then set CRATE_HOME to /opt/cratedb. Make sure to set CRATE_HOME before running bin/crate.\n\nThen, you could configure your data paths like this:\n\npath.conf: /opt/cratedb/config\npath.data: /opt/cratedb/data\npath.logs: /opt/cratedb/logs\npath.repo: /opt/cratedb/snapshots\n\n\nHere, the values given for path.conf, path.data, and path.logs reflect the default paths when CRATE_HOME is set to /opt/cratedb. The example above configures them for illustrative purposes. You do not have to configure these settings if you are happy with the defaults.\n\nNote\n\nNormally, configuration files, data files, log files, and so on would be kept under specialized directories such as /etc, /var/lib, and /var/log (see the Linux Filesystem Hierarchy for more information).\n\nHowever, if you want to customize your installation to make use of a single external volume, it is necessary to bring these directories together under a single mount point. You can do this by relocating all data directories under your mount point (/opt/cratedb in the example above). Other approaches are possible (for example, using symbolic links).\n\nIf you have installed CrateDB using a system package for Debian, Ubuntu, or Red Hat, the CRATE_HOME variable (as well as some data paths) are configured for by the systemd service file. You can view the crate service file, like so:\n\nsh$ systemctl cat crate\n\nTune the JVM\nHeap\n\nCrateDB is a Java application running on top of a Java Virtual Machine (JVM). The JVM uses a heap for memory allocations. For optimal performance, you must pay special attention to your heap configuration.\n\nBy default, CrateDB configures the JVM to dump out-of-memory exceptions to the file or directory specified by CRATE_HEAP_DUMP_PATH. You must make sure there is enough disk space available for heap dumps at this location.\n\nSee Also\n\nJVM environment variables\n\nGarbage collection\n\nCrateDB logs JVM garbage collection times using the built-in garbage collection (GC) logging provided by the JVM. You can configure this process with the GC logging environment variables.\n\nYou must ensure that the log directory is on a fast-enough disk and has enough space. When using Docker, use a path on a mounted volume.\n\nIf garbage collection takes too long, CrateDB will log this. You can adjust the timeout settings to suit your needs. However, the default settings should work in most instances.\n\nIf you are running CrateDB on Docker, you should configure the container to send debug logs to STDERR so that the container orchestrator handles the output.\n\nConfigure wire encryption\n\nFor security reasons, most production clusters should use wire encryption for network traffic between nodes and clients. Check out the reference manual on secured communications for more information."
  },
  {
    "title": "API — CrateDB Cloud",
    "url": "https://cratedb.com/docs/cloud/en/latest/reference/api.html",
    "html": "API\n\nSimilar to Croud, we offer an API to allow programmatic access to the Cloud products. The API can be accessed by generating a key and secret in your account page:\n\nClick the Generate new key button to create your key. A popup with your key and secret will appear. Make sure to store your secret safely, as you cannot access it again.\n\nAccess\n\nThe key and secret can be used as HTTP Basic Auth credentials when calling the API, e.g.\n\nsh$ $ curl -s -u $your_key:$your_secret https://console.cratedb.cloud/api/v2/users/me\n\n\nThis example will return details of the current user:\n\n{\"email\":\"some@example.com\",\"hmac\":\"...\",\"is_superuser\":false,\"name\":\"Some User\",\"organization_id\":\"123\",\"status\":\"active\",\"uid\":\"uid\",\"username\":\"some@example.com\"}\n\nExamples\n\nThe API is documented with Swagger (login required). It contains endpoints for:\n\nOrganizations\n\nRegions\n\nProjects\n\nClusters\n\nProducts\n\nUsers\n\nRoles\n\nSubscriptions\n\nAudit logs\n\nIt provides example requests with all the required parameters, expected responses, and all response codes. Access the API documentation here (login required)."
  },
  {
    "title": "Clustering — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/admin/clustering/index.html#clustering",
    "html": "Clustering\n\nOne of the benefits of CrateDB is that you can scale out your cluster by adding more nodes. This section of the documentation covers that topic.\n\nTable of contents\n\nCrateDB multi-node setup\nCrateDB multi-zone setup\nScaling Clusters Up and Down\nScaling CrateDB on Kubernetes\nLogical replication setup between CrateDB clusters"
  },
  {
    "title": "Billing — CrateDB Cloud",
    "url": "https://cratedb.com/docs/cloud/en/latest/reference/billing.html",
    "html": "Billing\n\nThis page documents the way billing and related subjects are handled by CrateDB Cloud. Since - depending on your chosen offer - billing and payment forms may appear in different places across the CrateDB Cloud interface, this guide should function as a single reference document for finding the information you need about billing, invoicing, and payments.\n\nYour current accumulated bill can be always found under the billing tab in Organization section:\n\nBilling principles\n\nCrateDB Cloud has three general principles for billing. Firstly, we only ever bill actual usage of any of the provided services. This means there are no flat fees or minimum payments.\n\nSecondly, we only bill for a given period. This means any usage costs are rounded up to the nearest hour of use for Marketplace customers and to the nearest minute of use for customers directly deploying via the CrateDB Cloud Console.\n\nThirdly, Billing is done in $0.001 increments for the compute + storage usage.\n\nBilling information & Payment methods\n\nYour billing information consists of your (company) address, credit card details, country of residence, VAT info, and so forth. This information can be filled out whenever you make use of an offer on CrateDB Cloud that is not free (when you only use a free offer, the billing info page will not be visible to you). There are several ways you can provide the necessary billing information:\n\nYou can add a new payment method even without deploying a new cluster. Simply navigate to the payment methods tab in the Organization section. You can then use this payment method when deploying a cluster later.\n\nAs part of deploying a cluster for a new organization. If you deploy a cluster that is not free, you will be prompted for your billing information as part of the configuration wizard.\n\nBy using the Billing tab in the Organization overview of the CrateDB Cloud Console. If you have a promoted cluster deployed and the free period expires, you can find the Billing tab by going to the Organization overview and clicking the fifth tab from the left. Here you can enter your billing details and your billing method (credit card). See the CrateDB Console walkthrough for more information.\n\nAddress information, along with the payment methods can be edited in the payment methods tab, in Organization section:\n\nInvoicing\n\nInvoicing is handled variously depending on which deployment method you use. If you deploy your cluster directly via the CrateDB Cloud Console, you will be invoiced at the email address you provided when signing up with CrateDB Cloud.\n\nIf you use one of the marketplace offers, the invoicing is handled by the marketplace provider in question and will be part of your general invoicing for services via that marketplace.\n\nCurrently, for direct deployments, VAT charges for EU customers are handled by CrateDB and are added to the invoices described above. For deployments via the marketplaces, any VAT charges due are handled by the respective marketplace owners (Microsoft Azure and AWS).\n\nCustom contract\n\nCrate also offers a special type of payment method suited for large customers that don’t want to use a credit card or marketplace subscriptions (AWS/Azure). With this type of subscription, a contract is created directly with Crate.\n\nNote\n\nThis type of payment method does not show up in your Cloud console automatically as it needs to be configured specifically for every customer.\n\nIf you’re interested in this option, don’t hesitate and contact us at sales@crate.io\n\nPayment processing {#billing-processing}\n\nFor clusters deployed in the regular way, using the CrateDB Console cluster deployment route, payment processing is handled by Stripe. For clusters deployed through the Microsoft Azure Marketplace and the AWS Marketplace, payment is handled by Stripe on behalf of the respective marketplaces."
  },
  {
    "title": "Resiliency Issues — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/resiliency.html",
    "html": "5.6\nResiliency Issues\n\nCrateDB uses Elasticsearch for data distribution and replication. Most of the resiliency issues exist in the Elasticsearch layer and can be tested by Jepsen.\n\nTable of contents\n\nKnown issues\n\nRetry of updates causes double execution\n\nFixed issues\n\nRepeated cluster partitions can cause lost cluster updates\n\nVersion number representing ambiguous row versions\n\nReplicas can fall out of sync when a primary shard fails\n\nLoss of rows due to network partition\n\nDirty reads caused by bad primary handover\n\nChanges are overwritten by old data in danger of lost data\n\nMake table creation resilient to closing and full cluster crashes\n\nUnaware master accepts cluster updates\n\nKnown issues\nRetry of updates causes double execution\nStatus\tWork ongoing (More info)\nSeverity\tModerate\nLikelihood\tVery rare\nCause\tNetwork issues, unresponsive nodes\nWorkloads\tNon-Idempotent writes\n\nScenario\n\nA node with a primary shard receives an update, writes it to disk, but goes offline before having sent a confirmation back to the executing node. When the node comes back online, it receives an update retry and executes the update again.\n\nConsequence\n\nIncorrect data for non-idempotent writes.\n\nFor example:\n\nAn double insert on a table without an explicit primary key would be executed twice and would result in duplicate data.\n\nA double update would incorrectly increment the row version number twice.\n\nFixed issues\nRepeated cluster partitions can cause lost cluster updates\nStatus\tFixed in CrateDB v4.0 (#32006, #32171)\nSeverity\tModerate\nLikelihood\tVery rare\nCause\tNetwork issues, unresponsive nodes\nWorkloads\tAll\n\nScenario\n\nA cluster is partitioned and a new master is elected on the side that has quorum. The cluster is repaired and simultaneously a change is made to the cluster state. The cluster is partitioned again before the new master node has a chance to publish the new cluster state and the partition the master lands on does not have quorum.\n\nConsequence\n\nThe node steps down as master and the uncommunicated state changes are lost.\n\nCluster state is very important and contains information like shard location, schemas, and so on. Lost cluster state updates can cause data loss, reset settings, and problems with table structures.\n\nPartially fixed\n\nThis problem is mostly fixed by #20384 (CrateDB v2.0.x), which uses committed cluster state updates during master election process. This does not fully solve this rare problem but considerably reduces the chance of occurrence. The reason is that if the second partition happens concurrently with a cluster state update and blocks the cluster state commit message from reaching a majority of nodes, it may be that the in flight update is lost. If the now-isolated master can still acknowledge the cluster state update to the client this will result to the loss of an acknowledged change.\n\nVersion number representing ambiguous row versions\nStatus\tFixed in CrateDB v4.0 (#19269, #10708)\nSeverity\tSignificant\nLikelihood\tVery rare\nCause\tNetwork issues, unresponsive nodes\nWorkloads\tVersioned reads with replicated tables while writing.\n\nScenario\n\nA client is writing to a primary shard. The node holding the primary shard is partitioned from the cluster. It usually takes between 30 and 60 seconds (depending on ping configuration) before the master node notices the partition. During this time, the same row is updated on both the primary shard (partitioned) and a replica shard (not partitioned).\n\nConsequence\n\nThere are two different versions of the same row using the same version number. When the primary shard rejoins the cluster and its data is replicated, the update that was made on the replicated shard is lost but the new version number matches the lost update. This will break Optimistic Concurrency Control.\n\nReplicas can fall out of sync when a primary shard fails\nStatus\tFixed in CrateDB v4.0 (#10708)\nSeverity\tModest\nLikelihood\tRare\nCause\tPrimary fails and in-flight writes are only written to a subset of its replicas\nWorkloads\tWrites on replicated table\n\nScenario\n\nWhen a primary shard fails, a replica shard will be promoted to be the primary shard. If there is more than one replica shard, it is possible for the remaining replicas to be out of sync with the new primary shard. This is caused by operations that were in-flight when the primary shard failed and may not have been processed on all replica shards. Currently, the discrepancies are not repaired on primary promotion but instead would be repaired if replica shards are relocated (e.g., from hot to cold nodes); this does mean that the length of time which replicas can be out of sync with the primary shard is unbounded.\n\nConsequence\n\nStale data may be read from replicas.\n\nLoss of rows due to network partition\nStatus\tFixed in Crate v2.0.x (#7572, #14252)\nSeverity\tSignificant\nLikelihood\tVery rare\nCause\tSingle node isolation\nWorkloads\tWrites on replicated table\n\nScenario\n\nA node with a primary shard is partitioned from the cluster. The node continues to accept writes until it notices the network partition. In the meantime, another shard has been elected as the primary. Eventually, the partitioned node rejoins the cluster.\n\nConsequence\n\nData that was written to the original primary shard on the partitioned node is lost as data from the newly elected primary shard replaces it when it rejoins the cluster.\n\nThe risk window depends on your ping configuration. The default configuration of a 30 second ping timeout with three retries corresponds to a 90 second risk window. However, it is very rare for a node to lose connectivity within the cluster but maintain connectivity with clients.\n\nDirty reads caused by bad primary handover\nStatus\tFixed in CrateDB v2.0.x (#15900, #12573)\nSeverity\tModerate\nLikelihood\tRare\nCause\tRace Condition\nWorkloads\tReads\n\nScenario\n\nDuring a primary handover, there is a small risk window when a shard can find out it has been elected as the new primary before the old primary shard notices that it is no longer the primary.\n\nA primary handover can happen in the following scenarios:\n\nA shard is relocated and then elected as the new primary, as two separate but sequential actions. Relocating a shard means creating a new shard and then deleting the old shard.\n\nAn existing replica shard gets promoted to primary because the primary shard was partitioned from the cluster.\n\nConsequence\n\nWrites that occur on the new primary during the risk window will not be replicated to the old shard (which still believes it is the primary) so any subsequent reads on the old shard may return incorrect data.\n\nChanges are overwritten by old data in danger of lost data\nStatus\tFixed in CrateDB v2.0.x (#14671)\nSeverity\tSignificant\nLikelihood\tVery rare\nCause\tNetwork problems\nWorkloads\tWrites\n\nScenario\n\nA node with a primary that contains new data is partitioned from the cluster.\n\nConsequence\n\nCrateDB prefers old data over no data, and so promotes an a shard with stale data as a new primary. The data on the original primary shard is lost. Even if the node with the original primary shard rejoins the cluster, CrateDB has no way of distinguishing correct and incorrect data, so that data replaced with data from the new primary shard.\n\nMake table creation resilient to closing and full cluster crashes\nStatus\tThe issue has been fixed with the following issues. Table recovery: #9126 Reopening tables: #14739 Allocation IDs: #15281\nSeverity\tModest\nLikelihood\tVery Rare\nCause\tEither the cluster fails while recovering a table or the table is closed during shard creation.\nWorkloads\tTable creation\n\nScenario\n\nRecovering a table requires a quorum of shard copies to be available to allocate a primary. This means that a primary cannot be assigned if the cluster dies before enough shards have been allocated. The same happens if a table is closed before enough shard copies were started, making it impossible to reopen the table. Allocation IDs solve this issue by tracking allocated shard copies in the cluster. This makes it possible to safely recover a table in the presence of a single shard copy. Allocation IDs can also distinguish the situation where a table has been created but none of the shards have been started. If such an table was inadvertently closed before at least one shard could be started, a fresh shard will be allocated upon reopening the table.\n\nConsequence\n\nThe primary shard of the table cannot be assigned or a closed table cannot be re-opened.\n\nUnaware master accepts cluster updates\nStatus\tFixed in CrateDB v2.0.x (#13062)\nSeverity\tModerate\nLikelihood\tVery rare\nCause\tNetwork problems\nWorkloads\tDDL statements\n\nScenario\n\nIf a master has lost quorum (i.e. the number of nodes it is in communication with has fallen below the configured minimum) it should step down as master and stop answering requests to perform cluster updates. There is a small risk window between losing quorum and noticing that quorum has been lost, depending on your ping configuration.\n\nConsequence\n\nIf a cluster update request is made to the node between losing quorum and noticing the loss of quorum, that request will be confirmed. However, those updates will be lost because the node will not be able to perform a successful cluster update.\n\nCluster state is very important and contains information like shard location, schemas, and so on. Lost cluster state updates can cause data loss, reset settings, and problems with table structures."
  },
  {
    "title": "Croud CLI — CrateDB Cloud: Croud CLI",
    "url": "https://cratedb.com/docs/cloud/cli/en/latest/",
    "html": "latest\nCroud CLI\n\nCroud is a command-line interface (CLI) tool for interacting with CrateDB Cloud. It is maintained as a pip package.\n\nIt allows to interact with these CrateDB Cloud features:\n\ncluster management\n\nuser management\n\nproject management\n\norganization management\n\nIt also allows to view available regions, active subscriptions, and available CrateDB Cloud products.\n\nTable of Contents\n\nGetting Started\nConfiguration\nCommands\nUser Roles"
  },
  {
    "title": "Bootstrap checks — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/admin/bootstrap-checks.html#bootstrap-checks",
    "html": "Bootstrap checks\n\nIf you are binding to a network reachable IP address, CrateDB performs a number of bootstrap checks during startup. These checks examine your setup and will prevent startup if a problem is detected.\n\nThis best practices guide is intended to help you configure your setup so that CrateDB passes the bootstrap checks and can perform optimally.\n\nIf you are binding to the loopback address, the bootstrap checks will not be run, but it is still a good idea to follow these instructions.\n\nTip\n\nIf you are using Docker, these checks are dependent on the host system. You must configure the host system appropriately if you wish to run CrateDB on Docker. Consult the additional documentation on Docker resource constraints for more information.\n\nTable of contents\n\nSystem settings\n\nOfficial packages\n\nTarball\n\nLinux\n\nGarbage collection\n\nSystem settings\nOfficial packages\n\nIf you are using one of the official packages, all of the necessary operating system configuration is handled for you.\n\nTarball\n\nIf you have installed CrateDB from a tarball, you must manually configure your operating system.\n\nHere’s what needs to be configured:\n\nFile descriptors\n\nSet hard and soft limit to unlimited\n\nMemory lock\n\nSet hard and soft limit to unlimited\n\nThreads\n\nSet hard and soft limit to 4096 (CrateDB versions < 3.0.0 will also work with 2048)\n\nVirtual memory\n\nSet hard and soft limit to unlimited (on Linux only)\n\nMemory map\n\nSet limit to 262144 (on Linux only)\n\nYou might be able to set these limits per process or per user, depending on your operating system and setup. And for this to take effect, you may also have to set these limits for the superuser.\n\nLinux\n\nIf you’re running Linux, you can perform the necessary configuration by following these instructions.\n\nEdit /etc/security/limits.conf and configure these lines:\n\ncrate soft nofile unlimited\ncrate hard nofile unlimited\n\ncrate soft memlock unlimited\ncrate hard memlock unlimited\n\ncrate soft nproc 4096\ncrate hard nproc 4096\n\ncrate soft as unlimited\ncrate hard as unlimited\n\n\nHere, crate is the user that runs the CrateDB daemon.\n\nEdit /etc/sysctl.conf and configure:\n\nvm.max_map_count = 262144\n\n\nTo apply this change, run:\n\n$ sudo sysctl -p\n\n\nThis command will this reload all settings from /etc/sysctl.conf.\n\nTip\n\nAlternatively, vm.max_map_count can be set directly, like so:\n\n$ sysctl -w vm.max_map_count=262144\n\n\nNote, however, this setting will be reset to the value in /etc/sysctl.conf when your system next boots.\n\nGarbage collection\n\nCrateDB has been tested using the CMS garbage collector (default up to and including CrateDB 4.0) and the G1 garbage collector (the default with CrateDB 4.1).\n\nG1GC can also be used in earlier CrateDB versions, but should only be used in combination with Java 11 or later.\n\nWarning\n\nOther garbage collectors have not been tested with CrateDB and we do not support using other GCs in production."
  },
  {
    "title": "User roles, types, and privileges — CrateDB Cloud",
    "url": "https://cratedb.com/docs/cloud/en/latest/reference/user-roles.html",
    "html": "User roles, types, and privileges\n\nOn this page you can find information about the different user roles and privileges relevant to CrateDB Cloud. The first section describes these for the users created within CrateDB Cloud: the organization roles. The second section describes a different type of users, namely CrateDB users, and their privileges and constraints.\n\nIn general, CrateDB Cloud users are created by the admins of their respective organizations, and their function is to support management for these organizations in CrateDB Cloud. By contrast, CrateDB users are either generated automatically or created as part of the CrateDB Cloud cluster deployment process, and support the operation of CrateDB Cloud clusters specifically.\n\nCrateDB Cloud user roles\n\nThis section describes the roles that can be set for users within CrateDB Cloud. For information on how to do so, see the documentation on adding users\n\nOrganization roles\n\nAn organization admin can add users to and remove users from an organization. Admins can perform all available operations for any services. They have access to the organization’s Audit Log.\n\nEach organization must have at least one admin.\n\nAn organization member is able to view the list of organization users but can’t edit, add, or remove users.\n\nCrateDB user roles\n\nThis section covers users and privileges that derive from the architecture of CrateDB as it operates for CrateDB Cloud. There are different types of CrateDB database users that are relevant for CrateDB Cloud customers, and their nature and constraints are described below.\n\nSystem user\n\nIn CrateDB Cloud, the backend uses a user called system in order to perform Cloud cluster upgrades, backups and scaling functions, among other things.\n\nWarning\n\nThe user system is essential for CrateDB Cloud to function as intended. While it is not normally accessible through the CrateDB Cloud Console or the Croud CLI, it can be accessed through the CrateDB admin UI or any other SQL client. It is important not to edit or delete this user in any way. Otherwise, the functioning of Cloud clusters may be compromised.\n\nCrate user\n\nIn CrateDB, when you create a cluster node (through whatever method), a crate user is automatically generated. crate is a superuser and can perform all possible operations. It is not possible to create additional superusers. Authentication for crate is restricted to localhost.\n\nRegular database user\n\nNext to the crate user there is the regular database user, created as part of the CrateDB Cloud cluster deployment wizard when deploying a cluster through AWS or Azure.\n\nBecause the regular database user has AL privileges, there are certain operations that they cannot perform. As of CrateDB 4.2.1, the list of such operations is as follows:\n\n| `ALTER CLUSTER`\n| `ANALYZE`\n| `DISCARD`\n| `KILL`\n| `KILL ALL`\n| `OPTIMIZE`\n| `SET LICENSE`\n| `SET TRANSACTION`\n\n\nMore information on CrateDB user privileges can be found in the CrateDB documentation on privileges."
  },
  {
    "title": "Delete cluster — CrateDB Cloud",
    "url": "https://cratedb.com/docs/cloud/en/latest/howtos/delete-cluster.html",
    "html": "Delete cluster\n\nThis is a guide on how to delete a CrateDB Cloud cluster. It consists of two parts: one describes the process of deleting a cluster created through Microsoft Azure, the other is for a cluster created through AWS. For Azure, there are in turn two methods: via the CrateDB Cloud Console and through the Azure Portal. Each are outlined in the Azure section.\n\nAlthough the general documentation for the CrateDB Cloud Console explains how you can delete a cluster specifically within the Console, this tutorial provides a step-by-step guide for all methods of deleting a CrateDB Cloud cluster. This is to make the process more transparent and easier to find and use.\n\nWarning\n\nAll cluster data will be lost on deletion. This action cannot be undone.\n\nMicrosoft Azure\n\nIf you followed the steps of the tutorial on how to set up a cluster from scratch via Microsoft Azure, you should have a cluster running now. If you created your CrateDB Cloud cluster via the Microsoft Azure offering, you have two different ways to delete a cluster, once it has been created. The first is through the CrateDB Cloud Console itself, the second is via the Microsoft Azure SaaS listing. We will describe each process below.\n\nDeleting a cluster via the CrateDB Cloud Console\n\nThe easiest and preferred way to delete a cluster is via the CrateDB Cloud Console. Make sure you are logged in to the Console with a user that has admin access in the organization where your cluster is deployed. For more on what that means, please see the documentation on user roles.\n\nFirst, you need to select the cluster in question. To do this, we need to access the Clusters page in the left-hand sidebar. There you should see a list of all your clusters.\n\nSimply click the “View” button on the one you want to delete. This will bring you to the overview of said cluster. There will be a “Delete cluster” button in the top right corner. After clicking this button, you will be prompted to input the name of the cluster to avoid accidental deletion of your cluster. After you submit by clicking Confirm, your cluster will be deleted.\n\nDeleting a cluster via the Microsoft Azure Portal\n\nAs an alternative option, you can also delete a cluster - once deployment has been confirmed in the CrateDB Cloud Console - directly via the Microsoft Azure Portal. If you followed the previous tutorial on deploying a cluster from scratch using the Azure Marketplace, you will already be familiar with it.\n\nTo delete a cluster via the Azure Portal, first we must head to the Azure Portal homepage. Make sure you are logged in with the correct account while on this page.\n\nIf all is well, you should be able to see a section titled Azure Services on the landing page. If you have recently used a SaaS resource, you should find in this overview the item “Software as a Service (SaaS)”. If this item is not present in the list, go to the search bar at the top (which says search resources, services, and docs) and enter “SaaS” there. The SaaS item should then appear in the list of results.\n\nClick on this icon. If you have subscribed to the CrateDB Cloud offer and have deployed a cluster previously, the cluster name should now appear in the list of SaaS items.\n\nAll that remains is to click directly on the relevant cluster name (you do not need to tick the box). This will take you to a screen with cluster details.\n\nTo delete the cluster, press the Delete button at the top right and confirm.\n\nAWS\nDeleting a cluster via the AWS Marketplace\n\nYou can deploy a cluster on CrateDB Cloud via AWS by subscribing to the offering on the AWS Marketplace. The offer will refer you to the CrateDB Cloud wizard where you can configure your plan and cluster. Finally, this process will lead you to the CrateDB Cloud Console.\n\nTo delete a cluster created in this way, you must unsubscribe from the AWS Marketplace offer. To do so, go to the AWS Marketplace website and make sure you are logged in with the account that you used to subscribe to the offer.\n\nOn the landing page, find your account name in the top right corner, and in the dropdown menu, select Your Marketplace Software.\n\nThis will take you to an overview of your AWS Marketplace subscriptions. You should see CrateDB Cloud there. Each subscription item has a button labeled Manage. Click this button for CrateDB Cloud.\n\nYou will now see a page with the CrateDB Cloud “pay as you go” offer on it. At the top right corner there is a button labelled Actions.\n\nThis generates a drop-down menu with various options for interacting with the offer. In this menu, click the option Cancel subscription.\n\nA warning will appear stating that canceling the subscription will not terminate your running services. Do not worry: upon receiving a notice of cancellation of the subscription, the CrateDB Cloud team will terminate your running services for you. Therefore, you can safely cancel your subscription at this prompt to delete your cluster.\n\nBilling\n\nCrate.io only bills for actual cluster usage. During cluster operation, this is on a per-hour basis. As soon as the cluster is deleted, nothing further will be billed for that cluster."
  },
  {
    "title": "Version 1.0.4 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/1.0.4.html",
    "html": "5.6\nVersion 1.0.4\n\nReleased on 2017/02/24.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 0.57.0 or higher before you upgrade to 1.0.4.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 1.0.0 or higher. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 1.0.0, and will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nChanges\n\nFixes\n\nChangelog\nChanges\n\nRemoved limitation which didn’t allow ordering on partition columns in a GROUP BY query.\n\nUpdated crate-admin to 1.0.5 which includes the following change:\n\nFixed a console results issue that caused the results table not to be displayed after horizontal scrolling.\n\nFixed an issue that caused the Admin UI to load only one plugin.\n\nDisplay warning in the console view when the query result contains an unsafe integer.\n\nRelocated the help resources section to be underneath the tweet import tutorial.\n\nShow loading indicator when Execute Query is in progress.\n\nFixes\n\nFixed an issue which caused restoring a whole partitioned table from a snapshot to fail.\n\nFixed the low/high disk-based shard allocation watermark settings. When a percentage value is provided for either of settings, it won’t be converted to an absolute byte value.\n\nIndex columns based on string arrays are correctly populated with values.\n\nFixed evaluation on UPDATE of generated columns without referenced columns, e.g., generated columns with a CURRENT_TIMESTAMP expression.\n\nFixed global aggregations on JOINs with 3 or more tables."
  },
  {
    "title": "Search — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/search.html",
    "html": "5.6\nSearch\n\nFrom here you can search these documents. Enter your search words into the box below and click \"search\". Note that the search function will automatically search for all of the words. Pages containing fewer words won't appear in the result list.\n\nSearch"
  },
  {
    "title": "Search — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/search.html",
    "html": "5.6\nSearch\n\nFrom here you can search these documents. Enter your search words into the box below and click \"search\". Note that the search function will automatically search for all of the words. Pages containing fewer words won't appear in the result list.\n\nSearch"
  },
  {
    "title": "Glossary — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/glossary.html",
    "html": "5.6\nGlossary\n\nThis glossary defines key terms used in the CrateDB reference manual.\n\nTable of contents\n\nTerms\n\nB\n\nC\n\nE\n\nF\n\nM\n\nN\n\nO\n\nP\n\nR\n\nS\n\nU\n\nV\n\nTerms\nB\nBinary operator\n\nSee operation.\n\nC\nCLUSTERED BY column\n\nSee routing column.\n\nE\nEvaluation\n\nSee expression.\n\nExpression\n\nAny valid SQL that produces a value (e.g., column references, comparison operators, and functions) through a process known as evaluation.\n\nContrary to a statement.\n\nSee Also\n\nSQL: Value expressions\n\nBuilt-ins: Subquery expressions\n\nData definition: Generation expressions\n\nScalar functions: Conditional functions and expressions\n\nAggregation: Aggregation expressions\n\nF\nFunction\n\nA token (e.g., replace) that takes zero or more arguments (e.g., three strings), performs a specific task, and may return one or more values (e.g., a modified string). Functions that return more than one value are called multi-valued functions.\n\nFunctions may be called in an SQL statement, like so:\n\ncr> SELECT replace('Hello world!', 'world', 'friend') as result;\n+---------------+\n| result        |\n+---------------+\n| Hello friend! |\n+---------------+\nSELECT 1 row in set (... sec)\n\n\nSee Also\n\nScalar functions\n\nAggregate functions\n\nTable functions\n\nWindow functions\n\nUser-defined functions\n\nM\nMetadata gateway\n\nPersists cluster metadata on disk every time the metadata changes. This data is stored persistently across full cluster restarts and recovered after nodes are started again.\n\nSee Also\n\nCluster configuration: Metadata gateway\n\nMulti-valued function\n\nA function that returns two or more values.\n\nSee Also\n\nTable functions\n\nWindow functions\n\nN\nNonscalar\n\nA data type that can have more than one value (e.g., arrays and objects).\n\nContrary to a scalar.\n\nSee Also\n\nGeographic types\n\nContainer types\n\nO\nOperand\n\nSee operator.\n\nOperation\n\nSee operator.\n\nOperator\n\nA reserved keyword (e.g., IN) or sequence of symbols (e.g., >=) that can be used in an SQL statement to manipulate one or more expressions and return a result (e.g., true or false). This process is known as an operation and the expressions can be called operands or arguments.\n\nAn operator that takes one operand is known as a unary operator and an operator that takes two is known as a binary operator.\n\nSee Also\n\nArithmetic operators\n\nComparison operators\n\nArray comparisons\n\nP\nPartition column\n\nA column used to partition a table. Specified by the PARTITIONED BY clause.\n\nAlso known as a PARTITIONED BY column or partitioned column.\n\nA table may be partitioned by one or more columns:\n\nIf a table is partitioned by one column, a new partition is created for every unique value in that partition column\n\nIf a table is partitioned by multiple columns, a new partition is created for every unique combination of row values in those partition columns\n\nSee Also\n\nData definition: Partitioned tables\n\nGenerated columns: Partitioning\n\nCREATE TABLE: PARTITIONED BY clause\n\nALTER TABLE: PARTITION clause\n\nREFRESH: PARTITION clause\n\nOPTIMIZE: PARTITION clause\n\nCOPY TO: PARTITION clause\n\nCOPY FROM: PARTITION clause\n\nCREATE SNAPSHOT: PARTITION clause\n\nRESTORE SNAPSHOT: PARTITION clause\n\nPARTITIONED BY column\n\nSee partition column.\n\nPartitioned column\n\nSee partition column.\n\nR\nRegular expression\n\nAn expression used to search for patterns in a string.\n\nSee Also\n\nWikipedia: Regular expression\n\nData definition: Fulltext analyzers\n\nQuerying: Regular expressions\n\nScalar functions: Regular expressions\n\nTable functions: regexp_matches\n\nRouting column\n\nValues in this column are used to compute a hash which is then used to route the corresponding row to a specific shard.\n\nAlso known as the CLUSTERED BY column.\n\nAll rows that have the same routing column row value are stored in the same shard.\n\nNote\n\nThe routing of rows to a specific shard is not the same as the routing of shards to a specific node (also known as shard allocation).\n\nSee Also\n\nStorage and consistency: Addressing documents\n\nSharding: Routing\n\nCREATE TABLE: CLUSTERED clause\n\nS\nScalar\n\nA data type with a single value (e.g., numbers and strings).\n\nContrary to a nonscalar.\n\nSee Also\n\nPrimitive types\n\nShard allocation\n\nThe process by which CrateDB allocates shards to a specific nodes.\n\nNote\n\nShard allocation is sometimes referred to as shard routing, which is not to be confused with row routing.\n\nSee Also\n\nShard allocation filtering\n\nCluster configuration: Routing allocation\n\nSharding: Number of shards\n\nAltering tables: Changing the number of shards\n\nAltering tables: Reroute shards\n\nShard recovery\n\nThe process by which CrateDB synchronizes a replica shard from a primary shard.\n\nShard recovery can happen during node startup, after node failure, when replicating a primary shard, when moving a shard to another node (i.e., when rebalancing the cluster), or during snapshot restoration.\n\nA shard that is being recovered cannot be queried until the recovery process is complete.\n\nSee Also\n\nCluster settings: Recovery\n\nSystem information: Checked node settings\n\nShard routing\n\nSee shard allocation.\n\nStatement\n\nAny valid SQL that serves as a database instruction (e.g., CREATE TABLE, INSERT, and SELECT) instead of producing a value.\n\nContrary to an expression.\n\nSee Also\n\nData definition\n\nData manipulation\n\nQuerying\n\nSQL Statements\n\nSubquery\n\nA SELECT statement used as a relation in the FROM clause of a parent SELECT statement.\n\nAlso known as a subselect.\n\nSubselect\n\nSee subquery.\n\nU\nUnary operator\n\nSee operation.\n\nUncorrelated subquery\n\nA scalar subquery that does not reference any relations (e.g., tables) in the parent SELECT statement.\n\nSee Also\n\nBuilt-ins: Subquery expressions\n\nV\nValue expression\n\nSee expression."
  },
  {
    "title": "Version 1.0.1 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/1.0.1.html",
    "html": "5.6\nVersion 1.0.1\n\nReleased on 2016/12/12.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 0.57.0 or higher before you upgrade to 1.0.1.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 1.0.0. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 1.0.0, and will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nChanges\n\nFixes\n\nChangelog\nChanges\n\nINSERT statements now support SELECT statements without parentheses.\n\nUpdated crate-admin to 1.0.2 which includes the following changes:\n\nRemoved pepper widget, support links are now in a Help section along with the Get Started tutorial.\n\nChanged read notification behaviour so that all items are marked as read upon opening the settings.\n\nLowered opacity of placeholder query in the console.\n\nFix intercom support that disappeared during the implementation of the new Admin UI layout.\n\nFix Radio button position in load overview.\n\nMade schema tabs more distinguishable from tables in the table list.\n\nUpdated link to support website in contact widget.\n\nFixes\n\nFixed scalar signature registration, NULL literals are now supported.\n\nFixed usage of aggregations with NULL values, no exception will be thrown anymore but instead NULL values are properly processed.\n\nThe chunk_size and buffer_size settings for creating repositories of type S3 are now parsed correctly.\n\nCrateDB no longer throws an error when ANY or ALL array comparison expressions are used in SELECT list. e.g.:\n\nselect 'foo' = any(some_array)\n\n\nFixed an issue that could lead to incorrect results if the WHERE clause contains primary key comparisons together with other functions like match.\n\nFixed an issue that caused select queries with bulk arguments to hang instead of throwing the proper error.\n\nFixed a rare race condition that could happen on select queries during a shard relocation leading to a ArrayIndexOutOfBoundsException or a wrong result.\n\nCreating new partitions could have failed if the partitioned table was created prior to CrateDB version 0.55.0 and the table contained a object column. This issue has been fixed."
  },
  {
    "title": "Version 1.0.3 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/1.0.3.html",
    "html": "5.6\nVersion 1.0.3\n\nReleased on 2017/02/10.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 0.57.0 or higher before you upgrade to 1.0.3.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 1.0.0 or higher. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 1.0.0, and will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nDeprecations\n\nChanges\n\nFixes\n\nChangelog\nDeprecations\n\nMulticast discovery has been deprecated in this release, and will be removed in the 1.1 release. Multicast discovery is disabled by default.\n\nChanges\n\nReturn correct affected row count instead of throwing an exception when trying to bulk insert values that don’t match the column type(s).\n\nRemoved OVER support from SQL parser because the clause was completely ignored when executing the query which led to misleading results.\n\nQueries with _doc reference comparison (e.g. _doc['name'] = 'foo') in the WHERE clause return the correct results instead of empty result.\n\nDynamically added string columns now have exactly the same characteristics as string columns created via CREATE TABLE or ALTER TABLE ADD COLUMN\n\nUpdated crate-admin to 1.0.4 which includes the following change:\n\nFixed getting started display issue on very wide screens.\n\nFixes\n\nScalar functions are now allowed on the HAVING clause if the scalar function is used as a GROUP BY symbol.\n\nFixed an issue in the PostgreSQL protocol that could cause a StackOverflow exception due to connection errors or malfunctioning clients.\n\nClosing a connection via the PostgreSQL Wire Protocol now correctly closes the internal resources.\n\nFixed issue that led to casting exception when comparing an object column with an object literal that contains a string value.\n\nFixed and issue that caused UPDATE statement on an empty partitioned table to throw UnsupportedOperationException.\n\nFixed an issue that caused fulltext search with fuzziness='AUTO' to throw NumberFormatException.\n\nFixed an issue in the LIKE predicate which prevented from using escaped backslash before the wildcard.\n\nFixed an issue that caused ORDER BY clause to be ignored if used in combination with GROUP BY in subselects. e.g.:\n\nSELECT x from (SELECT * from t1) as tt GROUP BY x ORDER BY x\n"
  },
  {
    "title": "Version 1.0.6 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/1.0.6.html",
    "html": "5.6\nVersion 1.0.6\n\nReleased on 2017/04/03.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 0.57.0 or higher before you upgrade to 1.0.6.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 1.0.0 or higher. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 1.0.0, and will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nChanges\n\nFixes\n\nChangelog\nChanges\n\nImproved error handling when using LIMIT/OFFSET that exceed Integer range.\n\nFixes\n\nFixed issue which prevents blob directories from being removed after a drop blob table command when a custom blob location is configured.\n\nFixed an issue that caused conditions like not x = any( ) to not find empty lists when used in WHERE clauses.\n\nFixed a race condition in an unsorted right outer join with one empty table that could lead to incorrect results.\n\nDivision by aggregation that resulted in 0. e.g. SELECT max(long_col) / min(long_col) from t) now correctly returns ArtithmeticException and does not hang.\n\nFixed a race condition in KILL statements which lead to a memory leak.\n\nFixed an issue that prevent a node from starting on Windows if the sigar-plugin is removed."
  },
  {
    "title": "Version 1.1.1 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/1.1.1.html",
    "html": "5.6\nVersion 1.1.1\n\nReleased on 2017/03/27.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 0.57.0 or higher before you upgrade to 1.1.1.\n\nYou cannot perform a rolling upgrade to this version. Any upgrade to this version will require a full restart upgrade and will introduce all of the breaking changes listed for Version 1.1.0.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nChanges\n\nFixes\n\nChangelog\nChanges\n\nQuerying the Admin UI from /_plugin/crate-admin/ will now redirect to /.\n\nAdded possible data type conversion to a timestamp array. e.g. cast(['2017-01-01','2017-12-31'] as array(timestamp))\n\nImproved error handling when using LIMIT/OFFSET that exceed Integer range.\n\nRemoved blog feed from side bar of the settings interface.\n\nIncreased base font size in the admin interface.\n\nAdded fallback to unformatted results in the console interface if no column types are returned.\n\nDisplay notification warning only when a new CrateDB version is released.\n\nAdded lineWrapping option to console editor in the Admin UI.\n\nFixes\n\nFixed a regression introduced in 1.1.1 which could cause queries to return wrong values.\n\nFixed bug introduced in 1.1.0 which caused all partitioned tables to become unusable.\n\nFixed an issue that caused conditions like not x = any( ) to not find empty lists when used in WHERE clauses.\n\nFixed a regression introduced in 1.1.0 that caused statements like COPY TO '/invalid/directory/' to get stuck instead of resulting in an error.\n\nFixed an issue where arrays in formatted objects were not displayed in the console interface.\n\nFixed an issue that caused tables interface to display a healthy status even though the partitions were in critical state.\n\nFixed an issue that caused the console text to appear on top of the settings tab.\n\nFixed load reading display when the readings were invalid (for example, on Windows)."
  },
  {
    "title": "Version 1.1.0 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/1.1.0.html",
    "html": "5.6\nVersion 1.1.0\n\nReleased on 2017/03/21.\n\nWarning\n\nDo not use this version. This release introduced a bug which caused all partitioned tables to become unusable. The bug was fixed in Version 1.1.1.\n\nThis version was removed from all release channels. This changelog is kept for information purposes only.\n\nTable of contents\n\nChangelog\n\nBreaking Changes\n\nChanges\n\nFixes\n\nChangelog\nBreaking Changes\n\nRemoved multicast discovery.\n\nThe ordinal column at the information_schema.columns will return NULL now for all sub-columns (all non top-level columns) as the order of object columns is not guaranteed.\n\nThe TableFunctionImplementation interface was streamlined with other function implementation semantics. This requires function implementation plugins to be adapted to the new interface.\n\nRemoved deprecated setting indices.fielddata.breaker that have been used as an alias for indices.breaker.fielddata.\n\nChanges\n\nServe Admin UI from /. Previously used URIs will direct to /.\n\nAdded the subscript function support for object literals.\n\nAdded cluster checks that warn if some tables need to be recreated or upgraded for compatibility with future versions of CrateDB.\n\nAdded functionality to monitor query runtime statistics via JMX. This feature can only be used with an enterprise license.\n\nAdded a new parameter upgrade_segments to the OPTIMIZE statement which enables the upgrade of tables and tables partitions to the current version of the storage engine.\n\nAdded new column min_lucene_version to sys.shards table, which shows the oldest Lucene segment version used in the shard.\n\nRemove restriction to run OPTIMIZE on blob tables.\n\nUDC: add the enterprise field to the UDC ping. The field identifies whether a user uses the enterprise version.\n\nAdded the license.enterprise setting to the cluster settings.\n\nIt is now supported to order and group by predicate functions in general with the exception of the match predicate.\n\nSelecting os['timestamp'] from ``sys.nodes returns the actual timestamp of each node clock at the time of collecting the metric instead of the timestamp on the handler node.\n\nAdded scalar function geohash that returns a GeoHash representation of a geo_point\n\nAdded support for casting JSON strings to object columns.\n\nThe array comparison no longer requires extra parentheses for subselects. Now it’s possible to use = ANY (SELECT ...) instead of = ANY ((SELECT ...)).\n\nAllow semi-colon (;) in the end of simple SQL statements.\n\nEnhanced performance optimisation of full joins by rewriting them to left, right or inner joins when conditions in WHERE exclude null values.\n\nAdded support for filtering and ordering on ignored object columns.\n\nAdded support for the double colon (::) cast operator.\n\nUpgraded the parser from ANTLR3 to ANTLR4.\n\nAdded monitoring plugin for the Enterprise edition in the Admin UI.\n\nAdded Lazy loading of the stylesheet and plugins depending on the Enterprise settings.\n\nAdded buttons to collapse and expand all schemas in the tables view.\n\nThe console now expands vertically to show the whole query if its size is larger than the standard size of the console.\n\nSQL console keywords are now CrateDB specific.\n\nImproved formatted results of the geo_area data type to include an external link to a visualisation of that geo_area.\n\nKeywords in the SQL console are capitalised.\n\nAdded node number to the status bar.\n\nRelocated the help resources section to be underneath the tweet import tutorial.\n\nImproved console results table, including data type based colorization, alternating row colorization, structured object/array formatting, human-readable timestamps, Google Maps link on geo-point results & lazy loading on result sets larger than 100 rows.\n\nFixes\n\nFixed an issue that prevent a node from starting on Windows if the sigar-plugin is removed.\n\nFixed validation of known configuration file settings. The settings are also validated upon start-up.\n\nShow loading indicator in the console interface when Execute Query is in progress.\n\nFixed issue that caused Cluster Offline message to not be displayed.\n\nFixed a console results issue that caused the results table not to be visible after horizontal scrolling.\n\nFixed styling issue that caused the last element in the side bar list to be hidden.\n\nFixed an issue that caused the notification date to be null in Safari.\n\nFixed a console results issue that caused the results table not to be displayed after horizontal scrolling.\n\nFixed an issue that caused the Admin UI to load only one plugin.\n\nDisplay warning in the console view when the query result contains an unsafe integer."
  },
  {
    "title": "Version 1.1.2 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/1.1.2.html",
    "html": "5.6\nVersion 1.1.2\n\nReleased on 2017/04/10.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 0.57.0 or higher before you upgrade to 1.1.2.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 1.1.1 or higher. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 1.1.0, and will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed a bug which caused non-grouping aggregations on a join with explicit CROSS JOIN syntax to fail.\n\nCorrectly expose stats.service.interval in the sys.cluster table.\n\nRe-added compatibility layer that was removed in 1.1 and resulted in exception thrown when trying to run aggregations on tables created with CrateDB < 0.52.0.\n\nFixed a race condition which could cause a memory leak.\n\nUpdated documentation to indicate that it’s not possible to use object, geo_point, geo_shape or array in the ORDER BY clause.\n\nFixed cluster check, which warns about tables that need upgrade for future compatibility, to include only tables that differ from current storage engine major version number.\n\nFixed issue which prevents blob directories from being removed after a drop blob table command when a custom blob location is configured."
  },
  {
    "title": "Version 2.0.6 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.0.6.html",
    "html": "5.6\nVersion 2.0.6\n\nReleased on 2017/07/19.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.0.6.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 2.0.0. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 2.0.0, and will require a full restart upgrade.\n\nConsult the upgrade notes for Version 2.0.0 when upgrading.\n\nWarning\n\nIf you’re using CrateDB’s BLOB storage you should consult the Upgrade Notes.\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nUpgrade Notes\n\nChangelog\n\nFixes\n\nUpgrade Notes\n\nDue to a bug introduced in Version 2.0.0 that can cause loss of BLOB data, it is necessary to perform a rolling upgrade if you’re running a version >= 2.0.0 and < 2.0.4 and using BLOB tables.\n\nAdditionally, the number of replicas needs to be set to at least 1 for all blob tables and you need to make sure that data is fully replicated before continuing.\n\nOnly then you may upgrade one node after each other.\n\nChangelog\nFixes\n\nAllow GROUP BY on any scalar functions, e.g.:\n\nSELECT id + 1 FROM t GROUP BY id\n\n\nAdded support for conditionals with NULL arguments.\n\nDo not include Lucene indices (metadata) of BLOB tables in snapshot when using CREATE SNAPSHOT with ALL, because the actual binary files of BLOB tables cannot be backed up using SNAPSHOT/RESTORE functionality.\n\nFixed rename table operation for empty partitioned tables.\n\nFixed an issue which resulted in an exception when using the same global aggregation symbol twice as a select item on a join query.\n\nFixed an issue that caused wrong results to be returned for global aggregation on JOINS when literal expression in WHERE clause is evaluated to false, e.g.:\n\nSELECT COUNT(*) FROM t1, t2 WHERE 1=2\n\n\nProvide comprehensive error message when using a NULL literal in GROUP BY.\n\nAllow DISTINCT to operate on literals, e.g.:\n\nSELECT DISTINCT ['val1', 'val2'], col1, col2, ... FROM t\n\n\nAllow GROUP BY on any literal, e.g.:\n\nSELECT {a=1, b=2}, COUNT(*) FROM t GROUP BY 1\n\n\nFixed an issue that can hang the nodes when running an insert from query statement for a large dataset when the target table is an empty partitioned table.\n\nFixed a race condition which could cause UPDATE or DELETE statements to get stuck.\n\nFixed a ClassCastException that could occur on certain queries if connected via the PostgreSQL protocol.\n\nFixed an issue that could cause the _score >= <minScore> filter to not work correctly if used in a query with an ORDER BY clause."
  },
  {
    "title": "Version 2.0.7 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.0.7.html",
    "html": "5.6\nVersion 2.0.7\n\nReleased on 2017/08/08.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.0.7.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 2.0.0. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 2.0.0, and will require a full restart upgrade.\n\nConsult the upgrade notes for Version 2.0.0 when upgrading.\n\nWarning\n\nIf you’re using CrateDB’s BLOB storage you should consult the Upgrade Notes.\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nUpgrade Notes\n\nChangelog\n\nChanges\n\nFixes\n\nUpgrade Notes\n\nDue to a bug introduced in Version 2.0.0 that can cause loss of BLOB data, it is necessary to perform a rolling upgrade if you’re running a version >= 2.0.0 and < 2.0.4 and using BLOB tables.\n\nAdditionally, the number of replicas needs to be set to at least 1 for all blob tables and you need to make sure that data is fully replicated before continuing.\n\nOnly then you may upgrade one node after each other.\n\nChangelog\nChanges\n\nEnabled mapping.total_fields.limit setting for tables in order to be able to increase the maximum number of columns higher than the default of 1000.\n\nFixes\n\nFixed an issue where COPY FROM, INSERT-BY-SUBQUERY or bulk INSERT statements could not be killed when high pressure is put on data node thread pools.\n\nFixed the error message to be more descriptive when the condition in a CASE/WHEN expression is not a boolean.\n\nFixed an issue which caused an exception if EXPLAIN is used on a statement that uses the ANY (array_expression) operator.\n\nAllow support of conditional expressions with different return types that can be converted to a single return type.\n\nFixed support for negate on null in conditional expressions.\n\nFixed support for setting write.wait_for_active_shards on a partitioned table.\n\nOptimized the algorithm that determines the best ordering of the tables in a JOIN.\n\nFixed a regression causing incorrect results for queries with DISTINCT on scalar functions. E.g.:\n\nSELECT DISTINCT upper(name) FROM t\n\n\nFixed a null pointer exception when running SELECT port FROM sys.nodes while psql.enabled: false was set.\n\nImplemented NOT NULL constraint validation for nested object columns, which was previously ignored. E.g.:\n\nCREATE TABLE test (\n  stuff object(dynamic) AS (\n    level1 object(dynamic) AS (\n      level2 string not null\n    ) NOT NULL\n  ) NOT NULL\n)\n"
  },
  {
    "title": "Version 2.1.0 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.1.0.html",
    "html": "5.6\nVersion 2.1.0\n\nReleased on 2017/07/11.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.1.0.\n\nYou cannot perform a rolling upgrade to this version. Any upgrade to this version will require a full restart upgrade.\n\nConsult the upgrade notes for Version 2.1.0 when upgrading.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nBreaking Changes\n\nChanges\n\nUpgrade Notes\n\nDatabase User\n\nUpgrading from version 2.0.x\n\nCreate columns syntax strictness\n\nUpgrading from versions prior to 2.0.0\n\nChangelog\nBreaking Changes\n\nCURRENT_USER, USER and SESSION_USER are now reserved words as we introduced them as system functions. These terms will not be available to be used as table, and column names and for already existing entities they will have to be quoted when referenced (otherwise the terms will be treated as function calls).\n\nSELECT statements without any FROM items are no longer executed against the sys.cluster table, but against a virtual table with no columns. Queries including sys.cluster columns but no explicit FROM item will now result in a ColumnUnknownException.\n\nThe onModule() method had been removed from io.crate.Plugin interface; createGuiceModules() must be used instead.\n\nsrv and azure are no longer valid configuration options for discovery.type. Instead there is a new discovery.zen.hosts_provider settings which can be set to either srv or azure.\n\nDuplicate definition of settings is neither allowed in the crate.yml file nor as command line options. However, settings provided via command line arguments can still override the settings defined in the crate.yml file.\n\nThe syntax for creating columns on Create Table and Alter Table has become more strict.\n\nThe sigar jar and object files have been moved from plugins/sigar to lib/sigar.\n\nChanges\n\nUpdated Elasticsearch to 5.2.2.\n\nUpdated Crash to 0.21.3.\n\nUpdated the Admin UI to 1.4.1.\n\nThe table setting recovery.initial_shards has been deprecated. You may set gateway.local.initial_shards per node instead. CrateDB will continue to read the old setting but applications should be migrated to the new setting.\n\nAdded support for GRANT and REVOKE privileges for accessing the cluster. Currently supported privilege types: DQL, DML and DDL.\n\nAdded support for GRANT, DENY and REVOKE privileges for accessing the tables and schemas.\n\nAdded column username to sys.jobs and sys.jobs_log that contains the username under which the job was invoked.\n\nAdded SSL/TLS support for HTTP endpoints.\n\nAdded SSL/TLS support for PostgreSQL Wire Protocol.\n\nAdded new HBA setting ssl which allows to control whether users have to connect with SSL enabled or disabled.\n\nAdded support for client certificate authentication via HBA.\n\nAdded support for joins on virtual tables.\n\nQueries which contain a correlated subquery will now result in an error stating that correlated subqueries are not supported, instead of a more confusing error indicating that a relation is unknown.\n\nExtended the output of the EXPLAIN statement.\n\nUpgrade Notes\nDatabase User\n\nClients that use the PostgreSQL Wire Protocol need to connect with a valid database user to get access to the server. See the official Crate JDBC Driver documentation for further information.\n\nUpgrading from version 2.0.x\n\nIf you’re using CrateDB’s BLOB storage and you need to run at least version 2.0.4 before upgrading to 2.1.0.\n\nPlease consult the Version 2.0.4 release notes for further details.\n\nCreate columns syntax strictness\n\nThe syntax strictness when creating new columns has been increased:\n\nColumns cannot contain a dot when using alter table. Instead, you can still use the subscript pattern to add an object column.\n\nThe use of references as a key of a subscript is not possible anymore. E.g. instead of col_name[index], you’ll need to use col_name['index']. Be aware that the use of single quotes will cause the index to be case sensitive.\n\nUpgrading from versions prior to 2.0.0\n\nPlease consult the Upgrade Notes for 2.0.0."
  },
  {
    "title": "Version 2.1.1 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.1.1.html",
    "html": "5.6\nVersion 2.1.1\n\nReleased on 2017/08/04.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.1.1.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 2.1.0. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 2.1.0, and will require a full restart upgrade.\n\nConsult the upgrade notes for Version 2.1.0 when upgrading.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nChanges\n\nFixes\n\nChangelog\nChanges\n\nMade the help messages of bin/crate and the Java code consistent. This change also removes the undocumented -E command line option which makes -C the only valid option for passing settings to CrateDB.\n\nChanged the Enterprise License missing check message, as well as lowering the severity from HIGH to LOW.\n\nAdded a warning message to the log if the unofficial Elasticsearch HTTP REST API is enabled via es.api.enabled setting.\n\nUpdated Elasticsearch to v5.4.3.\n\nEnabled mapping.total_fields.limit setting for tables in order to be able to increase maximum number of columns higher than the default of 1000.\n\nTable functions now support value expressions as arguments.\n\nFixes\n\nFixed an issue when using dots in an identifier for column creation in alter table, where they were interpreted as sub-fields of an object. The usage of dots as part of a column name is prohibited.\n\nFixed an issue that prevented CrateDB from starting when path.home was set via command line argument.\n\nFixed an issue causing JOINs on virtual tables (subselects) which contain LIMIT and/or OFFSET to return incorrect results. E.g.:\n\nSELECT * FROM\n  (SELECT * FROM t1 ORDER BY a LIMIT 5) t1,\nJOIN\n  (SELECT * FROM t2 ORDER BY b OFFSET 2) t2\nON t1.a = t2.max\n\n\nFixed an issue causing JOINs with aggregations on virtual tables (subselects) which also contain aggregations to return incorrect results. E.g.:\n\nSELECT t1.a, COUNT(*) FROM t1\nJOIN (SELECT b, max(i) as max FROM t2 GROUP BY b) t2\nON t1.a = t2.max\nGROUP BY t1.id\n\n\nFixed the error message to be more descriptive when the condition in a CASE/WHEN expression is not a boolean.\n\nFixed an issue which caused an exception if EXPLAIN is used on a statement that uses the ANY (array_expression) operators.\n\nAllow support of conditional expression with different return types that can be converted to a single return type.\n\nFixed support for negate on null in conditional expression.\n\nFixed support for setting write.wait_for_active_shards on a partitioned table.\n\nAdded missing documentation about the default value of the table setting write.wait_for_active_shards.\n\nImproved user privileges matching to constant time complexity.\n\nImproved the error message if an invalid table column reference is used in INSERT statements.\n\nOptimized the algorithm that determines the best ordering of the tables in a JOIN.\n\nUpdated Crash to 0.21.4 which fixes an issue with \\verbose command not working correctly when Crash is started without --verbose.\n\nImplemented flexible return type of sum function depending on the input types, which was previously only double.\n\nFixed a regression causing incorrect results for queries with DISTINCT on scalar functions. E.g.:\n\nSELECT DISTINCT upper(name) FROM t\n\n\nFixed a race condition which made it possible to create new columns in a partition of a partitioned table that didn’t match the type of the same column of sibling partitions.\n\nUpgraded Admin UI version to fix an issue with the Twitter tutorial.\n\nFixed a NPE when running select port from sys.nodes and psql.enabled: false was set.\n\nFixed an issue where the user that gets provided by the client on connect is not always used as current user if host based authentication is disabled.\n\nCorrected the documentation of the version column of the sys.snapshots table. It was described as the CrateDB version whereas it’s an internal version instead.\n\nDropping an empty partitioned table now drops the related table privileges.\n\nImplemented NOT NULL constraint validation for nested object columns, which was previously ignored. E.g.:\n\nCREATE TABLE test (\n  stuff object(dynamic) AS (\n    level1 object(dynamic) AS (\n      level2 string not null\n    ) NOT NULL\n  ) NOT NULL\n)\n\n\nInternal system queries are now executed under the crate superuser if user management is enabled.\n\n!= ANY() could not operate on arrays with more than 1024 elements. This limit has been increased by default to 8192. A new node setting: indices.query.bool.max_clause_count has been exposed to allow configuration of this limit.\n\nFixed an issue which caused unrelated table privileges to be lost after a table was renamed.\n\nFixed an issue that prevents CrateDB from bootstrapping on Windows hosts."
  },
  {
    "title": "Version 2.1.2 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.1.2.html",
    "html": "5.6\nVersion 2.1.2\n\nReleased on 2017/08/08.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.1.2.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 2.1.0. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 2.1.0, and will require a full restart upgrade.\n\nConsult the upgrade notes for Version 2.1.0 when upgrading.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed an issue where user-defined functions were not persisted across node restarts."
  },
  {
    "title": "Create Organization — CrateDB Cloud",
    "url": "https://cratedb.com/docs/cloud/en/latest/howtos/create-org.html",
    "html": "Create Organization\n\nThis is a guide on how to create a new organization in the CrateDB Cloud Console. If you follow the cluster deployment tutorial, your first organization will be automatically created for you. However, there are scenarios where you may want to create a new organization.\n\nIf you arrive at the Console for the first time, or if you have deleted your last organization, you will have to create an organization to use the CrateDB Cloud Console’s functionality.\n\nIf you wish to create multiple organizations when you are already member of one, the process is slightly different. This guide will outline both methods.\n\nCreate first organization\n\nWhen you first access the CrateDB Cloud Console after signing up, you will arrive at the Cluster overview page. You will also be informed, that your first organization named “My Organization” has been created for you.\n\nCreate multiple organizations\n\nIf you are already in an organization and want to create another one, go to the Account tab. Here you will see a list of all your organizations and your user role in them.\n\nTo create a new organization, click on Create new organization at the top right above the organization list. When prompted, simply input the name of your new organization and confirm by clicking Create organization. To delete an organization, click the trashcan icon next to the organization in the list.\n\nTo switch the active organization, click on the organization name in the list. All organization and cluster management options displayed in the CrateDB Cloud Console will then refer to that organization until you switch organizations again."
  },
  {
    "title": "Version 2.1.3 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.1.3.html",
    "html": "5.6\nVersion 2.1.3\n\nReleased on 2017/08/11.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.1.3.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 2.1.0. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 2.1.0, and will require a full restart upgrade.\n\nConsult the upgrade notes for Version 2.1.0 when upgrading.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed a single insert only related memory leak.\n\nImproved displaying of error messages if multiple errors are occur on ALTER TABLE statements."
  },
  {
    "title": "Getting Started — CrateDB Cloud: Croud CLI",
    "url": "https://cratedb.com/docs/cloud/cli/en/latest/getting-started.html#getting-started",
    "html": "latest\nGetting Started\n\nCroud CLI is available as a pip package.\n\nTable of Contents\n\nInstallation\n\nRun\n\nCommand-line options\n\nShell auto-complete\n\nInstallation\n\nTo install, run:\n\nsh$ pip install croud\n\n\nTo update, run:\n\nsh$ pip install -U croud\n\nRun\n\nVerify your installation, like so:\n\nsh$ croud --version\n\n\nWhen using Croud, you must supply a command.\n\nFor example:\n\nsh$ croud login\n\n\nHere, login is supplied as a command. The will open a browser window for you to authenticate with CrateDB Cloud. You will need to do this before issuing any further commands.\n\nFrom here, you can go on to use commands to manage your configuration values, projects, clusters, organizations, and users.\n\nCommand-line options\n\nCroud supports the following command-line options:\n\nArgument\n\n\t\n\nDescription\n\n\n\n\n-h, --help\n\n\t\n\nPrint the help message, then exits.\n\n\n\n\n-v, --version\n\n\t\n\nPrints the program’s version number, then exits.\n\nTip\n\nSome Croud commands take additional options.\n\nShell auto-complete\n\nCroud supports shell auto-completions for bash, zsh and tcsh:\n\nsh$ croud --print-completion {bash,zsh,tcsh}\n\n\nRefer to the documentation of your specific shell for installation instructions."
  },
  {
    "title": "Version 2.1.4 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.1.4.html",
    "html": "5.6\nVersion 2.1.4\n\nReleased on 2017/08/14.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.1.4.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 2.1.0. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 2.1.0, and will require a full restart upgrade.\n\nConsult the upgrade notes for Version 2.1.0 when upgrading.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed an issue that caused the cluster.name setting in the crate.yml settings file to be ignored."
  },
  {
    "title": "Full-Text: Exploring the Netflix Catalog — CrateDB Cloud",
    "url": "https://cratedb.com/docs/cloud/en/latest/tutorials/full-text.html",
    "html": "Full-Text: Exploring the Netflix Catalog\n\nCrateDB is an exceptional choice for handling complex queries and large-scale data sets. One of its standout features is its full-text search capabilities, using the BM25 ranking algorithm for information retrieval, built on top of the powerful Lucene indexing library. This makes CrateDB an excellent fit for organizing, searching, and analyzing extensive datasets.\n\nIn this tutorial, we will explore how to manage a dataset of Netflix titles, making use of CrateDB Cloud’s full-text search capabilities. Each entry in our imaginary dataset will have the following attributes:\n\nshow_id\n\nA unique identifier for each show or movie.\n\ntype\n\nSpecifies whether the title is a movie, TV show, or another format.\n\ntitle\n\nThe title of the movie or show.\n\ndirector\n\nThe name of the director.\n\ncast\n\nAn array listing the cast members.\n\ncountry\n\nThe country where the title was produced.\n\ndate_added\n\nA timestamp indicating when the title was added to the catalog.\n\nrelease_year\n\nThe year the title was released.\n\nrating\n\nThe content rating (e.g., PG, R, etc.).\n\nduration\n\nThe duration of the title in minutes or seasons.\n\nlisted_in\n\nAn array containing genres that the title falls under.\n\ndescription\n\nA textual description of the title, indexed using full-text search.\n\nTo begin, let’s create the schema for this dataset.\n\nCreating the Table\n\nCrateDB uses SQL, the most popular query language for database management. To store the data, create a table with columns tailored to the dataset using the CREATE TABLE command.\n\nImportantly, you will also take advantage of CrateDB’s full-text search capabilities by setting up a full-text index on the description column. This will enable you to perform complex textual queries later on.\n\nCREATE TABLE \"netflix_catalog\" (\n   \"show_id\" TEXT PRIMARY KEY,\n   \"type\" TEXT,\n   \"title\" TEXT,\n   \"director\" TEXT,\n   \"cast\" ARRAY(TEXT),\n   \"country\" TEXT,\n   \"date_added\" TIMESTAMP,\n   \"release_year\" TEXT,\n   \"rating\" TEXT,\n   \"duration\" TEXT,\n   \"listed_in\"  ARRAY(TEXT),\n   \"description\" TEXT INDEX using fulltext\n);\n\n\nRun the above SQL command in CrateDB to set up your table. With the table ready, you’re now set to insert the dataset.\n\nInserting Data\n\nNow, insert data into the table you just created, by using the COPY FROM SQL statement.\n\nCOPY netflix_catalog\nFROM 'https://github.com/crate/cratedb-datasets/raw/main/cloud-tutorials/data_netflix.json.gz'\nWITH (format = 'json', compression='gzip');\n\n\nRun the above SQL command in CrateDB to import the dataset. After this commands finishes, you are now ready to start querying the dataset.\n\nUsing Full-text Search\n\nStart with a basic SELECT statement on all columns, and limit the output to display only 10 records, in order to quickly explore a few samples worth of data.\n\nSELECT *\nFROM netflix_catalog\nLIMIT 10;\n\n\nCrateDB Cloud’s full-text search can be leveraged to find specific entries based on text matching. In this query, you are using the MATCH function on the description field to find all movies or TV shows that contain the word “love”. The results can be sorted by relevance score by using the synthetic _score column.\n\nSELECT title, description\nFROM netflix_catalog\nWHERE MATCH(description, 'love')\nORDER BY _score DESC\nLIMIT 10;\n\n\nWhile full-text search is incredibly powerful, you can still perform more traditional types of queries. For example, to find all titles directed by “Kirsten Johnson”, and sort them by release year, you can use:\n\nSELECT title, release_year\nFROM netflix_catalog\nWHERE director = 'Kirsten Johnson'\nORDER BY release_year DESC;\n\n\nThis query uses the conventional WHERE clause to find movies directed by Kirsten Johnson, and the ORDER BY clause to sort them by their release year in descending order.\n\nThrough these examples, you can see that CrateDB Cloud offers you a wide array of querying possibilities, from basic SQL queries to advanced full-text searches, making it a versatile choice for managing and querying your datasets."
  },
  {
    "title": "SQL standard compliance — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/compliance.html",
    "html": "5.6\nSQL standard compliance\n\nThis page documents the standard SQL (ISO/IEC 9075) features that CrateDB supports, along with implementation notes and any associated caveats.\n\nCaution\n\nThis list is approximate and features that are listed as supported might be nonconforming in their implementation. However, the main reference documentation always contains the most accurate information about the features CrateDB supports and how to use them.\n\nSee Also\n\nSQL compatibility\n\nID\n\n\t\n\nPackage\n\n\t\n\n#\n\n\t\n\nDescription\n\n\t\n\nComments\n\n\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n1\n\n\t\n\nINTEGER and SMALLINT data types\n\n\t\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n2\n\n\t\n\nREAL, DOUBLE PRECISION, and FLOAT data types\n\n\t\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n3\n\n\t\n\nDECIMAL and NUMERIC data types\n\n\t\n\nNot supported in DDL\n\n\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n4\n\n\t\n\nArithmetic operators\n\n\t\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n5\n\n\t\n\nNumeric comparison\n\n\t\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n6\n\n\t\n\nImplicit casting among the numeric data types\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n1\n\n\t\n\nCHARACTER data type\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n2\n\n\t\n\nCHARACTER VARYING data type\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n3\n\n\t\n\nCharacter literals\n\n\t\n\nOnly simple ‘ quoting\n\n\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n4\n\n\t\n\nCHARACTER_LENGTH function\n\n\t\n\nchar_length only\n\n\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n5\n\n\t\n\nOCTET_LENGTH function\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n6\n\n\t\n\nSUBSTRING function\n\n\t\n\nsubstr scalar\n\n\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n7\n\n\t\n\nCharacter concatenation\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n8\n\n\t\n\nUPPER and LOWER functions\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n9\n\n\t\n\nTRIM function\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n10\n\n\t\n\nImplicit casting among the character string types\n\n\t\n\njust one type\n\n\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n12\n\n\t\n\nCharacter comparison\n\n\t\n\n\nE031\n\n\t\n\nIdentifiers\n\n\t\t\t\n\n\nE031\n\n\t\n\nIdentifiers\n\n\t\n\n1\n\n\t\n\nDelimited identifiers\n\n\t\n\n\nE031\n\n\t\n\nIdentifiers\n\n\t\n\n2\n\n\t\n\nLower case identifiers\n\n\t\n\n\nE031\n\n\t\n\nIdentifiers\n\n\t\n\n3\n\n\t\n\nTrailing underscore\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\t\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n1\n\n\t\n\nSELECT DISTINCT\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n2\n\n\t\n\nGROUP BY clause\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n4\n\n\t\n\nGROUP BY can contain columns not in <select list>\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n5\n\n\t\n\nSelect list items can be renamed\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n6\n\n\t\n\nHAVING clause\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n7\n\n\t\n\nQualified * in select list\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n8\n\n\t\n\nCorrelation names in the FROM clause\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n9\n\n\t\n\nRename columns in the FROM clause\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n1\n\n\t\n\nComparison predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n2\n\n\t\n\nBETWEEN predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n3\n\n\t\n\nIN predicate with list of values\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n4\n\n\t\n\nLIKE predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n6\n\n\t\n\nNULL predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n8\n\n\t\n\nEXISTS predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n9\n\n\t\n\nSubqueries in comparison predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n11\n\n\t\n\nSubqueries in IN predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n12\n\n\t\n\nSubqueries in quantified comparison predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n13\n\n\t\n\nCorrelated subqueries\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n14\n\n\t\n\nSearch condition\n\n\t\n\n\nE071\n\n\t\n\nBasic query expressions\n\n\t\n\n1\n\n\t\n\nUNION DISTINCT table operator\n\n\t\n\n\nE071\n\n\t\n\nBasic query expressions\n\n\t\n\n2\n\n\t\n\nUNION ALL table operator\n\n\t\n\n\nE081\n\n\t\n\nBasic Privileges\n\n\t\t\t\n\n\nE081\n\n\t\n\nBasic Privileges\n\n\t\n\n1\n\n\t\n\nSELECT privilege\n\n\t\n\n\nE081\n\n\t\n\nBasic Privileges\n\n\t\n\n2\n\n\t\n\nDELETE privilege\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\t\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n1\n\n\t\n\nAVG\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n2\n\n\t\n\nCOUNT\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n3\n\n\t\n\nMAX\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n4\n\n\t\n\nMIN\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n5\n\n\t\n\nSUM\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n6\n\n\t\n\nALL quantifier\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n7\n\n\t\n\nDISTINCT quantifier\n\n\t\n\n\nE101\n\n\t\n\nBasic data manipulation\n\n\t\t\t\n\n\nE101\n\n\t\n\nBasic data manipulation\n\n\t\n\n1\n\n\t\n\nINSERT statement\n\n\t\n\n\nE101\n\n\t\n\nBasic data manipulation\n\n\t\n\n3\n\n\t\n\nSearched UPDATE statement\n\n\t\n\n\nE101\n\n\t\n\nBasic data manipulation\n\n\t\n\n4\n\n\t\n\nSearched DELETE statement\n\n\t\n\n\nE121\n\n\t\n\nBasic cursor support\n\n\t\n\n1\n\n\t\n\nDECLARE CURSOR\n\n\t\n\n\nE121\n\n\t\n\nBasic cursor support\n\n\t\n\n8\n\n\t\n\nCLOSE statement\n\n\t\n\n\nE121\n\n\t\n\nBasic cursor support\n\n\t\n\n10\n\n\t\n\nFETCH statement implicit NEXT\n\n\t\n\n\nE121\n\n\t\n\nBasic cursor support\n\n\t\n\n17\n\n\t\n\nWITH HOLD cursors\n\n\t\n\n\nE131\n\n\t\n\nNull value support (nulls in lieu of values)\n\n\t\t\t\n\n\nE141\n\n\t\n\nBasic integrity constraints\n\n\t\n\n1\n\n\t\n\nNOT NULL constraints\n\n\t\n\n\nE141\n\n\t\n\nBasic integrity constraints\n\n\t\n\n3\n\n\t\n\nPRIMARY KEY constraints\n\n\t\n\n\nE141\n\n\t\n\nBasic integrity constraints\n\n\t\n\n6\n\n\t\n\nCHECK constraints\n\n\t\n\n\nE141\n\n\t\n\nBasic integrity constraints\n\n\t\n\n7\n\n\t\n\nColumn defaults\n\n\t\n\n\nE141\n\n\t\n\nBasic integrity constraints\n\n\t\n\n8\n\n\t\n\nNOT NULL inferred on PRIMARY KEY\n\n\t\n\n\nE151\n\n\t\n\nTransaction support\n\n\t\n\n1\n\n\t\n\nCOMMIT statement\n\n\t\n\n\nE152\n\n\t\n\nBasic SET TRANSACTION statement\n\n\t\t\t\n\n\nE152\n\n\t\n\nBasic SET TRANSACTION statement\n\n\t\n\n1\n\n\t\n\nSET TRANSACTION statement: ISOLATION LEVEL SERIALIZABLE clause\n\n\t\n\nIs ignored\n\n\n\n\nE152\n\n\t\n\nBasic SET TRANSACTION statement\n\n\t\n\n2\n\n\t\n\nSET TRANSACTION statement: READ ONLY and READ WRITE clauses\n\n\t\n\nIs ignored\n\n\n\n\nE161\n\n\t\n\nSQL comments using leading double minus\n\n\t\t\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n1\n\n\t\n\nCOLUMNS view\n\n\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n2\n\n\t\n\nTABLES view\n\n\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n3\n\n\t\n\nVIEWS view\n\n\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n4\n\n\t\n\nTABLE_CONSTRAINTS view\n\n\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n5\n\n\t\n\nREFERENTIAL_CONSTRAINTS view\n\n\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n6\n\n\t\n\nCHECK_CONSTRAINTS view\n\n\t\n\n\nF031\n\n\t\n\nBasic schema manipulation\n\n\t\n\n1\n\n\t\n\nCREATE TABLE statement to create persistent base tables\n\n\t\n\n\nF031\n\n\t\n\nBasic schema manipulation\n\n\t\n\n2\n\n\t\n\nCREATE VIEW statement\n\n\t\n\n\nF031\n\n\t\n\nBasic schema manipulation\n\n\t\n\n3\n\n\t\n\nGRANT statement\n\n\t\n\n\nF031\n\n\t\n\nBasic schema manipulation\n\n\t\n\n4\n\n\t\n\nALTER TABLE statement: ADD COLUMN clause\n\n\t\n\n\nF033\n\n\t\n\nALTER TABLE statement: DROP COLUMN clause\n\n\t\t\t\n\n\nF034\n\n\t\n\nExtended REVOKE statement\n\n\t\t\t\n\n\nF034\n\n\t\n\nExtended REVOKE statement\n\n\t\n\n1\n\n\t\n\nREVOKE statement performed by other than the owner of a schema object\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\t\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n1\n\n\t\n\nInner join (but not necessarily the INNER keyword)\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n2\n\n\t\n\nINNER keyword\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n3\n\n\t\n\nLEFT OUTER JOIN\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n4\n\n\t\n\nRIGHT OUTER JOIN\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n5\n\n\t\n\nOuter joins can be nested\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n7\n\n\t\n\nThe inner table in a left or right outer join can also be used in an inner join\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n8\n\n\t\n\nAll comparison operators are supported (rather than just =)\n\n\t\n\n\nF051\n\n\t\n\nBasic date and time\n\n\t\n\n1\n\n\t\n\nDATE data type (including support of DATE literal)\n\n\t\n\n\nF051\n\n\t\n\nBasic date and time\n\n\t\n\n3\n\n\t\n\nTIMESTAMP data type (including support of TIMESTAMP literal) with fractional seconds precision of at least 0 and 6\n\n\t\n\n\nF051\n\n\t\n\nBasic date and time\n\n\t\n\n4\n\n\t\n\nComparison predicate on DATE, TIME, and TIMESTAMP data types\n\n\t\n\n\nF051\n\n\t\n\nBasic date and time\n\n\t\n\n5\n\n\t\n\nExplicit CAST between datetime types and character string types\n\n\t\n\n\nF051\n\n\t\n\nBasic date and time\n\n\t\n\n6\n\n\t\n\nCURRENT_DATE\n\n\t\n\n\nF052\n\n\t\n\nIntervals and datetime arithmetic\n\n\t\t\t\n\n\nF111\n\n\t\n\nIsolation levels other than SERIALIZABLE\n\n\t\t\t\n\n\nF111\n\n\t\n\nIsolation levels other than SERIALIZABLE\n\n\t\n\n1\n\n\t\n\nREAD UNCOMMITTED isolation level\n\n\t\n\nIs ignored\n\n\n\n\nF111\n\n\t\n\nIsolation levels other than SERIALIZABLE\n\n\t\n\n2\n\n\t\n\nREAD COMMITTED isolation level\n\n\t\n\nIs ignored\n\n\n\n\nF111\n\n\t\n\nIsolation levels other than SERIALIZABLE\n\n\t\n\n3\n\n\t\n\nREPEATABLE READ isolation level\n\n\t\n\nIs ignored\n\n\n\n\nF131\n\n\t\n\nGrouped operations\n\n\t\n\n1\n\n\t\n\nWHERE, GROUP BY, and HAVING clauses supported in queries with grouped views\n\n\t\n\n\nF131\n\n\t\n\nGrouped operations\n\n\t\n\n3\n\n\t\n\nSet functions supported in queries with grouped views\n\n\t\n\n\nF171\n\n\t\n\nMultiple schemas per user\n\n\t\t\t\n\n\nF201\n\n\t\n\nCAST function\n\n\t\t\t\n\n\nF221\n\n\t\n\nExplicit defaults\n\n\t\t\t\n\n\nF222\n\n\t\n\nINSERT statement: DEFAULT VALUES clause\n\n\t\t\t\n\n\nF261\n\n\t\n\nCASE expression\n\n\t\t\t\n\n\nF261\n\n\t\n\nCASE expression\n\n\t\n\n1\n\n\t\n\nSimple CASE\n\n\t\n\n\nF261\n\n\t\n\nCASE expression\n\n\t\n\n2\n\n\t\n\nSearched CASE\n\n\t\n\n\nF261\n\n\t\n\nCASE expression\n\n\t\n\n3\n\n\t\n\nNULLIF\n\n\t\n\n\nF261\n\n\t\n\nCASE expression\n\n\t\n\n4\n\n\t\n\nCOALESCE\n\n\t\n\n\nF262\n\n\t\n\nExtended CASE expression\n\n\t\t\t\n\n\nF311\n\n\t\n\nSchema definition statement\n\n\t\n\n2\n\n\t\n\nCREATE TABLE for persistent base tables\n\n\t\n\n\nF311\n\n\t\n\nSchema definition statement\n\n\t\n\n3\n\n\t\n\nCREATE VIEW\n\n\t\n\n\nF381\n\n\t\n\nExtended schema manipulation\n\n\t\t\t\n\n\nF381\n\n\t\n\nExtended schema manipulation\n\n\t\n\n1\n\n\t\n\nALTER TABLE statement: ALTER COLUMN clause\n\n\t\n\n\nF381\n\n\t\n\nExtended schema manipulation\n\n\t\n\n2\n\n\t\n\nALTER TABLE statement: ADD CONSTRAINT clause\n\n\t\n\n\nF381\n\n\t\n\nExtended schema manipulation\n\n\t\n\n3\n\n\t\n\nALTER TABLE statement: DROP CONSTRAINT clause\n\n\t\n\n\nF391\n\n\t\n\nLong identifiers\n\n\t\t\t\n\n\nF392\n\n\t\n\nUnicode escapes in identifiers\n\n\t\t\t\n\n\nF401\n\n\t\n\nExtended joined table\n\n\t\n\n2\n\n\t\n\nFULL OUTER JOIN\n\n\t\n\n\nF401\n\n\t\n\nExtended joined table\n\n\t\n\n4\n\n\t\n\nCROSS JOIN\n\n\t\n\n\nF431\n\n\t\n\nRead-only scrollable cursors\n\n\t\t\t\n\n\nF431\n\n\t\n\nRead-only scrollable cursors\n\n\t\n\n1\n\n\t\n\nFETCH with explicit NEXT\n\n\t\n\n\nF431\n\n\t\n\nRead-only scrollable cursors\n\n\t\n\n2\n\n\t\n\nFETCH FIRST\n\n\t\n\n\nF431\n\n\t\n\nRead-only scrollable cursors\n\n\t\n\n3\n\n\t\n\nFETCH LAST\n\n\t\n\n\nF431\n\n\t\n\nRead-only scrollable cursors\n\n\t\n\n4\n\n\t\n\nFETCH PRIOR\n\n\t\n\n\nF431\n\n\t\n\nRead-only scrollable cursors\n\n\t\n\n5\n\n\t\n\nFETCH ABSOLUTE\n\n\t\n\n\nF431\n\n\t\n\nRead-only scrollable cursors\n\n\t\n\n6\n\n\t\n\nFETCH RELATIVE\n\n\t\n\n\nF471\n\n\t\n\nScalar subquery values\n\n\t\t\t\n\n\nF481\n\n\t\n\nExpanded NULL predicate\n\n\t\t\t\n\n\nF501\n\n\t\n\nFeatures and conformance views\n\n\t\n\n1\n\n\t\n\nSQL_FEATURES view\n\n\t\n\n\nF571\n\n\t\n\nTruth value tests\n\n\t\t\t\n\n\nF651\n\n\t\n\nCatalog name qualifiers\n\n\t\t\t\n\n\nF763\n\n\t\n\nCURRENT_SCHEMA\n\n\t\t\t\n\n\nF791\n\n\t\n\nInsensitive cursors\n\n\t\t\t\n\n\nF850\n\n\t\n\nTop-level <order by clause> in <query expression>\n\n\t\t\t\n\n\nF851\n\n\t\n\n<order by clause> in subqueries\n\n\t\t\t\n\n\nF852\n\n\t\n\nTop-level <order by clause> in views\n\n\t\t\t\n\n\nF855\n\n\t\n\nNested <order by clause> in <query expression>\n\n\t\t\t\n\n\nF856\n\n\t\n\nNested <fetch first clause> in <query expression>\n\n\t\t\t\n\n\nF857\n\n\t\n\nTop-level <fetch first clause> in <query expression>\n\n\t\t\t\n\n\nF858\n\n\t\n\n<fetch first clause> in subqueries\n\n\t\t\t\n\n\nF859\n\n\t\n\nTop-level <fetch first clause> in views\n\n\t\t\t\n\n\nF860\n\n\t\n\n<fetch first row count> in <fetch first clause>\n\n\t\t\t\n\n\nS091\n\n\t\n\nBasic array support\n\n\t\t\t\n\nspecial syntax\n\n\n\n\nS091\n\n\t\n\nBasic array support\n\n\t\n\n1\n\n\t\n\nArrays of built-in data types\n\n\t\n\nspecial syntax\n\n\n\n\nS098\n\n\t\n\nARRAY_AGG\n\n\t\t\t\n\n\nT031\n\n\t\n\nBOOLEAN data type\n\n\t\t\t\n\n\nT051\n\n\t\n\nRow types\n\n\t\t\t\n\nLimited to built-in table functions\n\n\n\n\nT054\n\n\t\n\nGREATEST and LEAST\n\n\t\t\t\n\n\nT055\n\n\t\n\nString padding functions\n\n\t\t\t\n\n\nT056\n\n\t\n\nMulti-character trim functions\n\n\t\t\t\n\n\nT071\n\n\t\n\nBIGINT data type\n\n\t\t\t\n\n\nT081\n\n\t\n\nOptional string types maximum length\n\n\t\t\t\n\n\nT121\n\n\t\n\nWITH (excluding RECURSIVE) in query expression\n\n\t\t\t\n\n\nT122\n\n\t\n\nWITH (excluding RECURSIVE) in subquery\n\n\t\t\t\n\n\nT175\n\n\t\n\nGenerated columns\n\n\t\t\t\n\n\nT241\n\n\t\n\nSTART TRANSACTION statement\n\n\t\t\t\n\nIs ignored\n\n\n\n\nT321\n\n\t\n\nBasic SQL-invoked routines\n\n\t\n\n1\n\n\t\n\nUser-defined functions with no overloading\n\n\t\n\n\nT321\n\n\t\n\nBasic SQL-invoked routines\n\n\t\n\n3\n\n\t\n\nFunction invocation\n\n\t\n\n\nT321\n\n\t\n\nBasic SQL-invoked routines\n\n\t\n\n6\n\n\t\n\nROUTINES view\n\n\t\n\n\nT331\n\n\t\n\nBasic roles\n\n\t\t\t\n\n\nT351\n\n\t\n\nBracketed SQL comments (/…/ comments)\n\n\t\t\t\n\n\nT441\n\n\t\n\nABS and MOD functions\n\n\t\t\t\n\n\nT461\n\n\t\n\nSymmetric BETWEEN predicate\n\n\t\t\t\n\n\nT471\n\n\t\n\nResult sets return value\n\n\t\t\t\n\n\nT615\n\n\t\n\nLEAD and LAG functions\n\n\t\t\t\n\n\nT617\n\n\t\n\nFIRST_VALUE and LAST_VALUE function\n\n\t\t\t\n\n\nT618\n\n\t\n\nNTH_VALUE function\n\n\t\t\t\n\n\nT621\n\n\t\n\nEnhanced numeric functions\n\n\t\t\t\n\n\nT626\n\n\t\n\nANY_VALUE aggregation\n\n\t\t\t\n\n\nT631\n\n\t\n\nIN predicate with one list element\n\n\t\t\t\n\n\nT662\n\n\t\n\nUnderscores in numeric literals\n\n\t\t\t"
  },
  {
    "title": "SQL compatibility — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/compatibility.html",
    "html": "5.6\nSQL compatibility\n\nCrateDB provides a standards-based SQL implementation similar to many other SQL databases. In particular, CrateDB aims for compatibility with PostgreSQL. However, CrateDB’s SQL dialect does have some unique characteristics, documented on this page.\n\nSee Also\n\nSQL: Syntax reference\n\nTable of contents\n\nImplementation notes\n\nData types\n\nCreate table\n\nAlter table\n\nSystem information tables\n\nBLOB support\n\nTransactions (BEGIN, START, COMMIT, and ROLLBACK)\n\nUnsupported features and functions\n\nImplementation notes\nData types\n\nCrateDB supports a set of primitive data types. The following table defines how data types of standard SQL map to CrateDB Data types.\n\nStandard SQL\n\n\t\n\nCrateDB\n\n\n\n\ninteger\n\n\t\n\ninteger, int, int4\n\n\n\n\nbit[8]\n\n\t\n\nbyte, char\n\n\n\n\nboolean, bool\n\n\t\n\nboolean\n\n\n\n\nchar [(n)], varchar [(n)]\n\n\t\n\nstring, text, varchar, character varying\n\n\n\n\ntimestamp with time zone\n\n\t\n\ntimestamp with time zone, timestamptz\n\n\n\n\ntimestamp\n\n\t\n\ntimestamp without time zone\n\n\n\n\nsmallint\n\n\t\n\nshort, int2, smallint\n\n\n\n\nbigint\n\n\t\n\nlong, bigint, int8\n\n\n\n\nreal\n\n\t\n\nfloat, real\n\n\n\n\ndouble precision\n\n\t\n\ndouble, double precision\n\nCreate table\n\nCREATE TABLE supports additional storage and table parameters for sharding, replication and routing of data, and does not support inheritance.\n\nAlter table\n\nALTER COLUMN action is not currently supported (see ALTER TABLE).\n\nSystem information tables\n\nThe read-only System information and Information schema tables have a slightly different schema than specified in standard SQL. They provide schema information and can be queried to get real-time statistical data about the cluster, its nodes, and their shards.\n\nBLOB support\n\nStandard SQL defines a binary string type, called BLOB or BINARY LARGE OBJECT. With CrateDB, Binary Data is instead stored in separate BLOB Tables (see Blobs) which can be sharded and replicated.\n\nTransactions (BEGIN, START, COMMIT, and ROLLBACK)\n\nCrateDB is focused on providing analytical capabilities over supporting traditional transactional use cases, and thus it does not provide transaction control. Every statement commits immediately and is replicated within the cluster.\n\nHowever, every row in CrateDB has a version number that is incremented whenever the record is modified. This version number can be used to implement patterns like Optimistic Concurrency Control, which can be used to solve many of the use cases that would otherwise require traditional transactions.\n\nUnsupported features and functions\n\nThese features of standard SQL are not supported:\n\nStored procedures\n\nTriggers\n\nWITH Queries (Common Table Expressions)\n\nSequences\n\nInheritance\n\nConstraints\n\nUnique\n\nForeign key\n\nExclusion constraints\n\nThese functions of standard SQL are either not supported or only partly supported:\n\nAggregate functions\n\nVarious functions available (see Aggregation)\n\nWindow functions\n\nVarious functions available (see Window functions)\n\nENUM support functions\n\nIS DISTINCT FROM\n\nNetwork address functions and operators\n\nMathematical functions\n\nCertain functions supported (see Mathematical functions)\n\nSet returning functions\n\nTrigger functions\n\nXML functions\n\nNote\n\nThe currently supported and unsupported features in CrateDB are exposed in the Information schema table (see sql_features for usage).\n\nCrateDB also supports the PostgreSQL wire protocol.\n\nIf you have use cases for any missing features, functions, or dialect improvements, let us know on GitHub! We are always improving and extending CrateDB and would love to hear your feedback."
  },
  {
    "title": "Version 2.1.5 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.1.5.html",
    "html": "5.6\nVersion 2.1.5\n\nReleased on 2017/08/14.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.1.5.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 2.1.0. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 2.1.0, and will require a full restart upgrade.\n\nConsult the upgrade notes for Version 2.1.0 when upgrading.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed an issue that caused the cluster.name setting in crate.yml file to be overridden by default name crate.\n\nFixed a null pointer exception when using explicit join conditions with fields which were not contained in the select list. For example:\n\nSELECT t3.z FROM t1\nJOIN t2 ON t1.a = t2.b\nJOIN t3 ON t2.b = t3.c\n"
  },
  {
    "title": "Version 1.1.3 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/1.1.3.html",
    "html": "5.6\nVersion 1.1.3\n\nReleased on 2017/05/09.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 0.57.0 or higher before you upgrade to 1.1.3.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 1.1.1 or higher. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 1.1.0, and will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nAdmin UI improvements.\n\nImproved the accuracy of results if arithmetic operators (+, -, *, and /) are used in expressions that contain only float-type values.\n\nFixed COPY FROM to be able to copy data into a partitioned table with a generated column as both the primary key and a partition column.\n\nFixed a NullPointerException which occurred when selecting routing_hash_function or version columns from sys.shards for blob tables.\n\nFixed error thrown when applying filtering, ordering or limit on joins with more than 2 tables.\n\nFixed issue which lead to an object’s column policy being changed to the default DYNAMIC when adding a nested object column using the ALTER TABLE statement.\n\nFixed an issue with regexp_replace: In some cases it used the third argument as flags parameter instead of the fourth argument.\n\nImproved error message when trying to update an element of an array.\n\nFixed a regression that lead to ArrayIndexOutOfBoundsException if a JOIN query was made with a WHERE clause on partition columns.\n\nFixed a NullPointerException which could occur if an attempt was made to use match on two different relations within an explicit join condition. This now raises a proper error stating that it’s not supported.\n\nWrong results were returned from queries with more than one level of nested subselects.\n\nORDER BY on joins caused incorrect order of values when having multiple non-distinct values on the left part of the join. Now ORDER BY is correctly applied.\n\nThe usage of DISTINCT in a query with a GROUP BY was producing wrong results and was changed to throw UnsupportedOperationException. E.g.:\n\nSELECT DISTINCT col1 FROM t1 GROUP BY col2, col1\n\n\nAn error was thrown when using ORDER BY COUNT(*) on JOINS. E.g.:\n\nSELECT COUNT(*) FROM t1, t2 ORDER BY 1\n"
  },
  {
    "title": "Version 2.1.6 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.1.6.html",
    "html": "5.6\nVersion 2.1.6\n\nReleased on 2017/08/29.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.1.6.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 2.1.0. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 2.1.0, and will require a full restart upgrade.\n\nConsult the upgrade notes for Version 2.1.0 when upgrading.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed issue that caused the Monitoring tab to redirect to /401 when the user didn’t have privileges for sys.cluster or sys.jobs_log.\n\nFix issue where a DROP TABLE statement would return before the table privileges are dropped.\n\nFixed a bug which returned a malformed response for PSQL queries containing whitespace characters.\n\nFixed a bug in the detection of correlated subqueries which are currently unsupported.\n\nFix display of redundant parenthesis around expressions visible in SHOW CREATE and EXPLAIN statements.\n\nUpdated Crash to 0.21.5 which removes a deprecation warning logged in CrateDB server on every REST request.\n\nFixed an issue that caused path.logs setting in crate.yml to be ignored.\n\nFixed column name in output by removing new lines when select list contains subquery. E.g.:\n\nSELECT 1 =\n    (SELECT 1)\n\n\nFixed an issue that prevents CrateDB from bootstrap on Windows hosts.\n\nFixed an issue that caused queries with IS NULL or IS NOT NULL on columns of type geo_point to fail.\n\nChanged crate Unix/Linux startup script to use standard sh syntax instead of bash specific syntax."
  },
  {
    "title": "Version 2.1.7 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.1.7.html",
    "html": "5.6\nVersion 2.1.7\n\nReleased on 2017/09/14.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.1.7.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 2.1.0. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 2.1.0, and will require a full restart upgrade.\n\nConsult the upgrade notes for Version 2.1.0 when upgrading.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed an issue causing global aggregations with JOINs on virtual tables (subselects) which contain LIMIT to return incorrect results. E.g.:\n\nSELECT COUNT(*) FROM\n  (SELECT * FROM t1 ORDER BY a LIMIT 5) t1,\nJOIN\n  t2\nON t1.a = t2.b\n\n\nFixed an issue that caused an error when trying to create a table with a generated column expression of type array.\n\nFixed an issue that caused the account_user-column to be empty in the twitter tutorial plugin of the Admin UI.\n\nFixed an issue when using GRANT/REVOKE/DENY statements on a table with a custom schema set. The statements would result in permission changed on the default doc schema.\n\nFixed an issue with INNER and CROSS JOINS on more than 2 tables that could result in a Iterator is not on a row error."
  },
  {
    "title": "Version 1.1.4 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/1.1.4.html",
    "html": "5.6\nVersion 1.1.4\n\nReleased on 2017/06/02.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 0.57.0 or higher before you upgrade to 1.1.4.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 1.1.1 or higher. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 1.1.0, and will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nChanges\n\nFixes\n\nChangelog\nChanges\n\nImproved the resiliency of the retrieval of large results via HTTP. Queries are now aborted and result in an error if they consume too much memory.\n\nChanged the QueryStats JMX MBean to deliver node-based values instead of cluster-based values.\n\nThis makes it possible to spot performance discrepancies between nodes more easily.\n\nFixes\n\nFixed a COPY FROM issue that caused imports into tables with certain combinations of PARTITIONED BY, PRIMARY KEY and GENERATED COLUMNS to fail.\n\nFixed an issue with algorithm that tries to reorder the joined tables using the optimum ordering. The issue caused an exception to be thrown when join conditions contain table(s) which are not part of the adjacent joined tables. E.g.:\n\nSELECT * FROM t1 JOIN t2 JOIN t3 JOIN t4 ON t4.id = t2.id\n\n\nFixed an issue that led to ArrayIndexOutOfBoundsException on DISTINCT or GROUP BY queries on the sys.shards table.\n\nFixed an issue that could cause sys.operations entries to remain even after the operation has finished.\n\nOptimized the JMX QueryStats MBean to prevent it from putting too much load on the cluster.\n\nFixed the calculation of the OverallQueryAverageDuration QueryStats MBean.\n\nThe internal fetchSize is now dynamic based on configured heap and has an upper bound to prevent OutOfMemory errors if a PostgreSQL client retrieves a large result set without setting a fetchSize, or setting a fetchSize which is too large.\n\nFixed a race condition that could lead to KILL (ALL) causing queries to get stuck instead of interrupting them.\n\nFix an issued that cause a NullPointerException when ordering by system columns.\n\nFixed validation so that SELECT DISTINCT can be used only if there is no GROUP BY present or if the set of GROUP BY expressions is the same as the set SELECT expressions.\n\nAdded validation that ORDER BY symbols are included in the SELECT list when DISTINCT is used."
  },
  {
    "title": "Version 1.1.6 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/1.1.6.html",
    "html": "5.6\nVersion 1.1.6\n\nReleased on 2017/06/23.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 0.57.0 or higher before you upgrade to 1.1.6.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 1.1.1 or higher. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 1.1.0, and will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed an issue that caused an exception to be thrown when applying aggregations on generated columns of a table.\n\nFixed a bug in the memory accounting of the circuit breaker for HTTP results when querying for columns of undefined type.\n\nFixed issue that caused an exception when querying the _id column using all defined primary keys inside the WHERE clause over the HTTP API.\n\nFixed wrong results when querying IS NULL and IS NOT NULL on an array of objects.\n\nFixed an issue that caused an Exception to be thrown on JOIN queries with 4 or more tables when an ORDER BY is also applied.\n\nFixed an issue that resulted in rows being unable to be queried by primary keys, when the order of the primary key columns on insert differed from the order of the primary key columns on the table definition. Note: If records are already inserted by using a different primary key column order, they must be re-inserted, otherwise queries will still fail for these rows.\n\nImproved the resiliency of queries on sys.nodes: If a node disconnects during the execution of a query it will no longer fail.\n\nAdded proper handling when memory requirements of a JOIN query exceeds the available memory. Instead of having OutOfMemoryException thrown which led to killed nodes in the cluster, the issue is detected and the query is killed without affecting the cluster.\n\nFixed an issue that could cause DELETE statements to fail instead of “not matching” if there was a _version column in the WHERE clause.\n\nFixed an error handling issue that could lead to the termination of a node in very rare cases. (Usually if a user invoked the KILL statement)\n\nFixed an issue that caused an exception to be thrown when using unnest with 10 or more columns."
  },
  {
    "title": "Version 2.1.10 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.1.10.html",
    "html": "5.6\nVersion 2.1.10\n\nReleased on 2017/11/13.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.1.10.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 2.1.0. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 2.1.0, and will require a full restart upgrade.\n\nConsult the upgrade notes for Version 2.1.0 when upgrading.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed an issue that resulted in closed partitions showing a CRITICAL data state in the Admin UI.\n\nFixed an issue that caused INSERT INTO .. VALUES statements to fail if the VALUES clause contains multiple rows and the table has a generated column as primary key.\n\nFixed a performance regression for DELETE and UPDATE statements introduced in 2.1.9."
  },
  {
    "title": "Version 1.1.5 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/1.1.5.html",
    "html": "5.6\nVersion 1.1.5\n\nReleased on 2017/06/12.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 0.57.0 or higher before you upgrade to 1.1.5.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 1.1.1 or higher. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 1.1.0, and will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed an issue that leads to an exception if the statement evaluates on arrays that are provided in an aggregation function.\n\nFixed a performance regression that could cause JOIN queries to execute slower than they used to.\n\nReturn proper exception when group by is used on scalar functions that are applied to an aggregation.\n\nFixed an issue that causes aliases used in the select list to get lost on subselect queries.\n\nThe correct error messages and codes are now thrown for REST actions.\n\nFixed an issue that could cause non-grouping aggregations on virtual tables to return the wrong result.\n\nFixed a bug in the memory accounting of the circuit breaker for HTTP results when querying GEO_SHAPE columns.\n\nReflect internally used default size for translog.flush_threshold_size also in documentation and expose the correct default value in table settings."
  },
  {
    "title": "Version 2.2.0 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.2.0.html",
    "html": "5.6\nVersion 2.2.0\n\nReleased on 2017/09/19.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.2.0.\n\nYou cannot perform a rolling upgrade to this version. Any upgrade to this version will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nBreaking Changes\n\nChanges\n\nChangelog\nBreaking Changes\n\nGROUP BY now executes against the real columns and fallbacks to substituting possible aliases in the case that the columns do not exist.\n\nThis will cause statements that used alias values that shadowed multiple columns with the same name to fail (e.g. alias name in a join statement where multiple tables have the real column name).\n\nStatements with aliases that shadowed a real column and also retrieve that real column will no longer fail as the real column will be used in the GROUP BY clause (this will not be ambiguous anymore).\n\nChange semantics of IN operator to behave like = ANY.\n\nThe argument list for IN now has to be comprised of the same type. For example, this is now an illegal IN query because the list mixes integer and double type:\n\nSELECT * FROM t1 WHERE id IN (1, 1.2, 2)\n\n\nThe above would get translated into the following and throw an error:\n\nSELECT * FROM t1 WHERE id = ANY([1, 1.2, 3])\n\nChanges\n\nAdded the shards plugin for the Enterprise Edition in the Admin UI.\n\nThe shards plugin is a visualization that displays information about shards by table, partition and node.\n\nAdded new tab in Enterprise Edition of the Admin UI to display users and their privileges.\n\nGeneral Admin UI improvements.\n\nUpdated Crash to 0.22.1, which includes the following changes:\n\nAdded a status toolbar that prints the current session info.\n\nStart autocompletion for non-command keys at the third character.\n\nAllow IPv6 addresses in IP column type.\n\nUpgraded Elasticsearch to 5.5.2.\n\nAdded node cluster check for cluster-name folder in data path directory.\n\nIntroduce support for single column subselects in ANY and IN, e.g.:\n\nSELECT * FROM t1 WHERE id = ANY(SELECT id FROM t2)\nSELECT * FROM t1 WHERE id IN (SELECT id FROM t2)\n\n\nImproved resiliency of the table rename operation.\n\nRelaxed column naming restrictions.\n\nImproved resiliency of drop, close and open table operations.\n\nAdded empty tables KEY_COLUMN_USAGE and REFERENTIAL_CONSTRAINTS to INFORMATION_SCHEMA to be more compliant with the SQL99 standard."
  },
  {
    "title": "Version 1.2.0 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/1.2.0.html",
    "html": "5.6\nVersion 1.2.0\n\nReleased on 2017/04/24.\n\nNote\n\nThis release is superseded by Version 2.0.0 and should not be installed. You should directly install Version 2.0.0 instead."
  },
  {
    "title": "Version 2.0.0 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.0.0.html",
    "html": "5.6\nVersion 2.0.0\n\nReleased on 2017/05/16.\n\nWarning\n\nCrateDB 2.x versions prior 2.0.4 (including this version) contain a critical bug which leads to deletion of blob data upon node shutdown. It is recommended to not install those versions.\n\nTable of contents\n\nChangelog\n\nBreaking Changes\n\nChanges\n\nFixes\n\nUpgrade Notes\n\nDaemon User\n\nLogging\n\nSystem Properties\n\nConfiguration Changes\n\nSQL Changes\n\nBind Address\n\nHeap Size\n\nCluster name in path data\n\nBoolean Data Type\n\nChangelog\nBreaking Changes\n\nTo accommodate user-defined functions, some new reserved keywords have been added to the CrateDB SQL dialect: RETURNS, CALLED, REPLACE, FUNCTION, LANGUAGE, INPUT\n\nThe license.enterprise setting is set to true by default. This enables the CrateDB Enterprise Edition.\n\nEnabling this setting requires a valid enterprise license for production use.\n\nIf you disable this setting, CrateDB will run with the standard feature set.\n\nAll custom node.* style attributes must now be written as node.attr.* to distinguish them from attributes that CrateDB uses internally. Consult the node attribute docs for information.\n\nThe node.client setting has been removed.\n\nThe default value of the node.attr.max_local_storage_nodes node setting has been changed to 1 to prevent running multiple nodes on the same data path by default.\n\nPrevious versions of CrateDB defaulted to allowing up to 50 nodes running on the same data path. This was confusing where users accidentally started multiple nodes and ended up thinking they have lost data because the second node will start with an empty directory.\n\nRunning multiple nodes on the same data path tends to be an exception, so this is a safer default.\n\nParsing support of time values has been changed:\n\nThe unit w representing weeks is no longer supported.\n\nFractional time values (e.g. 0.5s) are no longer supported. For example, this means when setting timeouts, 0.5s will be rejected and should instead be input as 500ms.\n\nThe already unused path.work node setting has been removed.\n\nThe node setting bootstrap.mlockall has been renamed to bootstrap.memory_lock.\n\nThe keyword_repeat and type_as_payload built-in token filter have been removed.\n\nThe classic built-in analyzer has been removed.\n\nThe shard balance related cluster settings cluster.routing.allocation.balance.primary and cluster.routing.allocation.balance.replica have been removed.\n\nSome recovery related cluster settings have been removed or replaced:\n\nThe indices.recovery.concurrent_streams cluster setting is now superseded by cluster.routing.allocation.node_concurrent_recoveries.\n\nThe indices.recovery.activity_timeout cluster setting have been renamed to indices.recovery.recovery_activity_timeout.\n\nFollowing recovery cluster settings have been removed:\n\nindices.recovery.file_chunk_size\n\nindices.recovery.translog_ops\n\nindices.recovery.translog_size\n\nindices.recovery.compress\n\nLogging is now configured by log4j2.properties instead of logging.yml.\n\nThe plugin interface has changed, injecting classes on shard or index levels is no longer supported.\n\nIt’s no longer possible to run CrateDB as the Unix root user.\n\nSome translog related table settings have been removed or replaced:\n\nThe index.translog.interval, translog.disable_flush and translog.flush_threshold_period table settings have been removed.\n\nThe index.translog.sync_interval table setting doesn’t accept a value less than 100ms which prevents fsyncing too often if async durability is enabled. The special value 0 is no longer supported.\n\nThe index.translog.flush_threshold_ops table setting is not supported anymore. In order to control flushes based on the transaction log growth use index.translog.flush_threshold_size instead.\n\nThe COPY FROM statement now requires column names to be quoted in the JSON file being imported.\n\nQueries on columns with INDEX OFF will now fail instead of always resulting in an empty result.\n\nConfiguration support using system properties has been dropped.\n\nIt’s no longer possible to use Hadoop 1.x as a repository for snapshots.\n\nChanged default bind and publish address from 0.0.0.0 to the system loopback addresses which will result in CrateDB listening only to local ports.\n\nThe discovery.ec2.ping_timeout setting has been removed and the discovery.zen.ping_timeout setting is now also used for EC2 discovery.\n\nThe monitor.jvm.gc.[old|young].[debug|info|warn] settings used to configure logging of garbage collection have been renamed (adding collector) to monitor.jvm.gc.collector.[old|young].[debug|info|warn].\n\nRecovery timeout settings changes:\n\nindices.recovery.retry_internal_action_timeout has been renamed to indices.recovery.internal_action_timeout\n\nindices.recovery.retry_internal_long_action_timeout has been renamed to indices.recovery.internal_action_long_timeout\n\nindices.recovery.retry_activity_timeout has been renamed to indices.recovery.recovery_activity_timeout\n\nThread pool settings prefix have been changed from threadpool to thread_pool. E.g.: thread_pool.<name>.type.\n\nThe cluster name is not part of the effective path where data is stored anymore.\n\nThe blobs data directory layout has changed.\n\nChanges\n\nExtended the subselect support.\n\nAdded support for host based authentication (HBA).\n\nAdded support for renaming tables using the ALTER ... RENAME TO ... statement.\n\nAdded support for CREATE USER and DROP USER.\n\nAdded support for opening and closing a table or single partition.\n\nInformation on the state of tables/partitions is now exposed by a new column closed on the information_schema.tables and information_schema.table_partitions tables.\n\nAdded full support for DISTINCT on queries where GROUP BY is present.\n\nUDC pings will send licence.ident if defined from now on.\n\nAdded support for GROUP BY in combination with subselect. E.g.:\n\nSELECT x, COUNT(*) FROM (SELECT x FROM t LIMIT 1) AS tt GROUP BY x;\n\n\nImplemented hash sum scalar functions (MD5, SHA1). Please see sha1.\n\nVarious Admin UI improvements.\n\nAdded support for GROUP BY on joins.\n\nAdded support for user-defined functions.\n\nAdded JavaScript language for user-defined functions.\n\nAdded cluster check and warning for unlicensed usage of CrateDB Enterprise.\n\nAdded built-in fingerprint, keep_types, min_hash and serbian_normalization token filter.\n\nAdded a fingerprint built-in analyzer.\n\nUpgraded to Elasticsearch 5.0.2.\n\nImproved performance of blob stats computation by calculating them in an incremental manner.\n\nOptimized performance of negation queries on NOT NULL columns. E.g.:\n\nSELECT * FROM t WHERE not_null_col != 10\n\n\nUpdated documentation to indicate that it’s not possible to use object, geo_point, geo_shape, or array in the ORDER BY clause.\n\nRemoved psql.enabled and psql.port settings from sys.cluster because they where wrongly exposed in this table.\n\nUse the region of the EC2 instance for EC2 discovery when neither cloud.aws.ec2.endpoint nor cloud.aws.region are specified or do not resolve in a valid service endpoint.\n\nIt is now possible to restore an empty partitioned table.\n\nAdded validation that ORDER BY symbols are included in the SELECT list when DISTINCT is used.\n\nFixes\n\nFixed an issue which could result in queries being stuck if the thread pools are exhausted.\n\nFixed an issue which caused failing sys.snapshot queries if the data.path of an existing fs repository was not configured anymore.\n\nFixed that sys.snapshot queries hung instead of throwing an error if something went wrong.\n\nUpgrade Notes\nDaemon User\n\nYou can no longer run CrateDB as the superuser on Unix-like systems. You should create a new crate user for running the CrateDB daemon.\n\nLogging\n\nThe logging.yml has been removed. You must migrate your Logging configuration to the new log4j2.properties file.\n\nSystem Properties\n\nYou can no longer use the JAVA_OPTIONS or CRATE_JAVA_OPTS environment variables to pass configuration to CrateDB itself, for example:\n\nJAVA_OPTIONS=-Dcluster.name=crate\n\n\nOr:\n\nCRATE_JAVA_OPTS=-Dcluster.name=crate\n\n\nInstead, you must pass these options in on the CLI tools.\n\nYou can continue to use the JAVA_OPTIONS and CRATE_JAVA_OPTS environment variables to set general JVM properties and CrateDB specific JVM properties, respectively.\n\nConfiguration Changes\n\nMany configuration settings and files have been renamed or removed. You must review the Breaking Changes section above and update your setup as necessary.\n\nSQL Changes\n\nSeveral breaking changes were made to CrateDB’s SQL. This includes changes to time parsing, syntax changes, and new reserved keywords. You must review the Breaking Changes section above and update your client code as necessary.\n\nBind Address\n\nThe default bind address has been changed from 0.0.0.0 to the loopback address (meaning it will only be accessible on localhost). See Hosts for more.\n\nIf you want to keep the original behaviour (i.e. bind to every available network interface) you must add the following line to your Configuration file:\n\nnetwork.host: 0.0.0.0\n\n\nNote\n\nIf you bind to a network reachable IP address, you must follow the instructions in the new bootstrap checks guide.\n\nHeap Size\n\nIf you have previously set or configured CRATE_MIN_MEM or CRATE_MAX_MEM in your startup scripts or environment, you must remove both, and replace them with a single variable CRATE_HEAP_SIZE. The CRATE_HEAP_SIZE variable sets both the minimum and maximum memory to allocate, and should be set to whatever your previous CRATE_MAX_MEM was set to.\n\nCluster name in path data\n\nThe computation of the effective data directory path has changed in a way that the cluster name is not part of the path anymore. In previous versions it was $PATH_DATA_DIR/$CLUSTER_NAME/nodes/ and now it is $PATH_DATA_DIR/nodes/. There’s a fallback that still accepts the old data structure, which will be removed in future versions of CrateDB. It will be required that the data directory is either moved to the new location or the path.data setting gets changed to point to the old location by appending the cluster name to it (e.g /data/ becomes /data/yourclustername). Therefore it’s not possible anymore for multiple clusters to share the exact same path.data directory.\n\nBoolean Data Type\n\nTables that have been created with CrateDB version 0.54.x or smaller and that contain a column of type BOOLEAN must be re-created to be able to perform all supported operations on that column."
  },
  {
    "title": "Version 2.0.1 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.0.1.html",
    "html": "5.6\nVersion 2.0.1\n\nReleased on 2017/06/12.\n\nWarning\n\nCrateDB 2.x versions prior 2.0.4 (including this version) contain a critical bug which leads to deletion of blob data upon node shutdown. It is recommended to not install those versions.\n\nTable of contents\n\nChangelog\n\nBreaking Changes\n\nChanges\n\nFixes\n\nChangelog\nBreaking Changes\n\nChanged default required shard copies for table creation and write operations from quorum to all.\n\nChanged the default value of number_of_replicas from 1 to 0-1.\n\nChanges\n\nExpose new setting write.wait_for_active_shards to allow users to adjust the required shard copies for write operation to their needs.\n\nExpose fields for information_schema.columns and information_schema.tables so that it conforms to SQL-99 standard.\n\nFixes\n\nAdded missing table setting translog.durability which is required and must be set accordingly so that translog.sync_interval takes effect.\n\nFixed a NPE when querying sys.shards table.\n\nLog failed authentication attempts at log level WARN."
  },
  {
    "title": "Version 2.0.2 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.0.2.html",
    "html": "5.6\nVersion 2.0.2\n\nReleased on 2017/06/23.\n\nWarning\n\nCrateDB 2.x versions prior 2.0.4 (including this version) contain a critical bug which leads to deletion of blob data upon node shutdown. It is recommended to not install those versions.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed an issue that caused an exception to be thrown when applying aggregations on generated columns of a table.\n\nFixed a bug in the memory accounting of the circuit breaker for HTTP results when querying for columns of undefined type.\n\nFixed wrong results when querying IS NULL on an array of objects.\n\nFixed issue that caused an exception when querying the _id column using all defined primary keys inside the WHERE clause over the HTTP API.\n\nFixed an issue that caused an Exception to be thrown on JOIN queries with 4 or more tables when an ORDER BY is also applied.\n\nFixed an issue that resulted in rows being unable to be queried by primary keys, when the order of the primary key columns on insert differed from the order of the primary key columns on the table definition. Note: If records are already inserted by using a different primary key column order, they must be re-inserted, otherwise queries will still fail for these rows.\n\nFixed an issue that could cause DELETE by query and UPDATE statements to fail on datasets larger than 10,000 rows.\n\nImproved the resiliency of queries on sys.nodes: If a node disconnects during the execution of a query it will no longer fail.\n\nAdded proper handling when memory requirements of a JOIN query exceeds the available memory. Instead of having OutOfMemoryException thrown which led to killed nodes in the cluster, the issue is detected and the query is killed without affecting the cluster.\n\nFixed an issue that could cause DELETE statements to fail instead of “not matching” if there was a _version column in the WHERE clause.\n\nFixed an error handling issue that could lead to the termination of a node in very rare cases. (Usually if a user invoked the KILL statement)\n\nFixed issue where bulk operations like INSERT from dynamic queries and COPY FROM did not stop after being killed.\n\nCREATE USER and DROP USER statements will now only respond after all nodes in the cluster have processed the change.\n\nFixed an issue that caused an exception to be thrown when using unnest with 10 or more columns."
  },
  {
    "title": "Version 2.0.3 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.0.3.html",
    "html": "5.6\nVersion 2.0.3\n\nReleased on 2017/06/30.\n\nWarning\n\nCrateDB 2.x versions prior 2.0.4 (including this version) contain a critical bug which leads to deletion of blob data upon node shutdown. It is recommended to not install those versions.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed support for range queries on the _id and _uid columns. These queries were not working since version 2.0.0 (Issue: #5845).\n\nFixed an issue when RENAME is used on a partitioned table, which would result in data loss if a new table was created with the old name, and then afterwards was dropped (Issue: #5823)."
  },
  {
    "title": "Version 2.0.4 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.0.4.html",
    "html": "5.6\nVersion 2.0.4\n\nReleased on 2017/07/06.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.0.4.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 2.0.0. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 2.0.0, and will require a full restart upgrade.\n\nConsult the upgrade notes for Version 2.0.0 when upgrading.\n\nWarning\n\nIf you’re using CrateDB’s BLOB storage you should consult the Upgrade Notes.\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nUpgrade Notes\n\nChangelog\n\nChanges\n\nFixes\n\nUpgrade Notes\n\nDue to a bug introduced in Version 2.0.0 that can cause loss of BLOB data, it is necessary to perform a rolling upgrade if you’re running a version greater than or equal to 2.0.0 and using BLOB tables.\n\nAdditionally, the number of replicas needs to be set to at least 1 for all blob tables and you need to make sure that data is fully replicated before continuing.\n\nOnly then you may upgrade one node after each other.\n\nChangelog\nChanges\n\nThe recovery_after_time node check now fails if it would prevent the gateway.expected_nodes setting from having any effect.\n\nFixes\n\nFixed SELECT settings['gateway']['recover_after_time'] FROM sys.cluster so it returns the correct value.\n\nFixed an issue that caused blob data to be deleted upon node shutdown.\n\nFix thread-safety issue for scalar functions md5 and sha1.\n\ninformation.schema.tables now returns the default routing hash function if none is present in the table metadata.\n\nFixed an issue that caused an exception to be thrown when using COUNT(*) with subselects that return one row, e.g.:\n\nSELECT count(*) FROM t1 WHERE id > (SELECT max(col1) FROM t2)\n\n\nThrow proper exception when using the _raw column inside the where clause instead of silently ignoring it."
  },
  {
    "title": "Version 2.2.2 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.2.2.html",
    "html": "5.6\nVersion 2.2.2\n\nReleased on 2017/11/08.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.2.2.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 2.2.0. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 2.2.0, and will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nRemoved horizontal scroll from the console editor in the Admin UI.\n\nFixed an issue that cause the status bar of the Admin UI to show an error if the user has an Ad blocker enabled.\n\nFixed an issue that caused extra spaces to be added to the formatted console results in the Admin UI.\n\nFixed Admin UI shard view to show all nodes, even those with no shards.\n\nFixed issue that caused NULL values to be displayed as empty objects in the console view of the Admin UI.\n\nFixed Tables view of the Admin UI to show messages when the table list is empty.\n\nFixed an issue that prevented the complete graceful shutdown of CrateDB node.\n\nFixed an issue that caused a NullPointerException for queries that use the IN or ANY operators on timestamp fields.\n\nImproves resiliency of COPY FROM and INSERT FROM SUBQUERY statements when lot of new partitions will be created.\n\nFixed a NullPointerException which could occur when joining four tables with a join condition which referred to fields from the leftmost relation.\n\nFixed a problem that caused WITHIN queries to return no or incorrect results.\n\nFixed an issue where internal cluster state updates could reset the active ingestion rules, causing all ingest operations to stop.\n\nFixed the problem that stats.breaker.* settings were not applied to the correct circuit breaker.\n\nFixed an issue that caused an error Primary key value must not be NULL to be thrown when trying to insert rows in a table that has a generated column which is used both in PARTITION BY and PRIMARY KEY."
  },
  {
    "title": "Version 2.0.5 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.0.5.html",
    "html": "5.6\nVersion 2.0.5\n\nReleased on 2017/07/11.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.0.5.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 2.0.0. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 2.0.0, and will require a full restart upgrade.\n\nConsult the upgrade notes for Version 2.0.0 when upgrading.\n\nWarning\n\nIf you’re using CrateDB’s BLOB storage you should consult the Upgrade Notes.\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nUpgrade Notes\n\nChangelog\n\nFixes\n\nUpgrade Notes\n\nDue to a bug introduced in Version 2.0.0 that can cause loss of BLOB data, it is necessary to perform a rolling upgrade if you’re running a version >= 2.0.0 and < 2.0.4 and using BLOB tables.\n\nAdditionally, the number of replicas needs to be set to at least 1 for all blob tables and you need to make sure that data is fully replicated before continuing.\n\nOnly then you may upgrade one node after each other.\n\nChangelog\nFixes\n\nFixed an issue that caused an exception to be thrown when using COUNT(*) with a JOIN on 3 or more tables.\n\nFixed a regression that caused queries with a GROUP BY on array columns, which are wrapped in a scalar function to fail.\n\nFixed an issue that caused an exception if a table that has been created with CrateDB version <= 1.1 is modified using ALTER TABLE."
  },
  {
    "title": "Version 2.2.1 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.2.1.html",
    "html": "5.6\nVersion 2.2.1\n\nReleased on 2017/10/23.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.2.1.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 2.2.0. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 2.2.0, and will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nChanges\n\nFixes\n\nChangelog\nChanges\n\nAdded cluster checks that warn if some tables need to be recreated so that they are compatible with future versions of CrateDB >= 3.0.0.\n\nFixes\n\nVarious Admin UI improvements.\n\nFixed an issue that resulted in aliases overriding column names when a subselect is used and a column appears in the outer SELECT multiple times, without an alias and with alias or with multiple aliases. E.g.:\n\nSELECT a, a AS newcol FROM (SELECT a FROM t WHERE a > 1)\nSELECT a AS newcol1, a AS newcol2 FROM (SELECT a FROM t WHERE a > 1)\n\n\nFixed an issue that caused INSERT statements using a subquery on the sys.shards system table to fail.\n\nFixed race condition in COPY FROM that could display incorrect row count when inserting into table partition that does not exist upfront.\n\nFixed a bug that caused incorrect results to be returned for JOIN queries when the table stats indicated that the left table of a join is smaller than the right.\n\nFixed passing arguments that contain spaces in the crate shell script.\n\nFixed an issue that caused a table that is not part of the doc schema to be unavailable/hidden if it gets closed using ALTER TABLE.\n\nFixed an issue where the query circuit breaker would be tripped after running several queries due to incorrect memory tracking. Subsequent operations would’ve failed due to the lack of circuit breaker cleanup."
  },
  {
    "title": "Version 2.3.1 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.3.1.html",
    "html": "5.6\nVersion 2.3.1\n\nReleased on 2018/01/22.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.3.1.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 2.3.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nChanges\n\nFixes\n\nChangelog\nChanges\n\nIn certain cases it’s no longer necessary to explicitly add generated columns that are part of a composite primary key to the WHERE clause in order to get an execution plan with real-time semantics.\n\nGreatly improved performance of queries that uses scalar functions inside the WHERE clause.\n\nFixes\n\nAvoid downcasts in function expressions which could lead to reduced precision. For example\n\nSELECT round(float_col) + 1.1 would result in\n  SELECT round(float_col) + 1\nbut is now resolved to\n  SELECT to_float(round(float_col)) + 1.1\n\n\nEnsured natural order of configuration keys for Host Based Authentication.\n\nFixed encoding/decoding of Timestamp type for PostgreSQL wire protocol to support Golang psql drivers.\n\nFixed an Admin UI issue that caused the Cluster tab to not be loaded correctly.\n\nMap the Tab key to insert spaces instead of a tab character in the query console of the Admin UI.\n\nFixed an issue that caused the user name to not be displayed in the Admin UI."
  },
  {
    "title": "Version 2.3.2 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.3.2.html",
    "html": "5.6\nVersion 2.3.2\n\nReleased on 2018/01/25.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.3.2.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 2.3.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nChanges\n\nFixes\n\nChangelog\nChanges\n\nImplemented a performance optimization when IN() or = ANY() operators are used with a subquery. For further details see: https://github.com/crate/crate/issues/6755\n\nFixes\n\nRemoved unnecessary invocation of non-existent syscall that caused CrateDB to crash on startup with certain Ubuntu kernel versions."
  },
  {
    "title": "Version 2.3.3 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.3.3.html",
    "html": "5.6\nVersion 2.3.3\n\nReleased on 2018/02/15.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.3.3.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 2.3.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nImproved the handling of node disconnects to ensure there are no stale queries in case a node dies.\n\nFixed an issue that could result in a Bulk operation not supported for RootRelationBoundary error when querying sys.checks\n\nFixed a bug in parsing the UDF meta data when read from state file during node start that could cause other custom meta data (such as users) to disappear from cluster state.\n\nFixed an Admin UI issue that caused the translation strings to not be loaded correctly.\n\nFixed an issue that only occurred in Enterprise Edition, when privileges are enabled. This issue would result in an exception about missing privileges being thrown, rather than the expected Unsupported Feature exception, when attempting operations on closed tables/partitions as a user.\n\nFixed an issue that caused a NullPointerException if REFRESH TABLE was executed on a table with closed partitions.\n\nFixed an issue that caused MissingPrivilegeException for non-superusers for queries with UNION ALL.\n\nFixed a regression that caused assignments of arrays including parameter placeholders to not work correctly.\n\nFixed a regression which caused IS NOT NULL predicates on columns of type array(object) to not match correctly.\n\nFixed an issue that caused the percentile aggregation to fail if an array containing a single item was passed as fractions.\n\nFixed an issue which resulted in an exception when a routing column was compared against a subquery inside a WHERE clause.\n\nFixed a performance regression resulting in a table scan instead of a NO-MATCH if a subquery used inside a WHERE clause returns no result (https://github.com/crate/crate/issues/6773)."
  },
  {
    "title": "Version 2.3.4 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.3.4.html",
    "html": "5.6\nVersion 2.3.4\n\nReleased on 2018/03/06.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.3.4.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 2.3.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nChanges\n\nFixes\n\nChangelog\nChanges\n\nUpdated Elasticsearch to v5.6.8.\n\nFixes\n\nFixed the URL linking to stats collection documentation in the monitoring tab of the Admin UI.\n\nUpdated the table list search field to filter tables only by table name and table schema in the Admin UI.\n\nFixed an issue in the Admin UI that caused the table list to display wrong results.\n\nRe-enable implicit casting for columns whenever columns are used on both sides of an operator or function. The implicit cast will obey the type precedence (e.g. long > integer).\n\nFixed an issue that caused incorrect results to be returned when a WHERE clause filters on a partition column and a non partition column. E.g.:\n\nSELECT * FROM t WHERE parted_col='value' AND other_col='some_value'\n\n\nFixed an issue that causes JOINS with certain ORDER BY constructs to fail.\n\nFixed an issue where a NullPointerException would be thrown when trying to reroute shards to and from non-data nodes using ALTER TABLE REROUTE.\n\nFixed an issue that would cause queries and data manipulation statements with explicit WHERE conditions to throw NullPointerException when running against a table with a composite primary key.\n\nFixed a race condition that could cause the decommissioning of nodes using the graceful shutdown procedure to interrupt the execution of active queries.\n\nRead only queries should no longer fail with a TableUnknownException or IndexNotFoundException in case of a shard relocation."
  },
  {
    "title": "Version 2.3.6 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.3.6.html",
    "html": "5.6\nVersion 2.3.6\n\nReleased on 2018/04/04.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.3.6.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 2.3.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nChanges\n\nFixes\n\nChangelog\nChanges\n\nArrays can now contain mixed types if they’re safely convertible. JSON libraries tend to encode values like [ 0.0, 1.2] as [ 0, 1.2 ], this caused an error because of the strict type match we enforced before.\n\nFixes\n\nDo not allow renaming a table when connected to a read-only node.\n\nFixed an issue that caused an error to be thrown when filtering on a partitioned table using a column which is a source column for a GENERATED column which in turn is used as partition column. E.g.:\n\nCREATE TABLE parted_t(\n    t TIMESTAMP,\n    day TIMESTAMP GENERATED ALWAYS AS date_trunc('day', t)\n) PARTITIONED BY (day);\n\nSELECT * FROM parted_t WHERE t > 1522170000000;\n"
  },
  {
    "title": "Version 3.0.1 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.0.1.html",
    "html": "5.6\nVersion 3.0.1\n\nReleased on 2018/05/30.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.0.1.\n\nWe recommend that you upgrade to the latest 2.3 release before moving to 3.0.1.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 3.0.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nPlease consult the Upgrade Notes before upgrading.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.0 and must be recreated before moving to 3.0.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed an issue that caused the CrateDB process CPU calculation in the Admin UI to be false.\n\nFixed the default log file name to crate.log if no cluster name was explicitly specified in the settings.\n\nFixed an issue that caused a ClassCastException to be thrown when querying the sys.health table on a cluster that has blob tables.\n\nFixed support for views with aliased literals in select list / groupBy / orderBy.\n\nFixed a bug affecting the PostgreSQL Wire Protocol’s Simple Query mode and queries which contained strings with escaped single quotes and semicolons, e.g. 'Hello ''Joe'';'.\n\nFixed an issue in the PostgreSQL Wire Protocol’s Simple Query mode which would send an empty statement if the query string contained trailing whitespace."
  },
  {
    "title": "Version 2.1.8 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.1.8.html",
    "html": "5.6\nVersion 2.1.8\n\nReleased on 2017/10/12.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.1.8.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 2.1.0. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 2.1.0, and will require a full restart upgrade.\n\nConsult the upgrade notes for Version 2.1.0 when upgrading.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed an issue that caused INSERT statements using a subquery on the sys.shards system table to fail.\n\nFixed an issue that caused wrong results for JOIN queries on three or more tables when ORDER BY clause contains columns from more than one table.\n\nFixed a bug that caused incorrect results to be returned for JOIN queries when the table stats indicated that the left table of a join is smaller than the right.\n\nFixed passing arguments that contain spaces in the crate shell script.\n\nFixed an issue that caused a table that is not part of the doc schema to be unavailable/hidden if it gets closed using ALTER TABLE.\n\nFixed an issue where the query circuit breaker would be tripped after running several queries due to incorrect memory tracking. Subsequent operations would have failed due to the lack of circuit breaker cleanup."
  },
  {
    "title": "Version 2.1.9 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.1.9.html",
    "html": "5.6\nVersion 2.1.9\n\nReleased on 2017/11/08.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.1.9.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 2.1.0. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 2.1.0, and will require a full restart upgrade.\n\nConsult the upgrade notes for Version 2.1.0 when upgrading.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed an issue that caused extra spaces to be added to the formatted console results in the Admin UI.\n\nFixed issue that caused NULL values to be displayed as empty objects in the console view of the Admin UI.\n\nFixed an issue that prevented the complete graceful shutdown of CrateDB node.\n\nFixed an issue that caused a NullPointerException for queries that use the IN or ANY operators on timestamp fields.\n\nImproves resiliency of COPY FROM and INSERT FROM SUBQUERY statements when lot of new partitions will be created on demand.\n\nFixed a NullPointerException which could occur when joining four tables with a join condition which referred to fields from the leftmost relation.\n\nFixed a problem that caused WITHIN queries to return no or incorrect results.\n\nFixed the problem that stats.breaker.* settings were not applied to the correct circuit breaker.\n\nFixed an issue that caused an error Primary key value must not be NULL to be thrown when trying to insert rows in a table that has a generated column which is used both in PARTITION BY and PRIMARY KEY.\n\nThe PostgreSQL wire protocol service can now be bound to IPv6 addresses as documented.\n\nFixed an issue that resulted in aliases overriding column names when a subselect is used and a column appears in the outer SELECT multiple times, without an alias and with alias or with multiple aliases. E.g.:\n\nSELECT a, a AS newcol FROM (SELECT a FROM t WHERE a > 1)\nSELECT a AS newcol1, a AS newcol2 FROM (SELECT a FROM t WHERE a > 1)\n"
  },
  {
    "title": "Version 2.2.3 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.2.3.html",
    "html": "5.6\nVersion 2.2.3\n\nReleased on 2017/11/13.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.2.3.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 2.2.0. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 2.2.0, and will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nThe download URL, in the notifications section of the Admin UI, now links to the stable CrateDB version.\n\nReplica shards in the UNASSIGNED row of the Shards view of the Admin UI are now grouped by id.\n\nFixed an issue that resulted in closed partitions showing a CRITICAL data state.\n\nVarious bug fixes in crash CrateDB client.\n\nFixed a performance regression for DELETE and UPDATE statements introduced in 2.2.2.\n\nFixed an issue that caused INSERT INTO .. VALUES statements to fail if the VALUES clause contains multiple rows and the table has a generated column as primary key."
  },
  {
    "title": "Version 2.2.4 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.2.4.html",
    "html": "5.6\nVersion 2.2.4\n\nReleased on 2017/11/27.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.2.4.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 2.2.0. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 2.2.0, and will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nHandle MultiPolygon shapes in WITHIN queries correctly instead of throwing an exception.\n\nFixed an exception which occurred when using the _id system column in IN or ANY queries.\n\nThe target table name used in ALTER TABLE ... RENAME TO is now correctly validated.\n\nFixed a regression that caused DELETE statements with a filter on PRIMARY KEY columns that don’t match to fail instead of returning with a row count of 0."
  },
  {
    "title": "Version 2.2.5 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.2.5.html",
    "html": "5.6\nVersion 2.2.5\n\nReleased on 2017/12/14.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.2.5.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 2.2.0. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 2.2.0, and will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed an issue that caused an exception when trying to query the thread_pools['name'] field of the sys.nodes table and the settings['write']['wait_for_active_shards'] field of the information_schema.tables table.\n\nRemoved the horizontal scroll from the console editor in the Admin UI.\n\nReset the pagination of the console results in the Admin UI after each execution.\n\nCalculate “idle” process CPU usage correctly for the “CrateDB CPU Usage” graph in the Admin UI.\n\nFixed an issue that caused the Admin UI to not display any tables when the file system data is not available.\n\nRefresh the cluster information after query execution in the console view of the Admin UI, to ensure that the cluster info is always up-to-date.\n\nFixed an issue that caused incorrect results for collapsible subselects which contained a WHERE clause that resulted in no match. E.g.:\n\nSELECT * FROM (\n  SELECT * FROM t\n  WHERE false\n) vt\n\n\nFixed a performance regression introduced in 2.2.0 that caused SELECT statements with ORDER BY and LIMIT to execute significantly slower than before.\n\nFixed support for subscript expressions on the values column of information_schema.table_partitions."
  },
  {
    "title": "Version 2.2.6 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.2.6.html",
    "html": "5.6\nVersion 2.2.6\n\nReleased on 2018/01/17.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.2.6.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 2.2.0. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 2.2.0, and will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed a race condition that caused DELETE or UPDATE statements to result in an error if a shard was being relocated or in a RECOVERY state.\n\nImproved error message when using the DISTINCT clause on unsupported data types. Also improved documentation to note the data type restriction.\n\nFix log verbosity by only logging the HTTP SSL enabled/disabled message once at startup.\n\nFixed an issue that could cause INSERT INTO statements into partitioned tables to not work correctly. This only occurred if a query instead of the VALUES clause was used.\n\nFixed the evaluation of JavaScript user-defined functions that caused CrateDB to crash because of an unhandled assertion when providing the UDF with EcmaScript 6 arrow function syntax (var f = (x) => x;).\n\nFixed an issue where batch operations executed using the PosgreSQL wire protocol returned 0 as row count, even though the actual row count was different.\n\nFixed a bug which could cause job entries in sys.jobs not being removed when a connection error occurred while sending the results of the job execution to the client.\n\nFixed an issue that caused incorrect results for queries with joins on more than 2 tables with implicit join conditions using the ON clause and where LIMIT is applied. E.g.\n\nSELECT * from t1\nINNER JOIN t2 on t1.id = t2.id\nINNER JOIN t3 on t3.id = t2.id\nLIMIT 100\n\n\nFixed an issue that could cause an ALTER TABLE statement to fail with an exception on partitioned tables created with CrateDB < 1.2.\n\nEnforce validation of column constraints for INSERT statements with a subquery as source and in combination with ON DUPLICATE KEY UPDATE.\n\nFixed an issue that caused an error to be thrown when using GROUP BY or DISTINCT on a column of type IP in case there are rows with null values for that column."
  },
  {
    "title": "Version 2.2.7 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.2.7.html",
    "html": "5.6\nVersion 2.2.7\n\nReleased on 2018/01/25.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.2.7.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be Version 2.2.0. If you want to upgrade from a version prior to this, the upgrade will introduce all of the breaking changes listed for Version 2.2.0, and will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nRemoved unnecessary invocation of non-existent syscall that caused CrateDB to crash on startup with certain Ubuntu kernel versions.\n\nEnsured natural order of configuration keys for the Host Based Authentication.\n\nFixed encoding/decoding of Timestamp type for PostgreSQL wire protocol to support Golang PostgreSQL drivers."
  },
  {
    "title": "Version 2.3.0 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.3.0.html",
    "html": "5.6\nVersion 2.3.0\n\nReleased on 2017/12/22.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.3.0.\n\nYou cannot perform a rolling upgrade to this version. Any upgrade to this version will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nBreaking Changes\n\nChanges\n\nChangelog\nBreaking Changes\n\nCertain metrics in the sys.nodes table have been deprecated and now always return -1, except network metrics with still return 0 as previously when Sigar was not available.\n\nThe affected metrics are: network metrics, read/write filesystem metrics for individual disks, system/user/idle/stolen values for CPU, and system/user values for process CPU.\n\nThis is due to the removal of the Sigar library dependency.\n\nThe default value of the setting auth.host_based.enabled (false) is overwritten with true in the crate.yml that is shipped with the tarball and Linux distributions of CrateDB and also contains a sane default configuration for auth.host_based.config.\n\nThis is done in order to provide a better default installation for new users without breaking existing HBA configurations.\n\nAn implication of these settings is that whenever Enterprise is enabled (default behaviour) connections from remote hosts always require password authentication. Note, that when running CrateDB in Docker, the host of the Docker container is also considered a remote host.\n\nColumns aren’t implicitly cast to a type anymore. Whenever columns are compared to Literals (e.g. ‘string’, 1, 1.2), these literals will be converted to the column type but not vice-versa. The column can still be manually cast to a type by using a cast function.\n\nTable information_schema.table_constraints is now returning constraint_name as type string instead of type array. Constraint type PRIMARY_KEY has been changed to PRIMARY KEY. Also PRIMARY KEY constraint is not returned when not explicitly defined.\n\nScalar functions are resolved more strictly based on the argument types. This means that built-in functions with the same name as user-defined functions will always “hide” the latter, even if the UDF has a different set of arguments. Using the same name as a built-in function for a user defined function is considered bad practice.\n\nChanges\n\nAdded the hyperloglog_distinct aggregation function. This feature is only available in the Enterprise Edition.\n\nAdded a os['cpu']['used'] column to the sys.nodes table. This replaces the deprecated system/user/idle/stolen values.\n\nAdded a cgroup object column to the sys.nodes table. This column contains cgroup information which is relevant when running CrateDB inside a containers.\n\nSubqueries which filter on primary key columns now have the same realtime semantics as the equivalent top-level queries.\n\nAdded support for disabling the column store for STRING columns on table creation and when adding columns. In conjunction with disabling indexing on that columns, this will support storing strings greater than 32kb. Be aware, that the performance of aggregations, groupings and sorting on such columns will decrease.\n\nAdded support for ORDER BY, GROUP BY and global aggregates on columns with disabled indexing (INDEX OFF).\n\nAdded support for scalar subqueries in DELETE and UPDATE statements.\n\nAdded UNION ALL operator to produce the combined result of two or more queries, e.g.:\n\nSELECT id FROM t1\nUNION ALL\nSELECT id FROM t2\n\n\nAdded the “password” authentication method which is available for connections via the PostgreSQL wire protocol and HTTP. This method allows clients to authenticate using a valid database user and its password. For HTTP, the X-User header, used to provide a username, has been deprecated in favour of the standard HTTP Authorization header with the Basic Authentication Scheme.\n\nAdded a WITH clause to CREATE USER statement to specify user properties upon creation. The single property available right now is the password property which can be used for “password” authentication.\n\nThe passwords of existing users can be changed using the ALTER USER statement.\n\nNote that user passwords are never stored in clear-text inside CrateDB!\n\nThe “address” field of the auth.host_based.config setting allows the special _local_ identifier additionally to IP and CIDR notation. _local_ matches both IPv4 and IPv6 connections from localhost.\n\nTable information_schema.key_column_usage is now populated with primary key information of user generated tables.\n\nTable information_schema.table_constraints is now also returning the NOT_NULL constraint.\n\nAdded new cluster setting routing.rebalance.enable that allows to enable or disable shard rebalancing on the cluster.\n\nAdded support to manually control the allocation of shards using ALTER TABLE REROUTE. Supported reroute-options are: MOVE, ALLOCATE REPLICA, and CANCEL.\n\nAdded support to manually retry the allocation of shards that failed to allocate using ALTER CLUSTER REROUTE RETRY FAILED.\n\nAdded new table setting allocation.max_retries that defines the number of attempts to allocate a shard before giving up and leaving it unallocated.\n\nAdded new system table sys.allocations which lists shards and their allocation state including the reasoning why they are in a certain state.\n\nFunction arguments are now linked to each other, where possible. This enables type inference between arguments such that arguments can be converted to match a function’s signature. For example, coalesce(integer, long) would have resulted in an “unknown function” message. We now convert this call into coalesce(long, long). The conversion is possible through a type precedence list and convertibility checks on the data types.\n\nFunctions which accept regular expression flags now throw an error when invalid flags are provided.\n\nClients using the PostgreSQL wire protocol will now receive an additional crate_version ParameterStatus message when establishing a connection. This can be used to identify the server as CrateDB.\n\nAdded the typtype column to pg_catalog.pg_type for better compatibility with certain PostgreSQL client libraries.\n\nAdded the pg_backend_pid() function for enhanced PostgreSQL compatibility.\n\nAdded support for the PSQL ParameterDescription message which allows to get the parameter types in prepared statements up front without specifying the actual arguments first. This fixes compatibility issues with some drivers. This works for the most common use cases except for DDL statements.\n\nUpgraded Elasticsearch to version 5.6.3.\n\nUpdated the CrateDB command line shell (Crash) to version 0.23.0, which added support for password authentication and pasting multiple statements at once.\n\nUpdate the Admin UI to use new CPU metrics for its graphs.\n\nHadoop2 dependencies for the HDFS repository plugin have been upgraded to version 2.8.1."
  },
  {
    "title": "Version 2.3.7 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.3.7.html",
    "html": "5.6\nVersion 2.3.7\n\nReleased on 2018/05/03.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.3.7.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 2.3.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nChanges\n\nFixes\n\nChangelog\nChanges\n\nTable functions no longer require any permissions to be used. (Enterprise only.)\n\nFixes\n\nFixed an issue in binary encoding and decoding of array types in PostgreSQL wire protocol affecting the npgsql driver.\n\nAs Microsoft Windows does not support sending signals, stop attempting to handle the USR2 signal on Windows platforms as it pollutes the log with an unnecessary stack trace.\n\nFixed a performance regression which occurred in queries where a literal was compared against an array column of a different type. For example: SELECT * FROM t1 WHERE 1 = ANY(int_arr_column) The array column would have been converted to long although the literal on the left could have been converted to integer.\n\nFixed an issue where the order of the relations in a FROM clause would expose fields between the relations. An error was thrown during execution of the statements.\n\nFixed incorrect HTTP responses on the BLOB API of subsequent requests after an error response occurred, e.g., 404 Not Found.\n\nFixed an issue that caused ANY to not work correctly on nested arrays.\n\nFixed an issue that would cause a NullPointerException to be thrown when executing an EXPLAIN statement for a CROSS JOIN."
  },
  {
    "title": "Version 2.3.5 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.3.5.html",
    "html": "5.6\nVersion 2.3.5\n\nReleased on 2018/03/20.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.3.5.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 2.3.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed handling of NULL values correctly in the cluster tab of the Admin UI.\n\nUpdated the Size label in the table detail tab of the Admin UI to Size (Sum of primary shards).\n\nFixed an issue where ordering by a TIMESTAMP scalar would cause the query to get stuck.\n\nFixed a potential NullPointerException when running SHOW CREATE TABLE with plain type indices.\n\nFixed an issue that would cause queries ordered by a STRING scalar with possible NULL values to get stuck.\n\nFixed an issue that caused nodes to crash with OutOfMemoryException when executing queries with ORDER BY on large data sets. Instead, the query circuit breaker is tripped, the query is killed and the node’s health is unaffected.\n\nImproved the error handling for a race condition to make sure it doesn’t cause queries to get stuck.\n\nImproved the behaviour in case the cluster drops below the number of minimum nodes. The root REST endpoint will no longer timeout and sys.checks can still be queried, similar to how sys.nodes can still be queried."
  },
  {
    "title": "Version 2.3.8 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.3.8.html",
    "html": "5.6\nVersion 2.3.8\n\nReleased on 2018/05/16.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.3.8.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 2.3.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed an issue that caused a NullPointerException when using ANY on timestamp columns.\n\nFixed an issue that resulted in an unusable partitioned table when at least one column was defined with an explicit COLUMNSTORE setting.\n\nFixed an issue that would cause count(*) to return the wrong count when applied to a subquery.\n\nFixed an issue that could cause JOIN queries with ORDER BY to return incorrect results.\n\nFixed an issue that caused COPY TO with filters on primary key columns to fail."
  },
  {
    "title": "Version 2.3.10 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.3.10.html",
    "html": "5.6\nVersion 2.3.10\n\nReleased on 2018/05/25.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.3.10.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 2.3.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nChangelog\nChanges\n\nImproved the performance of password authentication for connections that are re-used.\n\nFixes\n\nFixed a streaming issue which could result in wrong version numbers of items on replica shards."
  },
  {
    "title": "Version 2.3.9 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.3.9.html",
    "html": "5.6\nVersion 2.3.9\n\nReleased on 2018/05/17.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.3.9.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 2.3.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed a stack overflow exception on upserts which could be caused by retries on version conflicts.\n\nFixed a memory leak caused by using the hyperloglog_distinct function."
  },
  {
    "title": "Version 3.1.3 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.1.3.html",
    "html": "5.6\nVersion 3.1.3\n\nReleased on 2018/11/19.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.1.3.\n\nWe recommend that you upgrade to the latest 3.0 release before moving to 3.1.3.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 3.1.1. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.1 and must be recreated before moving to 3.1.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nUpdated the Admin UI to 1.10.4 which includes the following fixes:\n\nFixed an issue that caused the cluster and node checks to not be refreshed when clicking on the refresh icon.\n\nFixed an issue that caused the Twitter importer to redirect to / instead of /help.\n\nFixed the navigation component to allow opening URLs in new tabs.\n\nFixed a race condition that could allow the creation of partitions with a different schema than other partitions within the same partitioned table.\n\nChanged the user HDFS repository parameter to security.principal, as that is the parameter used by the underlying repository-hdfs plugin.\n\nRemoved the conf_location HDFS repository parameter as it is no longer used.\n\nFixed an issue that caused zombie entries in sys.jobs if the fetchSize functionality of PostgreSQL wire protocol based clients is used.\n\nFixed a memory leak in the MQTT ingest service.\n\nFixed a memory leak that could occur with clients connecting via PostgreSQL protocol and invoking read queries with statements containing expressions that would fail (such as 1 / 0).\n\nFixed an issue in the PostgreSQL wire protocol that could result in nodes crashing with OutOfMemoryError if clients queried very large tables without specifying the fetch size.\n\nFixed an issue that caused missing privilege errors if table aliases were used."
  },
  {
    "title": "Version 3.1.2 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.1.2.html",
    "html": "5.6\nVersion 3.1.2\n\nReleased on 2018/10/18.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.1.2.\n\nWe recommend that you upgrade to the latest 3.0 release before moving to 3.1.2.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 3.1.1. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.1 and must be recreated before moving to 3.1.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nStore current created versions at new table partitions instead of using potentially older versions stored at the partition table.\n\nFixed an issue which caused tables created on versions < 3.0 using no longer supported table parameters to fail on ALTER TABLE statements.\n\nCORS pre-flight requests now no longer require authentication.\n\nFixed an issue which caused joins over multiple relations and implicit join conditions inside the WHERE clause to fail.\n\nThe Access-Control-Allow-Origin header is now correctly served by resources in the /_blobs endpoint if the relevant settings are enabled.\n\nFixed decoding of PostgreSQL specific array literal constants where unquoted elements and single element arrays were not decoded correctly and resulted in an empty array."
  },
  {
    "title": "Version 3.1.4 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.1.4.html",
    "html": "5.6\nVersion 3.1.4\n\nReleased on 2018/12/19.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.1.4.\n\nWe recommend that you upgrade to the latest 3.0 release before moving to 3.1.4.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 3.1.1. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.1 and must be recreated before moving to 3.1.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\narray_unique and array_difference now work for nested arrays and arrays with objects which contain arrays.\n\nFixed an issue that could cause some types of statements to remain listed within sys.jobs if their execution stopped with a failure.\n\nFixed an issue which caused EXPLAIN statements to use a wrong routing entries representation on versions >= 3.1.0."
  },
  {
    "title": "Version 3.1.6 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.1.6.html",
    "html": "5.6\nVersion 3.1.6\n\nReleased on 2019/02/05.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.1.6.\n\nWe recommend that you upgrade to the latest 3.0 release before moving to 3.1.6.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 3.1.1. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.1 and must be recreated before moving to 3.1.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed a performance regression on UPDATE and DELETE operations.\n\nFixed a performance regression when inserting data using unnest().\n\nFixed an issue where an ordered query with a specified limit that was much larger than the available rows would result in OutOfMemoryError even though the number of available rows could fit in memory.\n\nFixed a NullPointerException that occurs on OUTER joins which can be rewritten to INNER joins and uses a function as a select item."
  },
  {
    "title": "Version 3.1.5 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.1.5.html",
    "html": "5.6\nVersion 3.1.5\n\nReleased on 2019/01/14.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.1.5.\n\nWe recommend that you upgrade to the latest 3.0 release before moving to 3.1.5.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 3.1.1. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.1 and must be recreated before moving to 3.1.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed an issue that caused incorrectness in the navigation of the Admin UI.\n\nFixed a NullPointerException that could occur using array_difference.\n\nFixed a race condition that could lead to stuck queries, for example if a node was stopped or crashed.\n\nCasts to nested arrays are now properly supported.\n\nThe type of parameter placeholders in sub-queries in the FROM clause of a query can now be resolved to support PostgreSQL clients relying on the ParameterDescription message. This enables queries like select * from (select $1::int + $2) t\n\nFixed error readability of certain ALTER TABLE operations.\n\nFixed SQL parser to not allow repeated PARTITION BY or CLUSTERED BY | INTO tokens on CREATE TABLE statements.\n\nSupport ECS Task IAM profile credentials on AWS S3 repositories."
  },
  {
    "title": "Version 2.3.11 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/2.3.11.html",
    "html": "5.6\nVersion 2.3.11\n\nReleased on 2018/05/28.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB Version 1.1.3 or higher before you upgrade to 2.3.11.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 2.3.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nChangelog\nFixes\n\nFixed a bug which could produce invalid row version numbers under high load scenarios. These version numbers would prevent updating the affected rows."
  },
  {
    "title": "Version 3.2.0 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.2.0.html",
    "html": "5.6\nVersion 3.2.0\n\nReleased on 2018/12/19.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.2.0.\n\nWe recommend that you upgrade to the latest 3.1 release before moving to 3.2.0.\n\nYou cannot perform a rolling upgrade to this version. Any upgrade to this version will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nPlease consult the Upgrade Notes before upgrading.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.2 and must be recreated before moving to 3.2.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nUpgrade Notes\n\nLogging Configuration Changes\n\nDeprecated Settings and Features\n\nChangelog\n\nBreaking Changes\n\nChanges\n\nNew Features\n\nSQL Improvements\n\nPostgreSQL Compatibility\n\nDatabase Administration\n\nAdmin UI Upgrade to 1.11.3\n\nDeprecations\n\nOther\n\nUpgrade Notes\nLogging Configuration Changes\n\nThe default logging configuration for CrateDB in log4j2.properties has been changed. If you have customised this logging configuration, you should replace the $marker entry with [%node_name] %marker.\n\nDeprecated Settings and Features\n\nIn previous versions, the CrateDB license was set via the license.ident configuration setting. This has now been deprecated, and going forward the CrateDB license should be set using the SET LICENSE statement.\n\nThe INGEST framework has been deprecated, and will be removed in the future.\n\nThe http.enabled setting has been deprecated, and will be removed in the future.\n\nThe delimited_payload_filter built-in token filter for fulltext analyzers has been renamed to delimited_payload. The delimited_payload_filter name can still be used, but is deprecated, and will be removed in the future.\n\nChangelog\nBreaking Changes\n\nThe * of SELECT * statements in the query clause of view definitions is no longer expanded at view creation time but lazy whenever a view is evaluated. That means columns added to a table after the views initial creation will show up in queries on the view. It is generally recommended to avoid using * in views but always specify columns explicitly.\n\nChanges\nNew Features\n\nAdded support for executing existing aggregation functions as window functions using the OVER clause.\n\nAdded support for CrateDB license management enabling users to trial the enterprise-features, set a production enterprise license, or continue using the community edition. Additionally, a new SET LICENSE statement has been added for license registration, and the license.ident setting has become @deprecated.\n\nSQL Improvements\n\nAdded the REPLACE scalar function replacing a substring in a string with another string.\n\nAdded the GENERATE_SERIES(start, stop [, step ]) table function which can generate a series of numbers.\n\nImplemented the ARRAY_UPPER, ARRAY_LENGTH and ARRAY_LOWER scalar functions that return the upper and respectively lower bound of a given array dimension.\n\nAdded support for the ARRAY(subquery) expression, which can turn the result from a subquery into an array.\n\nThe = ANY operator now also supports operations on object arrays or nested arrays. This enables queries like WHERE ['foo', 'bar'] = ANY(object_array(string_array)).\n\nAdded support for SHOW parameter_name | ALL to retrieve one or all session setting value(s).\n\nAdded support for INITCAP(string) which capitalizes the first letter of every word while turning all others into lowercase.\n\nAdded the scalar expression CURRENT_DATABASE which returns the current database.\n\nFunctions like CURRENT_SCHEMA and CURRENT_USER, which depend on the active session can now be used as generated columns.\n\nAdded support for using table functions in the SELECT list of a query.\n\ngeo_shape columns can now be casted to object with cast in addition to try_cast.\n\nImproved the handling of function expressions inside subscripts used on object columns. This allows expressions like obj['x' || 'x'] to be used.\n\n<object_column> = <object_literal> comparisons now try to utilize the index for the objects contents and can therefore run much faster.\n\nValues of byte-size and time based configuration setting do not require a unit suffix anymore. Without a unit time values are treat as milliseconds since epoch and byte size values are treat as bytes.\n\nAdded support of using units inside byte-size or time bases statement parameters values. E.g. 1mb for one megabyte or 1s for one Second.\n\nPostgreSQL Compatibility\n\nAdded the pg_catalog.pg_database table.\n\nAdded pg_class, pg_namespace, pg_attribute, pg_attrdef, pg_index and pg_constraint tables to the pg_catalog schema for improved compatibility with PostgreSQL.\n\nImproved the compatibility with PostgreSQL clients that use the text type for parameter encoding.\n\nChanged PostgreSQL wire interface to emulate version 10.5.\n\nAdded some type aliases for improved compatibility with PostgreSQL.\n\nExpand the search_path setting to accept a list of schemas that will be searched when a relation (table, view or user-defined function) is referenced without specifying a schema. The system pg_catalog schema is implicitly included as the first one in the path.\n\nDatabase Administration\n\nAdded support for changing the number of shards on an existing table or partition using the ALTER TABLE SET statement.\n\nImproved resiliency of the ALTER TABLE: RENAME TO operation by making it an atomic operation.\n\nAdded an ALTER CLUSTER SWAP TABLE statement that can be used to switch the names of two tables.\n\nAdded a ALTER CLUSTER GC DANGLING ARTIFACTS statement that can be used to clean up internal structures that weren’t properly cleaned up due to cluster failures during operations which create such temporary artifacts.\n\nAdded support for per-table shard allocation filtering.\n\nAdmin UI Upgrade to 1.11.3\n\nChanged the license information (ident) to be taken from the sys.cluster.licence attribute instead of the license.ident setting, which is @deprecated.\n\nAddition of French language files and menu options.\n\nFixed an issue that caused incorrectness in the navigation of the Admin UI.\n\nUpdated the license container to be responsive in the Admin UI.\n\nVarious other improvements.\n\nDeprecations\n\nThe MQTT endpoint has been deprecated and will be removed in a future version.\n\nDeprecated the http.enabled setting which will be always on in future.\n\nOther\n\nUpgraded to Elasticsearch 6.5.1, which includes changes to the default logging configuration.\n\nAdded a remove_duplicates token filter.\n\nAdded a char_group tokenizer.\n\nRenamed the delimited_payload_filter token filter to delimited_payload. The old name can still be used, but is deprecated.\n\nFor further information on CrateDB 3.2.0 see our announcement blog post."
  },
  {
    "title": "Version 3.2.2 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.2.2.html",
    "html": "5.6\nVersion 3.2.2\n\nReleased on 2019/01/22.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.2.2.\n\nWe recommend that you upgrade to the latest 3.1 release before moving to 3.2.2.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 3.2.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nPlease consult the Upgrade Notes before upgrading.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.2 and must be recreated before moving to 3.2.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed a NullPointerException that could occur on queries on the information_schema.tables table.\n\nFixed a NullPointerException that occurs on OUTER joins which can be rewritten to INNER joins and uses a function as a select item."
  },
  {
    "title": "Version 3.2.1 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.2.1.html",
    "html": "5.6\nVersion 3.2.1\n\nReleased on 2019/01/14.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.2.1.\n\nWe recommend that you upgrade to the latest 3.1 release before moving to 3.2.1.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 3.2.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nPlease consult the Upgrade Notes before upgrading.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.2 and must be recreated before moving to 3.2.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed a NullPointerException that could occur using array_difference.\n\nFixed a race condition that could lead to stuck queries, for example if a node was stopped or crashed.\n\nFixed an issue that prevented queries on table functions from working if the cluster contains an expired license.\n\nCasts to nested arrays are now properly supported.\n\nThe type of parameter placeholders in sub-queries in the FROM clause of a query can now be resolved to support PostgreSQL clients relying on the ParameterDescription message. This enables queries like select * from (select $1::int + $2) t\n\nFixed error readability of certain ALTER TABLE operations.\n\nFixed SQL parser to not allow repeated PARTITION BY or CLUSTERED BY | INTO tokens on CREATE TABLE statements."
  },
  {
    "title": "Version 3.2.3 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.2.3.html",
    "html": "5.6\nVersion 3.2.3\n\nReleased on 2019/02/07.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.2.3.\n\nWe recommend that you upgrade to the latest 3.1 release before moving to 3.2.3.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 3.2.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nPlease consult the Upgrade Notes before upgrading.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.2 and must be recreated before moving to 3.2.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed a null pointer error that occurred if null arguments were supplied to unnest.\n\nFixed performance regression on UPDATE and DELETE operations.\n\nFixed performance regression when inserting data using unnest().\n\nFixed an issue where an ordered query with a specified limit that was much larger than the available rows would result in OutOfMemoryError even though the number of available rows could fit in memory."
  },
  {
    "title": "Version 3.0.0 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.0.0.html",
    "html": "5.6\nVersion 3.0.0\n\nReleased on 2018/05/16.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.0.0.\n\nWe recommend that you upgrade to the latest 2.3 release before moving to 3.0.0.\n\nYou cannot perform a rolling upgrade to this version. Any upgrade to this version will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nPlease consult the Upgrade Notes before upgrading.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.0 and must be recreated before moving to 3.0.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nBreaking Changes\n\nChanges\n\nUpgrade Notes\n\nConfiguration Changes\n\nRemoved Settings\n\nRenamed Settings\n\nAltered Settings\n\nOther Changes\n\nChangelog\nBreaking Changes\n\nDropped support for tables that have been created with CrateDB prior to version 2.0. Tables which require upgrading are indicated in the cluster checks, including visually shown in the Admin UI, if running the latest 2.2 or 2.3 release. The upgrade of tables needs to happen before updating CrateDB to this version. This can be done by exporting the data with COPY TO and importing it into a new table with COPY FROM. Alternatively you can use INSERT with query.\n\nData paths as defined in path.data must not contain the cluster name as a folder. Data paths which are not compatible with this version are indicated in the node checks, including visually shown in the Admin UI, if running the latest 2.2 or 2.3 release.\n\nThe region setting for CREATE REPOSITORY has been removed. It is automatically inferred but can still be manually specified by using the endpoint setting.\n\nStore level throttling settings indices.store.throttle.* have been removed.\n\nThe gateway recovery table setting recovery.initial_shards has been removed. Nodes will recover their unassigned local primary shards immediately after restart.\n\nThe discovery setting discovery.type has been removed. To enable EC2 discovery, the discovery.zen.hosts_provider setting must be set to ec2.\n\nDropped support for reading AWS credentials used for S3 and EC2 discovery from environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY as well as Java system properties aws.accessKeyId and aws.secretKey.\n\nEC2 cloud.aws.* settings have been renamed to discovery.ec2.*.\n\nThe setting that controls system call filters bootstrap.seccomp has been has been renamed to bootstrap.system_call_filter.\n\nThe columns number_of_shards, number_of_replicas, and self_referencing_column_name in information_schema.tables changed to return NULL for non-sharded tables.\n\nAdapted queries in the Admin UI to be compatible with CrateDB 3.0 and greater.\n\nFor HTTP authentication, support was dropped for the X-User header, used to provide a username, which has been deprecated in 2.3.0. in favour of the standard HTTP Authorization header.\n\nThe error_trace GET parameter of the HTTP endpoint only allows true and false in lower case. Other values are not allowed any more and will result in a parsing exception.\n\nThe _node column on sys.shards and sys.operations has been renamed to node, is now visible by default and has been trimmed to only include node['id'] and node['name']. In order to get all information a join query can be used with sys.nodes.\n\nChanges\n\nCrateDB is now based on Elasticsearch 6.1.4 and Lucene 7.1.0.\n\nMultiple Admin UI improvements.\n\nAdded a new tab for views in the Admin UI which lists available views and their properties.\n\nUpdated the bundled CrateDB Shell (crash) to version 0.24.0 which adds support for default schema for connections.\n\nAdded support in the PostgreSQL Wire Protocol’s SimpleQuery mode to process a query string which contains multiple queries delimited by semicolons.\n\nAdded support for DEALLOCATE statement which is used by certain PostgreSQL Wire Protocol clients (e.g. libpq) to deallocate a prepared statement and release its resources.\n\nAdded support for ordering on analysed columns and partition columns.\n\nAdded support for views which can be created using the new CREATE VIEW statement and dropped using the DROP VIEW statement. Views are listed in information_schema.views and they show up in information_schema.tables as well as information_schema.columns.\n\nEnterprise: Added the VIEW privilege class which can be used to grant/deny access to views.\n\nAdded support for INSERT INTO ... ON CONFLICT DO NOTHING. The statement ignores insert values which would cause duplicate keys.\n\nAdded support for ON CONFLICT clause in insert statements. INSERT INTO ...  ON CONFLICT (pk_col) DO UPDATE SET col = val is identical to INSERT INTO ... ON DUPLICATE KEY UPDATE col = val. The special EXCLUDED table can be used to refer to the insert values: INSERT INTO ... ON CONFLICT (pk_col) DO UPDATE SET col = EXCLUDED.col\n\nDEPRECATED: The ON DUPLICATE KEY UPDATE clause has been deprecated in favor of the ON CONFLICT DO UPDATE SET clause.\n\nImplemented the Block Hash Join algorithm which is now used for Equi-Joins.\n\nAdded new sys.health system information table to expose the health of all tables and table partitions.\n\nAdded new cluster.routing.allocation.disk.watermark.flood_stage setting, that controls at which disk usage indices should become read-only to prevent running out of disk space. There is also a new node check that indicates whether the threshold is exceeded.\n\nAdded a new bengali language analyzer and a bengali_normalization token filter.\n\nAdd max_token_length parameter to whitespace tokenizer.\n\nAdded new tokenizers simple_pattern and simple_pattern_split which allow to tokenize text for the fulltext index by a regular expression pattern.\n\nAdded support for CSV file inputs in COPY FROM statements. Input type is inferred using the file’s extension or can be set using the optional WITH clause and specifying the format.\n\nFully qualified column names including a schema name will no longer match on table aliases.\n\nThe default user if enterprise is disabled changed from null to crate. This causes entries in sys.jobs to show up with crate as username. Functions like user will also return crate if enterprise is enabled but the user module is not available.\n\nDisplay the node information (name and id) of jobs in the sys.jobs table.\n\nChanged the primary key constraints of the information schema tables table_constraints, referential_constraints, table_partitions, key_column_usage, columns, and tables to be SQL compliant.\n\nArrays can now contain mixed types if they’re safely convertible. JSON libraries tend to encode values like [0.0, 1.2] as [0, 1.2], this caused an error because of the strict type match we enforced before.\n\nImplemented constraint_schema and table_schema in information_schema.key_column_usage correctly and documented the full table schema.\n\nStatistics for jobs and operations are enabled by default. If you don’t need any statistics, please set stats.enabled to false.\n\nChanged BEGIN and SET SESSION to no longer require DQL permissions on the CLUSTER level.\n\nAdded epoch argument to the EXTRACT function which returns the number of seconds since Jan 1, 1970. For example: extract(epoch from '1970-01-01T00:00:01') returns 1.0 seconds.\n\nEnable logging of JVM garbage collection times that help to debug memory pressure and garbage collection issues. GC log files are stored separately to the standard CrateDB logs and the files are log-rotated.\n\nCrateDB will now by default create a heap dump in case of a crash caused by an out of memory error. This makes it necessary to account for the additional disk space requirements.\n\nImplemented a Ready node status JMX metric expressing if the node is ready for processing SQL statements.\n\nImplemented a NodeInfo JMX MBean to expose useful information (id, name) about the node.\n\nFixed path of log file name in rotation pattern in log4j2.properties. It now writes into the correct logging directory instead of the parent directory.\n\nALTER TABLE <name> OPEN will now wait for all shards to become active before returning to be consistent with the behaviour of other statements.\n\nAdded note about the newly available JMX HTTP Exporter to the monitoring documentation section.\n\nThe first argument (field) of the EXTRACT function has been limited to string literals and identifiers, as it was documented.\n\nUpgrade Notes\nConfiguration Changes\n\nThere are a few configuration changes that you should be aware of before restarting the nodes.\n\nRemoved Settings\n\nAll store level throttle settings (under indices.store.throttle.*) have been removed, and should be removed from your node configuration.\n\nSimilarly, the recovery.initial_shards configuration option has been removed, and should also be removed from your configuration.\n\nRenamed Settings\n\nThe discovery.type setting which was previously used to specify whether a cluster should use DNS discovery or the EC2 API, has been removed. Configuring the use of the EC2 API has now been moved to the discovery.zen.hosts_provider setting.\n\nThe bootstrap.seccomp setting, which controls system call filters, has been renamed to bootstrap.system_call_filter.\n\nAltered Settings\n\nThe path.data setting specifies the path or paths where the CrateDB node should store its table data and cluster metadata.\n\nIn CrateDB 3.0.0 and later, this path must not contain the cluster name as a directory. For example, if you have set cluster.name: abcdef, the setting path.data: /mnt/abcdef/data would be incompatible. Moving or renaming the directory, such as to /mnt/data, and altering your path.data setting accordingly will allow you to continue using the node’s data.\n\nData paths that are incompatible with 3.0.0 will be indicated visually in the Admin UI if you are running the latest 2.2.x or 2.3.x release.\n\nOther Changes\n\nThe CREATE REPOSITORY statement for creating backup repositories has been changed.\n\nPreviously, when using Amazon S3 for backup storage, bucket regions had to be configured explicitly. Bucket regions are now inferred automatically.\n\nIf you want to override this, you can use the endpoint parameter.\n\nPreviously, the X-User HTTP header could be used to provide a username. This head is now deprecated in favour of the standard HTTP Authorization header.\n\nThe _node column in the sys.shards and sys.operations tables has been renamed to node.\n\nAdditionally, node object now only includes id and name of the node, i.e. node['id'] and node['name'].\n\nTo get the full node information, use node['id'] to join the sys.nodes table."
  },
  {
    "title": "Version 3.0.2 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.0.2.html",
    "html": "5.6\nVersion 3.0.2\n\nReleased on 2018/06/12.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.0.2.\n\nWe recommend that you upgrade to the latest 2.3 release before moving to 3.0.2.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 3.0.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nPlease consult the Upgrade Notes before upgrading.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.0 and must be recreated before moving to 3.0.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed an issue that caused an UnsupportedFeatureException to be thrown for queries with a WHERE clause which contains an equality comparison that references a table column in both sides. E.g.:\n\nSELECT * FROM t\nWHERE t.i = abs(t.i)\n\n\nFixed an issue that could cause a cluster check warning, telling the user to upgrade tables which do not require upgrades.\n\nFixed an issue that could cause HTTP requests using empty bulk_args to fail.\n\nFixed a NullPointerException which could be thrown when deleting a table and querying the size column from the sys.shards table."
  },
  {
    "title": "Version 3.0.7 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.0.7.html",
    "html": "5.6\nVersion 3.0.7\n\nReleased on 2018/09/25.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.0.7.\n\nWe recommend that you upgrade to the latest 2.3 release before moving to 3.0.7.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 3.0.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nPlease consult the Upgrade Notes before upgrading.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.0 and must be recreated before moving to 3.0.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nBreaking Changes\n\nFixes\n\nChangelog\nBreaking Changes\n\nPreviously, Version 3.0.0 advertised a breaking change for the removal of the region setting when using the CREATE REPOSITORY query for S3 based snapshot repositories. However, due to a bug, using the region setting was valid syntax in versions 3.0.0 through to 3.0.6.\n\nThis bug has been fixed in 3.0.7. Due to this, CREATE REPOSITORY queries that were accepted (erroneously) in 3.0.6 may no longer work in 3.0.7.\n\nFixes\n\nCalling an unknown user-defined function now results in an appropriate error message instead of a NullPointerException.\n\nTrying to create a table with a generated column inside an object column now results in a friendly error message instead of a NullPointerException.\n\nFixed processing of the endpoint, protocol and max_retries S3 repository parameters."
  },
  {
    "title": "Version 3.0.3 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.0.3.html",
    "html": "5.6\nVersion 3.0.3\n\nReleased on 2018/06/29.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.0.3.\n\nWe recommend that you upgrade to the latest 2.3 release before moving to 3.0.3.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 3.0.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nPlease consult the Upgrade Notes before upgrading.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.0 and must be recreated before moving to 3.0.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nChanges\n\nFixes\n\nChangelog\nChanges\n\nAdded settings s3.client.default.access_key and s3.client.default.secret_key which can be used to set default credentials for S3 repositories, if they are not passed as parameters to the CREATE REPOSITORY SQL statement.\n\nImplemented a thread-utilization down-scaling logic which dynamically adapts the number of threads used for SELECT queries to avoid running into RejectedExcecution errors if there are many shards per node involved in the queries.\n\nImproved execution of DELETE and UPDATE queries, which should generally result in an increased performance for queries which only match a subset of the rows and avoid CircuitBreakingException errors. But it might result in a slight performance decrease if queries match all or almost all records.\n\nFixes\n\nFixed an issue where the Admin UI was not loaded when it was served from another location than / resulting in a blank browser canvas.\n\nMade table setting blocks.read_only_allow_delete configurable for partitioned tables.\n\nImproved performance for expressions involving literal type conversions, e.g. select count(*) from users group by name having max(bytes) = 4.\n\nFixed an issue which could result in lost entries at the sys.jobs_log and sys.operations_log tables when the related settings are changed while entries are written.\n\nFixed a dependency issue with the bundled crash that caused the application to be unable to connect to the server.\n\nFixed an issue that could prevent PostgreSQL clients from receiving an error and therefore getting stuck.\n\nFixed an issue that would cause a CAST from TIMESTAMP to LONG to be ignored.\n\nHandle STRING_ARRAY as argument type for user-defined functions correctly to prevent an ArrayStoreException."
  },
  {
    "title": "Version 3.2.5 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.2.5.html",
    "html": "5.6\nVersion 3.2.5\n\nReleased on 2019/03/11.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.2.5.\n\nWe recommend that you upgrade to the latest 3.1 release before moving to 3.2.5.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 3.2.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nPlease consult the Upgrade Notes before upgrading.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.2 and must be recreated before moving to 3.2.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed an issue that could cause queries on the sys.snapshots table to raise an error if a snapshot couldn’t be retrieved.\n\nFixed an issue that caused inserts into partitioned tables to fail with an unknown setting error if the table was created in an earlier version of CrateDB using settings that have been removed in later versions.\n\nFixed an issue that could result in a ConcurrentModificationException error when querying the sys.jobs_metrics table."
  },
  {
    "title": "Version 3.2.4 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.2.4.html",
    "html": "5.6\nVersion 3.2.4\n\nReleased on 2019/02/25.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.2.4.\n\nWe recommend that you upgrade to the latest 3.1 release before moving to 3.2.4.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 3.2.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nPlease consult the Upgrade Notes before upgrading.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.2 and must be recreated before moving to 3.2.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed an issue that would cause the results of a nested loop join statement ordered by fields from a single relation, in the form of SELECT t1.x, t2.x FROM t2 INNER JOIN t1 ON t1.x = t2.x ORDER BY t2.y, to be out of order.\n\nFixed an issue that caused the Admin UI monitoring graphs to be cut off.\n\nFixed an issue that caused a stream has already been operated upon or closed exception to be thrown when joining on a right subquery that contained a GROUP BY clause on one number column.\n\nFixed an issue that caused INSERT INTO with a subquery to not insert into partitioned tables where the partition columns had a NOT NULL constraint.\n\nFixed a regression that caused inserts which create new dynamic columns to fail if the table was created in an earlier version of CrateDB.\n\nFixed an issue that caused inserts into partitioned tables where the partition column is generated and based on the child of an object to fail.\n\nFixed an issue that caused the Basic Authentication prompt to fail in Safari."
  },
  {
    "title": "Version 3.2.7 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.2.7.html",
    "html": "5.6\nVersion 3.2.7\n\nReleased on 2019/04/09.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.2.7.\n\nWe recommend that you upgrade to the latest 3.1 release before moving to 3.2.7.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 3.2.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nPlease consult the Upgrade Notes before upgrading.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.2 and must be recreated before moving to 3.2.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed an issue that could cause window functions to compute incorrect results if multiple window functions with different window definitions were used.\n\nFixed an issue that could cause a query with window functions and limit to return too few rows or have the window functions compute wrong results.\n\nFixed an issue that would cause a ClassCastException for queries ordered by a window function.\n\nFix quoting of identifiers that contain leading digits or spaces when printing relation or column names."
  },
  {
    "title": "Version 3.2.6 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.2.6.html",
    "html": "5.6\nVersion 3.2.6\n\nReleased on 2019/03/25.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.2.6.\n\nWe recommend that you upgrade to the latest 3.1 release before moving to 3.2.6.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 3.2.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nPlease consult the Upgrade Notes before upgrading.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.2 and must be recreated before moving to 3.2.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed an issue which caused an IndexOutOfBoundsException when a window function with an ordered window was selected in a JOIN statement.\n\nFixed an issue which caused a NullPointerException when executing a window function over an ordered window which contained null values under the ordered column.\n\nFixed an issue which causes subselect queries with certain ORDER BY constructs to fail.\n\nFixed circuit breaker memory accounting of window functions to prevent Out Of Memory exceptions."
  },
  {
    "title": "Version 3.3.0 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.3.0.html",
    "html": "5.6\nVersion 3.3.0\n\nReleased on 2019/03/27.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.3.0.\n\nWe recommend that you upgrade to the latest 3.2 release before moving to 3.3.0.\n\nYou cannot perform a rolling upgrade to this version. Any upgrade to this version will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nPlease consult the Upgrade Notes before upgrading.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.3 and must be recreated before moving to 3.3.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nUpgrade Notes\n\nDeprecated Settings and Features\n\nChangelog\n\nChanges\n\nNew Features\n\nSQL Improvements\n\nPostgreSQL Compatibility\n\nDatabase Administration\n\nDeprecations\n\nOther\n\nUpgrade Notes\nDeprecated Settings and Features\n\nThe query frequency and average duration QueryStats MBean metrics has now been deprecated, in favour of the new total count and sum of durations metrics.\n\nThe cluster.graceful_stop.reallocate setting has been marked as deprecated. This setting was already being ignored, and setting the value to false had no effect.\n\nThe node decommission using the USR2 signals has now been deprecated in favour of the ALTER CLUSTER DECOMMISSION statement.\n\nThe CREATE INGEST and DROP INGEST rules have been marked as deprecated. Given that the only implementation (MQTT) was deprecated and will be removed, the framework itself will also be removed.\n\nChangelog\nChanges\nNew Features\n\nExposed the sum of durations, total, and failed count metrics under the QueryStats MBean for QUERY, INSERT, UPDATE, DELETE, MANAGEMENT, DDL, and COPY statement types.\n\nExposed the sum of statement durations, total, and failed count classified by statement type under the sum_of_durations, total_count and failed_count columns, respectively, in the Jobs metrics table.\n\nSQL Improvements\n\nAdded current_schemas(boolean) scalar function which will return the names of schemas in the search_path.\n\nAdded support for the first_value, last_value, and nth_value window functions as enterprise features.\n\nAdded the DROP ANALYZER statement to support removal of custom analyzer definitions from the cluster.\n\nOutput the custom analyzer, tokenizer, token_filter, and char_filter definition inside the information_schema.routines.routine_definition column.\n\nAdded support for the row_number() window function.\n\nAdded support for using any expression in the operand of a CASE clause.\n\nFix quoting of identifiers that contain leading digits or spaces when printing relation or column names.\n\nPostgreSQL Compatibility\n\nAdded pg_type columns: typlen, typarray, typnotnull, and typnamespace for improved PostgreSQL compatibility.\n\nAdded a pg_description table to the pg_catalog schema for improved PostgreSQL compatibility.\n\nFixed function resolution for PostgreSQL functions pg_backend_pid, pg_get_expr, and current_database when the schema prefix pg_catalog is included.\n\nDatabase Administration\n\nAdded a node check for the JVM version number.\n\nAdded ALTER CLUSTER DECOMMISSION <nodeId | nodeName> statement that triggers the existing node decommission functionality.\n\nChanged the trial license introduced in 3.2 to no longer have an expiration date, but instead be limited to three nodes.\n\nThe Usage Data Collector now includes information about the available number of processors.\n\nDeprecations\n\nThe query frequency and average duration QueryStats MBean metrics has been deprecated in favour of the new total count and sum of durations metrics.\n\nMarked the cluster.graceful_stop.reallocate setting as deprecated. This setting was already being ignored, setting the value to false has no effect.\n\nThe node decommission using the USR2 signal has been deprecated in favour of the ALTER CLUSTER DECOMMISSION statement.\n\nMarked CREATE INGEST and DROP INGEST as deprecated. Given that the only implementation (MQTT) was deprecated and will be removed, the framework itself will also be removed.\n\nOther\n\nBuffer the file output of COPY TO operations to improve performance by not writing to disk on every row."
  },
  {
    "title": "Version 3.2.8 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.2.8.html",
    "html": "5.6\nVersion 3.2.8\n\nReleased on 2019/04/16.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.2.8.\n\nWe recommend that you upgrade to the latest 3.1 release before moving to 3.2.8.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 3.2.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nPlease consult the Upgrade Notes before upgrading.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.2 and must be recreated before moving to 3.2.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed the processing of LIMIT clauses within queries in the INSERT INTO statement. A query like INSERT INTO target (SELECT * FROM (SELECT * FROM source LIMIT 10) t) could insert more than 10 rows if there is more than 1 node in the cluster. In addition, using LIMIT in the top level query of the INSERT INTO statement is now no longer prohibited. So the query can be written as follows: INSERT INTO target (SELECT * FROM source LIMIT 10).\n\nFixed an issue that would cause the wrong evaluation of nested subqueries in cases where the inner subquery returns a multi-value and the outer returns a single value result. For instance, the assignment subquery expression in the following update statement UPDATE t1 SET x = (SELECT COUNT(*) FROM t2 WHERE x IN (SELECT x FROM t3))\") might have produced an incorrect result."
  },
  {
    "title": "Version 3.0.6 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.0.6.html",
    "html": "5.6\nVersion 3.0.6\n\nReleased on 2018/08/28.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.0.6.\n\nWe recommend that you upgrade to the latest 2.3 release before moving to 3.0.6.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 3.0.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nPlease consult the Upgrade Notes before upgrading.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.0 and must be recreated before moving to 3.0.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed an issue where, when checking the privileges of an aliased relation, the default schema of doc would always be used, rather than the default schema of the current session.\n\nFixed correct processing of the operator option of the MATCH predicate."
  },
  {
    "title": "Version 3.0.5 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.0.5.html",
    "html": "5.6\nVersion 3.0.5\n\nReleased on 2018/07/30.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.0.5.\n\nWe recommend that you upgrade to the latest 2.3 release before moving to 3.0.5.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 3.0.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nPlease consult the Upgrade Notes before upgrading.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.0 and must be recreated before moving to 3.0.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed an issue which was introduced with 3.0.4 and could result in IllegalStateException being thrown during the startup of a CrateDB node, which prevents its successful bootstrap and one cannot recover from this state. 3.0.4 is a testing release and is not available in the stable channels."
  },
  {
    "title": "Version 3.0.4 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.0.4.html",
    "html": "5.6\nVersion 3.0.4\n\nReleased on 2018/07/23.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.0.4.\n\nWe recommend that you upgrade to the latest 2.3 release before moving to 3.0.4.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 3.0.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nPlease consult the Upgrade Notes before upgrading.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.0 and must be recreated before moving to 3.0.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nWarning\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed an issue which prevented adding new string typed columns into dynamic objects if a cluster was initially created with a version between 1.1.0 and 2.0.0.\n\nFixed an issue that caused runtime changes to the indices.breaker.query.limit and indices.breaker.query.overhead settings to be ignored when using the SET GLOBAL [TRANSIENT] command.\n\nStored the correct name (timestamptz) for the timestamp type in the pg_type table.\n\nFixed an issue that caused an UnsupportedFeatureException to be thrown when deleting or updating by query on an empty partitioned table, instead of just returning 0 rows deleted/updated."
  },
  {
    "title": "Version 3.1.1 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.1.1.html",
    "html": "5.6\nVersion 3.1.1\n\nReleased on 2018/09/25.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.1.1.\n\nWe recommend that you upgrade to the latest 3.0 release before moving to 3.1.1.\n\nYou cannot perform a rolling upgrade to this version. Any upgrade to this version will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.1 and must be recreated before moving to 3.1.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nChanges\n\nFixes\n\nChangelog\nChanges\n\nAdded support for using generated columns inside object columns.\n\nFixes\n\nCalling an unknown user-defined function now results in an appropriate error message instead of a NullPointerException.\n\nFixed processing of the endpoint, protocol and max_retries S3 repository parameters."
  },
  {
    "title": "Version 3.3.1 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.3.1.html",
    "html": "5.6\nVersion 3.3.1\n\nReleased on 2019/04/09.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.3.1.\n\nWe recommend that you upgrade to the latest 3.2 release before moving to 3.3.1.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 3.3.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nPlease consult the Upgrade Notes before upgrading.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.3 and must be recreated before moving to 3.3.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed a ClassCastException which could occur when selecting pg_catalog.pg_type.typlen.\n\nFixed an issue that could cause window functions to compute incorrect results if multiple window functions with different window definitions were used.\n\nFixed an issue that could cause a query with window functions and limit to return too few rows or have the window functions compute wrong results.\n\nFixed an issue that would cause a ClassCastException for queries ordered by a window function."
  },
  {
    "title": "Version 3.1.0 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.1.0.html",
    "html": "5.6\nVersion 3.1.0\n\nReleased on 2018/08/28.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.1.0.\n\nWe recommend that you upgrade to the latest 3.0 release before moving to 3.1.0.\n\nYou cannot perform a rolling upgrade to this version. Any upgrade to this version will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.1 and must be recreated before moving to 3.1.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nBreaking Changes\n\nChanges\n\nChangelog\nBreaking Changes\n\nCrash is no longer bundled with the CrateDB tarball distribution.\n\nChanges\n\nImproved performance and memory utilisation for queries against the sys tables.\n\nImproved performance and memory utilisation for unsorted distributed GROUP BY and aggregations statements by executing the reduce operation in an incremental way.\n\nImproved performance and memory utilisation of GROUP BY statements when the key column is a single string, byte, short, int or long.\n\nAdded a new CircuitBreakers MXBean for JMX which exposes statistics of all available circuit breakers.\n\nExposed the cluster state version in the sys.nodes table under the cluster_state_version column and under the NodeInfo MXBean in JMX.\n\nAdded a new ThreadPools MXBean for JMX which exposes statistics of all used thread pools.\n\nChanged the PostgreSQL wire protocol binary encoding format for timestamp columns to use the newer int64 format. This will enable compatibility with clients like pgx.\n\nAdded support for multi line SQL comments, e.g. /* multi line */.\n\nImproved performance of queries using an array access inside the WHERE clause. E.g.:\n\nSELECT * FROM t\nWHERE int_array_col[1] = 123\n\n\nAdded the full PostgreSQL syntax of the BEGIN statement and the COMMIT statement. This improves the support for clients that are based on the PostgreSQL wire protocol, such as the Golang lib/pg and pgx clients. The BEGIN and COMMIT statements and any of their parameters are simply ignored.\n\nAdded a new scalar function ignore3vl which eliminates the 3-valued logic of null handling for every logical expression beneath it. If 3-valued logic is not required, the use of this function in the WHERE clause beneath a NOT operator can boost the query performance significantly. E.g.:\n\nSELECT * FROM t\nWHERE NOT IGNORE3VL(5 = ANY(t.int_array_col))\n\n\nAdded a new Connections MBean for JMX which exposes the number of open connections per protocol.\n\nAdded a new connections column to the sys.nodes table which contains the number of currently open connections per protocol and the total number of connections per protocol opened over the life-time of a node.\n\nAdded support for COPY FROM ... RETURN SUMMARY which will return a result set with detailed error reporting of imported rows.\n\nAdded a new stats.jobs_log_filter setting which can be used to control what kind of entries are recorded into the sys.jobs_log table. In addition there is a new stats.jobs_log_persistent_filter setting which can be used to record entries also in the regular CrateDB log file.\n\nExposed statement classification in sys.jobs_log table.\n\nAdded a sys.jobs_metrics table which contains query latency information.\n\nThe setting es.api.enabled has been marked as deprecated and will be removed in a future version. Once removed it will no longer be possible to use the Elasticsearch API. Please create a feature request if you’re using the ES API and cannot use the SQL interface as substitute.\n\nIntroduced the EXPLAIN ANALYZE statement for query profiling.\n\nAdded typbasetype column to the pg_catalog.pg_type table.\n\nAdded support for the SHOW TRANSACTION_ISOLATION statement.\n\nAdded TimeZone parameter response to PostgreSQL Wire Protocol.\n\nExtended syntax support for certain ALTER BLOB TABLE RENAME, REROUTE and OPEN/CLOSE queries."
  },
  {
    "title": "Version 3.3.2 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.3.2.html",
    "html": "5.6\nVersion 3.3.2\n\nReleased on 2019/04/17.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.3.2.\n\nWe recommend that you upgrade to the latest 3.2 release before moving to 3.3.2.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 3.3.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nPlease consult the Upgrade Notes before upgrading.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.3 and must be recreated before moving to 3.3.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed the processing of LIMIT clauses within queries in the INSERT INTO statement. A query like INSERT INTO target (SELECT * FROM (SELECT * FROM source LIMIT 10) t) could insert more than 10 rows if there is more than 1 node in the cluster. In addition, using LIMIT in the top level query of the INSERT INTO statement is now no longer prohibited. So the query can be written as follows: INSERT INTO target (SELECT * FROM source LIMIT 10).\n\nFixed an issue that would cause the wrong evaluation of nested subqueries in cases where the inner subquery returns a multi-value and the outer returns a single value result. For instance, the assignment subquery expression in the following update statement UPDATE t1 SET x = (SELECT COUNT(*) FROM t2 WHERE x IN (SELECT x FROM t3))\") might have produced an incorrect result."
  },
  {
    "title": "Version 3.3.3 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.3.3.html",
    "html": "5.6\nVersion 3.3.3\n\nReleased on 2019/05/23.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.3.3.\n\nWe recommend that you upgrade to the latest 3.2 release before moving to 3.3.3.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 3.3.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nPlease consult the Upgrade Notes before upgrading.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.3 and must be recreated before moving to 3.3.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed an issue that will prevent CrateDB from bootstrapping when running on Java 8 and a Java agent is specified using JAVA_OPTS or CRATE_JAVA_OPTS.\n\nFixed an issue that prevented parameter placeholders from being resolved when creating a view. A view definition like CREATE VIEW v1 AS SELECT ? would get stored without the ? being resolved to the actual parameter value, causing queries on the view to fail and also breaking information_schema.views.\n\nIncreased the precedence of the double colon cast operator, so that a statement like x::double / y::double applies both casts before the division.\n\nFixed an issue with the disk watermark sys checks which would incorrectly report all of them as failed if cluster.routing.allocation.disk.threshold_enabled was set to false.\n\nFixed an issue were a query on a subquery with ambiguous columns would return the same values for all of the ambiguous columns. An example is SELECT * FROM (SELECT * FROM t1, t2) AS tjoin where both t1 and t2 have a column named x. In this case the value for t1.x would be output twice.\n\nFixed a race condition when setting an enterprise license very early on node startup while a trial license is generated concurrently and such may used instead of the user given license.\n\nImprove error message for the unsupported window definition ordered or partitioned by an array column type in the context of window functions"
  },
  {
    "title": "Version 3.3.4 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.3.4.html",
    "html": "5.6\nVersion 3.3.4\n\nReleased on 2019/06/19.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.3.4.\n\nWe recommend that you upgrade to the latest 3.2 release before moving to 3.3.4.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 3.3.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nPlease consult the Upgrade Notes before upgrading.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.3 and must be recreated before moving to 3.3.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed an issue that caused conflicting rows in an INSERT INTO .. ON CONFLICT (..) DO NOTHING statement to be reported as failed.\n\nFixed an issue that caused wrong results when running LEFT or RIGHT outer joins on a single node cluster and the rows inside each table differs.\n\nFixed an issue in the PostgreSQL wire protocol implementation that could cause clients to receive a Only write operations are allowed in Batch statements if the client relied on the behavior that closing prepared statements should implicitly close related portals.\n\nFixed a bug that led to is null predicates against ignored object fields to always evaluate to true.\n\nFixed collect_set to return an array type in order to be able to return the results over JDBC. collection_count and collection_avg are also changed to receive arrays as arguments, instead of sets.\n\nFixed an issue that caused an error when trying to create a table with a column definition that contains a predefined array data type and generated expression. For instance, a statement like CREATE TABLE foo (col ARRAY(TEXT) AS ['bar']) would fail.\n\nFixed a bug that led to failures of group by a single text column queries on columns with the cardinality ration lower than 0.5."
  },
  {
    "title": "Version 3.3.5 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.3.5.html",
    "html": "5.6\nVersion 3.3.5\n\nReleased on 2019/07/08.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.3.5.\n\nWe recommend that you upgrade to the latest 3.2 release before moving to 3.3.5.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 3.3.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nPlease consult the Upgrade Notes before upgrading.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.3 and must be recreated before moving to 3.3.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed the tables compatibility check to correctly indicate when tables need to be recreated in preparation for a CrateDB upgrade towards the next major version of CrateDB.\n\nThe values provided in INSERT or UPDATE statements for object columns which contain generated expressions are now validated. The computed expression must match the provided value. This makes the behavior consistent with how top level columns of a table are treated.\n\nFixed support for ordering by literal constants.\n\nExample: SELECT 1, * FROM t ORDER BY 1\""
  },
  {
    "title": "Version 4.0.1 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.0.1.html",
    "html": "5.6\nVersion 4.0.1\n\nReleased on 2019/07/08.\n\nNote\n\nPlease consult the Upgrade Notes before upgrading. Before upgrading to 4.0.1 you should be running a CrateDB cluster that is at least on 3.0.7.\n\nAn upgrade to Version 4.0.1 requires a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nWarning\n\nTables that were created prior CrateDB 3.x will not function with 4.x and must be recreated before moving to 4.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.0.0 release notes for a full list of changes in the 4.0 series.\n\nFixes\n\nFixed a possible NPE which could occur when querying the sys.allocations table.\n\nFixed the tables compatibility check to correctly indicate when tables need to be recreated in preparation for a CrateDB upgrade towards the next major version of CrateDB.\n\nFixed an issue that led to DEFAULT constraints of inner columns of object columns to be ignored.\n\nThe values provided in INSERT or UPDATE statements for object columns which contain generated expressions are now validated. The computed expression must match the provided value. This makes the behavior consistent with how top level columns of a table are treated.\n\nFixed support for ordering by literal constants. Example: SELECT 1, * FROM t ORDER BY 1\""
  },
  {
    "title": "Version 4.0.10 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.0.10.html",
    "html": "5.6\nVersion 4.0.10\n\nReleased on 2019/12/10.\n\nNote\n\nPlease consult the Upgrade Notes before upgrading from CrateDB 3.x or earlier. Before upgrading to 4.0.10 you should be running a CrateDB cluster that is at least on 3.0.7.\n\nWe recommend that you upgrade to the latest 3.3 release before moving to 4.0.10.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 4.0.2. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nWarning\n\nTables that were created prior CrateDB 3.x will not function with 4.x and must be recreated before moving to 4.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.0.0 release notes for a full list of changes in the 4.0 series.\n\nFixes\n\nFixed an issue that would lead to incorrect behaviour of the insert from sub query statement for the scenario when the target table contains a generated column with the NOT NULL constraint and a value for the generated column is not provided explicitly. For example, the insert statement below failed due to the NOT NULL constraint violation:\n\nCREATE TABLE t (x INT, y AS x + 1 NOT NULL)\nINSERT INTO t (x) (SELECT 1)\n\n\nFixed a potential memory accounting leak for queries with WHERE clauses on primary keys. This could lead to a node eventually rejecting all further queries with a CircuitBreakingException.\n\nImproved snapshot error handling by assuring a snapshot is declared as failed when a shard or node failure happens during the snapshot process.\n\nFixed an issue which may result in an ArrayIndexOutOfBoundsException while altering an empty partitioned table.\n\nFixed a regression introduced in 2.3.2 which optimizes subqueries when used inside multi-valued functions and operators like IN(), ANY() or ARRAY() by applying an implicit ordering. When using data types (e.g. like an object: select array(select {a = col} from test) which do not support ordering, an NPE was raised.\n\nFixed a possible OutOfMemory issue which may happen on GROUP BY statement using a group key of type TEXT on tables containing at least one shard with a low to medium cardinality on the group key.\n\nFixed an issue that caused an error when using ALTER TABLE .. ADD on a table which contains nested primary key columns.\n\nFixed an issue where values of type array(varchar) were decoded incorrectly if they contained a , character. This occurred when the PostgreSQL wire protocol was used in text mode.\n\nImproved performance of snapshot finalization as https://github.com/crate/crate/pull/9327 introduced a performance regression on the snapshot process.\n\nFixed a ClassCastException that could occur when using unnest on multi dimensional arrays."
  },
  {
    "title": "Version 4.0.2 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.0.2.html",
    "html": "5.6\nVersion 4.0.2\n\nReleased on 2019/07/12.\n\nNote\n\nPlease consult the Upgrade Notes before upgrading. Before upgrading to 4.0.2 you should be running a CrateDB cluster that is at least on 3.0.7.\n\nAn upgrade to Version 4.0.2 requires a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nWarning\n\nTables that were created prior CrateDB 3.x will not function with 4.x and must be recreated before moving to 4.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.0.0 release notes for a full list of changes in the 4.0 series.\n\nFixes\n\nFixed an issue in the Admin UI that prevented partitions from showing up in the table detail view.\n\nFixed an issue in the version handling that would prevent rolling upgrades to future versions of CrateDB.\n\nArithmetic operations now work on expressions of type timestamp without time zone, to make it consistent with timestamp with time zone."
  },
  {
    "title": "Version 4.0.11 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.0.11.html",
    "html": "5.6\nVersion 4.0.11\n\nReleased on 2020/01/15.\n\nNote\n\nPlease consult the Upgrade Notes before upgrading from CrateDB 3.x or earlier. Before upgrading to 4.0.11 you should be running a CrateDB cluster that is at least on 3.0.7.\n\nWe recommend that you upgrade to the latest 3.3 release before moving to 4.0.11.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 4.0.2. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nWarning\n\nTables that were created prior CrateDB 3.x will not function with 4.x and must be recreated before moving to 4.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.0.0 release notes for a full list of changes in the 4.0 series.\n\nFixes\n\nFixed an issue that prevented statements from showing up in sys.jobs_log if they run into an error.\n\nFixed an NPE which occurred when using the current_timestamp inside the WHERE clause on a view relation.\n\nFixed the data type of the sys.jobs_metrics.classification['labels'] column, should be text_array instead of an undefined type.\n\nFixed an issue that caused a type cast error in INSERT statements if the target table contained a array(object() as (...) column where a child of the object array contained a NOT NULL constraint.\n\nFixed a NullPointerException that could prevent a node from starting up. This could occur if the node crashed or disconnected while a user deleted a table.\n\nImproved the memory accounting for values of type geo_shape, object or undefined. Previously an arbitrary fixed value was used for memory accounting. If the actual payloads are large, this could have led to out of memory errors as the memory usage was under-estimated.\n\nFixed the type information of the fs['data'] and fs['disks'] column in the sys.nodes table. Querying those columns could have resulted in serialization errors.\n\nFixed the support for the readonly property in CREATE REPOSITORY.\n\nFixed an issue that may cause a SELECT query to hang on multiple nodes cluster if a resource error like a CircuitBreakingException occurs.\n\nFixed an issue that caused a INSERT INTO ... (SELECT ... FROM ..) statement to fail if not all partition columns appeared in the target list of the INSERT INTO statement."
  },
  {
    "title": "Version 4.1.0 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.1.0.html",
    "html": "5.6\nVersion 4.1.0\n\nReleased on 2020/01/15.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.1.0.\n\nWe recommend that you upgrade to the latest 4.0 release before moving to 4.1.0.\n\nA rolling upgrade to 4.1.0 from 4.0.2+ is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nBreaking Changes\n\nDeprecations\n\nChanges\n\nResiliency improvements\n\nPerformance improvements\n\nSQL Standard and PostgreSQL compatibility improvements\n\nWindow function extensions\n\nFunctions and operators\n\nNew statements and clauses\n\nObservability improvements\n\nOthers\n\nBreaking Changes\n\nChanged arithmetic operations *, +, and - of types integer and bigint to throw an exception instead of rolling over from positive to negative or the other way around.\n\nRemap CrateDB Objects array data type from the PostgreSQL JSON to JSON array type. That might effect some drivers that use the PostgreSQL wire protocol to insert data into tables with object array typed columns. For instance, when using the Npgsql driver, it is not longer possible to insert an array of objects into a column of the object array data type by using the parameter of a SQL statement that has the JSON data type and an array of CLR as its value. Instead, use a string array with JSON strings that represent the objects. See the Npgsql documentation for more details.\n\nChanged how columns of type Geographic types are being communicated to PostgreSQL clients.\n\nBefore, clients were told that those columns are double arrays. Now, they are correctly mapped to the PostgreSQL point type. This means that applications using clients like JDBC will have to be adapted to use PgPoint. (See Geometric DataTypes in JDBC)\n\nChanged the behavior of unnest to fully unnest multi dimensional arrays to their innermost type to be compatible with PostgreSQL.\n\nDeprecations\n\nDeprecated the node.store.allow_mmapfs setting in favour of node.store.allow_mmap.\n\nChanges\nResiliency improvements\n\nAllow user to limit the number of threads on a single shard that may be merging at once via the merge.scheduler.max_thread_count table parameter.\n\nSome ALTER TABLE operations now internally invoke a single cluster state update instead of multiple cluster state updates. This change improves resiliency because there is no longer a window where the cluster state could be inconsistent.\n\nChanged the default garbage collector from Concurrent Mark Sweep to G1GC. This should lead to shorter GC pauses.\n\nAdded a dynamic bulk sizing mechanism that should prevent INSERT INTO ... FROM query operations from running into out-of-memory errors when the individual records of a table are large.\n\nAdded the cluster.routing.allocation.total_shards_per_node setting.\n\nPerformance improvements\n\nOptimized SELECT DISTINCT .. LIMIT n queries. On high cardinality columns, these types of queries now execute up to 200% faster and use less memory.\n\nThe optimizer now utilizes internal statistics to approximate the number of rows returned by various parts of a query plan. This should result in more efficient execution plans for joins.\n\nReduced recovery time by sending file-chunks concurrently. This change only applies when transport communication is secured or compressed. The number of chunks is controlled by the indices.recovery.max_concurrent_file_chunks setting.\n\nAdded an optimization that allows WHERE clauses on top of derived tables containing table functions to run more efficiently in some cases.\n\nAllow user to control how table data is stored and accessed on a disk via the store.type table parameter and node.store.allow_mmap node setting.\n\nChanged the default table data store type from mmapfs to hybridfs.\n\nSQL Standard and PostgreSQL compatibility improvements\nWindow function extensions\n\nAdded support for the lag and lead window functions as enterprise features.\n\nAdded support for ROWS frame definitions in the context of window functions window definitions.\n\nAdded support for the named window definition. This change allows a user to define a list of window definitions in the WINDOW clause that can be referenced in OVER clauses.\n\nAdded support for offset PRECEDING and offset FOLLOWING window definitions.\n\nFunctions and operators\n\nAdded support for the ALL operator for array and subquery comparisons.\n\nAdded a PG_GET_KEYWORDS table function.\n\nExtended CONCAT to do implicit casts, so that calls like SELECT 't' || 5 are supported.\n\nAdded support for casting values of type object to text. This casting will cause the object to be converted to a JSON string.\n\nAdded support for casting to Geographic types, Geometric shapes and Objects array data types.\n\nFor example:\n\ncast(['POINT(2 3)','POINT(1 3)'] AS array(geo_point))\n\n\nAdded the PG_TYPEOF system function.\n\nAdded the INTERVAL data type and extended pg_catalog.generate_series(start, stop, [step]) to work with timestamps and the new INTERVAL type.\n\nAdded LPAD and RPAD scalar functions.\n\nAdded the LTRIM and RTRIM scalar functions.\n\nAdded LEFT and RIGHT scalar functions.\n\nAdded TIMEZONE scalar function.\n\nAdded AT TIME ZONE syntax.\n\nAdded support for the operator ILIKE, the case insensitive complement to LIKE.\n\nAdded support for CIDR notation comparisons through special purpose operator << associated with type IP.\n\nStatements like 192.168.0.0 << 192.168.0.1/24 evaluate as true, meaning SELECT ip FROM ips_table WHERE ip << 192.168.0.1/24 returns matching IP addresses.\n\nNew statements and clauses\n\nAdded a ANALYZE command that can be used to update statistical data about the contents of the tables in the CrateDB cluster. This data is visible in a newly added pg_stats table.\n\nAdded a PROMOTE REPLICA subcommand to ALTER TABLE.\n\nAdded support for the filter clause in aggregate expressions and window functions that are aggregates.\n\nAdded support for using VALUES as a top-level relation.\n\nObservability improvements\n\nAdded a failures column to the sys.snapshots table.\n\nImproved the error messages that were returned if a relation or schema is not found.\n\nThe error messages may now include suggestions for similarly named tables, which should make typos more apparent and help users figure out they are missing double quotes (e.g., when a table name contains upper case letters).\n\nAdded a seq_no_stats and a translog_stats column to the sys.shards table.\n\nAdded new system table sys.segments which contains information about the Lucene segments of a shard.\n\nAdded a node column to sys.jobs_log.\n\nStatements containing limits, filters, window functions, or table functions will now be labelled accordingly in Jobs metrics.\n\nOthers\n\nChanged the default for write.wait_for_active_shards from ALL to 1. This update improves the out of the box experience by allowing a subset of nodes to become unavailable without blocking write operations. See the documentation linked above for more details about the implications.\n\nAdded phonetic token filter with following encoders: metaphone, double_metaphone, soundex, refined_soundex, caverphone1, caverphone2, cologne, nysiis, koelnerphonetik, haasephonetik, beider_morse, and daitch_mokotoff.\n\nRemoved a restriction for predicates in the WHERE clause involving partitioned columns, which could result in a failure response with the message: logical conjunction of the conditions in the WHERE clause which involve partitioned columns led to a query that can't be executed.\n\nSupport implicit object creation in update statements. For example, UPDATE t SET obj['x'] = 10 will now implicitly set obj to {obj: {x: 10}} on rows where obj was null.\n\nAdded the codec parameter to CREATE TABLE to control the compression algorithm used to store data.\n\nThe node argument of the REROUTE commands of ALTER TABLE can now either be the ID or the name of a node.\n\nAdded support for the PostgreSQL array string literal notation."
  },
  {
    "title": "Version 4.0.12 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.0.12.html",
    "html": "5.6\nVersion 4.0.12\n\nReleased on 2020-01-30.\n\nNote\n\nPlease consult the Upgrade Notes before upgrading from CrateDB 3.x or earlier. Before upgrading to 4.0.12 you should be running a CrateDB cluster that is at least on 3.0.7.\n\nWe recommend that you upgrade to the latest 3.3 release before moving to 4.0.12.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 4.0.2. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nWarning\n\nTables that were created prior CrateDB 3.x will not function with 4.x and must be recreated before moving to 4.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.0.0 release notes for a full list of changes in the 4.0 series.\n\nFixes\n\nFixed a bug that would lead to insertion of records via INSERT INTO ... (SELECT ... FROM ..) and INSERT INTO ... VALUES (...) into different partitions while using the same partition by value. This occurs only when the partition key is an object field of the timestamp data type.\n\nFixed an issue that caused queries on more than one relation and a literal FALSE in the WHERE clause to match all rows instead of no rows.\n\nFixed the following issues in the Admin UI:\n\nFixed an issue that prevents the value for nested partition columns showing up in the table partitions overview.\n\nFixed capitalization of Shards tab label\n\nUpdated keywords list so that they are recognised and painted in red.\n\nFixed a bug which could lead to stuck queries when an error happens inside distributed execution plans, e.g. a CircuitBreakingException due to exceeded memory usage.\n\nFixed an issue that resulted in the values for nested partitioned columns to be missing from the result.\n\nFixed an issue that caused SELECT * to include nested columns of type geo_shape instead of only selecting top-level columns.\n\nFixed an issue that caused subscript expressions on top of child relations in which an object column is selected to fail.\n\nFixed a ClassCastException that occurred when querying certain columns from information_schema.tables, sys.jobs_log or sys.jobs_metrics with a client connected via PostgreSQL wire protocol.\n\nFixed a regression introduced in 4.0.11 which caused a ClassCastException when querying sys.allocations."
  },
  {
    "title": "Version 4.1.2 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.1.2.html",
    "html": "5.6\nVersion 4.1.2\n\nReleased on 2020-02-14.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.1.2.\n\nWe recommend that you upgrade to the latest 4.0 release before moving to 4.1.2.\n\nA rolling upgrade to 4.1.2 from 4.0.2+ is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.1.0 release notes for a full list of changes in the 4.1 series.\n\nFixes\n\nFixed a regression that caused a ArithmeticException: / by zero error when querying an empty table or a non-empty table with outdated table statistics.\n\nFixed a regression that caused INSERT INTO statements containing an object column as target and a matching JSON string literal in the source to fail with a type cast error.\n\nFixed an issue which may result in showing the CE Admin UI view even if running in Enterprise mode on early node startup."
  },
  {
    "title": "Version 4.1.1 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.1.1.html",
    "html": "5.6\nVersion 4.1.1\n\nReleased on 2020-01-30.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.1.1.\n\nWe recommend that you upgrade to the latest 4.0 release before moving to 4.1.1.\n\nA rolling upgrade to 4.1.1 from 4.0.2+ is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.1.0 release notes for a full list of changes in the 4.1 series.\n\nFixes\n\nFixed an issue that caused queries on more than one relation and a literal FALSE in the WHERE clause to match all rows instead of no rows.\n\nFixed the following issues in the Admin UI:\n\nFixed an issue that prevents the value for nested partition columns showing up in the table partitions overview.\n\nFixed capitalization of Shards tab label.\n\nUpdated keywords list so that they are recognised and painted in red.\n\nFixed a bug which could lead to stuck queries when an error happens inside distributed execution plans, e.g., a CircuitBreakingException due to exceeded memory usage.\n\nFixed an issue that resulted in the values for nested partition columns to be missing from the result.\n\nFixed an issue that caused SELECT * to include nested columns of type geo_shape instead of only selecting top-level columns.\n\nFixed a bug that would lead to insertion of records via INSERT INTO ... (SELECT ... FROM ..) and INSERT INTO ... VALUES (...) into different partitions while using the same partition by value. This occurs only when the partition key is an object field of the timestamp data type.\n\nFixed an issue that caused subscript expressions on top of child relations in which an object column is selected to fail.\n\nFixed an issue in VALUES that would not allow combining expressions that can be explicitly casted or NULL literals in the same column.\n\nFixed a ClassCastException that occurred when querying certain columns from information_schema.tables, sys.jobs_log, or sys.jobs_metrics with a client connected via the PostgreSQL wire protocol.\n\nFixed a regression introduced in 4.0.11 which caused a ClassCastException when querying sys.allocations."
  },
  {
    "title": "Version 4.1.5 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.1.5.html",
    "html": "5.6\nVersion 4.1.5\n\nReleased on 2020-04-24.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.1.5.\n\nWe recommend that you upgrade to the latest 4.0 release before moving to 4.1.5.\n\nA rolling upgrade to 4.1.5 from 4.0.2+ is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.1.0 release notes for a full list of changes in the 4.1 series.\n\nFixes\n\nFixed an issue that caused COPY FROM from a HTTPS URL to fail with No X509TrustManager implementation available if CrateDB is configured to use SSL.\n\nFixed an issue that would lead to incorrect result when selecting the cluster license object column: fields of the object would contain null values even though the license was set.\n\nFixed an issue that caused OFFSET as part of a UNION to be applied incorrectly.\n\nFixed an issue that could lead to incorrect ordering if using ORDER BY on a column of type IP or on a scalar function.\n\nFixed an issue that caused a NullPointerException if using COPY TO with a WHERE clause with filters on primary key columns."
  },
  {
    "title": "Version 4.1.3 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.1.3.html",
    "html": "5.6\nVersion 4.1.3\n\nReleased on 2020-03-05.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.1.3.\n\nWe recommend that you upgrade to the latest 4.0 release before moving to 4.1.3.\n\nA rolling upgrade to 4.1.3 from 4.0.2+ is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.1.0 release notes for a full list of changes in the 4.1 series.\n\nFixes\n\nFixed an issue that led to more than one expression in the form <literalValue> AS <alias> to be interpreted as the same column if all <literalValue> expressions are equal.\n\nFixed an issue that led to a NullPointerException when using GROUP BY on a nested partition column.\n\nFixed an issue that led to an ArrayIndexOutOfBoundsException if using ON CONFLICT (...) UPDATE SET in an INSERT statement.\n\nFixed an issue that could lead to a Values less than -1 bytes are not supported error if one or more CrateDB nodes have insufficient disk space available.\n\nFixed an issue that would cause COPY FROM statements that used a HTTPS source using a Let’s Encrypt certificate to fail.\n\nFixed an issue that caused RTRIM to behave like LRTRIM."
  },
  {
    "title": "Version 4.1.6 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.1.6.html",
    "html": "5.6\nVersion 4.1.6\n\nReleased on 2020-06-08.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.1.6.\n\nWe recommend that you upgrade to the latest 4.0 release before moving to 4.1.6.\n\nA rolling upgrade to 4.1.6 from 4.0.2+ is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.1.0 release notes for a full list of changes in the 4.1 series.\n\nFixes\n\nFixed an issue that prevented CrateDB from detecting changes to the SSL keystore or truststore if one the paths is a symlink.\n\nFixed an issue that could prevent updating settings that depend on other settings. For example \"cluster.routing.allocation.disk.watermark.low\"\n\nFixed an issue that caused the ANALYZE statement to fail if there are tables with object arrays in the cluster.\n\nFixed a performance issue that can lead to queries like SELECT text_column FROM tbl GROUP BY 1 to run more than 150% faster."
  },
  {
    "title": "Version 4.1.4 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.1.4.html",
    "html": "5.6\nVersion 4.1.4\n\nReleased on 2020-03-20.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.1.4.\n\nWe recommend that you upgrade to the latest 4.0 release before moving to 4.1.4.\n\nA rolling upgrade to 4.1.4 from 4.0.2+ is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.1.0 release notes for a full list of changes in the 4.1 series.\n\nFixes\n\nImproved the resiliency of INSERT INTO .. queries that have a table function like generate_series as a source.\n\nFixed a regression introduced in 4.1 that led to a ClassCastException running queries with GROUP BY, no aggregations, and a LIMIT clause."
  },
  {
    "title": "Version 4.1.7 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.1.7.html",
    "html": "5.6\nVersion 4.1.7\n\nReleased on 2020-06-24.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.1.7.\n\nWe recommend that you upgrade to the latest 4.0 release before moving to 4.1.7.\n\nA rolling upgrade to 4.1.7 from 4.0.2+ is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.1.0 release notes for a full list of changes in the 4.1 series.\n\nFixes\n\nFixed an issue that caused ORDER BY expressions referencing table functions used in the SELECT list to fail.\n\nFixed an issue that prevented an optimization for SELECT DISTINCT <single_text_column> FROM <table> from working if used within a INSERT INTO statement.\n\nRe-enabled the IAM role authentication for s3 repositories\n\nChanged the required privileges to execute RESET statements to include the AL privilege. Users with AL could change settings using SET GLOBAL already.\n\nFixed an issue that caused a NullPointerException if the ANALYZE statement was executed on tables with primitive array type columns that contain NULL values.\n\nFixed an issue that caused the OFFSET clause to be ignored in SELECT DISTINCT queries."
  },
  {
    "title": "Version 4.2.2 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.2.2.html",
    "html": "5.6\nVersion 4.2.2\n\nReleased on 2020-07-28.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.2.2.\n\nWe recommend that you upgrade to the latest 4.1 release before moving to 4.2.2.\n\nA rolling upgrade from 4.1.7+ to 4.2.2 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.2.0 release notes for a full list of changes in the 4.2 series.\n\nFixes\n\nFixed an issue that occasionally caused EXPLAIN ANALYZE to return invalid query breakdown results or fail with the index out of bound exception.\n\nFixed a regression that caused INSERT INTO statements in tables with a nested CLUSTERED BY column to fail.\n\nFixed a regression introduced in 4.2.0 that caused subscript lookups on ignored object columns to raise an error instead of a null value if the key doesn’t exist.\n\nFixed a performance regression introduced in 4.2.0 which caused queries with a ORDER BY but without LIMIT to execute slower than they used to.\n\nFixed an issue that prevented queries executed using a query-then-fetch strategy from being labelled correctly in the sys.jobs_metrics and sys.jobs_log tables.\n\nFixed a regression introduced in 4.2.0 which caused queries including a virtual table, and both a ORDER BY and LIMIT clause to fail.\n\nFixed a regression introduced in 4.2.0 that resulted in NULL values being returned for partition columns instead of the actual values.\n\nAllow all users to execute DISCARD and SET TRANSACTION statement. These are session local statements and shouldn’t require special privileges.\n\nUpdated the bundled JDK to 14.0.2-12\n\nIncreased the default interval for stats.service.interval <stats.service.interval> from one hour to 24 hours because invoking it every hour caused significant extra load on a cluster.\n\nFixed an issue that caused SHOW CREATE TABLE to print columns of type VARCHAR(n) as TEXT, leading to a loss of the length information when using the SHOW CREATE TABLE statement to re-create a table.\n\nFixed an issue that prevented ALTER TABLE .. ADD COLUMN statements from working on tables containing a PRIMARY KEY column with a INDEX OFF definition."
  },
  {
    "title": "Version 4.2.3 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.2.3.html",
    "html": "5.6\nVersion 4.2.3\n\nReleased on 2020-08-18.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.2.3.\n\nWe recommend that you upgrade to the latest 4.1 release before moving to 4.2.3.\n\nA rolling upgrade from 4.1.7+ to 4.2.3 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.2.0 release notes for a full list of changes in the 4.2 series.\n\nFixes\n\nFixed a regression introduced in 4.2.0 which caused fulltext search queries of the phase match type to ignore the fuzziness option.\n\nFixed a regression introduced in 4.2.0 that caused sum and avg global aggregates to return incorrect results when used on columns of real or double precision data types.\n\nFixed an issue that caused primary key lookups to return an empty result instead of the row identified by the primary key values, if the primary key consists of multiple columns and if one of them is of type BOOLEAN."
  },
  {
    "title": "Version 4.0.0 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.0.0.html",
    "html": "5.6\nVersion 4.0.0\n\nReleased on 2019/06/25.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 3.0.4 or higher before you upgrade to 4.0.0.\n\nWe recommend that you upgrade to the latest 3.3 release before moving to 4.0.0.\n\nAn upgrade to Version 4.0.0 requires a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nPlease consult the Upgrade Notes before upgrading.\n\nWarning\n\nTables that were created prior CrateDB 3.x will not function with 4.x and must be recreated before moving to 4.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nUpgrade Notes\n\nDiscovery Changes\n\nBreaking Changes\n\nGeneral\n\nRemoved Settings\n\nSystem table changes\n\nRemoved Functionality\n\nDeprecations\n\nChanges\n\nSQL Standard and PostgreSQL compatibility improvements\n\nUsers and Access Control\n\nRepositories and Snapshots\n\nPerformance and resiliency improvements\n\nOthers\n\nUpgrade Notes\nDiscovery Changes\n\nThis version of CrateDB uses a new cluster coordination (discovery) implementation which improves resiliency and master election times. A new voting mechanism is used when a node is removed or added which makes the system capable of automatically maintaining an optimal level of fault tolerance even in situations of network partitions.\n\nThis eliminates the need of the easily miss configured minimum_master_nodes setting.\n\nAdditionally a rare resiliency failure, recorded as Repeated cluster partitions can cause cluster state updates to be lost can no longer occur.\n\nDue to this some discovery settings are added, renamed and removed.\n\nOld Name\n\n\t\n\nNew Name\n\n\n\n\nNew, required on upgrade.\n\n\t\n\ncluster.initial_master_nodes\n\n\n\n\ndiscovery.zen.hosts_provider\n\n\t\n\ndiscovery.seed_providers\n\n\n\n\ndiscovery.zen.ping.unicast.hosts\n\n\t\n\ndiscovery.seed_hosts\n\n\n\n\ndiscovery.zen.minimum_master_nodes\n\n\t\n\nRemoved\n\n\n\n\ndiscovery.zen.ping_interval\n\n\t\n\nRemoved\n\n\n\n\ndiscovery.zen.ping_timeout\n\n\t\n\nRemoved\n\n\n\n\ndiscovery.zen.ping_retries\n\n\t\n\nRemoved\n\n\n\n\ndiscovery.zen.publish_timeout\n\n\t\n\nRemoved\n\nCaution\n\nThe cluster.initial_master_nodes setting is required to be set at production (non loopback bound) clusters on upgrade, see the setting documentation for details.\n\nNote\n\nOnly a single port value is allowed for each discovery.seed_hosts setting entry. Defining a port range as it was allowed but ignored in previous versions under the old setting name discovery.zen.ping.unicast.hosts, will be rejected.\n\nNote\n\nCrateDB will refuse to start when it encounters an unknown setting, like the above mentioned removed ones. Please make sure to adjust your crate.yml or CMD arguments upfront.\n\nBreaking Changes\nGeneral\n\nRenamed CrateDB data types to the corresponding PostgreSQL data types.\n\nCurrent Name\n\n\t\n\nNew Name\n\n\n\n\nshort\n\n\t\n\nsmallint\n\n\n\n\nlong\n\n\t\n\nbigint\n\n\n\n\nfloat\n\n\t\n\nreal\n\n\n\n\ndouble\n\n\t\n\ndouble precision\n\n\n\n\nbyte\n\n\t\n\nchar\n\n\n\n\nstring\n\n\t\n\ntext\n\n\n\n\ntimestamp\n\n\t\n\ntimestamp with time zone\n\nSee Data types for more detailed information. The old data type names, are registered as aliases for backward comparability.\n\nChanged the ordering of columns to be based on their position in the CREATE TABLE statement. This was done to improve compatibility with PostgreSQL and will affect queries like SELECT * FROM or INSERT INTO <table> VALUES (...)\n\nChanged the default Column policy on tables from dynamic to strict. Columns of type object still default to dynamic.\n\nRemoved the implicit soft limit of 10000 that was applied for clients using HTTP.\n\nDropped support for Java versions < 11\n\nRemoved Settings\n\nRemoved the deprecated setting cluster.graceful_stop.reallocate.\n\nRemoved the deprecated http.enabled setting. HTTP is now always enabled and can no longer be disabled.\n\nRemoved the deprecated license.ident setting. Licenses must be set using the SET LICENSE statement.\n\nRemoved the deprecated license.enterprise setting. To use CrateDB without any enterprise features one should use the community edition instead.\n\nRemoved the experimental enable_semijoin session setting. As this defaulted to false, this execution strategy cannot be used anymore.\n\nRemoved the possibility of configuring the AWS S3 repository client via the crate.yaml configuration file and command line arguments. Please, use the CREATE REPOSITORY statement parameters for this purpose.\n\nRemoved HDFS repository setting: concurrent_streams as it is no longer supported.\n\nThe zen1 related discovery settings mentioned in Discovery Changes.\n\nSystem table changes\n\nChanged the layout of the version column in the information_schema.tables and information_schema.table_partitions tables. The version is now displayed directly under created and upgraded. The cratedb and elasticsearch sub-category has been removed.\n\nRemoved deprecated metrics from sys.nodes:\n\nMetric name\n\n\n\n\nfs['disks']['reads']\n\n\n\n\nfs['disks']['bytes_read']\n\n\n\n\nfs['disks']['writes']\n\n\n\n\nfs['disks']['bytes_written']\n\n\n\n\nos['cpu']['system']\n\n\n\n\nos['cpu']['user']\n\n\n\n\nos['cpu']['idle']\n\n\n\n\nos['cpu']['stolen']\n\n\n\n\nprocess['cpu']['user']\n\n\n\n\nprocess['cpu']['system']\n\nRenamed column information_schema.table_partitions.schema_name to table_schema.\n\nRenamed information_schema.columns.user_defined_type_* columns to information_schema_columns.udt_* for SQL standard compatibility.\n\nChanged type of column information_schema.columns.is_generated to STRING with value NEVER or ALWAYS for SQL standard compatibility.\n\nRemoved Functionality\n\nThe Elasticsearch REST API has been removed.\n\nRemoved the deprecated ingest framework, including the MQTT endpoint.\n\nRemoved the HTTP pipelining functionality. We are not aware of any client using this functionality.\n\nRemoved the deprecated average duration and query frequency JMX metrics. The total counts and sum of durations as documented in QueryStats MBean should be used instead.\n\nRemoved the deprecated ON DUPLICATE KEY syntax of INSERT statements. Users can migrate to the ON CONFLICT syntax.\n\nRemoved the index thread-pool and the bulk alias for the write thread-pool. The JMX getBulk property of the ThreadPools bean has been renamed too getWrite.\n\nRemoved deprecated nGram, edgeNGram token filter and htmlStrip char filter, they are superseded by ngram, edge_ngram and html_strip.\n\nRemoved the deprecated USR2 signal handling. Use ALTER CLUSTER DECOMMISSION instead. Be aware that the behavior of sending USR2 signals to a CrateDB process is now undefined and up to the JVM. In some cases it may still terminate the instance but without clean shutdown.\n\nDeprecations\n\nDeprecate the usage of the _version column for Optimistic Concurrency Control in favour of the _seq_no and _primary_term columns.\n\nDeprecate the usage of the TIMESTAMP alias data type as a timestamp with time zone, use the TIMESTAMP WITH TIME ZONE or the TIMESTAMPTZ data type alias instead. The TIMESTAMP data type will be an equivalent to data type without time zone in future CrateDB releases.\n\nMarked SynonymFilter tokenizer as deprecated.\n\nMarked LowerCase tokenizer as deprecated.\n\nChanges\nSQL Standard and PostgreSQL compatibility improvements\n\nAdded support for using relation aliases with column aliases. Example: SELECT x, y from unnest([1], ['a']) as u(x, y)\n\nAdded support for column Default clause for CREATE TABLE.\n\nExtended the support for window functions. The PARTITION BY definition and the CURRENT ROW -> UNBOUNDED FOLLOWING frame definitions are now supported.\n\nAdded the string_agg(column, delimiter) aggregation function.\n\nAdded support for SQL Standard Timestamp Format to the Dates and times.\n\nAdded the TIMESTAMP WITHOUT TIME ZONE data type.\n\nAdded the TIMESTAMPTZ alias for the TIMESTAMP WITH TIME ZONE data type.\n\nAdded support for the type ‘string’ cast operator, which is used to initialize a constant of an arbitrary type.\n\nAdded the pg_get_userbyid() scalar function to enhance PostgreSQL compatibility.\n\nEnabled scalar function evaluation when used in the query FROM clause in place of a relation.\n\nShow the session setting description in the output of the SHOW ALL statement.\n\nAdded information for the internal PostgreSQL data type: name in pg_catalog.pg_type for improved PostgreSQL compatibility.\n\nAdded the pg_catalog.pg_settings table.\n\nAdded support for String literals with C-Style escapes.\n\nAdded trim scalar function that trims the (leading, trailing or both) set of characters from an input string.\n\nAdded string_to_array scalar function that splits an input string into an array of string elements using a separator and a null-string.\n\nAdded missing PostgreSQL type mapping for the array(ip) collection type.\n\nAdded current_setting system information scalar function that yields the current value of the setting.\n\nAllow User-defined functions to be registered against the pg_catalog schema. This also extends CURRENT_SCHEMA to be addressable with pg_catalog included.\n\nAdded quote_ident scalar function that quotes a string if it is needed.\n\nUsers and Access Control\n\nMask sensitive user account information in sys.repositories for repository types: azure, s3.\n\nRestrict access to log entries in sys.jobs and sys.jobs_log to the current user. This doesn’t apply to superusers.\n\nAdded a new Administration Language (AL) privilege type which allows users to manage other users and use SET GLOBAL. See Privileges.\n\nRepositories and Snapshots\n\nAdded support for the Azure Storage repositories.\n\nChanged the default value of the fs repository type setting compress, to true. See fs repository parameters.\n\nImproved resiliency of the CREATE SNAPSHOT operation.\n\nPerformance and resiliency improvements\n\nExposed the _seq_no and _primary_term system columns which can be used for Optimistic Concurrency Control. By introducing _seq_no and _primary_term, the following resiliency issues were fixed:\n\nVersion Number Representing Ambiguous Row Versions\n\nReplicas can fall out of sync when a primary shard fails\n\nPredicates like abs(x) = 1 which require a scalar function evaluation and cannot operate on table indices directly are now candidates for the query cache. This can result in order of magnitude performance increases on subsequent queries.\n\nRouting awareness attributes are now also taken into consideration for primary key lookups. (Queries like SELECT * FROM t WHERE pk = 1)\n\nChanged the circuit breaker logic to measure the real heap usage instead of the memory reserved by child circuit breakers. This should reduce the chance of nodes running into an out of memory error.\n\nAdded a new optimization that allows to run predicates on top of views or sub-queries more efficiently in some cases.\n\nOthers\n\nAdded support for dynamical reloading of SSL certificates. See Configuring the Keystore.\n\nAdded minimum_index_compatibility_version and minimum_wire_compatibility_version to sys.version to expose the current state of the node’s index and wire protocol version as part of the sys.nodes table.\n\nUpgraded to Lucene 8.0.0, and as part of this the BM25 scoring has changed. The order of the scores remain the same, but the values of the scores differ. Fulltext queries including _score filters may behave slightly different.\n\nAdded a new _docid system column.\n\nAdded support for subscript expressions on an object column of a sub-relation. Examples: SELECT a['b'] FROM (SELECT a FROM t1) or SELECT a['b'] FROM my_view where my_view is defined as SELECT a FROM t1."
  },
  {
    "title": "Version 3.3.6 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/3.3.6.html",
    "html": "5.6\nVersion 3.3.6\n\nReleased on 2019/09/27.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 2.0.4 or higher before you upgrade to 3.3.6.\n\nWe recommend that you upgrade to the latest 3.2 release before moving to 3.3.6.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 3.3.0. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nPlease consult the Upgrade Notes before upgrading.\n\nWarning\n\nTables that were created prior to upgrading to CrateDB 2.x will not function with 3.3 and must be recreated before moving to 3.3.x.\n\nYou can recreate tables using COPY TO and COPY FROM while running a 2.x release into a new table, or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nChangelog\n\nFixes\n\nChangelog\nFixes\n\nFixed an issue that could prevent accounted memory from being properly de-accounted on queries using hyperloglog_distinct, leading clients to eventually receive CircuitBreakingException error messages and also breaking internal recovery operations.\n\nRemoved a case where a NullPointerException was logged if a HTTP client disconnected before a pending response could be sent to the client."
  },
  {
    "title": "Version 4.0.3 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.0.3.html",
    "html": "5.6\nVersion 4.0.3\n\nReleased on 2019/08/06.\n\nNote\n\nPlease consult the Upgrade Notes before upgrading. Before upgrading to 4.0.3 you should be running a CrateDB cluster that is at least on 3.0.7.\n\nWe recommend that you upgrade to the latest 3.3 release before moving to 4.0.3.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 4.0.2. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nWarning\n\nTables that were created prior CrateDB 3.x will not function with 4.x and must be recreated before moving to 4.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.0.0 release notes for a full list of changes in the 4.0 series.\n\nFixes\n\nFixed an issue with using the same column under a different name/alias inside more complex queries. Example: SELECT count(*), t.x, t.x AS tx FROM t GROUP BY t.x\n\nFixed an issue with the collect_set function. It could compute an incorrect result if used as a window function with shrinking window frames.\n\nFixed an issue with the version payload returned by HTTP, which resulted in falsely displaying CrateDB’s version as a -SNAPSHOT version at the Admin UI.\n\nFixed parsing of timestamps with a time zone offset of +0000.\n\nFixed an issue that could cause startup to fail with early access builds of Java."
  },
  {
    "title": "Version 4.2.4 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.2.4.html",
    "html": "5.6\nVersion 4.2.4\n\nReleased on 2020-08-26.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.2.4.\n\nWe recommend that you upgrade to the latest 4.1 release before moving to 4.2.4.\n\nA rolling upgrade from 4.1.7+ to 4.2.4 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.2.0 release notes for a full list of changes in the 4.2 series.\n\nFixes\n\nFixed a performance regression that caused SELECT statements on tables with generated partition columns and a predicate that uses a column used to compute the partition column to hit all partitions instead of only a subset.\n\nFixed an issue that could lead to a IndexOutOfBoundsException when using virtual tables and joins.\n\nFixed an issue that declared the rule optimizer settings as global. The settings are now session local.\n\nFixed an issue that prevented the MATCH predicate from working in mixed clusters running 4.1.8 and 4.2.\n\nFixed an issue that prevented user-defined functions in a custom schema from working if used in a generated column expression.\n\nFixed an issue that allowed users to use a function in a generated column that didn’t fully match the given arguments, leading to a subsequent runtime failure when trying to access tables.\n\nFixed exposure of the full qualified name of a sub-script column in information_schema.tables.partitioned_by and pg_catalog.pg_attribute.attname to use the CrateDB SQL compatible identifier.\n\nFixed an issue that led to a Message not fully read error when trying to decommission a node using ALTER CLUSTER DECOMMISSION.\n\nFixed an issue that resulted in incorrect results when querying the sys.nodes table. Predicates used in the WHERE clause on columns that were absent in the select-list never matched."
  },
  {
    "title": "Version 4.2.5 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.2.5.html",
    "html": "5.6\nVersion 4.2.5\n\nReleased on 2020-09-22.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.2.5.\n\nWe recommend that you upgrade to the latest 4.1 release before moving to 4.2.5.\n\nA rolling upgrade from 4.1.7+ to 4.2.5 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.2.0 release notes for a full list of changes in the 4.2 series.\n\nFixes\n\nFixed an issue that caused the IP restriction in the host based authentication to not work correctly in all cases\n\nFixed a BWC issue with aggregation functions resolving in a mixed version cluster where at least one node is on version < 4.2.\n\nImproved the throttling behavior of INSERT INTO .. <query>, it is now more aggressive to reduce the amount of memory used by a INSERT INTO operation.\n\nFixed an issue which resulted in an error when a parameter symbol (placeholder) is used inside an aggregation.\n\nFixed an issue that could lead to the incorrect result of joining more than two tables even if the join condition is satisfied. Only the hash join implementation was affected by the issue.\n\nFixed a regression introduced in 4.2.3 that prevented primary key lookups with parameter placeholders from working in some cases.\n\nFixed an issue resulting wrongly in a RED sys.health.health state for healthy partitions with less shards configured than the actual partitioned table.\n\nFixed the resulting value for sys.health.partition_ident for non-partitioned tables. As documented, a NULL value should be returned instead of an empty string.\n\nFixed a performance regression that caused unnecessary traffic and load to the active master node when processing INSERT statements."
  },
  {
    "title": "Version 4.2.7 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.2.7.html",
    "html": "5.6\nVersion 4.2.7\n\nReleased on 2020-10-15.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.2.7.\n\nWe recommend that you upgrade to the latest 4.1 release before moving to 4.2.7.\n\nA rolling upgrade from 4.1.7+ to 4.2.7 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.2.0 release notes for a full list of changes in the 4.2 series.\n\nFixes\n\nFixed an issue that allowed users to create a self-referencing view which broke any further operations accessing table meta data.\n\nPrevent dropping of a UDF if it is still used inside any generated column expressions, throw an error instead."
  },
  {
    "title": "Version 4.0.4 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.0.4.html",
    "html": "5.6\nVersion 4.0.4\n\nReleased on 2019/08/21.\n\nNote\n\nPlease consult the Upgrade Notes before upgrading from CrateDB 3.x or earlier. Before upgrading to 4.0.4 you should be running a CrateDB cluster that is at least on 3.0.7.\n\nWe recommend that you upgrade to the latest 3.3 release before moving to 4.0.4.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 4.0.2. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nWarning\n\nTables that were created prior CrateDB 3.x will not function with 4.x and must be recreated before moving to 4.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.0.0 release notes for a full list of changes in the 4.0 series.\n\nFixes\n\nFixed an issue that would lead EXPLAIN of invalid statements to stuck instead of fail.\n\nFixed a regression introduced in 4.0 that broke the MATCH predicate if used on aliased relations.\n\nImproved error handling if an argument of a window function is not used as a grouping symbol.\n\nFixed an OUTER JOIN issue resulting in an ArrayOutOfBoundException if the gap between matching rows of the tables was growing to big numbers.\n\nFixed serialization issue that might occur in distributed queries that contain window function calls with the partition by clause in the select list.\n\nFixed a race condition which could result in a AlreadyClosedException when querying the sys.shards table."
  },
  {
    "title": "Version 4.0.5 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.0.5.html",
    "html": "5.6\nVersion 4.0.5\n\nReleased on 2019/09/19.\n\nNote\n\nPlease consult the Upgrade Notes before upgrading from CrateDB 3.x or earlier. Before upgrading to 4.0.5 you should be running a CrateDB cluster that is at least on 3.0.7.\n\nWe recommend that you upgrade to the latest 3.3 release before moving to 4.0.5.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 4.0.2. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nWarning\n\nTables that were created prior CrateDB 3.x will not function with 4.x and must be recreated before moving to 4.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.0.0 release notes for a full list of changes in the 4.0 series.\n\nFixes\n\nImproved the help section of the Admin UI including the Spanish translations.\n\nFixed an issue in the Admin UI to no longer display all columns as being generated columns in the table/column view section.\n\nFixed an issue introduced in CrateDB 4.0 resulting in dysfunctional disk-based allocation thresholds.\n\nFixed an issue resulting in pg_catalog.pg_attribute.attnum and information_schema.columns.ordinal_position being NULL on tables created with CrateDB < 4.0.\n\nFixed an issue resulting in NULL values when the ORDER BY symbol is a child of an ignored object column.\n\nFixed the Tables need to be recreated cluster check to list partitioned tables only once instead of once per partition.\n\nFixed the ssl.resource_poll_interval setting processing and documentation."
  },
  {
    "title": "Version 4.0.6 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.0.6.html",
    "html": "5.6\nVersion 4.0.6\n\nReleased on 2019/10/03.\n\nNote\n\nPlease consult the Upgrade Notes before upgrading from CrateDB 3.x or earlier. Before upgrading to 4.0.6 you should be running a CrateDB cluster that is at least on 3.0.7.\n\nWe recommend that you upgrade to the latest 3.3 release before moving to 4.0.6.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 4.0.2. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nWarning\n\nTables that were created prior CrateDB 3.x will not function with 4.x and must be recreated before moving to 4.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.0.0 release notes for a full list of changes in the 4.0 series.\n\nFixes\n\nFixed an issue that could prevent accounted memory from being properly de-accounted on queries using hyperloglog_distinct, leading clients to eventually receive CircuitBreakingException error messages and also breaking internal recovery operations.\n\nFixed an issue that caused the users list in the privileges tab to not displayed when the CrateDB Admin UI is not served from /.\n\nFixed various issues in the CrateDB Admin UI console.\n\nFixed an issue that caused the Twitter tutorial to not start automatically after the login redirect in the CrateDB Admin UI.\n\nFixed an issue that prevented subqueries from being used in select item expressions that also contain a reference accessed via a relation alias. For example: SELECT t.y IN (SELECT x FROM t2) FROM t1 t\n\nFail the storage engine if indexing on a replica shard fails after it was successfully done on a primary shard. It prevents replica and primary shards from going out of sync.\n\nFixed bug in the disk threshold decider logic that would ignore to account new relocating shard (STARTED to RELOCATING) when deciding how to allocate or relocate shards with respect to cluster.routing.allocation.disk.watermark.low and cluster.routing.allocation.disk.watermark.high settings.\n\nFixed regression that prevented shards from reallocation when a node passes over cluster.routing.allocation.disk.watermark.high.\n\nRemoved a case where a NullPointerException was logged if a HTTP client disconnected before a pending response could be sent to the client."
  },
  {
    "title": "Version 4.2.6 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.2.6.html",
    "html": "5.6\nVersion 4.2.6\n\nReleased on 2020-10-08.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.2.6.\n\nWe recommend that you upgrade to the latest 4.1 release before moving to 4.2.6.\n\nA rolling upgrade from 4.1.7+ to 4.2.6 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.2.0 release notes for a full list of changes in the 4.2 series.\n\nFixes\n\nFixed a regression introduced in 4.2 which prevented DELETE FROM ... statements with the subquery expression in the where clause from complete deletion of the matching partitions in partitioned tables.\n\nFixed an issue that prevented casts from DOUBLE PRECISION to REAL for the minimal supported REAL number -3.4028235e38.\n\nFixed an issue that prevented an access to the properties of object type arguments in JavaScript user-defined functions.\n\nFixed a regression introduced in 4.2 that could cause subscript expressions to fail with a Base argument to subscript must be an object, not null or Can't handle Symbol error.\n\nFixed an issue causing a node crash due to OOM when running the analyze on large tables.\n\nFixed an issue that caused queries involving a JOIN operation on system tables like sys.cluster to fail.\n\nChanged the memory reservation and circuit breaker behavior for INSERT FROM QUERY operations to allow for more concurrent operations. After the change introduced in 4.2.5, individual operations could reserve too much memory, causing other operations to fail with a circuit breaker exception."
  },
  {
    "title": "Version 4.3.0 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.3.0.html",
    "html": "5.6\nVersion 4.3.0\n\nReleased on 2020-10-16.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.3.0.\n\nWe recommend that you upgrade to the latest 4.2 release before moving to 4.3.0.\n\nA rolling upgrade from 4.2.6+ to 4.3.0 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nBreaking Changes\n\nDeprecations\n\nChanges\n\nPerformance improvements\n\nSQL Standard and PostgreSQL compatibility improvements\n\nAdministration\n\nError handling improvements\n\nBreaking Changes\n\nAdded support for the g flag to function regexp_matches and changed its type from scalar to table. It now returns a table where each row contains a single column groups of type array(text).\n\nChanged values of information_schema.columns.ordinal_position and pg_catalog.pg_attribute.attnum for object data type sub-columns from NULL to an incremented ordinal. Values of top-level columns may change if their position is after an object type column.\n\nDeprecations\n\nDeprecated the *.overhead setting for all circuit breakers. It now defaults to 1.0 for all of them and changing it has no effect.\n\nDeprecated the indices.breaker.fielddata.limit and indices.breaker.fielddata.overhead settings. These no longer have any effect as there is no fielddata cache anymore.\n\nChanges\nPerformance improvements\n\nImproved the performance of aggregations in various use cases. In some scenarios we saw performance improvements by up to 70%.\n\nChanged the default for soft deletes. Soft deletes are now enabled by default for new tables. This should improve the speed of recovery operations when replica shard copies were unavailable for a short period.\n\nSQL Standard and PostgreSQL compatibility improvements\n\nAdded scalar function translate(string, from, to).\n\nAdded support for SET AND RESET SESSION AUTHORIZATION SQL statements.\n\nAdded scalar function pg_get_function_result.\n\nAdded scalar function pg_function_is_visible.\n\nAdded table function generate_subscripts\n\nAdded the pg_catalog.pg_roles table\n\nAdded full support for quoted subscript expressions like \"myObj['x']\". This allows to use tools like PowerBI with tables that contain object columns.\n\nAdministration\n\nAdded a new cluster.max_shards_per_node cluster setting that limits the amount of shards that can be created per node. Once the limit is reached, operations that would create new shards will be rejected.\n\nAdded the read_only_allow_delete setting to the settings['blocks'] column of the information_schema.tables and information_schema.table_partitions tables.\n\nChanged OPTIMIZE to no longer implicitly refresh a table.\n\nChanged the privileges for KILL, all users are now allowed to kill their own statements.\n\nRemoved the Twitter tutorial from the Admin Console.\n\nError handling improvements\n\nAdded detailed information on the error when a column with an undefined type is used to GROUP BY.\n\nAdded detailed information to possible errors on repository creation to give better insights on the root cause of the error.\n\nChanged the error code for the PostgreSQL wire protocol from XX000 internal_error when:\n\na user defined function is missing to 42883 undefined_function\n\na column alias is ambiguous to 42P09 ambiguous_alias\n\na schema name is invalid to 3F000 invalid_schema_name\n\na column reference is ambiguous to 42702 ambiguous_column\n\na relation exists already to 42P07 duplicate_table\n\na column does not exist to 42703 undefined_column\n\na relation does not exist to 42P01 undefined_table\n\na document exists already to 23505 unique_violation\n\nChanged the error code for dropping a missing view from the undefined 4040 to 4041.\n\nChanged the error handling so it returns the error message and the related exception without being wrapped in a SqlActionException. Error codes remain the same."
  },
  {
    "title": "Version 4.1.8 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.1.8.html",
    "html": "5.6\nVersion 4.1.8\n\nReleased on 2020-07-07.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.1.8.\n\nWe recommend that you upgrade to the latest 4.0 release before moving to 4.1.8.\n\nA rolling upgrade to 4.1.8 from 4.0.2+ is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.1.0 release notes for a full list of changes in the 4.1 series.\n\nFixes\n\nFixed an issue where access_key and secret_key settings for s3 repositories were exposed as unmasked settings.\n\nFixed an issue that would prevent the pg_stats table from querying if the most_common_vals or histogram_bounds columns contain values of the OBJECT or ARRAY types."
  },
  {
    "title": "Version 4.3.1 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.3.1.html",
    "html": "5.6\nVersion 4.3.1\n\nReleased on 2020-10-29.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.3.1.\n\nWe recommend that you upgrade to the latest 4.2 release before moving to 4.3.1.\n\nA rolling upgrade from 4.2.6+ to 4.3.1 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nFixes\n\nFixed an issue that could cause COPY or INSERT FROM QUERY operations to fail with a circuit breaking exception, even if more memory would’ve been available.\n\nFixed an issue that would prevent function resolution for arguments that contain both text types with and without length limit. For example, statements as following would fail with the unknown function exception:\n\nCREATE TABLE tbl (str VARCHAR(3))\nSELECT * FROM tbl WHERE str = 'x'\n\n\nImproved the validation logic for CREATE TABLE and ALTER TABLE statements to prevent users from creating tables that cannot be used due to an invalid schema definition.\n\nChanged the DROP TABLE logic to allow super users to drop tables with a corrupted schema.\n\nFixed an issue that could lead to a IndexShardClosedException when querying the sys.shards table.\n\nFixed an issue that led to a spike in the snapshot thread pool queue when taking snapshots.\n\nFixed an issue that caused an error when a client using the PostgreSQL wire protocol is used to retrieve array values containing a escaped double quotes. For example, the JDBC client failed with Can't extract array data from JDBC array."
  },
  {
    "title": "Version 4.3.4 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.3.4.html",
    "html": "5.6\nVersion 4.3.4\n\nReleased on 2021-01-19.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.3.4.\n\nWe recommend that you upgrade to the latest 4.2 release before moving to 4.3.4.\n\nA rolling upgrade from 4.2.6+ to 4.3.4 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nFixes\n\nFixed an issue inside the table version and node version compatibility check which prevented downgrading from one hotfix version to another one.\n\nFixed an issue that could lead to a String index out of range error when streaming values of type TIMESTAMP WITH TIME ZONE using a PostgreSQL client.\n\nFixed an issue that could lead to errors like Can't map PGType with oid=26 to Crate type using a PostgreSQL client.\n\nFixed an issue that could result in a The assembled list of ParameterSymbols is invalid. Missing parameters. error if using the MATCH predicate and parameter placeholders within a query.\n\nBumped JNA library to version 5.6.0. This will make CrateDB start flawlessly and without warnings on recent versions of Windows."
  },
  {
    "title": "Version 4.0.7 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.0.7.html",
    "html": "5.6\nVersion 4.0.7\n\nReleased on 2019/10/24.\n\nNote\n\nPlease consult the Upgrade Notes before upgrading from CrateDB 3.x or earlier. Before upgrading to 4.0.7 you should be running a CrateDB cluster that is at least on 3.0.7.\n\nWe recommend that you upgrade to the latest 3.3 release before moving to 4.0.7.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 4.0.2. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nWarning\n\nTables that were created prior CrateDB 3.x will not function with 4.x and must be recreated before moving to 4.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.0.0 release notes for a full list of changes in the 4.0 series.\n\nFixes\n\nFixed the handling of array values when used in the SET GLOBAL statement.\n\nImproved the handling of NULL values in SET GLOBAL statement. They now no longer cause a NullPointerException but instead advice users to use RESET GLOBAL to reset settings to their default value.\n\nFixed evaluation of generated columns when they are based on columns with default constraints and no user given values. Default constraints where not taken into account before.\n\nFixed an issue when using try_cast('invalid-ts' as timestamp) which resulted in a parsing exception instead of an expected NULL value.\n\nTuned the circuit breaker mechanism to reduce the chance of it rejecting queries under low cluster load.\n\nClosed some gaps in the memory accounting used for the circuit breaker mechanism. This should help prevent memory intense queries from triggering long GC pauses as they’ll be rejected earlier.\n\nMade the documented indices.breaker.total.limit setting public, so that it can be adjusted using SET GLOBAL.\n\nImproved the migration logic for partitioned tables which have been created in CrateDB 2.x. If all current partitions of a partitioned tables have been created in CrateDB 3.x, the table won’t have to be re-indexed anymore to upgrade to CrateDB 4.0+.\n\nChanged the error message returned when a CREATE REPOSITORY statement fails so that it includes more information about the cause of the failure."
  },
  {
    "title": "Version 4.0.8 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.0.8.html",
    "html": "5.6\nVersion 4.0.8\n\nReleased on 2019/11/07.\n\nNote\n\nPlease consult the Upgrade Notes before upgrading from CrateDB 3.x or earlier. Before upgrading to 4.0.8 you should be running a CrateDB cluster that is at least on 3.0.7.\n\nWe recommend that you upgrade to the latest 3.3 release before moving to 4.0.8.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 4.0.2. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nWarning\n\nTables that were created prior CrateDB 3.x will not function with 4.x and must be recreated before moving to 4.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.0.0 release notes for a full list of changes in the 4.0 series.\n\nFixes\n\nImproved the handling of sorted queries with a large limit, to reduce the chance of them causing a out of memory error.\n\nFixed a NullPointerException that could occur when querying the settings column of information_schema.table_partitions.\n\nFixed an issue in the Admin interface that caused the pagination Previous button to not display the whole list of results for that page in the console view.\n\nFixed an issue that could prevent CREATE SNAPSHOT from succeeding, resulting in a partial snapshot which contained failure messages incorrectly indicating that the index is corrupt.\n\nFixed an issue resulting in a parsing exception on SHOW TABLE statements when a default expression is implicitly cast to the related column type and the column type contains a SPACE character (like e.g. double precision)."
  },
  {
    "title": "Version 4.2.1 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.2.1.html",
    "html": "5.6\nVersion 4.2.1\n\nReleased on 2020-07-14.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.2.1.\n\nWe recommend that you upgrade to the latest 4.1 release before moving to 4.2.1.\n\nA rolling upgrade from 4.1.7+ to 4.2.1 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.2.0 release notes for a full list of changes in the 4.2 series.\n\nFixes\n\nFixed an issue with the quote_ident scalar function that caused it to quote subscript expressions like \"col['x']\" instead of \"col\"['x'].\n\nFixed an issue that prevented the use of subscript expressions as conflict target in ON CONFLICT clauses of INSERT statements.\n\nFixed an issue where drop snapshot on an azure repository would not delete all the related data.\n\nFixed an issue that could lead to a Field is not streamable error message when using window functions."
  },
  {
    "title": "Version 4.2.0 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.2.0.html",
    "html": "5.6\nVersion 4.2.0\n\nReleased on 2020-07-07.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.2.0.\n\nWe recommend that you upgrade to the latest 4.1 release before moving to 4.2.0.\n\nA rolling upgrade from 4.1.7+ to 4.2.0 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nBreaking Changes\n\nDeprecations\n\nChanges\n\nAdministration\n\nSQL Standard and PostgreSQL compatibility improvements\n\nFunctions and operators\n\nNew statements and clauses\n\nPerformance improvements\n\nBreaking Changes\n\nChanged the logic how the array_unique scalar function infers the argument types. Previously if the arguments had a different type it used the type of the first argument. For example:\n\ncr> select array_unique(['1'], [1.0, 2.0]);\n+---------------------------------+\n| array_unique(['1'], [1.0, 2.0]) |\n+---------------------------------+\n| [\"1\", \"1.0\", \"2.0\"]             |\n+---------------------------------+\n\n\nThis logic has been changed to instead use a type precedence logic to be consistent with how other functions behave:\n\ncr> select array_unique(['1'], [1.0, 2.0]);\n+------------+\n| [1.0, 2.0] |\n+------------+\n| [1.0, 2.0] |\n+------------+\n\n\nBulk INSERT INTO ... VALUES (...) statements do not throw an exception any longer when one of the bulk operations fails. The result of the execution is only available via the results array represented by a row count for each bulk operation.\n\nNumeric literals fitting into the integer range are now treated as integer literals instead of bigint literals. Thus a statement like select 1 will return an integer column type. This shouldn’t be an issue for most clients as the HTTP endpoint uses JSON for serialization and PostgreSQL clients usually use a typed getLong.\n\nA consequence of this change is that arithmetic with literals used in statements will also happen within the integer range, and it can lead to an integer overflow if the result does not fit into the integer range. For example, a calculation like 365 * 24 * 60 * 60 * 1000 needs to be explicitly cast into the bigint range to avoid an integer overflow. This can be done like this: 365::bigint * 24 * 60 * 60 * 1000. If working with timestamps it is recommended to instead use the more readable interval type instead: '1 year'::interval.\n\nIn the PostgreSQL Wire Protocol, ReadyForQuery messages now contain the IN TRANSACTION or FAILED TRANSACTION indicator based on previous BEGIN and COMMIT SQL statements. Before this change, the status always defaulted to IDLE. This change may have the side-effect that an explicit rollback call in a client library will result in an unsupported ROLLBACK statement.\n\nDeprecations\n\nThe index.warmer.enabled setting has been deprecated and doesn’t have any effect anymore.\n\nChanges\nAdministration\n\nThe JavaScript user-defined function language is now enabled by default in the CrateDB enterprise edition.\n\nAdded the optimizer session setting to configure query optimizer rules.\n\nInclude the bundled version of OpenJDK (14.0.1+7) into the CrateDB built. It means that CrateDB doesn’t rely on JAVA_HOME of the host system any longer.\n\nIncreased the default interval to detect keystore or truststore changes to five minutes.\n\nAdded a tables column to the sys.snapshots table which lists the fully qualified name of all tables contained within the snapshot.\n\nLimit the output of COPY FROM RETURN SUMMARY in the presence of errors to display up to 50 line_numbers to avoid buffer pressure at clients and to improve readability.\n\nSQL Standard and PostgreSQL compatibility improvements\n\nAdded scalar function CURRENT_TIME, that returns the system’s time as microseconds since midnight UTC, at the time the SQL statement is handled. The actual return type is the new data type timetz.\n\nAdded new type time with time zone, a.k.a timetz, which is to be used as return type for time related functions such as the future current_time.\n\nAdded the OIDVECTOR data type which is used in some pg_catalog tables.\n\nAdded the REGPROC alias data type that is used to reference functions in the pg_catalog tables.\n\nAdded the varchar(n) and character varying(n) types, where n is an optional length limit.\n\nAdded the server_version_num and server_version read-only session settings.\n\nAdded the pg_catalog.pg_proc table.\n\nAdded the pg_catalog.pg_range table.\n\nAdded the pg_catalog.pg_enum table.\n\nAdded the information_schema.character_sets table.\n\nAdded pg_type columns: typbyval, typcategory, typowner, typisdefined, typrelid, typndims, typcollation, typinput, typoutput, and typndefault for improved PostgreSQL compatibility.\n\nAdded support for JOIN USING, e.g. SELECT * FROM t1 JOIN t2 USING (col), an alternative to JOIN ON, when the column name(s) are the same in both relations.\n\nAdded entries for primary keys to pg_class and pg_index table.\n\nAdded support for record subscript syntax as alternative to the existing object subscript syntax.\n\nAdded support for using columns of type long inside subscript expressions (e.g., array_expr[column]).\n\nMade generate_series addressable by specifying the pg_catalog schema explicitly. So, for example, both generate_series(1, 2) and pg_catalog.generate_series(1, 2) are valid.\n\nAdded support for the PostgreSQL notation to refer to array types. For example, it is now possible to use text[] instead of array(test).\n\nAdded support for GROUP BY operations on analysed columns of type text.\n\nFunctions and operators\n\nFixed arithmetics containing a non-floating numeric column type and a floating literal which resulted wrongly in a non-floating return type.\n\nReplaced the Nashorn JavaScript engine with GraalVM for JavaScript user-defined functions. This change upgrades ECMAScript support from 5.1 to 10.0.\n\nAdded the chr scalar function.\n\nAdded length and repeat scalar functions.\n\nAdded the array_agg aggregation function.\n\nAdded the trunc scalar function.\n\nAdded the now scalar function.\n\nAdded a mod alias for the modulus function for improved PostgreSQL compatibility.\n\nAdded the atan2 trigonometric scalar function.\n\nAdded the exp scalar function.\n\nAdded the degrees and radians scalar functions.\n\nAdded support for using table functions with more than one column within the select list part of a SELECT statement.\n\nAdded the cot trigonometric scalar function.\n\nAdded the pi scalar function.\n\nAdded a ceiling alias for the ceil function for improved PostgreSQL compatibility.\n\nAdded the encode(bytea, format) and decode(text, format) scalar functions.\n\nAdded the ascii scalar function.\n\nAdded the obj_description(integer, text) scalar function for improved PostgreSQL compatibility.\n\nAdded the format_type(integer, integer) scalar function for improved PostgreSQL compatibility.\n\nAdded the version() system information scalar function.\n\nNew statements and clauses\n\nExtended the supported syntax for SET TRANSACTION.\n\nAdded the DISCARD statement.\n\nAdded the CHECK constraint syntax, which specifies that the values of certain columns must satisfy a boolean expression on insert and update.\n\nIntroduced new optional RETURNING clause for INSERT and UPDATE to return specified values from each row written.\n\nPerformance improvements\n\nOptimized <column> IS NOT NULL queries."
  },
  {
    "title": "Version 4.3.2 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.3.2.html",
    "html": "5.6\nVersion 4.3.2\n\nReleased on 2020-11-25.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.3.2.\n\nWe recommend that you upgrade to the latest 4.2 release before moving to 4.3.2.\n\nA rolling upgrade from 4.2.6+ to 4.3.2 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nFixes\n\nFixed an issue that could cause browsers to prompt for a client certificate if SSL is enabled on the server side, even if no cert authentication method is configured.\n\nFixed a regression introduced in CrateDB 4.1 resulting in duplicated recovery file chunk responses sent. This causes log entries of Transport handler not found ....\n\nFixed an issue that resulted in records in pg_catalog.pg_proc which wouldn’t be joined with pg_catalog.pg_type. Clients like npgsql use this information and without it, the users received an error like The CLR array type System.Int32[] isn't supported by Npgsql or your PostgreSQL if using array types.\n\nFixed an issue that could lead to stuck INSERT INTO .. RETURNING queries.\n\nFixed a regression introduced in CrateDB >= 4.3 which prevents using regexp_matches() wrapped inside a subscript expression from being used as a GROUP BY expression.\n\nThis fixed the broken Admin UI -> Monitoring tab as it uses such a statement.\n\nFixed validation of GROUP BY expressions if an alias is used. The validation was by passed and resulted in an execution exception instead of an user friendly validation exception.\n\nFixed an issue that caused IS NULL and IS NOT NULL operators on columns of type OBJECT with the column policy IGNORED to match incorrect records.\n\nFixed an issue that led to an error like UnsupportedOperationException: Can't handle Symbol [ParameterSymbol: $1] if using INSERT INTO with a query that contains parameter place holders and a LIMIT clause.\n\nFixed an issue that led to an error when nesting multiple table functions."
  },
  {
    "title": "Version 4.3.3 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.3.3.html",
    "html": "5.6\nVersion 4.3.3\n\nReleased on 2021-01-06.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.3.3.\n\nWe recommend that you upgrade to the latest 4.2 release before moving to 4.3.3.\n\nA rolling upgrade from 4.2.6+ to 4.3.3 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nFixes\n\nFixed an issue in the PostgreSQL wire protocol that prevented values from being serialized correctly if the client didn’t send a describe message and used the binary serialization format.\n\nFixed an issue resulting in an error when using a ORDER BY clause inside the subquery of a INSERT INTO statement.\n\nFixed a regression introduced in CrateDB 4.2.0 leading to a NPE when copying data from one table to another using INSERT INTO ... while the source table contains more than 128 columns.\n\nFixed an issue resulting in the full generated expression as the column name inside data exported by COPY TO statements.\n\nFixed an issue resulting double-quoted column names inside data exported by COPY TO statements.\n\nFixed a memory leak in the DNS discovery seed provider. The memory leak occurred if you configured discovery.seed_providers=srv.\n\nFixed a regression introduced in CrateDB 4.0 preventing the global setting cluster.info.update.interval to be changed.\n\nFixed handling of spaces in $CRATE_HOME. Users would get a No such file or directory error if the path set via $CRATE_HOME contained spaces."
  },
  {
    "title": "Version 4.4.0 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.4.0.html",
    "html": "5.6\nVersion 4.4.0\n\nReleased on 2021-01-19.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.4.0.\n\nWe recommend that you upgrade to the latest 4.3 release before moving to 4.4.0.\n\nA rolling upgrade from 4.3.x to 4.4.0 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nDeprecations\n\nChanges\n\nPerformance improvements\n\nSQL and PostgreSQL compatibility improvements\n\nAdministration and Operations\n\nNew scalar and window functions\n\nDeprecations\n\nThe settings discovery.zen.publish_timeout, discovery.zen.commit_timeout, discovery.zen.no_master_block, discovery.zen.publish_diff.enable have been marked as deprecated and will be removed in a future version.\n\nChanges\nPerformance improvements\n\nImproved the performance of queries on the sys.health table.\n\nAdded support for using the optimized primary key lookup plan if additional filters are combined via AND operators.\n\nImproved the performance of queries on the sys.allocations table in cases where there are filters restricting the result set or if only a sub-set of the columns is selected.\n\nImproved the performance for queries which select a subset of the columns available in a wide table.\n\nSQL and PostgreSQL compatibility improvements\n\nAdded arithmetic operation support for the numeric type.\n\nAdded support for the numeric data type and allow the sum aggregation on the numeric type. Note that the storage of the numeric data type is not supported.\n\nExtended the RowDescription message that can be sent while communicating with PostgreSQL clients to include a table_oid and a attr_num based on the values that are also exposed via the pg_catalog.pg_attribute table. This improves compatibility with clients which make use of these attributes.\n\nChanged the format_type function to use the PostgreSQL compatible type name notation with [] suffixes for arrays, instead of _array.\n\nAdded the delimiter option for COPY FROM CSV files. The option is used to specify the character that separates columns within a row.\n\nAdded the empty_string_as_null option for COPY FROM CSV files. If the option is enabled, all column’s values represented by an empty string, including a quoted empty string, are set to NULL.\n\nAdministration and Operations\n\nAdded information about the shards located on the node to the NodeInfo MXBean which is available as an enterprise feature.\n\nAdded the sys.snapshot_restore table to track the progress of the snapshot restore operations.\n\nNew scalar and window functions\n\nAdded the to_char scalar function for timestamp and interval argument data types.\n\nAdded the split_part scalar function.\n\nAdded the dense_rank window function, which is available as an enterprise feature.\n\nAdded the rank window function, which is available as an enterprise feature."
  },
  {
    "title": "Version 4.0.9 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.0.9.html",
    "html": "5.6\nVersion 4.0.9\n\nReleased on 2019/11/25.\n\nNote\n\nPlease consult the Upgrade Notes before upgrading from CrateDB 3.x or earlier. Before upgrading to 4.0.9 you should be running a CrateDB cluster that is at least on 3.0.7.\n\nWe recommend that you upgrade to the latest 3.3 release before moving to 4.0.9.\n\nIf you want to perform a rolling upgrade, your current CrateDB version number must be at least Version 4.0.2. Any upgrade from a version prior to this will require a full restart upgrade.\n\nWhen restarting, CrateDB will migrate indexes to a newer format. Depending on the amount of data, this may delay node start-up time.\n\nWarning\n\nTables that were created prior CrateDB 3.x will not function with 4.x and must be recreated before moving to 4.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.0.0 release notes for a full list of changes in the 4.0 series.\n\nFixes\n\nFixed an incompatibility in the PostgreSQL wire protocol which could cause queries being sent using the asyncpg python client to get stuck. (Using version 0.20 of the client).\n\nFixed two display issues in the Admin UI:\n\n0 values in the partitions view for tables could be incorrectly displayed as NULL.\n\nThe node health in the cluster view was not displayed when the name of a node was too long.\n\nFixed a regression introduced in 4.0.8 which could cause queries with an explicit limit larger than 1000 to fail with a numHits must be > 0 error.\n\nFixed an resiliency issue on snapshot creation while dynamic columns are created concurrently which may result in incompatibility problems on restore.\n\nFixed case sensitivity of unquoted column names inside ON CONFLICT clauses."
  },
  {
    "title": "Version 4.4.2 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.4.2.html",
    "html": "5.6\nVersion 4.4.2\n\nReleased on 2021-03-02.\n\nNote\n\nIf upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.4.2.\n\nWe recommend that you upgrade to the latest 4.3 release before moving to 4.4.2.\n\nA rolling upgrade from 4.3.0+ to 4.4.2 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.4.0 release notes for a full list of changes in the 4.4 series.\n\nFixes\n\nFixed an issue in the error handling of CREATE REPOSITORY statements which could lead to a NullPointerException instead of a more meaningful error message.\n\nFixed an issue that could lead to a Can't handle Symbol error when using views which are defined with column aliases.\n\nFixed a regression that led to max aggregations on columns of type double precision or real to return null instead of 0.0 if all aggregated values are 0.0.\n\nFixed an issue that could lead to an OutOfMemoryError when retrieving large result sets with large individual records.\n\nFixed an issue that could lead to a NullPointerException when using a SELECT statement containing an INNER JOIN.\n\nFixed an issue in the PostgreSQL wire protocol that would cause de-serialization of arrays to fail if they contained unquoted strings starting with digits.\n\nFixed an issue that could lead to a serialization error when streaming values of the TIMESTAMP WITHOUT TIME ZONE type in text format using the PostgreSQL wire protocol."
  },
  {
    "title": "Version 4.4.3 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.4.3.html",
    "html": "5.6\nVersion 4.4.3\n\nReleased on 2021-03-23.\n\nNote\n\nIf upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.4.3.\n\nWe recommend that you upgrade to the latest 4.3 release before moving to 4.4.3.\n\nA rolling upgrade from 4.3.0+ to 4.4.3 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.4.0 release notes for a full list of changes in the 4.4 series.\n\nFixes\n\nFixed an issue that led to a CircuitBreakingException when using the ANALYZE statement.\n\nFixed an issue that led to Values less than -1 bytes errors if TRACE logging was activated for the circuit breaker package.\n\nFixed shard allocation on downgraded nodes where only the HOTFIX version part differs to fully support rolling downgrades to same MAJOR.MINOR versions.\n\nFixed an issue that could lead to a stuck INNER JOIN query involving the sys.shards table on a cluster without user tables.\n\nAdjusted crate.bat to work with spaces in directory names.\n\nAdjusted crate-node auxiliary program to use the bundled Java runtime.\n\nFixed a NullPointerException when trying to kill a job as normal user which is no longer running."
  },
  {
    "title": "Version 4.4.1 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.4.1.html",
    "html": "5.6\nVersion 4.4.1\n\nReleased on 2021-02-03.\n\nNote\n\nIf upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.4.1.\n\nWe recommend that you upgrade to the latest 4.3 release before moving to 4.4.1.\n\nA rolling upgrade from 4.3.0+ to 4.4.1 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.4.0 release notes for a full list of changes in the 4.4 series.\n\nFixes\n\nFixed an issue that could lead to stuck queries when joining more than two relations and if the WHERE clause contained filters on primary key columns that led to an execution plan using a primary key lookup on at least one relation.\n\nFixed an issue in the PostgreSQL wire protocol implementation that could lead to incorrect results on the client side if a client invoked a write operation and sent an explicit flush message before the sync message. The node-postgres client is one client that uses such a message sequence.\n\nFixed an issue that could lead to a stream has already been operated upon or closed error when using primary key lookup operations as part of a query with several JOIN clauses.\n\nFixed a regression that caused queries which used a parameter placeholder in a join condition to fail with an The assembled list of ParameterSymbols is invalid. Missing parameters error."
  },
  {
    "title": "Version 4.5.1 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.5.1.html",
    "html": "5.6\nVersion 4.5.1\n\nReleased on 2021-05-03.\n\nNote\n\nIf upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.5.1.\n\nWe recommend that you upgrade to the latest 4.4 release before moving to 4.5.1.\n\nA rolling upgrade from 4.4.0+ to 4.5.1 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.5.0 release notes for a full list of changes in the 4.5 series.\n\nFixes\n\nFixed an issue that could cause queries on virtual tables to return an incorrect result if a table function is used in the select-list of a sub-query, but not used in the outputs of the parent relation. An example:\n\nSELECT name FROM (SELECT name, unnest(tags) FROM metrics) m;\n\n\nUpdated the bundled JDK to 16.0.1+9\n\nFixed an issue that would cause columns of type varchar with a length limited to be incorrectly casted to another type if used as argument in a function that has several type overloads.\n\nFixed an issue that caused ALTER TABLE ADD COLUMN statements to remove constraints like analyzers or NOT NULL from existing columns in the same table.\n\nAllow executing CREATE TABLE .. AS as a regular user with DDL permission on the target schema, and DQL permission on the source relations.\n\nChanged the RowDescription message that is sent to PostgreSQL clients to avoid that the JDBC client triggers queries against pg_catalog schema tables each time information from the MetaData of a ResultSet is accessed.\n\nFixed crate-node auxiliary program to use the bundled Java runtime on Linux."
  },
  {
    "title": "Version 4.5.3 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.5.3.html",
    "html": "5.6\nVersion 4.5.3\n\nReleased on 2021-06-22.\n\nNote\n\nIf upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.5.3.\n\nWe recommend that you upgrade to the latest 4.4 release before moving to 4.5.3.\n\nA rolling upgrade from 4.4.0+ to 4.5.3 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.5.0 release notes for a full list of changes in the 4.5 series.\n\nFixes\n\nFixed a regression introduced in 4.5.2 which caused aggregations on virtual tables using a primary key lookup to fail. An example:\n\nSELECT count(*) FROM (\n  SELECT * FROM users WHERE id = ? AND (addr is NULL)\n) AS u;\n"
  },
  {
    "title": "Version 4.5.5 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.5.5.html",
    "html": "5.6\nVersion 4.5.5\n\nReleased on 2021-07-20.\n\nNote\n\nIf upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.5.5.\n\nWe recommend that you upgrade to the latest 4.4 release before moving to 4.5.5.\n\nA rolling upgrade from 4.4.0+ to 4.5.5 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.5.0 release notes for a full list of changes in the 4.5 series.\n\nFixes\n\nFixed an issue that could lead to NULL values getting returned when using a _doc['columnName'] expression in the ORDER BY clause. Prior to 4.6.0 this also affected INSERT INTO statements on top level columns.\n\nFixed an issue that caused a class cast error when trying to import records from a CSV file into a partitioned table using the COPY FROM statement."
  },
  {
    "title": "Version 4.5.4 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.5.4.html",
    "html": "5.6\nVersion 4.5.4\n\nReleased on 2021-07-13.\n\nNote\n\nIf upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.5.4.\n\nWe recommend that you upgrade to the latest 4.4 release before moving to 4.5.4.\n\nA rolling upgrade from 4.4.0+ to 4.5.4 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.5.0 release notes for a full list of changes in the 4.5 series.\n\nFixes\n\nFixed a regression that caused the job_id column within the sys.operations_log table to return the id values instead of the job_id values.\n\nFixed an issue that could result in a IOException: can not write type ... error when combining values of type TIMETZ, NUMERIC, GEO_POINT or INTERVAL with values of type UNDEFINED.\n\nFixed an issue that caused INSERT FROM VALUE statements to insert records, despite failing validation and returning an error to the client.\n\nFixed an issue that caused the NOW() and CURRENT_USER functions to get normalized to a literal value when used as part of a generated column or DEFAULT expression in a CREATE TABLE statement.\n\nFixed a HBA SSL configuration parsing issue. The on value for the ssl configuration option was not recognized and got interpreted as ‘true’."
  },
  {
    "title": "Version 4.6.3 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.6.3.html",
    "html": "5.6\nVersion 4.6.3\n\nReleased on 2021-09-08.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.6.3.\n\nWe recommend that you upgrade to the latest 4.3 release before moving to 4.6.3.\n\nA rolling upgrade from 4.5.x to 4.6.3 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.6.0 release notes for a full list of changes in the 4.6 series.\n\nFixes\n\nFixed a performance regression introduced in 4.2 which caused queries with a LIMIT on top of views or virtual tables with an ORDER BY to be slow.\n\nFixed an issue in the Query View function of the administration console. It generated queries with extra quotes around identifiers.\n\nFixed an issue that could cause clients to receive a 400 Bad Request error when using the HTTP interface early during node startup.\n\nFixed an issue that resulted in broken values when selecting multiple object columns with different inner types using the UNION statement.\n\nFixed an issue which caused object data types definitions with sub-column identifiers containing white spaces to result in a validation exception at CrateDB 4.6.2 or a unusable object type column (write/reads fail) at CrateDB 4.2.0 to 4.6.1."
  },
  {
    "title": "Version 4.6.6 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.6.6.html",
    "html": "5.6\nVersion 4.6.6\n\nReleased on 2021-12-13.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.6.6.\n\nWe recommend that you upgrade to the latest 4.3 release before moving to 4.6.6.\n\nA rolling upgrade from 4.5.x to 4.6.6 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.6.0 release notes for a full list of changes in the 4.6 series.\n\nFixes\n\nUpdated log4j to 2.15.0 to fix a security vulnerability. See Log4Shell for details.\n\nFixed an issue that could result in an error if a client sent multiple statements in a single string using the PostgreSQL simple protocol mode.\n\nImproved the detection of the cgroup version to support kernels where the cpu.stat and memory.stat files weren’t available at the cgroup root level.\n\nFixed an issue that could cause a deadlock, leading to an unavailable cluster if using blob tables and uploading multiple files in parallel.\n\nFixed an issue that was triggered by adding a column to a table with existing generated columns with ALTER TABLE. It caused the generated columns to become non-generated such that it could no longer generate the values."
  },
  {
    "title": "Version 4.6.4 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.6.4.html",
    "html": "5.6\nVersion 4.6.4\n\nReleased on 2021-09-30.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.6.4.\n\nWe recommend that you upgrade to the latest 4.3 release before moving to 4.6.4.\n\nA rolling upgrade from 4.5.x to 4.6.4 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.6.0 release notes for a full list of changes in the 4.6 series.\n\nFixes\n\nImproved the internal dynamic batching mechanism for operations like INSERT INTO FROM QUERY and COPY FROM. It should more aggressively throttle these operations in case of memory pressure, reducing the chance of them failing with a CircuitBreakingException.\n\nFixed a second issue that could cause clients to receive a 400 Bad Request error when using the HTTP interface early during node startup. The previous fix within the 4.6.3 release was incomplete.\n\nReduced the default initial concurrency limit for operations like INSERT INTO FROM QUERY from 50 to 5. This is closer to the behavior before 4.6.0. If the nodes have spare capacity for a higher concurrency the effective concurrency limit will grow dynamically over time, but it will start out lower to avoid overloading a cluster with an initial spike of internal requests.\n\nAdded various overload protection settings to control the concurrency of operations like INSERT INTO FROM QUERY.\n\nFixed an issue that caused ALTER TABLE <tbl> ADD COLUMN <columName> INDEX USING FULLTEXT statements to ignore the INDEX USING FULLTEXT part.\n\nFixed a performance regression introduced in 4.2 which could cause queries including joins, virtual tables and LIMIT operators to run slower than before.\n\nFixed an issue that caused INSERT INTO statements to fail on partitioned tables where the partitioned column is generated and the column and value are provided in the statement.\n\nFixed an issue that caused showing an incorrect log message in case of an authentication failure. “Password authentication” used to be shown instead of the actual authentication method name.\n\nFixed an issue that caused NullPointerException when inserting into previously altered tables that were partitioned and had generated columns.\n\nFixed an issue in the administration console: Nested arrays in OBJECT(IGNORED) columns will now be displayed correctly."
  },
  {
    "title": "Version 4.6.5 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.6.5.html",
    "html": "5.6\nVersion 4.6.5\n\nReleased on 2021-11-12.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.6.5.\n\nWe recommend that you upgrade to the latest 4.3 release before moving to 4.6.5.\n\nA rolling upgrade from 4.5.x to 4.6.5 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.6.0 release notes for a full list of changes in the 4.6 series.\n\nFixes\n\nFixed an issue that caused UNION ALL statements to succeed or throw unexpected exceptions when the SELECT results for UNION ALL included object types with identically named but differently typed sub-columns.\n\nFixed an issue that caused date_format() to return wrong values when used with the %D specifier (day of month as ordinal number) for 11th, 12th and 13th.\n\nFixed a performance regression introduced in 4.2 which caused queries with WHERE clause on aliased column on top of views or virtual tables to be slow.\n\nFixed an issue in HBA which caused entries with method cert for one protocol affect entries of other protocols in the way that client certificate is requested for trust or password entries of other protocols.\n\nFixed an issue that threw an unexpected exception while converting a WKT string representing a valid polygon to geo_shape."
  },
  {
    "title": "Version 4.8.2 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.8.2.html",
    "html": "5.6\nVersion 4.8.2\n\nReleased on 2022-07-11.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.8.2.\n\nWe recommend that you upgrade to the latest 4.7 release before moving to 4.8.2.\n\nA rolling upgrade from 4.7.x to 4.8.2 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.8.0 release notes for a full list of changes in the 4.8 series.\n\nFixes\n\nFixed an issue that caused queries reading values of type BIT to fail if the query contains a WHERE clause pk_col = ? condition.\n\nFixed an issue in the serialization logic of the BIT type. This could cause issues with PostgreSQL clients using the text serialization mode.\n\nFixed an issue that caused col IS NULL to match empty arrays.\n\nFixed an issue that caused col IS NULL expressions to match rows where col is not null if col had INDEX OFF and STORAGE WITH (columnstore = false) set.\n\nFixed an issue that caused queries with ORDER BY clause and LIMIT 0 to fail.\n\nFixed an issue that prevented rows inserted after the last refresh from showing up in the result if a shard had been idle for more than 30 seconds. This affected tables without an explicit refresh_interval setting.\n\nFixed an issue that caused NPE to be thrown, instead of a user-friendly error message when NULL is passed as shardId for the ALTER TABLE XXX REROUTE XXX statements (MOVE, ALLOCATE, PROMOTE, CANCEL).\n\nFixed an issue that caused queries operating on expressions with no defined type to fail. Examples are queries with GROUP BY on an ignored object column or UNION on NULL literals.\n\nFixed an issue that caused GROUP BY and ORDER BY statements with NULL ordinal casted to a specific type, throw an error. Example: SELECT NULL, count(*) from unnest([1, 2]) GROUP BY NULL::integer.\n\nFixed an issue that not-null constraints used to be shown in the pg_constraint table which contradicts with PostgreSQL.\n\nFixed an issue that caused IllegalArgumentException to be thrown when attempting to insert values into a partitioned table, using less columns than the ones defined in the table’s PARTITIONED BY clause.\n\nFixed an issue that caused failure of ALTER TABLE statements when updating dynamic or non-dynamic table settings on closed tables.\n\nFixed an issue that caused clients using PostrgreSQL wire protocol’s simple query to hang, when an error occurred during planning.\n\nFixed casts of TEXT to REGCLASS data types which were resulting in wrong REGCLASS values as the current_schema was not taken into account.\n\nFixed an issue which caused PRIMARY KEY columns to be shown as nullable in the pg_catalog.pg_attribute table.\n\nFixed possible infinitive loops on COPY FROM statements if an IO error happen while trying to read lines from an URI.\n\nFixed an issue that caused ARRAY_LENGTH in combination with a greater than `` (>``) comparison to exclude rows if an array contained duplicates or nulls."
  },
  {
    "title": "Version 4.8.1 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.8.1.html",
    "html": "5.6\nVersion 4.8.1\n\nReleased on 2022-05-24.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.8.1.\n\nWe recommend that you upgrade to the latest 4.7 release before moving to 4.8.1.\n\nA rolling upgrade from 4.7.x to 4.8.1 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.8.0 release notes for a full list of changes in the 4.8 series.\n\nFixes\n\nEnabled users to alter settings on tables subscribed in a logical replication.\n\nEnabled to alter the setting blocks.read_only_allow_delete on blob tables to make it possible to drop read-only blob tables.\n\nFixed an issue that could cause queries on sys.snapshots to get stuck and consume a significant amount of resources.\n\nFixed an issue with primary key columns that have a DEFAULT clause. That could lead to queries on the primary key column not matching the row.\n\nFixed an issue with the logical replication of tables metadata which caused to stop if the master node of the subscriber changed.\n\nFixed an issue with aliased sub-relation outputs when used inside the outer where clause expression, resulting in a planner error. Example: SELECT * FROM (SELECT id, true AS a FROM t1) WHERE a\n\nFixed an edge case with the initial restore of subscribed tables when the restore operation finish almost instantly (e.g. restoring small tables).\n\nFixed an issue with table functions parameter binding in SELECT queries without FROM clause. Example: SELECT unnest(?).\n\nImproved error handling when creating a subscription with unknown publications. Instead of successfully creating the subscription, an error is now presented to the user.\n\nFixed an issue with client caching which lead to authentication error when creating a subscription with bad credentials and pg_tunnel followed by re-creating it second time with the same name and valid credentials.\n\nFixed an issue with VARCHAR and BIT columns with a length limited used in primary key, generated or default column expression. An ALTER TABLE statement removed the length limit from such columns.\n\nFixed an issue resulting in a broken subscription when a subscription is dropped and re-created within a short period of time."
  },
  {
    "title": "Version 4.8.3 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.8.3.html",
    "html": "5.6\nVersion 4.8.3\n\nReleased on 2022-09-06.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.8.3.\n\nWe recommend that you upgrade to the latest 4.7 release before moving to 4.8.3.\n\nA rolling upgrade from 4.7.x to 4.8.3 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.8.0 release notes for a full list of changes in the 4.8 series.\n\nFixes\n\nFixed a regression introduced in CrateDB 4.3.0 causing an IndexOutOfBoundsException when applying aggregations on literals. Example:\n\nSELECT SUM(10) FROM test HAVING COUNT(1) > 0\n\n\nFixed an issue, preventing users from defining a constraint on a generated column, when creating a table or when adding a generated column. Example:\n\nCREATE TABLE test(\n    col1 INT,\n    col2 INT GENERATED ALWAYS AS col1*2 CHECK (col2 > 0)\n)\n\n\nFixed an issue causing IndexOutOfBoundsException to be thrown when using LEFT/RIGHT or FULL OUTER JOIN and one of the tables (or sub-selects) joined has 0 rows.\n\nUpdated the bundled JDK from 17.0.3+7 to 17.0.4+8.\n\nFixed a race condition that could cause a INSERT INTO operation to get stuck.\n\nFixed an issue that could cause queries with objectColumn = ? expressions to fail if the object contains inner arrays.\n\nFixed a NullPointerException when using a IS NULL expression on an object column that just had a child column added.\n\nFixed an issue that caused array_upper and array_lower scalar functions return wrong results on multidimensional arrays.\n\nFixed exposure of the correct version a partitioned table was created on at the information_schema.tables.version.created column for newly created partitioned tables. This won’t fix it for existing tables as the version information the table was created with was not stored and such is not known. The issue doesn’t affect the version exposed for individual partitions.\n\nFixed a column positions issue that caused an INSERT or any other statements that adds columns dynamically to throw an exception.\n\nUpdated the Admin UI to version 1.22.2. It includes a fix for a rendering issue causing jumping behavior on selected views in view list.\n\nFixed UPDATE, INSERT and COPY FROM to preserve the implied column order when columns are added.\n\nFixed casts of strings to the DATE type, any possible time parts of a timestamp formatted will be ignored instead of raising a cast error.\n\nFixed casts of numeric and timestamp values to the DATE type, any time values weren’t removed from the returning epoch in milliseconds.\n\nFixed an issue causing error when adding a check constraint to a nested object sub column."
  },
  {
    "title": "Version 4.8.4 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.8.4.html",
    "html": "5.6\nVersion 4.8.4\n\nReleased on 2022-09-15.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.8.4.\n\nWe recommend that you upgrade to the latest 4.7 release before moving to 4.8.4.\n\nA rolling upgrade from 4.7.x to 4.8.4 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.8.0 release notes for a full list of changes in the 4.8 series.\n\nFixes\n\nFixed a file descriptor leak that was triggered by querying the os column of the sys.nodes table.\n\nFixed an issue that could lead to stuck queries.\n\nFixed EXPLAIN plan output for queries with a WHERE clause containing implicit cast symbols. A possible optimization of our planner/optimizer was not used, resulting in different output than actually used on plan execution.\n\nFixed an issue that could lead to a NoSuchElementException when using the JDBC client and mixing different DML statements using the addBatch functionality."
  },
  {
    "title": "Version 5.0.0 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.0.0.html",
    "html": "5.6\nVersion 5.0.0\n\nReleased on 2022-07-11.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.0.0.\n\nWe recommend that you upgrade to the latest 4.8 release before moving to 5.0.0.\n\nA rolling upgrade from 4.8.x to 5.0.0 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nBreaking Changes\n\nDeprecations\n\nChanges\n\nSQL Statements\n\nNew Types\n\nSQL Standard And PostgreSQL Schema Compatibility\n\nScalar Functions\n\nPerformance Improvements\n\nAdministration and Operations\n\nBreaking Changes\n\nUpdated Lucene to the version 9.2.0.\n\nChanged the type of TIMESTAMP from TIMESTAMP WITH TIME ZONE to TIMESTAMP WITHOUT TIME ZONE to be compatible with the SQL standard.\n\nTables created prior to updating to CrateDB 5.0 will remain unchanged. Tables created after updating to 5.0 while using the TIMESTAMP alias will use TIMESTAMP WITHOUT TIME ZONE. Change the type definition to TIMESTAMP WITH TIME ZONE if you want to preserve the old behavior.\n\nCreating tables with soft deletes disabled is no longer supported. The setting soft_deletes.enabled will always be set to true and removed in CrateDB 6.0.\n\nRemoved deprecated node.max_local_storage_nodes setting. Set different path.data values to run multiple CrateDB processes on the same machine.\n\nRemoved deprecated delimited_payload_filter built-in token filter which has been renamed to delimited_payload since CrateDB 3.2.0.\n\nRemoved simplefs store type as a follow-up of its removal in Lucene 9.0.0. Refer to store types for alternatives.\n\nChanged the default column name for the generate_series, regexp_matches and unnest table functions. Previously they’d default to col1 as column name. The behavior changed to use the table function’s name as column name unless the function returns more than one column. Some examples:\n\ncr> SELECT * FROM generate_series(1, 2);\n\nThis used to return a column named col1 and now returns a column named generate_series.\n\ncr> SELECT * FROM generate_series(1, 2) as numbers;\n\nThis used to return a column named col1 and now returns a column named numbers.\n\nTo ease migration, we added a new setting (legacy.table_function_column_naming.) to opt-out of this new behavior.\n\nFields of type pg_node_tree in PostgreSQL in the pg_class and pg_index tables now return TEXT instead of ARRAY(OBJECT) for improved compatibility with PostgreSQL.\n\nThe single byte byte data type was falsely exposed as the char data type. It will now be correctly exposed as the special single byte \"char\".\n\nDeprecations\n\nDeprecated the azure discovery functionality. It only works for classic VM and Microsoft deprecated classic VM support on Azure.\n\nChanges\nSQL Statements\n\nAdded support for non-recursive WITH Queries (Common Table Expressions).\n\nAdded support for using subscript expressions on top of aliases within the ORDER BY clause. An example: SELECT percentile(x, [0.90, 0.95]) AS percentiles FROM tbl ORDER BY percentiles[1].\n\nAdded support for array element access on top of a subscript on an object array. An example: object_array['subelement'][1]\n\nAdded support for casts from bigint to regclass for improved compatibility with PostgreSQL clients.\n\nAdded support for FETCH [FIRST | NEXT] <noRows> [ROW | ROWS] ONLY clause as and alternative to the LIMIT clause.\n\nAllowed LIMIT and OFFSET clauses to be declared in any order, i.e.: SELECT * FROM t LIMIT 10 OFFSET 5 or SELECT * FROM t OFFSET 5 LIMIT 10.\n\nAdded support for LIMIT NULL, LIMIT ALL, OFFSET NULL, OFFSET 10 ROW and OFFSET 10 ROWS.\n\nAdded support for using NULL literals in a UNION without requiring an explicit cast.\n\nChanged UNION to support implicit casts if the type of expressions in the first relation don’t match the types in the second relation.\n\nNew Types\n\nAdded full support, incl. storage and indexing, for the fixed-length, blank padded CHARACTER(n) data type. Previously, the single byte byte was exposed as char which has been fixed, see Breaking Changes.\n\nAdded decimal type as alias to numeric\n\nSQL Standard And PostgreSQL Schema Compatibility\n\nAdded typsend column to pg_catalog.pgtype table for improved compatibility with PostgreSQL.\n\nAdded primary key and check constraint column positions into conkey field of the pg_constraint table for improved compatibility with PostgreSQL.\n\nAdded pg_catalog.pg_tables and pg_catalog.pg_views tables for improved PostgreSQL compatibility.\n\nAdded identity columns information to information_schema.columns table for improved PostgreSQL compatibility. CrateDB does not support identity columns.\n\nAdded an empty pg_catalog.pg_shdescription table for improved PostgreSQL compatibility.\n\nScalar Functions\n\nAdded SUBSTRING to the non-reserved SQL keywords in order to support the generic function call syntax for improved PostgreSQL compatibility. Example: SUBSTRING('crate', 1, 3)\n\nAdded the concat_ws scalar function which allows concatenation with a custom separator.\n\nAdded the object_keys scalar function which returns the set of first level keys of an object.\n\nAdded the pg_get_serial_sequence scalar function for improved compatibility with PostgreSQL. CrateDB does not support sequences.\n\nAdded has_schema_privilege scalar function which checks whether user (or current user if not specified) has specific privilege(s) for the specific schema.\n\nAdded support for an optional boolean argument pretty at the pg_get_expr scalar function for improved PostgreSQL compatibility.\n\nAdded the pg_get_partkeydef scalar function for improved compatibility with PostgreSQL. Partitioning in CrateDB is different from PostgreSQL, therefore this function always returns NULL.\n\nMoved the quote_ident(text) function to pg_catalog for improved compatibility with PostgreSQL.\n\nPerformance Improvements\n\nAdded an optimization to push down constant join conditions to the relation in an inner join, which results in a more efficient execution plan.\n\nAdded an optimization for array_column = [] queries.\n\nAdministration and Operations\n\nUpdated the bundled JDK to 18.0.1+10\n\nUsers with AL privileges can now run ANALYZE\n\nUpdated the Admin UI. It includes a new color, new colors and no longer loads resources like web fonts from external services."
  },
  {
    "title": "Version 5.0.1 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.0.1.html",
    "html": "5.6\nVersion 5.0.1\n\nReleased on 2022-09-06.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.0.1.\n\nWe recommend that you upgrade to the latest 4.8 release before moving to 5.0.1.\n\nA rolling upgrade from 4.8.x to 5.0.1 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nFixes\n\nSee the Version 5.0.0 release notes for a full list of changes in the 5.0 series.\n\nFixes\n\nFixed a regression introduced in CrateDB 4.3.0 causing an IndexOutOfBoundsException when applying aggregations on literals. Example:\n\nSELECT SUM(10) FROM test HAVING COUNT(1) > 0\n\n\nFixed an issue, preventing users from defining a constraint on a generated column, when creating a table or when adding a generated column. Example:\n\nCREATE TABLE test(\n    col1 INT,\n    col2 INT GENERATED ALWAYS AS col1*2 CHECK (col2 > 0)\n)\n\n\nFixed an issue causing IndexOutOfBoundsException to be thrown when using LEFT/RIGHT or FULL OUTER JOIN and one of the tables (or sub-selects) joined has 0 rows.\n\nUpdated the bundled JDK from 18.0.1+10 to 18.0.2+9.\n\nFixed a race condition that could cause a INSERT INTO operation to get stuck.\n\nFixed an issue that could cause queries with objectColumn = ? expressions to fail if the object contains inner arrays.\n\nFixed a NullPointerException when using a IS NULL expression on an object column that just had a child column added.\n\nFixed an issue that caused array_upper and array_lower scalar functions return wrong results on multidimensional arrays.\n\nFixed exposure of the correct version a partitioned table was created on at the information_schema.tables.version.created column for newly created partitioned tables. This won’t fix it for existing tables as the version information the table was created with was not stored and such is not known. The issue doesn’t affect the version exposed for individual partitions.\n\nFixed a column positions issue that caused an INSERT or any other statements that adds columns dynamically to throw an exception.\n\nUpdated the Admin UI to version 1.22.2. It includes a fix for a rendering issue causing jumping behavior on selected views in view list.\n\nFixed UPDATE, INSERT and COPY FROM to preserve the implied column order when columns are added.\n\nFixed casts of strings to the DATE type, any possible time parts of a timestamp formatted will be ignored instead of raising a cast error.\n\nFixed casts of numeric and timestamp values to the DATE type, any time values weren’t removed from the returning epoch in milliseconds.\n\nFixed an issue causing error when adding a check constraint to a nested object sub column."
  },
  {
    "title": "Version 4.5.2 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.5.2.html",
    "html": "5.6\nVersion 4.5.2\n\nReleased on 2021-06-15.\n\nNote\n\nIf upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.5.2.\n\nWe recommend that you upgrade to the latest 4.4 release before moving to 4.5.2.\n\nA rolling upgrade from 4.4.0+ to 4.5.2 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.5.0 release notes for a full list of changes in the 4.5 series.\n\nFixes\n\nFixed an issue that resulted in an unknown column error if trying to access a fulltext index from an aliased table. For example the following statement failed:\n\nSELECT * FROM users u WHERE MATCH (u.name_ft, 'Arthur');\n\n\nFixed an issue that prevented DEFAULT clauses from being evaluated per record in INSERT statements with multiple source values. This resulted in the same values being inserted when using nondeterministic functions like gen_random_text_uuid as default expression.\n\nFixed an issue that prevented aggregations or grouping operations on virtual tables to run parallel on shard level, even if the inner query would support it.\n\nFixed an issue that prevented INSERT INTO statements where the source is a query that selects an object column which contains a different set of columns than the target object column.\n\nFixed an issue that could lead to errors when using DISTINCT or GROUP BY with duplicate columns.\n\nFixed an issue that could cause GROUP BY queries with a LIMIT clause and aliased columns to fail.\n\nFixed an issue that prevented LIKE operators from using the index if the left operand was a varchar column with length limit, and the right operand a literal.\n\nFixed an issue that resulted in more data being snapshot than expected if only concrete tables were snapshot by the CREATE SNAPSHOT ... TABLE [table, ...]. Instead of just the concrete tables, also the metadata of partitioned table, views, users, etc. were falsely stored.\n\nFixed an issue that resulted in a non-executable plan if a windows function result from a sub-select is used inside a query filter. An example:\n\nSELECT * FROM (\n  SELECT ROW_NUMBER() OVER(PARTITION by col1) as row_num\n  FROM (VALUES('x')) t1\n) t2\nWHERE row_num = 2;\n\n\nFixed an issue that caused valid values for number_of_routing_shards in CREATE TABLE statements to be rejected because the validation always used a fixed value of 5 instead of the actual number of shards declared within the CREATE TABLE statement.\n\nFixed an issue that caused incorrect classification for DELETE and UPDATE queries with sub-select. Statement type for those queries was always SELECT.\n\nFixed an issue that threw an exception when ORDER BY clauses contain the output column position or the alias name of an aliased column."
  },
  {
    "title": "Version 5.0.2 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.0.2.html",
    "html": "5.6\nVersion 5.0.2\n\nReleased on 2022-10-10.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.0.2.\n\nWe recommend that you upgrade to the latest 4.8 release before moving to 5.0.2.\n\nA rolling upgrade from 4.8.x to 5.0.2 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nFixes\n\nSee the Version 5.0.0 release notes for a full list of changes in the 5.0 series.\n\nFixes\n\nFixed an issue causing queries with matching on _id to not get rows from translog, and therefore only rows that were visible from the latest manual or automatic REFRESH were returned.\n\nFixed an issue causing an IllegalArgumentException to be thrown when the optimizer attempts to convert a LEFT JOIN to an INNER JOIN and there is also a subquery in the WHERE clause.\n\nFixed a file descriptor leak that was triggered by querying the os column of the sys.nodes table.\n\nFixed an issue that could lead to a NoSuchElementException when using the JDBC client and mixing different DML statements using the addBatch functionality.\n\nFixed an issue that could lead to stuck queries.\n\nFixed EXPLAIN plan output for queries with a WHERE clause containing implicit cast symbols. A possible optimization of our planner/optimizer was not used, resulting in different output than actually used on plan execution.\n\nFixed an issue that leads to stuck write queries if the indices.breaker.query.limit is set to -1 as the value was interpreted as a byte value instead of disabling any breaking."
  },
  {
    "title": "Version 5.2.2 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.2.2.html",
    "html": "5.6\nVersion 5.2.2\n\nReleased on 2023-02-09.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.2.2.\n\nWe recommend that you upgrade to the latest 5.1 release before moving to 5.2.2.\n\nA rolling upgrade from 5.1.x to 5.2.2 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nFixes\n\nSee the Version 5.2.0 release notes for a full list of changes in the 5.2 series.\n\nFixes\n\nFixed crate-node cli tool to work without asking for path.home and/or path.conf parameters when run from inside the CrateDB distribution directory, for Linux, MacOSX and Windows environments.\n\nRemoved the automatic fix mechanism, introduced in 5.2.1, for corrupted metadata due to table swap statements like:\n\nALTER CLUSTER SWAP TABLE \"myschema\".\"mytable\" TO \"myschema.mytable\";\n\n\nand provide a manual way to fix such issues by running:\n\nbin/crate-node fix-metadata\n\n\nChanged the typsend and typreceive values in the pg_catalog.pg_type table to match PostgreSQL for improved compatibility. Clients like Postgrex depend on this.\n\nFixed a race condition that could lead to an error reporting that a partition was not found when running DELETE FROM <table> WHERE partition_column = ? statements.\n\nFixed runtime ClassCastException when attempting to ORDER BY an INTERVAL type, or when attempting to use MIN or MAX aggregations on an INTERVAL type, and return an error message about unsupported type during analysis of a query.\n\nDisallowed comparison operators between INTERVAL types, except for = and <>, Previously, null was returned in any of the >, >=, <, <=, comparisons."
  },
  {
    "title": "Version 4.5.0 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.5.0.html",
    "html": "5.6\nVersion 4.5.0\n\nReleased on 2021-03-23.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.5.0.\n\nWe recommend that you upgrade to the latest 4.3 release before moving to 4.5.0.\n\nA rolling upgrade from 4.3.x to 4.5.0 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nDeprecations\n\nChanges\n\nLicensing\n\nPerformance improvements\n\nSQL and PostgreSQL compatibility improvements\n\nAdministration and Operations\n\nNew scalar and window functions\n\nDeprecations\n\nThe table setting soft_deletes.enabled has been marked as deprecated and will be removed in a future version. Soft deletes will become mandatory in CrateDB 5.0.\n\nChanges\nLicensing\n\nRelicensed all enterprise features under the Apache License 2.0 and removed licensing related code. The SET LICENSE statement can still be used, but it won’t have any effect.\n\nBecause of this, all users have now access to features like\n\nUser management.\n\nAuthentication and SSL/TLS support.\n\nUser defined function support.\n\nAll window functions.\n\nJMX monitoring features.\n\nThe hyperloglog_distinct aggregation function.\n\nPerformance improvements\n\nOptimized how NULL values are stored, reducing the amount of disk space required. This can also improve the performance of value lookups on tables with a lot of null values.\n\nSQL and PostgreSQL compatibility improvements\n\nAdded the regclass data type for improved compatibility with PostgreSQL tools.\n\nAdded an empty pg_tablespace table in the pg_catalog schema for improved support for PostgreSQL tools.\n\nImproved the error messages for cast errors for values of type object.\n\nAdded support for the CREATE TABLE AS statement.\n\nAdministration and Operations\n\nAdded support for restoring metadata and settings from snapshots. If ALL is used, everything (tables, settings, views, etc.) will be restored. On the other hand, using TABLES will only restore tables. In previous releases, only tables could be restored, even if ALL was used.\n\nImproved language selection, translations and general usability at the Admin UI.\n\nUpdated the bundled JDK to 16+36\n\nNew scalar and window functions\n\nAdded the gen_random_text_uuid() scalar function.\n\nAdded the pg_postmaster_start_time() scalar function.\n\nAdded CURDATE and CURRENT_DATE scalar functions."
  },
  {
    "title": "Version 4.6.0 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.6.0.html",
    "html": "5.6\nVersion 4.6.0\n\nReleased on 2021-07-13.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.6.0.\n\nWe recommend that you upgrade to the latest 4.3 release before moving to 4.6.0.\n\nA rolling upgrade from 4.5.x to 4.6.0 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nDeprecations\n\nChanges\n\nPerformance Improvements\n\nSQL Statements and Compatibility\n\nNew Scalars\n\nAdministration and Operations improvements\n\nAdministration Console\n\nDeprecations\n\nDeprecated the node.max_local_storage_nodes setting.\n\nChanges\nPerformance Improvements\n\nImproved the performance of the hyperloglog_distinct aggregation function.\n\nImproved the performance of SELECT statements with WHERE conditions that are in the form of WHERE COL=COL.\n\nImproved the performance of INSERT FROM query statements where the query contains a GROUP BY clause.\n\nImproved the internal throttling mechanism used for INSERT FROM QUERY and COPY FROM operations. This should lead to these queries utilizing more resources if the cluster can spare them.\n\nAdded an optimization that improves the performance of count() aggregations on object columns that have at least one inner column with a NOT NULL constraint.\n\nSQL Statements and Compatibility\n\nAdded the bit(n) data type.\n\nCrateDB now accepts the START TRANSACTION statement for PostgreSQL wire protocol compatibility. However, CrateDB does not support transactions and will silently ignore this statement.\n\nAdded support for directory-level wild card expansion for URIs passed to COPY FROM statements.\n\nNew Scalars\n\nAdded array_to_string scalar function that concatenates array elements into a single string using a separator and an optional null-string.\n\nAdded array_min and array_max scalar functions returning the minimal and maximal element in array respectively.\n\nAdded the array_sum scalar function that returns the sum of all elements in an array.\n\nAdded the array_avg scalar function that returns the average of all elements in an array.\n\nAdministration and Operations improvements\n\nUsers can now read tables within the pg_catalog schema without explicit DQL permission. They will only see records the user has privileges on.\n\nUsers with AL privileges (or DDL on both tables) can now run the following ALTER CLUSTER commands: ALTER CLUSTER SWAP TABLE source TO target, ALTER CLUSTER REROUTE RETRY FAILED, ALTER CLUSTER GC DANGLING ARTIFACTS.\n\nAdded support for encrypting node-to-node communication.\n\nChanged the privileges model to allow users with DDL privileges on a table to use the OPTIMIZE TABLE statement.\n\nIncluded the shard information for closed tables in sys.shards table.\n\nAdded a closed column to sys-shards exposing the state of the table associated with the shard.\n\nAdded support for reading cgroup information in the cgroup v2 format.\n\nAdded support of hostnames in HBA configuration.\n\nAdministration Console\n\nRemoved all analytics (UDC, Segment)\n\nRemoved the “Notifications” section in the status bar\n\nRemoved min-width for columns in console to reduce scrolling\n\nChanged syntax highlighting in console. Keywords in double quotes are now longer highlighted. Types are highlighted with a different color.\n\nActivated codemirror code hints for keywords.\n\nChanged the look of the scroll bar to appear more modern.\n\nAdded length limit for varchar(n) and bit(n) types in table view."
  },
  {
    "title": "Version 4.6.1 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.6.1.html",
    "html": "5.6\nVersion 4.6.1\n\nReleased on 2021-07-20.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.6.1.\n\nWe recommend that you upgrade to the latest 4.3 release before moving to 4.6.1.\n\nA rolling upgrade from 4.5.x to 4.6.1 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.6.0 release notes for a full list of changes in the 4.6 series.\n\nFixes\n\nFixed an issue that could lead to NULL values getting returned when using a _doc['columnName'] expression in the ORDER BY clause. Prior to 4.6.0 this also affected INSERT INTO statements on top level columns.\n\nFixed an issue that caused a class cast error when trying to import records from a CSV file into a partitioned table using the COPY FROM statement.\n\nFixed an issue that caused a node with default configuration to fail on startup with error “No valid auth.host_based entry found”."
  },
  {
    "title": "Version 4.6.2 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.6.2.html",
    "html": "5.6\nVersion 4.6.2\n\nReleased on 2021-08-26.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.6.2.\n\nWe recommend that you upgrade to the latest 4.3 release before moving to 4.6.2.\n\nA rolling upgrade from 4.5.x to 4.6.2 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.6.0 release notes for a full list of changes in the 4.6 series.\n\nFixes\n\nFixed a validation issue resulting in an unusable broken table when a sub-column identifier of an object type column contains invalid whitespace characters.\n\nFixed an issue that could cause queries on sys.snapshots to fail with an error if a repository is in the cluster state that cannot be accessed - for example due to invalid credentials.\n\nFixed a regression introduced in CrateDB 4.6.0 that broke the functionality of restoring only concrete custom metadata like USERS, PRIVILEGES, VIEWS and UDFS.\n\nFixed an issue that caused the SHOW TRANSACTION_ISOLATION statement to require privileges for the sys schema.\n\nFixed an issue in the execution plan generation for SELECT COUNT(*) FROM ... statements with predicates like 'a' in ANY(varchar_array_column). Such predicates resulted in a cast on the column ('a' in ANY(varchar_array_column::array(varchar(1)))), leading to poor performance because the indices couldn’t get utilized. This fix significantly improves the performance of such queries. In a test over 100000 records, the query runtime improved from 320ms to 2ms.\n\nFixed an issue that could cause a NullPointerException if a user invoked a SELECT statement with a predicate on a OBJECT (ignored) column immediately after a DELETE statement.\n\nFixed an issue that could cause a wrong cast of numeric with parameters in arithmetical expressions."
  },
  {
    "title": "Version 5.2.3 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.2.3.html",
    "html": "5.6\nVersion 5.2.3\n\nReleased on 2023-02-23.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.2.3.\n\nWe recommend that you upgrade to the latest 5.1 release before moving to 5.2.3.\n\nA rolling upgrade from 5.1.x to 5.2.3 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nFixes\n\nSee the Version 5.2.0 release notes for a full list of changes in the 5.2 series.\n\nFixes\n\nFixed an issue that caused correlated sub-queries to fail if using columns in the filter clause that were otherwise not selected.\n\nFixed an issue that caused correlated sub-queries to fail with an IllegalStateException when the outer query contained multiple joins. An example\n\nCREATE TABLE a (x INT, y INT, z INT); // tables b, c, d created as a\nSELECT\n  (SELECT 1 WHERE a.x=1 AND b.y=1 AND c.z=1)\nFROM a, b, c, d;\nIllegalStateException[OuterColumn `y` must appear in input of\nCorrelatedJoin]\n\n\nFixed an issue that caused DROP FUNCTION to throw a ColumnUnknownException instead of an IllegalArgumentException justifying why a function cannot be dropped.\n\nUpdated to Admin UI 1.24.3, which fixed a compatibility issue where graphs have not been working on the “Overview” page with CrateDB 5.2, and added syntax highlighting for functions added in CrateDB 5.2: MIN_BY, MAX_BY, HAS_DATABASE_PRIVILEGE, PARSE_URI, and PARSE_URL."
  },
  {
    "title": "Version 4.6.8 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.6.8.html",
    "html": "5.6\nVersion 4.6.8\n\nReleased on 2022-03-09.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.6.8.\n\nWe recommend that you upgrade to the latest 4.3 release before moving to 4.6.8.\n\nA rolling upgrade from 4.5.x to 4.6.8 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.6.0 release notes for a full list of changes in the 4.6 series.\n\nFixes\n\nFixed an issue from COPY FROM that caused the tables’ default expressions to be evaluated only once, causing a single value to be used throughout the table instead of per row. This only affected non-deterministic functions such as gen_random_text_uuid(), random(), etc.\n\nFixed an issue that could lead to errors when reading translog files from CrateDB versions < 4.0.\n\nFixed an issue that could lead to an Couldn't create execution plan error when using a join condition referencing multiple other relations.\n\nFixed an issue that led to an Invalid query used in CREATE VIEW error if using a scalar subquery within the query part of a CREATE VIEW statement.\n\nFixed an issue that caused truncation of milliseconds in timezone scalar function.\n\nFixed an issue that caused a failure when a window function over a partition is not used in an upper query. For example: select x from (select x, ROW_NUMBER() OVER (PARTITION BY y) from t) t1\n\nFixed an incorrect optimization of comparison function arguments for explicit cast arguments resulting in a wrong result set. Example: WHERE strCol::bigint > 3"
  },
  {
    "title": "Version 5.2.4 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.2.4.html",
    "html": "5.6\nVersion 5.2.4\n\nReleased on 2023-03-14.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.2.4.\n\nWe recommend that you upgrade to the latest 5.1 release before moving to 5.2.4.\n\nA rolling upgrade from 5.1.x to 5.2.4 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nFixes\n\nSee the Version 5.2.0 release notes for a full list of changes in the 5.2 series.\n\nFixes\n\nFixed an issue that caused JOIN queries to fail with an internal error, when USING is used to define the join condition in combination with a nested join e.g.:\n\nSELECT * FROM t1 JOIN (t2 JOIN t3 ON t2.y = t3.y) USING(x)\n\n\nFurthermore, validation of USING was added, so that a meaningful error message is thrown in case it’s misused.\n\nFixed an issue that caused nested joins to fail with an internal error e.g.:\n\nSELECT * FROM t1 JOIN (t2 JOIN t3 ON t2.x = t3.x) ON t1.x = t2.x\n\n\nFixed an issue that could cause errors for queries with aggregations and UNION, e.g.\n\nSELECT a, avg(c), b FROM t1 GROUP BY 1, 3\nUNION\nSELECT x, avg(z), y FROM t2 GROUP BY 1, 3\nUNION\nSELECT i, avg(k), j FROM t3 GROUP BY 1, 3\n\n\nFixed a performance regression for IS NOT NULL expressions on object columns which was introduced in 5.0.3 and 5.1.1.\n\nFixed an issue that could cause DELETE FROM statements which match a large amount of records to cause a node to crash with an out of memory error.\n\nFixed an issue that caused expressions like <column> != ANY(<array-literal>) to match on partitions where the column didn’t exist or on records where <column> had a null value.\n\nFixed an issue that allowed users to execute user-defined functions without DQL privileges on the schemas that the functions are defined in.\n\nFixed an issue that translated ColumnUnknownException to a misleading SchemaUnknownException when users without DQL privilege on doc schema queried unknown columns from table functions. An example\n\nSELECT unknown_col FROM abs(1);\nSchemaUnknownException[Schema 'doc' unknown]\n\n\nFixed an issue that translated an AmbiguousColumnException to a misleading IllegalStateException when aliased columns are queried that are also ambiguous. An example\n\nSELECT r FROM (SELECT a AS r, a AS r FROM t) AS q\nIllegalStateException[Symbol 'io.crate.expression.symbol.Symbol' not supported]\n// r is an alias of a and is ambiguous from the perspective of the outer query\n\n\nFixed an issue that translated UnsupportedOperationException to a misleading MissingPrivilegeException when executing functions with invalid names or signatures.\n\nFixed an issue causing nested join statements using the NESTED LOOP plan to return incorrect results in some scenarios when issued on a multi-node cluster."
  },
  {
    "title": "Version 5.2.5 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.2.5.html",
    "html": "5.6\nVersion 5.2.5\n\nReleased on 2023-03-20.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.2.5.\n\nWe recommend that you upgrade to the latest 5.1 release before moving to 5.2.5.\n\nA rolling upgrade from 5.1.x to 5.2.5 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nFixes\n\nSee the Version 5.2.0 release notes for a full list of changes in the 5.2 series.\n\nFixes\n\nFixed an issue that caused nested join queries to return incorrect results because a WHERE clause filter referencing a parent join was discarded by the planner. Example:\n\nSELECT * FROM t1 JOIN t2 ON t1.a = t2.b JOIN t3 ON t2.c = t3.d WHERE t1.x = t3.d\n\n\nIn this case, the WHERE clause filter t1.x = t3.d was discarded."
  },
  {
    "title": "Version 4.6.7 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.6.7.html",
    "html": "5.6\nVersion 4.6.7\n\nReleased on 2022-01-18.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.6.7.\n\nWe recommend that you upgrade to the latest 4.3 release before moving to 4.6.7.\n\nA rolling upgrade from 4.5.x to 4.6.7 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.6.0 release notes for a full list of changes in the 4.6 series.\n\nFixes\n\nFixed an issue that caused the npgsql PostgreSQL client to fail with an System.Exception: Received unexpected backend message ParseComplete error if using the EntityFramework.Core framework to insert records.\n\nFixed an issue that could lead to errors like Received resultset tuples, but no field structure for them when fetching a subset of rows from one query, and then intermediately triggering a different query before finishing the first query.\n\nFixed an issue that could cause clients using the PostgreSQL wire protocol to receive row counts in incorrect orders when using APIs that allow to execute multiple statements in a batch.\n\nFixed an issue that could cause inserts into partitioned tables to fail with a IndexNotFoundException if concurrently deleting partitions.\n\nFixed a BWC translog issue for indices created with CrateDB < 3.2.\n\nUpdated log4j to 2.17.1 because of CVE-2021-45046 and CVE-2021-45105. CrateDB isn’t affected by default. The log4j configuration shipped with CrateDB doesn’t include any of the problematic layout patterns. This is a pre-caution in case users changed the default configuration."
  },
  {
    "title": "Version 5.2.7 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.2.7.html",
    "html": "5.6\nVersion 5.2.7\n\nReleased on 2023-04-28.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.2.7.\n\nWe recommend that you upgrade to the latest 5.1 release before moving to 5.2.7.\n\nA rolling upgrade from 5.1.x to 5.2.7 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nFixes\n\nSee the Version 5.2.0 release notes for a full list of changes in the 5.2 series.\n\nFixes\n\nFixed an issue introduced in 5.2.0 which led INNER JOIN queries to produce 0 or wrong results when filtering with a constant value is used in the join condition, e.g.:\n\nSELECT * FROM t1 INNER JOIN t2 ON t1.a = 10 AND t1.x = t2.y\n\n\nFixed a performance regression introduced in 5.2.3 which led to filters on object columns resulting in a table scan if used with views or virtual tables. See #14015 for details.\n\nFixed an issue that caused geo_shape_array IS NULL expressions to fail with an IllegalStateException.\n\nFixed an issue that caused the actual cast/type conversion error to be hidden when it failed for a sub-column of an object column, when using a client statement with parameters i.e (python).:\n\nCREATE TABLE a (b OBJECT(DYNAMIC) AS (c REAL));\n# create a connection and a cursor and then:\ncursor.execute(\"INSERT INTO a VALUES (?)\", [({\"c\": True},)])\n\n\nFixed a regression that caused the -h option in bin/crate to fail with an Error parsing arguments! error.\n\nAdded a null_or_empty scalar function that can be used as a faster alternative to IS NULL if it’s acceptable to match on empty objects. This makes it possible to mitigate a performance regression introduced in 5.0.3 and 5.1.1\n\nFixed an issue that led to NullPointerException when trying to query an OBJECT field with no values, using the NOT operator, e.g.:\n\nCREATE TABLE test (obj OBJECT(DYNAMIC)); -- no data\nSELECT myobj FROM test WHERE (obj::TEXT) NOT LIKE '%value%';\n\n\nFixed an issue in the PostgreSQL wire protocol implementation that could lead to ClientInterrupted errors with some clients. An example client is pg-cursor.\n\nFixed an issue that allowed creating columns with names conflicting with subscript pattern, such as \"a[1]\", a subscript expression enclosed in double quotes.\n\nFixed an issue that caused SQLParseException when quoted subscript expressions contained quotes. An example would be querying an array with the name containing quotes like SELECT \"arr\"\"[1]\";.\n\nFixed an issue that caused ALTER TABLE ADD COLUMN statement to assign PRIMARY KEY to wrong columns, when adding multiple primary key columns, having none-primary columns in-between.\n\nFixed an issue that caused ALTER TABLE ADD COLUMN statement to assign a wrong type to ARRAY(GEO_SHAPE) column and create a GEO_SHAPE column instead.\n\nFixed an issue that caused ALTER TABLE ADD COLUMN statement to assign a wrong type to ARRAY(TEXT) column and create a TEXT column instead if column has a FULLTEXT index.\n\nFixed an issue that prevented assigning default expression to ARRAY columns."
  },
  {
    "title": "Version 4.7.0 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.7.0.html",
    "html": "5.6\nVersion 4.7.0\n\nReleased on 2022-01-25.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.7.0.\n\nWe recommend that you upgrade to the latest 4.3 release before moving to 4.7.0.\n\nA rolling upgrade from 4.6.x to 4.7.0 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nDeprecations\n\nChanges\n\nSQL Statements and Compatibility\n\nScalar and Aggregation Functions\n\nNew Tables and Schema Extensions\n\nAdministration and Operations\n\nNew Types\n\nPerformance\n\nDeprecations\n\nDeprecated support for HDFS snapshot repositories.\n\nThe gateway.expected_nodes cluster setting has been marked as deprecated and will be removed in CrateDB 5.0. The gateway.expected_data_nodes must be used instead.\n\nThe gateway.recover_after_nodes cluster setting has been marked as deprecated and will be removed in CrateDB 5.0. The gateway.recover_after_data_nodes must be used instead.\n\nChanges\nSQL Statements and Compatibility\n\nAdded support for the END statement for improved PostgreSQL compatibility.\n\nAdded support to use an aggregation in an order-by clause without having them in the select list like select x from tbl group by x order by count(y)\n\nChanged the type precedence rules for INSERT FROM VALUES statements. The target column types now take higher precedence to avoid errors in statements like INSERT INTO tbl (text_column) VALUES ('a'), (3). Here 3 (INTEGER) used to take precedence, leading to a cast error because a cannot be converted to an INTEGER.\n\nThis doesn’t change the behavior of standalone VALUES statements. VALUES ('a'), (3) as a standalone statement will still fail.\n\nIntroduced RESPECT NULLS and IGNORE NULLS flags to window function calls. The following window functions can now utilize the flags: LEAD, LAG, NTH_VALUE, FIRST_VALUE, and LAST_VALUE.\n\nAdded FAIL_FAST option to COPY FROM statement that when it is set to true, any errors observed while processing the statement will trigger an exception and the on-going executions will terminate in best effort.\n\nScalar and Aggregation Functions\n\nAdded the scalar function array_append which adds a value at the end of an array\n\nRegistered the scalar function array_to_string under the pg_catalog schema.\n\nAdded the scalar function pg_encoding_to_char which converts an PostgreSQL encoding’s internal identifier to a human-readable name.\n\nAdded the scalar function age which returns interval between 2 timestamps.\n\nAdded the date_bin scalar function that truncates timestamp into specified interval aligned with specified origin.\n\nAdded the array_slice(anyarray, from, to) scalar function.\n\nAdded support for the array slice access expression anyarray[from:to].\n\nAdded support of numeric type to the avg aggregation function.\n\nAdded the area(geo_shape) scalar function that calculates the area for a GEO_SHAPE.\n\nEnabled the setting of most prototype methods for JavaScript Objects (e.g. Array.prototype, Object.prototype) in user-defined functions\n\nNew Tables and Schema Extensions\n\nAdded an empty pg_catalog.pg_locks table for improved PostgreSQL compatibility.\n\nAdded an empty pg_catalog.pg_indexes table for compatibility with PostgreSQL.\n\nAdded a new table_partitions column to the sys.snapshots table.\n\nAdded the column_details column to the information_schema.columns table including the top level column name and path information of object elements.\n\nAdministration and Operations\n\nAdded a sys node check for max shards per node to verify that the amount of shards on the current node is less than 90 % of cluster.max_shards_per_node. The check is exposed via sys.node_checks.\n\nAdded error_on_unknown_object_key session setting. This will either allow or suppress an error when unknown object keys are queried from dynamic objects.\n\nEnabled HTTP connections to preserve session settings across the requests as long as the connection is re-used.\n\nNote that connections are established on an individual node to node basis. If a client sends requests to different nodes, those won’t share the same session settings, unless the client sets the session settings on each node individually.\n\nImproved the visual layout of the administration console: Remove dedicated “Monitoring” page and move its contents to the “Overview” page.\n\nNew Types\n\nAdded float4 type as alias to real and float8 type as alias to double precision\n\nAdded the JSON type.\n\nPerformance\n\nImproved optimizer rewrite rules for outer join to inner joins rewrites. Previously using aliases could prevent the rewrite from working."
  },
  {
    "title": "Version 5.2.8 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.2.8.html",
    "html": "5.6\nVersion 5.2.8\n\nReleased on 2023-05-09.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.2.8.\n\nWe recommend that you upgrade to the latest 5.1 release before moving to 5.2.8.\n\nA rolling upgrade from 5.1.x to 5.2.8 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nFixes\n\nSee the Version 5.2.0 release notes for a full list of changes in the 5.2 series.\n\nFixes\n\nImproved output representation of timestamp subtraction, by normalizing to bigger units, but no further than days, to be consistent with PostgreSQL behavior. e.g:\n\nSELECT '2022-12-05T11:22:33.123456789+05:30'::timestamp - '2022-12-03T11:22:33.123456789-02:15'::timestamp\n\n\npreviously would return: PT40H15M and now returns: P1DT16H15M.\n\nImproved error message for date_bin scalar function when the first argument of INTERVAL data type contains month and/or year units.\n\nFixed an issue that caused AssertionError to be thrown when referencing previous relations, not explicitly joined, in an join condition, e.g.:\n\nSELECT * FROM t1\nCROSS JOIN t2\nINNER JOIN t3 ON t3.x = t1.x AND t3.y = t2\n\n\nFixed an issue that caused DROP TABLE IF EXISTS to wrongly return 1 row affected or SQLParseException (depending on user privileges), when called on an existent schema, a non-existent table and with the crate catalog prefix, e.g.:\n\nDROP TABLE IF EXISTS crate.doc.non_existent_table\n\n\nImproved an optimization rule to enable index lookups instead of table scans in more cases. This is a follow up to a fix in 5.2.7 which fixed a regression introduced in 5.2.3."
  },
  {
    "title": "Version 4.7.1 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.7.1.html",
    "html": "5.6\nVersion 4.7.1\n\nReleased on 2022-03-10.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.7.1.\n\nWe recommend that you upgrade to the latest 4.6 release before moving to 4.7.1.\n\nA rolling upgrade from 4.6.x to 4.7.1 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.7.0 release notes for a full list of changes in the 4.7 series.\n\nFixes\n\nFixed an issue from COPY FROM that caused the tables’ default expressions to be evaluated only once, causing a single value to be used throughout the table instead of per row. This only affected non-deterministic functions such as gen_random_text_uuid(), random(), etc.\n\nFixed an issue that could lead to errors when reading translog files from CrateDB versions < 4.0.\n\nFixed an issue that could lead to an Couldn't create execution plan error when using a join condition referencing multiple other relations.\n\nFixed an issue that caused an NPE when executing COPY FROM with a globbed URI having a parent directory that does not exist in the filesystem. This affected copying from filesystems local to CrateDB nodes only.\n\nFixed an issue that led to an Invalid query used in CREATE VIEW error if using a scalar subquery within the query part of a CREATE VIEW statement.\n\nFixed an issue that caused a failure when a window function over a partition is not used in an upper query. For example: select x from (select x, ROW_NUMBER() OVER (PARTITION BY y) from t) t1\n\nFixed an incorrect optimization of comparison function arguments for explicit cast arguments resulting in a wrong result set. Example: `WHERE strCol::bigint > 3`\n\nUpdated the bundled JDK to 17.0.2+8"
  },
  {
    "title": "Version 5.2.9 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.2.9.html",
    "html": "5.6\nVersion 5.2.9\n\nReleased on 2023-09-22.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.2.9.\n\nWe recommend that you upgrade to the latest 5.1 release before moving to 5.2.9.\n\nA rolling upgrade from 5.1.x to 5.2.9 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nFixes\n\nSee the Version 5.2.0 release notes for a full list of changes in the 5.2 series.\n\nFixes\n\nFixed an issue that led to file not found errors when trying to restore a snapshot that was taken after a table had been swapped. A new snapshot must be taken to apply the fix and solve the issue."
  },
  {
    "title": "Version 5.2.6 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.2.6.html",
    "html": "5.6\nVersion 5.2.6\n\nReleased on 2023-04-04.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.2.6.\n\nWe recommend that you upgrade to the latest 5.1 release before moving to 5.2.6.\n\nA rolling upgrade from 5.1.x to 5.2.6 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nFixes\n\nSee the Version 5.2.0 release notes for a full list of changes in the 5.2 series.\n\nFixes\n\nFixed an issue that prevented PostgreSQL wire protocol clients from being able to describe the query of a cursor created using DECLARE. An example of a client that uses the functionality is psycopg3. A snippet like the following failed:\n\nimport psycopg\nconn = psycopg.connect(\"host=localhost port=5432 user=crate\")\ncur = conn.cursor(name=\"foo\")\ncur.execute(\"select 1\")\nfor row in cur.fetchall():\n    print(row)\n\n\nFixed an issue in the PostgreSQL wire protocol implementation that could prevent protocol level fetch from working correctly with some clients. An example client is pg-cursor.\n\nFixed a performance regression for queries that used a scalar sub-query in the WHERE which itself also filtered on columns in a WHERE without selecting those columns. An example:\n\nSELECT name FROM users\n  WHERE id IN (SELECT user_id FROM hits WHERE ts > '2023-01-01')\n\n\nFixed an issue that a wrong HTTP response was sent, when trying to POST to an invalid URL, causing the HTTP client to stall.\n\nFixed response for HTTP GET request to not expose internal paths when the requested URL doesn’t exist.\n\nFixed default behaviour for CURSOR’s SCROLL. When neither SCROLL nor NO SCROLL is provided in the statement, NO SCROLL is now assumed.\n\nFixed a race condition that could lead to a ShardNotFoundException when executing UPDATE statements.\n\nFixed an issue that caused a subset of a WHERE clause to be lost from a JOIN statement. The first trigger condition was using a column by itself as a boolean expression. For example from WHERE NOT b AND c the column c representing a boolean expression c = TRUE overrode NOT b. The second trigger condition was MATCH predicate which also overrode preceding WHERE conditions.\n\nFixed an issue that caused a ColumnUnknownException when creating a table with a generated column involving a subscript expression with a root column name containing upper cases. An example:\n\nCREATE TABLE t (\"OBJ\" OBJECT AS (intarray int[]), firstElement AS \"OBJ\"['intarray'][1]);\nColumnUnknownException[Column obj['intarray'] unknown]\n\n\nFixed a NullPointerException which occurs when using NULL as a setting value.\n\nFixed a resource leak that could happen when inserting data which causes constraints violation or parsing errors."
  },
  {
    "title": "Version 5.2.10 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.2.10.html",
    "html": "5.6\nVersion 5.2.10\n\nReleased on 2023-10-12.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.2.10.\n\nWe recommend that you upgrade to the latest 5.1 release before moving to 5.2.10.\n\nA rolling upgrade from 5.1.x to 5.2.10 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nFixes\n\nSee the Version 5.2.0 release notes for a full list of changes in the 5.2 series.\n\nFixes\n\nFixed an issue that caused privileges checks to be bypassed when using scalar sub-selects in various clauses of a query: SELECT, WHERE, HAVING, etc.\n\nFixed an issue that led to file not found errors when trying to restore a snapshot that was taken after a table had been swapped. A new snapshot must be taken to apply the fix and solve the issue. This issue was not fully addressed with Version 5.2.9."
  },
  {
    "title": "Version 4.7.3 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.7.3.html",
    "html": "5.6\nVersion 4.7.3\n\nReleased on 2022-05-24.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.7.3.\n\nWe recommend that you upgrade to the latest 4.6 release before moving to 4.7.3.\n\nA rolling upgrade from 4.6.x to 4.7.3 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.7.0 release notes for a full list of changes in the 4.7 series.\n\nFixes\n\nEnabled to alter the setting blocks.read_only_allow_delete on blob tables to make it possible to drop read-only blob tables.\n\nFixed an issue that could cause queries on sys.snapshots to get stuck and consume a significant amount of resources.\n\nFixed an issue with primary key columns that have a DEFAULT clause. That could lead to queries on the primary key column not matching the row.\n\nFixed an issue with aliased sub-relation outputs when used inside the outer where clause expression, resulting in a planner error. Example: SELECT * FROM (SELECT id, true AS a FROM t1) WHERE a\n\nFixed an issue with table functions parameter binding in SELECT queries without FROM clause. Example: SELECT unnest(?).\n\nFixed an issue with VARCHAR and BIT columns with a length limited used in primary key, generated or default column expression. An ALTER TABLE statement removed the length limit from such columns."
  },
  {
    "title": "Version 4.7.2 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.7.2.html",
    "html": "5.6\nVersion 4.7.2\n\nReleased on 2022-04-28.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.7.2.\n\nWe recommend that you upgrade to the latest 4.6 release before moving to 4.7.2.\n\nA rolling upgrade from 4.6.x to 4.7.2 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nFixes\n\nSee the Version 4.7.0 release notes for a full list of changes in the 4.7 series.\n\nFixes\n\nUpdated the bundled JDK to 17.0.3+7\n\nFixed an issue with the handling of intervals in generated columns. The table creation failed when an interval is included in a function call as part of a generated column.\n\nFixed an issue with the handling of quoted identifiers in column names where certain characters break the processing. This makes sure any special characters can be used as column name.\n\nFixed a race condition that could cause a blocked by: [FORBIDDEN/4/Table or partition preparing to close error when inserting into a partitioned table where a single partition got closed.\n\nFixed an issue that caused an Relation unknown error while trying to close an empty partitioned table using ALTER TABLE ... CLOSE.\n\nFixed an issue that caused COPY FROM RETURN SUMMARY fail non-gracefully in case of import from CSV containing invalid line(s).\n\nBumped JNA library to version 5.10.0. This will make CrateDB start without JNA library warnings on M1 chip based MacOS systems.\n\nUpdated to Admin UI 1.20.2, which fixes duplicate entries in query history.\n\nFixed an issue that threw SQLParseException when a ILIKE operand contained ‘{‘ or ‘}’.\n\nFixed an issue that caused ALTER TABLE ADD COLUMN to lose an optional routing_column information provided at table creation."
  },
  {
    "title": "Version 4.8.0 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/4.8.0.html",
    "html": "5.6\nVersion 4.8.0\n\nReleased on 2022-04-28.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 4.8.0.\n\nWe recommend that you upgrade to the latest 4.3 release before moving to 4.8.0.\n\nA rolling upgrade from 4.7.x to 4.8.0 is supported.\n\nBefore upgrading, you should back up your data.\n\nTable of Contents\n\nBreaking Changes\n\nChanges\n\nSQL Statements and Compatibility\n\nNew Scalars\n\nAdministration and Operations\n\nPerformance\n\nBreaking Changes\n\nRemoved support for HDFS snapshot repositories. We suspect nobody uses it anymore. If you require HDFS support please reach out to us, if there is enough interest we may be able to provide a plugin with the functionality.\n\nChanges\nSQL Statements and Compatibility\n\nAdded support for UNION DISTINCT or UNION statement to be able to retrieve unique rows from multiple relations without using sub-queries with extra GROUP BY clauses.\n\nImplemented cancelling requests section of PostgreSQL wire protocol.\n\nAdded a WITH clause parameter, validation to COPY FROM which can enable or disable the newly added type validation feature.\n\nAdded type validation logic to COPY FROM. Now raw data will be parsed and validated against the target table schema and casted if possible utilizing type casting.\n\nAllow users to be able to specify different S3 compatible storage endpoints to COPY FROM/TO statements by embedding the host and port to the URI parameter and also a WITH clause parameter protocol to choose between HTTP or HTTPS.\n\nAdded the option to import CSV files without field headers using the COPY FROM statement.\n\nAdded the option to import only a subset of columns using COPY FROM when importing CSV files with headers.\n\nAdded the option to run COPY FROM and COPY TO operations in the background without waiting for them to complete.\n\nNew Scalars\n\nAdded the array_position function which returns the position of the first occurrence of the provided value in an array. A starting position can be optionally provided.\n\nAdministration and Operations\n\nAdded the Logical Replication feature allowing to replicate data across multiple clusters.\n\nWrite blocks added due to low disk space are now automatically removed if a node again drops below the high watermark.\n\nAdded a flush_stats column to the sys.shards table.\n\nUpdated to Admin UI 1.21.0, which improves console highlighting by adding various keywords and data types.\n\nPerformance\n\nOptimized the casting from string to arrays by avoiding an unnecessary string to byte conversion.\n\nImproved the evaluation performance of implicit casts by utilizing the compile step of the function to determine the return type."
  },
  {
    "title": "Version 5.2.11 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.2.11.html",
    "html": "5.6\nVersion 5.2.11\n\nReleased on 2023-12-21.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.2.11.\n\nWe recommend that you upgrade to the latest 5.1 release before moving to 5.2.11.\n\nA rolling upgrade from 5.1.x to 5.2.11 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nSecurity Fixes\n\nPackaging Changes\n\nSee the Version 5.2.0 release notes for a full list of changes in the 5.2 series.\n\nSecurity Fixes\n\nThe HTTP transport will not trust any X-Real-IP header by default anymore. This prevents a client from spoofing its IP address by setting these headers and thus bypassing IP based authentication with is enabled by default for the crate superuser. To keep allowing the X-Real-IP header to be trusted, you have to explicitly enable it via the auth.trust.http_support_x_real_ip node setting.\n\nPackaging Changes\n\nThe RPM and DEB packages changed slightly to unify the build process. The most important change is that the crate service no longer automatically starts after package installation, to allow changing the configuration first.\n\nOther than that, the structure is now:\n\nbin, jdk and lib are installed into /usr/share/crate. In the RPM package this used to be in /opt/crate.\n\nThe home directory of the crate user is /usr/share/crate\n\nchanges, notice, license are in /usr/share/doc/crate\n\nservice file is in /usr/lib/systemd/system\n\nThe crate.yml configuration file is in /etc/crate/\n\nThe default environment configuration file at RPM packages changed to /etc/default/crate to be consistent with the DEB package. The old location at /etc/sysconfig/crate is not supported anymore.\n\nIf you haven’t made any significant configuration changes the new packages should keep working out of the box.\n\nImportant for Debian and Ubuntu users: There is now a new repository.\n\nYou’ll have to update the repository configuration to install CrateDB newer than 5.5.1.\n\nThis new repository keeps old CrateDB versions in the Package index and also contains packages for ARM64."
  },
  {
    "title": "Version 5.3.1 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.3.1.html",
    "html": "5.6\nVersion 5.3.1\n\nReleased on 2023-04-28.\n\nWarning\n\nCrateDB 5.3.x versions up to 5.3.3 (excluding) contain a critical bug which can lead to data corruption/loss when using a column definition with a number data type and disabled index (INDEX OFF). It is not recommended to use those versions, use CrateDB >= 5.3.3 instead.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.3.1.\n\nWe recommend that you upgrade to the latest 5.2 release before moving to 5.3.1.\n\nA rolling upgrade from 5.3.x to 5.3.1 is supported. For upgrades from 5.2.x, see the warning below. Before upgrading, you should back up your data.\n\nWarning\n\nDue to a bug in the replication layer, rolling upgrades from 5.2.x to 5.3.1 with ongoing write traffic can lead to corrupted shards and in worse case, data loss. We recommend that you stop all write traffic before upgrading and/or perform a full cluster restart.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nFixes\n\nSee the Version 5.3.0 release notes for a full list of changes in the 5.3 series.\n\nFixes\n\nFixed a performance regression introduced in 5.2.3 which led to filters on object columns resulting in a table scan if used with views or virtual tables. See #14015 for details.\n\nFixed an issue that caused geo_shape_array IS NULL expressions to fail with an IllegalStateException.\n\nFixed an issue that caused the actual cast/type conversion error to be hidden when it failed for a sub-column of an object column, when using a client statement with parameters i.e (python).:\n\nCREATE TABLE a (b OBJECT(DYNAMIC) AS (c REAL));\n# create a connection and a cursor and then:\ncursor.execute(\"INSERT INTO a VALUES (?)\", [({\"c\": True},)])\n\n\nFixed a regression that caused the -h option in bin/crate to fail with an Error parsing arguments! error.\n\nAdded a null_or_empty scalar function that can be used as a faster alternative to IS NULL if it’s acceptable to match on empty objects. This makes it possible to mitigate a performance regression introduced in 5.0.3 and 5.1.1\n\nFixed an issue that led to NullPointerException when trying to query an OBJECT field with no values, using the NOT operator, e.g.:\n\nCREATE TABLE test (obj OBJECT(DYNAMIC)); -- no data\nSELECT myobj FROM test WHERE (obj::TEXT) NOT LIKE '%value%';\n\n\nFixed an issue in the PostgreSQL wire protocol implementation that could lead to ClientInterrupted errors with some clients. An example client is pg-cursor.\n\nFixed an issue that allowed creating columns with names conflicting with subscript pattern, such as \"a[1]\", a subscript expression enclosed in double quotes.\n\nFixed an issue that caused SQLParseException when quoted subscript expressions contained quotes. An example would be querying an array with the name containing quotes like SELECT \"arr\"\"[1]\";.\n\nFixed an issue that caused ALTER TABLE ADD COLUMN statement to assign PRIMARY KEY to wrong columns, when adding multiple primary key columns, having none-primary columns in-between.\n\nFixed an issue that caused ALTER TABLE ADD COLUMN statement to assign a wrong type to ARRAY(TEXT) column and create a TEXT column instead if column has a FULLTEXT index.\n\nFixed an issue that prevented assigning default expression to ARRAY columns.\n\nReverted base image change in the Docker image as it broke downstream components which rely on some of the bundled tools like rev."
  },
  {
    "title": "Version 5.3.2 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.3.2.html",
    "html": "5.6\nVersion 5.3.2\n\nReleased on 2023-05-24.\n\nWarning\n\nCrateDB 5.3.x versions up to 5.3.3 (excluding) contain a critical bug which can lead to data corruption/loss when using a column definition with a number data type and disabled index (INDEX OFF). It is not recommended to use those versions, use CrateDB >= 5.3.3 instead.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.3.2.\n\nWe recommend that you upgrade to the latest 5.2 release before moving to 5.3.1.\n\nA rolling upgrade from 5.2.x to 5.3.2 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nFixes\n\nSee the Version 5.3.0 release notes for a full list of changes in the 5.3 series.\n\nFixes\n\nFixed an issue that would exclude empty partitioned tables from being listed in sys.snapshots.\n\nFixed a regression introduced in 5.3.0 that prevented the evaluation of DEFAULT clauses on children of OBJECT columns if the object was missing entirely from an INSERT INTO statement.\n\nImproved error message when providing DEFAULT clause for columns of type OBJECT.\n\nFixed a regression introduced in 5.3.0 that could lead to INSERT INTO statements with a ON CONFLICT clause to mix up values and target columns, leading to validation errors or storing the wrong values in the wrong columns.\n\nFixed an issue that LIKE and ILIKE operators would produce wrong results when the ? is used in the pattern string, e.g.:\n\nSELECT * FROM tbl WHERE q ILIKE '%.com?apiPath%'\n\n\nFixed an issue that would cause all tables within a Snapshot to be restored, when trying to restore an empty partitioned table, e.g.:\n\nRESTORE SNAPSHOT repo1.snap1 TABLE empty_parted\n\n\nFixed an issue with Azure repositories, which could lead to wrong results for queries to sys.snapshots, create snapshots to a wrong repository, or drop snapshots from a wrong repository when more than 1 repositories are configured to the same CrateDB cluster.\n\nFixed an issue that could lead to queries to become stuck instead of failing with a circuit breaker error if a node is under memory pressure.\n\nImproved an optimization rule to enable index lookups instead of table scans in more cases. This is a follow up to a fix in 5.2.7 which fixed a regression introduced in 5.2.3.\n\nFixed an issue that caused DROP TABLE IF EXISTS to wrongly return 1 row affected or SQLParseException (depending on user privileges), when called on an existent schema, a non-existent table and with the crate catalog prefix, e.g.:\n\nDROP TABLE IF EXISTS crate.doc.non_existent_table\n\n\nImproved output representation of timestamp subtraction, by normalizing to bigger units, but no further than days, to be consistent with PostgreSQL behavior. e.g:\n\nSELECT '2022-12-05T11:22:33.123456789+05:30'::timestamp - '2022-12-03T11:22:33.123456789-02:15'::timestamp\n\n\npreviously would return: PT40H15M and now returns: P1DT16H15M.\n\nImproved error message for date_bin scalar function when the first argument of INTERVAL data type contains month and/or year units.\n\nFixed an issue that allowed inserting a non-array value into a column that is dynamically created by inserting an empty array, ultimately modifying the type of the column. The empty arrays will be convert to nulls when queried. For example:\n\nCREATE TABLE t (o OBJECT);\nINSERT INTO t VALUES ({x=[]});\nINSERT INTO t VALUES ({x={}});  /* this is the culprit statement, inserting an object onto an array typed column */\nSHOW CREATE TABLE t;\n+-----------------------------------------------------+\n| SHOW CREATE TABLE doc.t                             |\n+-----------------------------------------------------+\n| CREATE TABLE IF NOT EXISTS \"doc\".\"t\" (              |\n|    \"o\" OBJECT(DYNAMIC) AS (                         |\n|       \"x\" OBJECT(DYNAMIC)  /* an array type modified to an object type */\nSELECT * FROM t;\n+-------------+\n| o           |\n+-------------+\n| {\"x\": {}}   |\n| {\"x\": null} |  /* an empty array converted to null */\n+-------------+\n\n\nFixed an issue that caused AssertionError to be thrown when referencing previous relations, not explicitly joined, in an join condition, e.g.:\n\nSELECT * FROM t1\nCROSS JOIN t2\nINNER JOIN t3 ON t3.x = t1.x AND t3.y = t2\n\n\nFixed an issue that caused the default expressions on columns of type GEO_SHAPE to be ignored on writes.\n\nFixed a race condition issue while concurrently accessing S3 repositories with different settings, e.g. by queries against sys.snapshots.\n\nFixed an issue in a mixed cluster scenario that may cause incoming writes written on a node < 5.3.0 to fail when replicated to a node >= 5.3.0."
  },
  {
    "title": "Version 5.3.0 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.3.0.html",
    "html": "5.6\nVersion 5.3.0\n\nReleased on 2023-04-04.\n\nWarning\n\nCrateDB 5.3.x versions up to 5.3.3 (excluding) contain a critical bug which can lead to data corruption/loss when using a column definition with a number data type and disabled index (INDEX OFF). It is not recommended to use those versions, use CrateDB >= 5.3.3 instead.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.3.0.\n\nWe recommend that you upgrade to the latest 5.2 release before moving to 5.3.0.\n\nBefore upgrading, you should back up your data.\n\nWarning\n\nDue to a bug in the replication layer, rolling upgrades from 5.2.x to 5.3.0 with ongoing write traffic can lead to corrupted shards and in worse case, data loss. We recommend that you stop all write traffic before upgrading and/or perform a full cluster restart.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nBreaking Changes\n\nDeprecations\n\nChanges\n\nSQL Standard And PostgreSQL Schema Compatibility\n\nSQL statements\n\nPerformance Improvements\n\nBreaking Changes\n\nRemoved support for SET LICENSE. The statement had no effect since CrateDB 4.5 and was only kept for backward compatibility.\n\nDeprecations\n\nNone\n\nChanges\nSQL Standard And PostgreSQL Schema Compatibility\n\nChanged the behavior of SHOW search_path to omit the implicit pg_catalog schema, unless the user set it explicitly. This matches the PostgreSQL behavior.\n\nAllowed schema and table names to contain upper case letters. This can be achieved by quoting the names. Unquoted names with upper case letters are converted to lower cases which has been the existing behaviour.\n\nAllowed schema and table names to start with _.\n\nAdded the col_description(integer, integer) scalar function for improved PostgreSQL compatibility. CrateDB does not support comments for columns, so this function always returns NULL.\n\nSQL statements\n\nChanged the behavior of INSERT INTO on tables with generated columns which use non-deterministic functions. Previously, if the columns were included in the column target list, the provided values were validated against re-computed values of the generated expression. Given that the functions are not deterministic, this validation always failed and made it impossible to copy data between tables with such columns.\n\nIn the new behavior the values are no longer validated for non-deterministic functions but are accepted as is. If the column is not present in the target list, they are re-computed as before. This lets users do copy operations between such tables and decide if the source value should be used as is, or if they should be re-computed by either adding or removing the columns from the INSERT INTO column target list.\n\nPerformance Improvements\n\nOptimized the evaluation of CASE expressions to prevent stack overflows for very large expressions.\n\nImproved the performance of queries using a correlated sub-query inside the WHERE clause in conjunction with a non-correlated filter clause.\n\nImproved performance of statements that create multiple partitions at once, which can occur during COPY FROM or INSERTS with multi-values into partitioned tables.\n\nImproved ingestion performance by up to 30%."
  },
  {
    "title": "Version 5.1.1 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.1.1.html",
    "html": "5.6\nVersion 5.1.1\n\nReleased on 2022-11-09.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.1.1.\n\nWe recommend that you upgrade to the latest 5.0 release before moving to 5.1.1.\n\nA rolling upgrade from 5.0.x to 5.1.1 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nFixes\n\nSee the Version 5.1.0 release notes for a full list of changes in the 5.1 series.\n\nFixes\n\nFixed an issue that prevented _id IN (SELECT ...) from matching records.\n\nFixed an issue that could lead to a class_cast_exception error when using ORDER BY on a column of type TEXT or VARCHAR.\n\nChanged the logic to resolve functions. Previously it would first look for built-ins for all schemas within the search path before looking up user defined functions. Now it will search for built-in and UDF per schema to prioritize UDFs earlier in the search path over built-ins later in the search path.\n\nFixed an issue that could lead to a encoded value getting returned for bit columns that are part of an object column.\n\nFixed an issue that caused incorrect results to be returned when using array(subquery) when the subquery is using ORDER BY on a different column than the one returned, i.e.:\n\nSELECT array(SELECT country FROM sys.summits ORDER BY height DESC LIMIT 3)\n\n\nFixed an issue that prevented defining a bit column with the same name as the parent object within a table. i.e.:\n\nCREATE TABLE tbl (x OBJECT AS (x bit(1)))\n                  ^            ^\n\n\nFixed an issue that could lead to out of memory errors if using the percentile aggregation.\n\nFixed an issue that could lead to serialization errors when using the bit type in objects.\n\nFixed an issue that could lead to IllegalIndexShardStateException errors when running a SELECT count(*) FROM tbl on partitioned tables.\n\nFixed an issue which caused PRIMARY KEY columns to be required on insert even if they are generated and their source columns are default not-null, i.e.:\n\nCREATE TABLE test (\n  id INT NOT NULL PRIMARY KEY,\n  created TIMESTAMP WITH TIME ZONE DEFAULT current_timestamp NOT NULL,\n  month TIMESTAMP GENERATED ALWAYS AS date_trunc('month', created) PRIMARY KEY\n);\n\nINSERT INTO test(id) VALUES(1);\n\n\nFixed an issue that could cause COPY FROM, INSERT INTO, UPDATE and DELETE operations to get stuck if under memory pressure.\n\nFixed an issue that didn’t allow queries with a greater than 0 OFFSET but without LIMIT to be executed successfully, i.e.:\n\nSELECT * FROM test OFFSET 10\nSELECT * FROM test LIMIT null OFFSET 10\nSELECT * FROM test LIMIT ALL OFFSET 10\n\n\nFixed an issue that caused col IS NULL to match empty objects.\n\nFixed an issue that caused ARRAY_COL = [] to throw an exception on OBJECT, GEO_SHAPE, IP or BIT array element types.\n\nFixed an issue that caused queries reading values of type BIT to return a wrong result if the query contains a WHERE clause pk_col = ? condition.\n\nFixed an issue that prevented NOT NULL constraints on GEO_SHAPE columns from showing up in SHOW CREATE TABLE statements."
  },
  {
    "title": "Version 5.0.3 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.0.3.html",
    "html": "5.6\nVersion 5.0.3\n\nReleased on 2022-10-18.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.0.3.\n\nWe recommend that you upgrade to the latest 4.8 release before moving to 5.0.3.\n\nA rolling upgrade from 4.8.x to 5.0.3 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nFixes\n\nSee the Version 5.0.0 release notes for a full list of changes in the 5.0 series.\n\nFixes\n\nFixed an issue which caused PRIMARY KEY columns to be required on insert even if they are generated and their source columns are default not-null, i.e.:\n\nCREATE TABLE test (\n  id INT NOT NULL PRIMARY KEY,\n  created TIMESTAMP WITH TIME ZONE DEFAULT current_timestamp NOT NULL,\n  month TIMESTAMP GENERATED ALWAYS AS date_trunc('month', created) PRIMARY KEY\n);\n\nINSERT INTO test(id) VALUES(1);\n\n\nFixed an issue that could cause COPY FROM, INSERT INTO, UPDATE and DELETE operations to get stuck if under memory pressure.\n\nFixed an issue that didn’t allow queries with a greater than 0 OFFSET but without LIMIT to be executed successfully, i.e.:\n\nSELECT * FROM test OFFSET 10\nSELECT * FROM test LIMIT null OFFSET 10\nSELECT * FROM test LIMIT ALL OFFSET 10\n\n\nFixed an issue that caused col IS NULL to match empty objects."
  },
  {
    "title": "Version 5.1.0 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.1.0.html",
    "html": "5.6\nVersion 5.1.0\n\nReleased on 2022-10-11.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.1.0.\n\nWe recommend that you upgrade to the latest 5.0 release before moving to 5.1.0.\n\nA rolling upgrade from 5.0.x to 5.1.0 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nBreaking Changes\n\nDeprecations\n\nChanges\n\nSQL Statements\n\nSQL Standard And PostgreSQL Schema Compatibility\n\nPerformance Improvements\n\nAdministration and Operations\n\nBreaking Changes\n\nRemoved the node.store.allow_mmapfs setting. It was deprecated in 4.1.0 in favour of the node.store.allow_mmap setting.\n\nRemoved the indices.breaker.fielddata.limit setting and the *.overhead settings for all circuit breakers. They were deprecated in 4.3.0 and had no effect since then.\n\nRemoved the deprecated discovery.zen.publish_timeout, discovery.zen.commit_timeout, discovery.zen.no_master_block, discovery.zen.publish_diff.enable settings. They had no effect since 4.0.0 and have been deprecated in 4.4.0.\n\nRemoved the deprecated azure discovery functionality.\n\nFields referencing catalog in information_schema tables now return 'crate' (the only catalog in CrateDB) instead of the table schema.\n\nDeprecations\n\nDeprecated the upgrade_segments option of the OPTIMIZE TABLE statement. The option will now longer have any effect and will be removed in the future.\n\nChanges\nSQL Statements\n\nAdded initial support for cursors. See DECLARE, FETCH and CLOSE.\n\nAdded support for the EXISTS expression.\n\nAdded support for correlated scalar sub-queries within the select list of a query. See Scalar subquery.\n\nAdded support of GROUP BY on ARRAY typed columns.\n\nSQL Standard And PostgreSQL Schema Compatibility\n\nAdded support for SET TIME ZONE to improve PostgreSQL Compatibility. Timezone will be ignored on the server side.\n\nAdded a application_name session setting that can be used to identify clients or applications which connect to a CrateDB node.\n\nAdded support for catalog in fully qualified table and column names, i.e.:\n\nSELECT * FROM crate.doc.t1;\nSELECT crate.doc.t1.a, crate.doc.t1.b FROM crate.doc.t1;\n\n\nMade the commas between successive transaction_modes of the BEGIN and its SQL equivalent START TRANSACTION statement optional to support compatibility with clients and tools using an older (< 8.0) PostgreSQL syntax.\n\nChanged the interval parameter of date_trunc to be case insensitive.\n\nAdded support for 'YES', 'ON' and '1' as alternative way to specify a TRUE boolean constant and 'NO', 'OFF' and '0' as alternative way to specify FALSE boolean constant improving compatibility with PostgreSQL.\n\nAdded support for casting TIMESTAMP and TIMESTAMP WITHOUT TIME ZONE values to the DATE data type and vice versa.\n\nPerformance Improvements\n\nImprove performance of queries on sys.snapshots.\n\nAdministration and Operations\n\nUpdated to Admin UI 1.23.1, which improves scrolling behavior on wide result sets, and fixes formatting of TIMESTAMP WITHOUT TIME ZONE values in query console result table.\n\nAdded I/O throughput throttling of the ANALYZE statement as well as of the periodic statistic collection controlled by the stats.service.interval setting to lower the impact on the cluster load. This throttling can be controlled by a new setting stats.service.max_bytes_per_sec and is set 40MB/s by default."
  },
  {
    "title": "Version 5.1.2 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.1.2.html",
    "html": "5.6\nVersion 5.1.2\n\nReleased on 2022-12-07.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.1.2.\n\nWe recommend that you upgrade to the latest 5.0 release before moving to 5.1.2.\n\nA rolling upgrade from 5.0.x to 5.1.2 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nFixes\n\nSee the Version 5.1.0 release notes for a full list of changes in the 5.1 series.\n\nFixes\n\nFixed privileges for DECLARE for cursors to make it accessible for non-super-users. All users can declare cursors as long as they have the required permissions for the query used within DECLARE.\n\nAdded dynamic bulk sizing for update by query or delete by query operations to reduce memory pressure and fix out of memory errors when running update statements with large assignment expressions.\n\nFixed inefficient join optimizations on hash and nested-loop joins when table statistics aren’t available.\n\nFixed a race condition that could lead to a NullPointerException when using IS NULL on an object that was just added to a table.\n\nFixed an issue that caused the generated expressions on columns of type GEO_SHAPE not being evaluated on writes and such being ignored.\n\nFixed an issue that could generate duplicate data on COPY FROM while some internal retries were happening due to I/O errors e.g. socket timeouts.\n\nFixed an issue that caused routing.allocation.*.{attribute} settings to be ignored when executing ALTER TABLE SET on a partitioned table.\n\nFixed an issue that caused the setting, number_of_replicas to be ignored when executing ALTER TABLE SET on a partitioned table."
  },
  {
    "title": "Version 5.1.3 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.1.3.html",
    "html": "5.6\nVersion 5.1.3\n\nReleased on 2023-01-11.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.1.3.\n\nWe recommend that you upgrade to the latest 5.0 release before moving to 5.1.3.\n\nA rolling upgrade from 5.0.x to 5.1.3 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nFixes\n\nSee the Version 5.1.0 release notes for a full list of changes in the 5.1 series.\n\nFixes\n\nFixed an issue that led to table not being dropped when swapping two tables with drop_source = true, when either or both source and target tables were partitioned.\n\nFixed an issue that allowed users without the related privileges to check other users’ privileges by calling has_schema_privilege function.\n\nFixed an issue that prevented UDFs from accessing nested objects.\n\nFixed an issue that caused SELECT * statements to fail if a table has an object with inner null object and a sibling column with the same name with one of the sub-columns. An example:\n\nCREATE TABLE IF NOT EXISTS \"t\" (\n  \"obj1\" OBJECT(DYNAMIC) AS (\n   \"target\" text,\n   \"obj2\" OBJECT(DYNAMIC) AS (\n      \"target\" REAL\n   )\n  )\n);\nINSERT INTO t VALUES ('{\"obj2\": null, \"target\": \"Sensor\"}');\nSELECT * FROM t;\n\n\nFixed an issue that caused swap table to accept invalid table names provided in a double-quoted string format containing . such as \"table.t\" by mis-interpreting it as \"table\".\"t\", which is a two double-quoted strings joined by a ..\n\nFixed an issue that caused failure of the statements containing comparison of bit strings with different length. An example:\n\nSELECT B'01' = B'1'\n\n\nFixed an issue that caused failures of queries joining a table to a virtual table where virtual table is another JOIN on aliased column and having a LIMIT clause. An example:\n\nCREATE TABLE t1 (x INTEGER, i INTEGER);\nCREATE TABLE t2 (y INTEGER);\n\nSELECT * from GENERATE_SERIES(1, 2)\nCROSS JOIN\n  (SELECT t1.i, t2.y AS aliased from t1 inner join t2 on t1.x = t2.y) v\nLIMIT 10\n"
  },
  {
    "title": "Version 5.1.4 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.1.4.html",
    "html": "5.6\nVersion 5.1.4\n\nReleased on 2023-02-01.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.1.4.\n\nWe recommend that you upgrade to the latest 5.0 release before moving to 5.1.4.\n\nA rolling upgrade from 5.0.x to 5.1.4 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nFixes\n\nSee the Version 5.1.0 release notes for a full list of changes in the 5.1 series.\n\nFixes\n\nAdded validation to reject inner column names containing special whitespace characters to avoid invalid schema definitions.\n\nFixed an issue that caused the returned column names to be missing the subscripts when querying sub-columns of nested object arrays.\n\nFixed an issue that caused ClassCastException when accessing a sub-column of a nested object array where the sub-column resolves to a nested array. e.g.:\n\nCREATE TABLE test (\n  \"a\" ARRAY(OBJECT AS (\n    \"b\" ARRAY(OBJECT AS (\n      \"s\" STRING\n    )))));\nINSERT INTO test (a) VALUES ([{b=[{s='1'}, {s='2'}, {s='3'}]}]);\nSELECT a['b'] FROM test; // a['b'] is type of array(array(object))\n\n\nFixed an issue in the PostgreSQL wire protocol that would cause de-serialization of arrays to fail if they contained unquoted strings consisting of more than 2 words."
  },
  {
    "title": "Version 5.2.1 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.2.1.html",
    "html": "5.6\nVersion 5.2.1\n\nReleased on 2023-02-02.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.2.1.\n\nWe recommend that you upgrade to the latest 5.1 release before moving to 5.2.1.\n\nA rolling upgrade from 5.1.x to 5.2.1 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nFixes\n\nSee the Version 5.2.0 release notes for a full list of changes in the 5.2 series.\n\nFixes\n\nFixed behaviour of FETCH RELATIVE, which previously behaved identically to FETCH FORWARD and FETCH BACKWARD, whereas it should behave similarly to FETCH ABSOLUTE, with the difference that the position of the 1 row to return is calculated relatively to the current cursor position.\n\nFixed an issue that caused accounted memory not to be released when using cursors, even if the CURSOR was explicitly or automatically (session terminated) closed.\n\nAdded validation to reject inner column names containing special whitespace characters to avoid invalid schema definitions.\n\nFixed an issue that caused the returned column names to be missing the subscripts when querying sub-columns of nested object arrays.\n\nFixed an issue that caused IndexOutOfBoundsException to be thrown when trying to FETCH backwards from a CURSOR, past the 1st row.\n\nFixed an issue that caused wrong rows to be returned when scrolling backwards and then forwards through a CURSOR.\n\nImproved error message when fetching using ABSOLUTE, past the last row returned by the cursor query.\n\nFixed and issue that caused wrong or 0 rows to be returned when trying to fetch all rows backwards from a CURSOR using: FETCH BACKWARDS ALL.\n\nFixed an issue that caused swap table to consume invalid table names provided in a double-quoted string format containing . such as \"table.t\" by mis-interpreting it as \"table\".\"t\", which is a two double-quoted strings joined by a .. This caused metadata corruptions leading to StartupExceptions and data losses. Corrupted metadata recovery is in place to prevent the exceptions but not all data can be recovered.\n\nFixed an issue that caused ClassCastException when accessing a sub-column of a nested object array where the sub-column resolves to a nested array. An example\n\nCREATE TABLE test (\n  \"a\" ARRAY(OBJECT AS (\n    \"b\" ARRAY(OBJECT AS (\n      \"s\" STRING\n    )))));\nINSERT INTO test (a) VALUES ([{b=[{s='1'}, {s='2'}, {s='3'}]}]);\nSELECT a['b'] FROM test; // a['b'] is type of array(array(object))\n\n\nFixed an issue in the PostgreSQL wire protocol that would cause de-serialization of arrays to fail if they contained unquoted strings consisting of more than 2 words."
  },
  {
    "title": "Version 5.3.3 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.3.3.html",
    "html": "5.6\nVersion 5.3.3\n\nReleased on 2023-06-26.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.3.3.\n\nWe recommend that you upgrade to the latest 5.2 release before moving to 5.3.3.\n\nA rolling upgrade from 5.2.x to 5.3.3 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nFixes\n\nSee the Version 5.3.0 release notes for a full list of changes in the 5.3 series.\n\nFixes\n\nFixed an issue introduced with CrateDB 5.3.0 resulting in failing writes, broken replica shards, or even un-recoverable tables on tables using a column definition with a number data type and an explicit INDEX OFF. Any table that was created with INDEX OFF on a number column and already written to with CrateDB version >= 5.3.0 should be recreated using e.g. INSERT INTO new_table SELECT * FROM old_table (followed by swap table ALTER CLUSTER SWAP TABLE new_table TO old_table) or restored from a backup.\n\nFixed a regression introduced in 4.7.0 which caused aggregations used in INSERT INTO statements returning null instead of the aggregation result.\n\nFixed a regression introduced in 5.3.0 which caused CAST(<generated_partition_column> AS <targetType>) expressions to return NULL instead of the cast result.\n\nFixed a regression introduced in 5.3.0 which caused INSERT INTO statements with a ON CONFLICT clause on tables with generated primary key columns to fail with an ArrayIndexOutOfBoundsException.\n\nFixed an issue that caused an NullPointerException while inserting a TIMETZ typed value dynamically which is not supported.\n\nFixed a regression introduced in 5.3.0 which caused INSERT INTO statements to reject invalid dynamic columns and their value without raising an error or skipping the whole record. An example\n\nCREATE TABLE t(a INT) WITH (column_policy='dynamic');\nINSERT INTO t(a, _b) VALUES (1, 2);\nINSERT OK, 1 row affected  (0.258 sec)\nINSERT INTO t(a, _b) (SELECT 2, 2);\nINSERT OK, 1 row affected  (0.077 sec)\nSELECT * FROM t;\n+---+\n| a |\n+---+\n| 1 |\n| 2 |\n+---+\nSELECT 2 rows in set (0.594 sec)\n\n\nIn 5.2.0 neither variant inserted a record. The first INSERT raised an error, and the second resulted in row count 0.\n\nFixed an issue which caused INSERT INTO statements to skip generated column validation for sub-columns if provided value is NULL."
  },
  {
    "title": "Version 5.3.8 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.3.8.html",
    "html": "5.6\nVersion 5.3.8\n\nReleased on 2023-12-21.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.3.8.\n\nWe recommend that you upgrade to the latest 5.3 release before moving to 5.3.8.\n\nA rolling upgrade from 5.2.x to 5.3.8 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nSecurity Fixes\n\nPackaging Changes\n\nSee the Version 5.3.0 release notes for a full list of changes in the 5.3 series.\n\nSecurity Fixes\n\nThe HTTP transport will not trust any X-Real-IP header by default anymore. This prevents a client from spoofing its IP address by setting these headers and thus bypassing IP based authentication with is enabled by default for the crate superuser. To keep allowing the X-Real-IP header to be trusted, you have to explicitly enable it via the auth.trust.http_support_x_real_ip node setting.\n\nPackaging Changes\n\nThe RPM and DEB packages changed slightly to unify the build process. The most important change is that the crate service no longer automatically starts after package installation, to allow changing the configuration first.\n\nOther than that, the structure is now:\n\nbin, jdk and lib are installed into /usr/share/crate. In the RPM package this used to be in /opt/crate.\n\nThe home directory of the crate user is /usr/share/crate\n\nchanges, notice, license are in /usr/share/doc/crate\n\nservice file is in /usr/lib/systemd/system\n\nThe crate.yml configuration file is in /etc/crate/\n\nThe default environment configuration file at RPM packages changed to /etc/default/crate to be consistent with the DEB package. The old location at /etc/sysconfig/crate is not supported anymore.\n\nIf you haven’t made any significant configuration changes the new packages should keep working out of the box.\n\nImportant for Debian and Ubuntu users: There is now a new repository.\n\nYou’ll have to update the repository configuration to install CrateDB newer than 5.5.1.\n\nThis new repository keeps old CrateDB versions in the Package index and also contains packages for ARM64."
  },
  {
    "title": "Version 5.3.4 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.3.4.html",
    "html": "5.6\nVersion 5.3.4\n\nReleased on 2023-07-11.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.3.4.\n\nWe recommend that you upgrade to the latest 5.2 release before moving to 5.3.4.\n\nA rolling upgrade from 5.2.x to 5.3.4 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nFixes\n\nSee the Version 5.3.0 release notes for a full list of changes in the 5.3 series.\n\nFixes\n\nFixed an issue introduced with CrateDB 5.3.0 resulting in failing writes, broken replica shards, or even un-recoverable tables on tables using a column definition with a IP data type and an explicit INDEX OFF. Any table that was created with INDEX OFF on a IP column and already written to with CrateDB version >= 5.3.0 should be recreated using e.g. INSERT INTO new_table SELECT * FROM old_table (followed by swap table ALTER CLUSTER SWAP TABLE new_table TO old_table) or restored from a backup.\n\nImproved error message to be user-friendly, for definition of CHECK at column level for object sub-columns, instead of a ConversionException.\n\nAdded validation to prevent creation of invalid nested array columns via INSERT INTO and dynamic column policy.\n\nFixed parsing of ARRAY literals in PostgreSQL simple query mode.\n\nFixed value of sys.jobs_log.stmt for various statements when issued via the PostgreSQL simple query mode by using the original query string instead of the statements string representation.\n\nFixed an issue that caused UPDATE and DELETE on tables with PRIMARY KEYs from ignoring non primary key symbols in WHERE clauses if the WHERE clauses contain PRIMARY KEYS, e.g.\n\nUPDATE test SET x = 10 WHERE pk_col = 1 AND x = 2; -- executed update with 'pk_col = 1' only, ignoring 'x = 2'\n\n\nFixed an issue that could cause errors for queries with aggregations, UNION and LIMIT, e.g.\n\nSELECT a, avg(c), b FROM t1 GROUP BY 1, 3\nUNION\nSELECT x, avg(z), y FROM t2 GROUP BY 1, 3\nUNION\nSELECT i, avg(k), j FROM t3 GROUP BY 1, 3\nLIMIT 10\n\n\nFixed an issue which prevented INSERT INTO ... SELECT ... from inserting any records if the target table had a partitioned column of a non-string type, used in any expressions of GENERATED or CHECK definitions.\n\nFixed an issue which caused INSERT INTO ... SELECT ... statements to skip NULL checks of CLUSTERED BY column values.\n\nFixed an issue that resulted in enabled indexing for columns defined as the BIT data type even when explicitly turning it of using INDEX OFF.\n\nFixed an issue resulting in an exception when writing data into a column of type Boolean with disabled indexing using INDEX OFF.\n\nFixed an issue that caused an exception to be thrown when inserting a non-array value into a column that is dynamically created by inserting an empty array, ultimately modifying the type of the column and then selecting this column by the row’s primary key, for example:\n\nCREATE TABLE t (id int primary key, o OBJECT(dynamic));\nINSERT INTO t VALUES (1, {x=[]});\nINSERT INTO t VALUES (2, {x={}});  /* this is the culprit statement, inserting an object onto an array typed column */\n\nSELECT * FROM t WHERE id=1;\nSQLParseException[Cannot cast object element `x` with value `[]` to type `object`]\n\n\nafter the fix:\n\nSELECT * FROM t WHERE id=1;\n+----+-------------+\n| id | o           |\n+----+-------------+\n|  1 | {\"x\": null} |\n+----+-------------+\n"
  },
  {
    "title": "Version 5.3.5 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.3.5.html",
    "html": "5.6\nVersion 5.3.5\n\nReleased on 2023-08-04.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.3.5.\n\nWe recommend that you upgrade to the latest 5.3 release before moving to 5.3.5.\n\nBefore upgrading, you should back up your data.\n\nWarning\n\nA rolling upgrade from 5.3.x to 5.3.5 is not supported if your cluster contains view definitions. If you want to do a rolling upgrade, skip 5.3.5 and upgrade to 5.3.6 or 5.4.2.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nFixes\n\nSee the Version 5.3.0 release notes for a full list of changes in the 5.3 series.\n\nFixes\n\nAllowed user to RESET cluster settings with archived. prefix.\n\nFixed an issue that could lead to a class_cast_exception when using a subscript expression on an aliased expression that shadows another column. For example:\n\nSELECT a['b'] FROM (SELECT UNNEST(a) AS a FROM tbl) t;\n\n\nFixed an issue that caused a object_column = {} query to not match if the object column doesn’t have any child columns.\n\nFixed an issue that caused queries against views to evaluate the view definition with the current search path. This could cause issues if the search path diverged. The fix only applies to newly created views.\n\nFixed a Failed to validate IP error that occurred when using the any_value aggregation on a column of type IP.\n\nImproved error message when passing settings with WITH clause on a CREATE SUBSCRIPTION statement, which is not currently supported.\n\nFixed an issue that resulted in hiding errors with the connection URL of CREATE SUBSCRIPTION, when executed from a non-superuser but with AL privileges.\n\nAllowed trailing / without a database name for connection URL of CREATE SUBSCRIPTION.\n\nFixed a NullPointerException which could happen if using a cross join on a sub-query, where the sub-query was executed using a Fetch operator. An example query:\n\nSELECT\n  *\nFROM\n  (SELECT a FROM tbl1 ORDER BY b DESC LIMIT 1) i,\n  tbl2\nWHERE\n  c >= 50;\n\n\nFixed a NullPointerException` which was thrown, instead of using default ``no compression behavior, when compression parameter of COPY TO statement is set to null.\n\nFixed IndexOutOfBoundsException caused by an IS [NOT] NULL filter on a sub-column of an object or object array in a WHERE clause, e.g.\n\nCREATE TABLE test (o1 ARRAY(OBJECT AS (col INT)), o2 OBJECT);\nSELECT * FROM test WHERE o1[1]['col'] IS NULL;\n=> IndexOutOfBoundsException[Index: 1 Size: 1]\nSELECT * FROM test AS T WHERE T.o2['unknown_col'] IS NOT NULL;\n=> IndexOutOfBoundsException[Index: 1 Size: 1]\n\n\nFixed an issue which caused INSERT INTO statements to skip generated expression validation for partitioned columns.\n\nFixed an issue which caused arrays in IGNORED objects to be converted to nulls.\n\nFixed an issue which caused INSERT INTO ... SELECT ... statements to leave behind empty partitions if NULL or CHECK constraint on partitioned by column failed.\n\nFixed an issue which caused errors on querying information_schema tables when the query of a VIEW is erroneous due to changes made to the underlying tables/views. Also, added a comment to view definition in pg_catalog.pg_views and information_schema.views tables to denote that a VIEW’s query is erroneous.\n\nFixed SQLParseException caused by querying an unknown key from an object column of a table that is aliased and with the session setting error_on_unknown_object_key, set to false, e.g.\n\nCREATE TABLE test (o OBJECT);\nSELECT T.o['unknown'] from (SELECT * FROM test) AS T;\n=> SQLParseException[Couldn't create execution plan from logical plan because of: Couldn't find o['unknown'] in SourceSymbols{inputs={}, nonDeterministicFunctions={}}\n"
  },
  {
    "title": "Version 5.3.6 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.3.6.html",
    "html": "5.6\nVersion 5.3.6\n\nReleased on 2023-08-011.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.3.6.\n\nWe recommend that you upgrade to the latest 5.3 release before moving to 5.3.6.\n\nA rolling upgrade from 5.2.x to 5.3.4 or 5.3.6 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nFixes\n\nSee the Version 5.3.0 release notes for a full list of changes in the 5.3 series.\n\nFixes\n\nFixed an issue that led to serialization errors on a rolling upgrade to 5.3.5 if the cluster contained user defined views."
  },
  {
    "title": "Version 5.3.7 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.3.7.html",
    "html": "5.6\nVersion 5.3.7\n\nReleased on 2023-10-12.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.3.7.\n\nWe recommend that you upgrade to the latest 5.3 release before moving to 5.3.7.\n\nA rolling upgrade from 5.2.x to 5.3.4, 5.3.6 or 5.3.7 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nFixes\n\nSee the Version 5.3.0 release notes for a full list of changes in the 5.3 series.\n\nFixes\n\nFixed an issue that caused privileges checks to be bypassed when using scalar sub-selects in various clauses of a query: SELECT, WHERE, HAVING, etc.\n\nFixed an issue that led to file not found errors when trying to restore a snapshot that was taken after a table had been swapped. A new snapshot must be taken to apply the fix and solve the issue."
  },
  {
    "title": "Version 5.3.9 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.3.9.html",
    "html": "5.6\nVersion 5.3.9\n\nReleased on 2024-01-29.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.3.9.\n\nWe recommend that you upgrade to the latest 5.3 release before moving to 5.3.9.\n\nA rolling upgrade from 5.2.x to 5.3.9 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nSecurity Fixes\n\nSee the Version 5.3.0 release notes for a full list of changes in the 5.3 series.\n\nSecurity Fixes\n\nFixed a security issue where any CrateDB user could read/import the content of any file on the host system, the CrateDB process user has read access to, by using the COPY FROM command with a file URI. This access is now restricted to the crate superuser only. See CVE-2024-24565 for more details. (Thanks to @Tu0Laj1 for reporting this issue)"
  },
  {
    "title": "Version 5.2.0 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.2.0.html",
    "html": "5.6\nVersion 5.2.0\n\nReleased on 2023-01-12.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.2.0.\n\nWe recommend that you upgrade to the latest 5.1 release before moving to 5.2.0.\n\nA rolling upgrade from 5.1.x to 5.2.0 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nBreaking Changes\n\nChanges\n\nSQL Statements\n\nSQL Standard And PostgreSQL Schema Compatibility\n\nScalar Functions\n\nPerformance Improvements\n\nAdministration and Operations\n\nBreaking Changes\n\nRemoved support for the CRATE_INCLUDE environment variable from the bin/crate start script. Configuration of CrateDB should happen via the crate.yml, the CRATE_HEAP_SIZE environment variable and optionally CRATE_JAVA_OPTS.\n\nRemoved support for the -d and -p options from the bin/crate start script. It’s recommended to run CrateDB either via a container runtime like Docker, or via a service manager like systemd where these options are not required.\n\nSubtraction of timestamps was returning their difference in milliseconds, but with result type TIMESTAMP which was wrong and led to issues with several PostgreSQL compliant clients. Instead of just fixing the result type, and change it to LONG, the subtraction of timestamps was changed to return an INTERVAL and be 100% compliant with PostgreSQL behaviour.\n\nBefore:\n\nSELECT '2022-12-05T11:22:33.123456789'::timestamp - '2022-11-21T10:11:22.0012334'::timestamp;\n+-----------------------+\n| 1213871122::timestamp |\n+-----------------------+\n|            1213871122 |\n+-----------------------+\n\n\nAfter:\n\nSELECT '2022-12-05T11:22:33.123456789'::timestamp - '2022-11-21T10:11:22.0012334'::timestamp;\n+------------------------------+\n| 'PT337H11M11.122S'::interval |\n+------------------------------+\n| 337:11:11.122                |\n+------------------------------+\n\n\nTo use the previous behaviour, timestamps can simply be cast to longs before subtracting them:\n\nSELECT (ts_end::long - ts_start::long) FROM test\n\n\nAlternatively, epoch can be extracted from the result of the subtraction:\n\nSELECT EXTRACT(epoch FROM ts_end - ts_start) FROM test\n\nChanges\nSQL Statements\n\nAdded support for adding multiple columns in a single ALTER TABLE ADD COLUMN statement.\n\nExtended the syntax for CREATE VIEW to allow parenthesis surrounding the query.\n\nSQL Standard And PostgreSQL Schema Compatibility\n\nBumped the version of PostgreSQL wire protocol to 11 since 10 has been deprecated.\n\nAdded has_database_privilege scalar function which checks whether user (or current user if not specified) has specific privilege(s) for the database.\n\nAdded a datestyle session setting that shows the display format for date and time values. Only the ISO style is supported. Optionally provided pattern conventions for the order of date parts (Day, Month, Year) are ignored.\n\nAdded support for SCROLL and backward movement to cursors. See DECLARE and FETCH.\n\nAdded the MAX_BY and MIN_BY aggregation functions\n\nAdded support for bit operators on integral and BIT types.\n\nAdded support for dollar quoted strings, see String Literal for further details.\n\ncancel messages sent from a client via the PostgreSQL wire protocol are now internally forwarded to other nodes to support setups with load-balancers.\n\nAdded support for SUM() aggregations on INTERVAL type. e.g.:\n\nSELECT SUM(tsEnd - tsStart) FROM test\n\nScalar Functions\n\nAdded the concat(object, object) scalar function which combines two objects into a new object containing the union of their first level properties, taking the second object’s values for duplicate properties.\n\nAdded the parse_uri(text) scalar function which parses a valid URI string into an object containing the URI components, making it easier to query them.\n\nAdded the parse_url(text) scalar function which parses a valid URL string into an object containing the URL components, including parsed query parameters, making it easier to query them.\n\nAdded support for EXTRACT(field FROM interval). e.g.:\n\nSELECT EXTRACT(MINUTE FROM INTERVAL '49 hours 127 minutes')\n\nPerformance Improvements\n\nImprove performance of snapshots related operations.\n\nAdministration and Operations\n\nAdded attributes column to sys.nodes table to expose custom node settings.\n\nExposed the require, include and exclude routing.allocation settings per partition within information_schema.table_partitions.\n\nUpdated to Admin UI 1.24.1, which added Italian translations, updated some dependency packages across the board, and its tool chain."
  },
  {
    "title": "Version 5.5.1 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.5.1.html",
    "html": "5.6\nVersion 5.5.1\n\nReleased on 2023-11-30.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.5.1.\n\nWe recommend that you upgrade to the latest 5.4 release before moving to 5.5.1.\n\nA rolling upgrade from 5.4.x to 5.5.1 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of contents\n\nFixes\n\nSee the Version 5.5.0 release notes for a full list of changes in the 5.5 series.\n\nFixes\n\nFixed an issue that caused queries with a NOT expression in the WHERE clause to fail evaluating NULL correctly.\n\nFixed an issue that caused the value for generated primary key columns to evaluate to NULL in INSERT INTO .. ON CONFLICT statements if the column wasn’t part of the target column list.\n\nCreating a table that uses a table-function as part of a default expression or generated expression now results in an error on table creation, instead of never inserting records due to runtime failures.\n\nImproved the error message when using COPY FROM with wait_for_completion=false and RETURN SUMMARY. It now reports that the combination is not supported instead of running into a ClassCastException.\n\nFixed an issue that caused queries with a NOT (a AND b) expression in the WHERE clause to not evaluate correctly with NULL values.\n\nFixed an issue that caused queries with a NOT or != on a CASE expression containing a nullable column to exclude NULL entries.\n\nRe-added jcmd to the bundled JDK distribution.\n\nReturn meaningful error when trying to drop a column which itself or its sub-columns participates in a table level constraint with other columns. For example, we cannot drop column col_to_drop from the following tables:\n\nCREATE TABLE t1 (i int, col_to_drop int,\n                 CHECK (col_to_drop + i > 0))\nCREATE TABLE t2 (i int, col_to_drop object AS (subcol_a int),\n                 CHECK (col_to_drop['subcol_a'] + i > 0))\n\n\nFixed an issue that caused a ColumnUnknownException to be thrown when attempting to drop a column with a CHECK. All relevant constraints are dropped together with the column.\n\nFixed an issue that caused the hash-join operator generate invalid hashes which lead to a broken join operation when there were more than two relations involved e.g.::\n\nSELECT * FROM t1, t2, t3 WHERE t3.c = t1.a AND t3.c = t2.b AND t1.a = t2.b;\n\n\nwould generate the logical plan::\n\nHashJoin[(t3.c = t2.b AND t1.a = t2.b)]\n  ├ HashJoin[(t3.c = t1.a)]\n  │  ├ Collect[doc.t3 | [c] | true]\n  │  └ Collect[doc.t1 | [a] | true]\n  └ Collect[doc.t2 | [b] | true]\n\n\nThe hash-symbol generation for the join t3.c = t2.b AND t1.a = t2.b was broken and would not join the data.\n\nFixed a regression introduced in 5.5.0 that caused the loss of filter conditions in nested-joins when the query was optimized by the rule optimizer_move_filter_beneath_join.\n\nFixed a regression introduced in 5.5.0 that caused empty string to be accepted as a valid column name while table creation."
  },
  {
    "title": "Version 5.5.3 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.5.3.html",
    "html": "5.6\nVersion 5.5.3\n\nReleased on 2024-01-17.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.5.3.\n\nWe recommend that you upgrade to the latest 5.4 release before moving to 5.5.3.\n\nA rolling upgrade from 5.4.x to 5.5.3 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of contents\n\nFixes\n\nSee the Version 5.5.0 release notes for a full list of changes in the 5.5 series.\n\nFixes\n\nFixed an issue, that led to no results being returned when trying to filter on a PRIMARY KEY column with an implicit cast. For example:\n\nCREATE TABLE tbl(a string PRIMARY KEY);\nSELECT * FROM tbl WHERE a = 10;\n\n\nRejected renaming of views and tables if the target table or view already exists, and return an error message. Previously, such a rename was allowed which caused the existing view or table to be lost.\n\nFixed a regression introduced in 4.2.0 that caused queries with UNNEST with a single nested array as parameter to fail with a ClassCastException. For example:\n\nSELECT x FROM unnest([[1, 2], [3]]) as t (x);\n\n\nFixed an issue that could cause subscript expressions to raise a ColumnUnknownException error despite the object having a defined schema. An example case where this happened is:\n\nCREATE TABLE tbl (obj ARRAY(OBJECT(STRICT) AS (x INT)));\nINSERT INTO tbl VALUES ([{}]);\nSELECT unnest(obj)['x'] FROM tbl;\n\n\nThere were two workarounds for this:\n\nUsing SELECT unnest(obj['x']) FROM tbl\n\nDisable errors on unknown object keys via set error_on_unknown_object_key = false;\n\nFixed a regression introduced in 5.5.0 which caused subscript expressions on object arrays to fail in some cases. For example, the following case failed with a ClassCastException:\n\nCREATE TABLE tbl (obj ARRAY(OBJECT(STRICT) AS (x BIGINT)));\nINSERT INTO tbl VALUES ([{x = 1}]);\nSELECT unnest(obj)['x'] FROM tbl;\n\n\nFixed an issue that caused UPDATE and DELETE statements to match records if the WHERE clause contained an equality condition on all primary keys, and it included an additional clause in an AND that should have evaluated to FALSE.\n\nFixed a regression introduced in 5.3.0 that caused storing an object as NULL if object had generated sub-columns and the object column wasn’t part of the INSERT targets. An object with generated sub-columns is stored now.\n\nFixed a regression introduced in 5.3.0 that caused failure for INSERT statements if a target table had 1 or more replicas, an object column with non-deterministic generated or default sub-column and the object column wasn’t part of the INSERT targets.\n\nFixed a performance regression introduced in 5.5.0 for aggregations on columns with the column store disabled.\n\nFixed a regression introduced in 5.3.0 that caused replication failure leading to an unstable cluster when INSERT... ON CONFLICT... UPDATE SET was run on a table with non-deterministic column and some of table’s columns weren’t part of the INSERT targets. An example case where this happened is:\n\nCREATE TABLE tbl (\n    id INT PRIMARY KEY,\n    a INT,\n    b TEXT,\n    modification_date TIMESTAMP AS current_timestamp\n)\nINSERT INTO tbl (id, a) VALUES (1, 2) ON CONFLICT (id) DO UPDATE SET a = 3;\n\n\nFixed an issue that caused joins with the join conditions that referred to columns from a single table only from returning invalid results. i.e.:\n\nSELECT * FROM t1 INNER JOIN t2 ON t1.col = t1.col;\n"
  },
  {
    "title": "Version 5.5.0 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.5.0.html",
    "html": "5.6\nVersion 5.5.0\n\nReleased on 2023-10-27.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.5.0.\n\nWe recommend that you upgrade to the latest 5.4 release before moving to 5.5.0.\n\nA rolling upgrade from 5.4.x to 5.5.0 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of contents\n\nBreaking Changes\n\nChanges\n\nSQL Statements\n\nSQL Standard and PostgreSQL Compatibility\n\nData Types\n\nScalar and Aggregation Functions\n\nPerformance and Resilience Improvements\n\nAdministration and Operations\n\nBreaking Changes\n\nAdded CURRENT_DATE, CURRENT_TIME and CURRENT_TIMESTAMP to non-reserved keywords.\n\nValidation for COPY FROM is now enforced. The validation parameter is deprecated and using it results in a deprecation message in the logs.\n\nUPDATE statements will continue on row failures instead of always showing an error. The affected rows are displayed by the resulting row count.\n\nRestricted Logical Replication of tables created on a cluster with higher major/minor version to a cluster with major/minor lower version.\n\nChanges\nSQL Statements\n\nChanged the CREATE TABLE analysis to be more strict. Before it was possible to use unqualified names in place of string literals to define values within WITH clauses. Now string literals are required.\n\nFor example, instead of:\n\nmy_column text index using fulltext with (analyzer = myanalyzer)\n\n\nIt is necessary to use:\n\nmy_column text index using fulltext with (analyzer = 'myanalyzer')\n\n\nChanged CREATE TABLE to allow defining CHECK constraints inline on sub-columns of object columns. Before it was necessary to use table check constraints.\n\nAdded support for ALTER TABLE DROP COLUMN statement.\n\nSQL Standard and PostgreSQL Compatibility\n\nAllowed statements that set standard_conforming_strings session setting to the default value (on).\n\nAdded a new read-only session setting max_identifier_length.\n\nAdded support for INTERVAL multiplication by integers.\n\nAdded an empty pg_catalog.pg_event_trigger table.\n\nData Types\n\nAdded support for explicit casts from strings in JSON list format to object[].\n\nAdded a FLOAT_VECTOR type to store dense vectors of float values which can be searched using a k-nearest neighbour algorithm via a new KNN_MATCH scalar.\n\nScalar and Aggregation Functions\n\nAdded a KNN_MATCH scalar.\n\nAdded a (string FROM string) overload to substr('string', from, [ count ]) to extract a substring from a string that matches a POSIX regular expression pattern.\n\nPerformance and Resilience Improvements\n\nLimited the amount of errors returned by RETURN SUMMARY of COPY FROM to prevent running into circuit breaker errors due to memory constraints.\n\nAdministration and Operations\n\nAdded an optimizer rule for cross-join elimination which will reorder the joined relations of a query to eliminate cross-joins e.g.:\n\nSELECT * FROM t1 CROSS JOIN t2 INNER JOIN t3 ON t3.z = t1.x AND t3.z = t2.y\n\n\nThis query can be reordered to t1, t3, t2 to eliminate the cross-join between t1 and t2. This will result in the following logical plan:\n\n Eval[x, y, z]\n   └ Join[INNER | (z = y)]\n     ├ Join[INNER | (x = z)]\n     │  ├ Collect[doc.t1 | [x] | true]\n     │  └ Collect[doc.t3 | [z] | true]\n     └ Collect[doc.t2 | [y] | true]\n\nThis optimizer rule can be disabled with the session settings::\n\n SET optimizer_eliminate_cross_join = false\n\n\nNote that this setting is experimental, and may change in the future.\n\nAdded support for renaming views via ALTER TABLE <view> RENAME TO <newName>.\n\nChanged permissions on sys.jobs and sys.jobs_log to allow users with the AL privileges to see entries from other users.\n\nAdded a new memory.operation_limit cluster and session setting.\n\nAdded support for endpoint and secondary endpoint to CREATE REPOSITORY for Azure storage."
  },
  {
    "title": "PostgreSQL wire protocol — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/interfaces/postgres.html",
    "html": "5.6\nPostgreSQL wire protocol\n\nCrateDB supports the PostgreSQL wire protocol v3.\n\nIf a node is started with PostgreSQL wire protocol support enabled it will bind to port 5432 by default. To use a custom port, set the corresponding Ports in the Configuration.\n\nHowever, even though connecting PostgreSQL tools and client libraries is supported, the actual SQL statements have to be supported by CrateDB’s SQL dialect. A notable difference is that CrateDB doesn’t support transactions, which is why clients should generally enable autocommit.\n\nNote\n\nIn order to use setFetchSize in JDBC it is possible to set auto commit to false.\n\nThe client will utilize the fetchSize on SELECT statements and only load up to fetchSize rows into memory.\n\nSee the PostgreSQL JDBC Query docs for more information.\n\nWrite operations will still behave as if auto commit was enabled and commit or rollback calls are ignored.\n\nTable of contents\n\nServer compatibility\n\nStart-up\n\nSSL Support\n\nAuthentication\n\nParameterStatus\n\nDatabase selection\n\nQuery modes\n\nSimple query\n\nExtended query\n\nCopy operations\n\nFunction call\n\nCanceling requests\n\npg_catalog\n\npg_type\n\nOID types\n\nShow transaction isolation\n\nBEGIN, START, and COMMIT statements\n\nClient compatibility\n\nJDBC\n\nLimitations\n\nConnection failover and load balancing\n\nImplementation differences\n\nCopy operations\n\nData types\n\nDates and times\n\nObjects\n\nArrays\n\nDeclaration of arrays\n\nType casts\n\nText search functions and operators\n\nServer compatibility\n\nCrateDB emulates PostgreSQL server version 14.\n\nStart-up\nSSL Support\n\nSSL can be configured using Secured communications (SSL/TLS).\n\nAuthentication\n\nAuthentication methods can be configured using Host-Based Authentication (HBA).\n\nParameterStatus\n\nAfter the authentication succeeded, the server has the possibility to send multiple ParameterStatus messages to the client. These are used to communicate information like server_version (emulates PostgreSQL 9.5) or server_encoding.\n\nCrateDB also sends a message containing the crate_version parameter. This contains the current CrateDB version number.\n\nThis information is useful for clients to detect that they’re connecting to CrateDB instead of a PostgreSQL instance.\n\nDatabase selection\n\nSince CrateDB uses schemas instead of databases, the database parameter sets the default schema name for future queries. If no schema is specified, the schema doc will be used as default. Additionally, the only supported charset is UTF8.\n\nQuery modes\nSimple query\n\nThe PostgreSQL simple query protocol mode is fully implemented.\n\nExtended query\n\nThe PostgreSQL extended query protocol mode is implemented with the following limitations:\n\nThe ParameterDescription message works for the most common use cases except for DDL statements.\n\nTo optimize the execution of bulk operations the execution of statements is delayed until the Sync message is received\n\nCopy operations\n\nCrateDB does not support the COPY sub-protocol, see also Copy operations.\n\nFunction call\n\nThe function call sub-protocol is not supported since it’s a legacy feature.\n\nCanceling requests\n\nPostgreSQL cancelling requests is fully implemented.\n\npg_catalog\n\nFor improved compatibility, the pg_catalog schema is implemented containing following tables:\n\npg_am\n\npg_attrdef\n\npg_attribute\n\npg_class\n\npg_constraint\n\npg_cursors\n\npg_database\n\npg_depend\n\npg_description\n\npg_enum\n\npg_event_trigger\n\npg_index\n\npg_indexes\n\npg_locks\n\npg_namespace\n\npg_proc\n\npg_publication\n\npg_publication_tables\n\npg_range\n\npg_roles\n\npg_settings\n\npg_shdescription\n\npg_stats\n\npg_subscription\n\npg_subscription_rel\n\npg_tables\n\npg_tablespace\n\npg_type\n\npg_views\n\npg_type\n\nSome clients require the pg_catalog.pg_type in order to be able to stream arrays or other non-primitive types.\n\nFor compatibility reasons, there is a trimmed down pg_type table available in CrateDB:\n\ncr> SELECT oid, typname, typarray, typelem, typlen, typtype, typcategory\n... FROM pg_catalog.pg_type\n... ORDER BY oid;\n+------+--------------+----------+---------+--------+---------+-------------+\n|  oid | typname      | typarray | typelem | typlen | typtype | typcategory |\n+------+--------------+----------+---------+--------+---------+-------------+\n|   16 | bool         |     1000 |       0 |      1 | b       | N           |\n|   18 | char         |     1002 |       0 |      1 | b       | S           |\n|   19 | name         |       -1 |       0 |     64 | b       | S           |\n|   20 | int8         |     1016 |       0 |      8 | b       | N           |\n|   21 | int2         |     1005 |       0 |      2 | b       | N           |\n|   23 | int4         |     1007 |       0 |      4 | b       | N           |\n|   24 | regproc      |     1008 |       0 |      4 | b       | N           |\n|   25 | text         |     1009 |       0 |     -1 | b       | S           |\n|   26 | oid          |     1028 |       0 |      4 | b       | N           |\n|   30 | oidvector    |     1013 |      26 |     -1 | b       | A           |\n|  114 | json         |      199 |       0 |     -1 | b       | U           |\n|  199 | _json        |        0 |     114 |     -1 | b       | A           |\n|  600 | point        |     1017 |       0 |     16 | b       | G           |\n|  700 | float4       |     1021 |       0 |      4 | b       | N           |\n|  701 | float8       |     1022 |       0 |      8 | b       | N           |\n|  705 | unknown      |        0 |       0 |     -2 | p       | X           |\n| 1000 | _bool        |        0 |      16 |     -1 | b       | A           |\n| 1002 | _char        |        0 |      18 |     -1 | b       | A           |\n| 1005 | _int2        |        0 |      21 |     -1 | b       | A           |\n| 1007 | _int4        |        0 |      23 |     -1 | b       | A           |\n| 1008 | _regproc     |        0 |      24 |     -1 | b       | A           |\n| 1009 | _text        |        0 |      25 |     -1 | b       | A           |\n| 1014 | _bpchar      |        0 |    1042 |     -1 | b       | A           |\n| 1015 | _varchar     |        0 |    1043 |     -1 | b       | A           |\n| 1016 | _int8        |        0 |      20 |     -1 | b       | A           |\n| 1017 | _point       |        0 |     600 |     -1 | b       | A           |\n| 1021 | _float4      |        0 |     700 |     -1 | b       | A           |\n| 1022 | _float8      |        0 |     701 |     -1 | b       | A           |\n| 1042 | bpchar       |     1014 |       0 |     -1 | b       | S           |\n| 1043 | varchar      |     1015 |       0 |     -1 | b       | S           |\n| 1082 | date         |     1182 |       0 |      8 | b       | D           |\n| 1114 | timestamp    |     1115 |       0 |      8 | b       | D           |\n| 1115 | _timestamp   |        0 |    1114 |     -1 | b       | A           |\n| 1182 | _date        |        0 |    1082 |     -1 | b       | A           |\n| 1184 | timestamptz  |     1185 |       0 |      8 | b       | D           |\n| 1185 | _timestamptz |        0 |    1184 |     -1 | b       | A           |\n| 1186 | interval     |     1187 |       0 |     16 | b       | T           |\n| 1187 | _interval    |        0 |    1186 |     -1 | b       | A           |\n| 1231 | _numeric     |        0 |    1700 |     -1 | b       | A           |\n| 1266 | timetz       |     1270 |       0 |     12 | b       | D           |\n| 1270 | _timetz      |        0 |    1266 |     -1 | b       | A           |\n| 1560 | bit          |     1561 |       0 |     -1 | b       | V           |\n| 1561 | _bit         |        0 |    1560 |     -1 | b       | A           |\n| 1700 | numeric      |     1231 |       0 |     -1 | b       | N           |\n| 2205 | regclass     |     2210 |       0 |      4 | b       | N           |\n| 2210 | _regclass    |        0 |    2205 |     -1 | b       | A           |\n| 2249 | record       |     2287 |       0 |     -1 | p       | P           |\n| 2276 | any          |        0 |       0 |      4 | p       | P           |\n| 2277 | anyarray     |        0 |    2276 |     -1 | p       | P           |\n| 2287 | _record      |        0 |    2249 |     -1 | p       | A           |\n+------+--------------+----------+---------+--------+---------+-------------+\nSELECT 50 rows in set (... sec)\n\n\nNote\n\nThis is just a snapshot of the table.\n\nCheck table information_schema.columns to get information for all supported columns.\n\nOID types\n\nObject Identifiers (OIDs) are used internally by PostgreSQL as primary keys for various system tables.\n\nCrateDB supports the oid type and the following aliases:\n\nName\n\n\t\n\nReference\n\n\t\n\nDescription\n\n\t\n\nExample\n\n\n\n\nregproc\n\n\t\n\npg_proc\n\n\t\n\nA function name\n\n\t\n\nsum\n\n\n\n\nregclass\n\n\t\n\npg_class\n\n\t\n\nA relation name\n\n\t\n\npg_type\n\nCrateDB also supports the oidvector type.\n\nNote\n\nCasting a string or an integer to the regproc type does not result in a function lookup (as it does with PostgreSQL).\n\nInstead:\n\nCasting a string to the regproc type results in an object of the regproc type with a name equal to the string value and an oid equal to an integer hash of the string.\n\nCasting an integer to the regproc type results in an object of the regproc type with a name equal to the string representation of the integer and an oid equal to the integer value.\n\nConsult the CrateDB data types reference for more information about each OID type (including additional type casting behaviour).\n\nShow transaction isolation\n\nFor compatibility with JDBC the SHOW TRANSACTION ISOLATION LEVEL statement is implemented:\n\ncr> show transaction isolation level;\n+-----------------------+\n| transaction_isolation |\n+-----------------------+\n| read uncommitted      |\n+-----------------------+\nSHOW 1 row in set (... sec)\n\nBEGIN, START, and COMMIT statements\n\nFor compatibility with clients that use the PostgresSQL wire protocol (e.g., the Golang lib/pq and pgx drivers), CrateDB will accept the BEGIN, COMMIT, and START TRANSACTION statements. For example:\n\ncr> BEGIN TRANSACTION ISOLATION LEVEL READ UNCOMMITTED,\n...                   READ ONLY,\n...                   NOT DEFERRABLE;\nBEGIN OK, 0 rows affected  (... sec)\n\ncr> COMMIT\nCOMMIT OK, 0 rows affected  (... sec)\n\n\nCrateDB will silently ignore the COMMIT, BEGIN, and START TRANSACTION statements and all respective parameters.\n\nClient compatibility\nJDBC\n\npgjdbc JDBC drivers version 9.4.1209 and above are compatible.\n\nLimitations\n\nReflection methods like conn.getMetaData().getTables(...) won’t work since the required tables are unavailable in CrateDB.\n\nAs a workaround it’s possible to use SHOW TABLES or query the information_schema tables manually using SELECT statements.\n\nOBJECT and GEO_SHAPE columns can be streamed as JSON but require pgjdbc version 9.4.1210 or newer.\n\nMultidimensional arrays will be streamed as JSON encoded string to avoid a protocol limitation where all sub-arrays are required to have the same length.\n\nThe behavior of PreparedStatement.executeBatch in error cases depends on in which stage an error occurs: A BatchUpdateException is thrown if no processing has been done yet, whereas single operations failing after the processing started are indicated by an EXECUTE_FAILED (-3) return value.\n\nTransaction limitations as described above.\n\nHaving escape processing enabled could prevent the usage of Object Literals in case an object key’s starting character clashes with a JDBC escape keyword (see also JDBC escape syntax). Disabling escape processing will remedy this appropriately for pgjdbc version >= 9.4.1212.\n\nConnection failover and load balancing\n\nConnection failover and load balancing is supported as described here: PostgreSQL JDBC connection failover.\n\nNote\n\nIt is not recommended to use the targetServerType parameter since CrateDB has no concept of master-replica nodes.\n\nImplementation differences\n\nThe PostgreSQL Wire Protocol makes it easy to use many PostgreSQL compatible tools and libraries directly with CrateDB. However, many of these tools assume that they are talking to PostgreSQL specifically, and thus rely on SQL extensions and idioms that are unique to PostgreSQL. Because of this, some tools or libraries may not work with other SQL databases such as CrateDB.\n\nCrateDB’s SQL query engine enables real-time search & aggregations for online analytic processing (OLAP) and business intelligence (BI) with the benefit of the ability to scale horizontally. The use-cases of CrateDB are different than those of PostgreSQL, as CrateDB’s specialized storage schema and query execution engine addresses different needs (see Clustering).\n\nThe features listed below cover the main differences in implementation and dialect between CrateDB and PostgreSQL. A detailed comparison between CrateDB’s SQL dialect and standard SQL is outlined in SQL compatibility.\n\nCopy operations\n\nCrateDB does not support the distinct sub-protocol that is used to serve COPY operations and provides another implementation for transferring bulk data using the COPY FROM and COPY TO statements.\n\nData types\nDates and times\n\nAt the moment, CrateDB does not support TIME without a time zone.\n\nAdditionally, CrateDB does not support the INTERVAL input units MILLENNIUM, CENTURY, DECADE, MILLISECOND, or MICROSECOND.\n\nObjects\n\nThe definition of structured values by using JSON types, composite types or HSTORE are not supported. CrateDB alternatively allows the definition of nested documents (of type OBJECT) that store fields containing any CrateDB supported data type, including nested object types.\n\nArrays\nDeclaration of arrays\n\nWhile multidimensional arrays in PostgreSQL must have matching extends for each dimension, CrateDB allows different length nested arrays as this example shows:\n\ncr> select [[1,2,3],[1,2]] from sys.cluster;\n+---------------------+\n| [[1, 2, 3], [1, 2]] |\n+---------------------+\n| [[1, 2, 3], [1, 2]] |\n+---------------------+\nSELECT 1 row in set (... sec)\n\nType casts\n\nCrateDB accepts the Type casting syntax for conversion of one data type to another.\n\nSee Also\n\nPostgreSQL value expressions\n\nCrateDB value expressions\n\nText search functions and operators\n\nThe functions and operators provided by PostgreSQL for full-text search (see PostgreSQL fulltext Search) are not compatible with those provided by CrateDB.\n\nIf you are missing features, functions or dialect improvements and have a great use case for it, let us know on GitHub. We’re always improving and extending CrateDB and we love to hear feedback."
  },
  {
    "title": "Version 5.5.2 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.5.2.html",
    "html": "5.6\nVersion 5.5.2\n\nReleased on 2023-12-21.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.5.2.\n\nWe recommend that you upgrade to the latest 5.4 release before moving to 5.5.2.\n\nA rolling upgrade from 5.4.x to 5.5.2 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of contents\n\nSecurity Fixes\n\nPackaging Changes\n\nFixes\n\nSee the Version 5.5.0 release notes for a full list of changes in the 5.5 series.\n\nSecurity Fixes\n\nThe HTTP transport will not trust any X-Real-IP header by default anymore. This prevents a client from spoofing its IP address by setting these headers and thus bypassing IP based authentication with is enabled by default for the crate superuser. To keep allowing the X-Real-IP header to be trusted, you have to explicitly enable it via the auth.trust.http_support_x_real_ip node setting.\n\nPackaging Changes\n\nThe RPM and DEB packages changed slightly to unify the build process. The most important change is that the crate service no longer automatically starts after package installation, to allow changing the configuration first.\n\nOther than that, the structure is now:\n\nbin, jdk and lib are installed into /usr/share/crate. In the RPM package this used to be in /opt/crate.\n\nThe home directory of the crate user is /usr/share/crate\n\nchanges, notice, license are in /usr/share/doc/crate\n\nservice file is in /usr/lib/systemd/system\n\nThe crate.yml configuration file is in /etc/crate/\n\nThe default environment configuration file at RPM packages changed to /etc/default/crate to be consistent with the DEB package. The old location at /etc/sysconfig/crate is not supported anymore.\n\nIf you haven’t made any significant configuration changes the new packages should keep working out of the box.\n\nImportant for Debian and Ubuntu users: There is now a new repository.\n\nYou’ll have to update the repository configuration to install CrateDB newer than 5.5.1.\n\nThis new repository keeps old CrateDB versions in the Package index and also contains packages for ARM64.\n\nFixes\n\nFixed an issue that caused queries with a NULLIF or OR expression in the WHERE clause to fail evaluating NULL correctly.\n\nFixed a race condition that could lead to a memory leak when relocating a shard from one node to another and concurrently running queries.\n\nFixed a performance regression introduced in 5.5.0 for queries with\n\nGROUP BY on a single column.\n\nFixed an issue that allowed adding a column under the same name as an existing index definition.\n\nFixed an issue with wrong escaping of backslash in C-style escaped strings. SELECT E'\\%' used to return \\% instead of %.\n\nFixed an issue which led to SQLParseException when using an escaped quote \\' at the end of C-style string.\n\nFixed an issue that caused ALTER TABLE DROP COLUMN to falsely report success on dropping system columns despite any follow up queries on the dropped columns work as expected. Now an exception is thrown.\n\nFixed the SQL parser to be lenient with the position of constraint definitions at the column definition of CREATE TABLE and ALTER TABLE statements. E.g. CREATE TABLE t (a INT NULL DEFAULT 1) is now accepted while before the NULL constraint had to be placed after the DEFAULT constraint.\n\nFixed an issue introduced with CrateDB 5.5.0 that prevented importing JSON file via COPY FROM if the file contained JSON entries with same keys but in different order."
  },
  {
    "title": "JMX monitoring — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/monitoring.html",
    "html": "5.6\nJMX monitoring\n\nThe JMX monitoring feature exposes query metrics via the JMX API.\n\nTable of contents\n\nSetup\n\nEnable collecting stats\n\nEnable the JMX API\n\nUsing Docker\n\nJMX Beans\n\nQueryStats MBean\n\nNodeStatus MBean\n\nNodeInfo MXBean\n\nConnections MBean\n\nThreadPools MXBean\n\nCircuitBreakers MXBean\n\nExposing JMX via HTTP\n\nSetup\nEnable collecting stats\n\nBy default, Collecting stats is enabled. You can disable collecting stats via the CrateDB configuration file or by running this statement:\n\ncr> SET GLOBAL \"stats.enabled\" = FALSE;\n\nEnable the JMX API\n\nTo monitor CrateDB using the JMX API, you must set the following system properties before you start CrateDB:\n\ncom.sun.management.jmxremote\ncom.sun.management.jmxremote.port=<JMX_PORT>\ncom.sun.management.jmxremote.ssl=false\ncom.sun.management.jmxremote.authenticate=false\n\n\nHere, <JMX_PORT> sets the port number of your JMX server. JMX SSL and authentication are currently not supported.\n\nMore information about the JMX monitoring properties can be found in the JMX documentation.\n\nYou can set the Java system properties with the -D option:\n\nsh$ ./bin/crate -Dcom.sun.management.jmxremote \\\n...             -Dcom.sun.management.jmxremote.port=7979 \\\n...             -Dcom.sun.management.jmxremote.ssl=false \\\n...             -Dcom.sun.management.jmxremote.authenticate=false\n\n\nHowever, the recommended way to set system properties is via the CRATE_JAVA_OPTS environment variable, like so:\n\nsh$ export CRATE_JAVA_OPTS=\"$CRATE_JAVA_OPTS \\\n      -Dcom.sun.management.jmxremote \\\n      -Dcom.sun.management.jmxremote.port=7979 \\\n      -Dcom.sun.management.jmxremote.ssl=false \\\n      -Dcom.sun.management.jmxremote.authenticate=false\"\nsh$ ./bin/crate\n\n\nIf you’re using the CrateDB Debian or RPM packages, you can set this environment variable via the /etc/default/crate configuration file.\n\nUsing Docker\n\nTo enable JMX monitoring when running CrateDB in a Docker container you have to set the following additional Java system properties:\n\n-Djava.rmi.server.hostname=<RMI_HOSTNAME>\n-Dcom.sun.management.jmxremote.rmi.port=<RMI_PORT>\n\n\nHere, <RMI_HOSTNAME> is the IP address or hostname of the Docker host and <RMI_PORT> is the statically assigned port of the RMI server. For convenience, <RMI_PORT> can be set to the same port the JMX server listens on.\n\nThe <RMI_HOSTNAME> and <RMI_PORT> can be used by JMX clients (e.g. JConsole or VisualVM) to connect to the JMX server.\n\nHere’s an example Docker command:\n\nsh> docker run -d --env CRATE_HEAP_SIZE=1g -e CRATE_JAVA_OPTS=\"\\\n      -Dcom.sun.management.jmxremote\n      -Dcom.sun.management.jmxremote.port=7979 \\\n      -Dcom.sun.management.jmxremote.ssl=false \\\n      -Dcom.sun.management.jmxremote.authenticate=false \\\n      -Dcom.sun.management.jmxremote.rmi.port=7979 \\\n      -Djava.rmi.server.hostname=<RMI_HOSTNAME>\" \\\n      -p 7979:7979 crate -Cnetwork.host=_site_\n\n\nHere, again, <RMI_HOSTNAME> is the IP address or hostname of the Docker host.\n\nJMX Beans\nQueryStats MBean\n\nThe QueryStats MBean exposes the sum of durations, in milliseconds, total and failed count of all statements executed since the node was started, grouped by type, for SELECT, UPDATE, DELETE, INSERT, MANAGEMENT, DDL, COPY and UNDEFINED queries.\n\nMetrics can be accessed using the JMX MBean object name io.crate.monitoring:type=QueryStats and the following attributes:\n\nStatements total count since the node was started:\n\nSelectQueryTotalCount\n\nInsertQueryTotalCount\n\nUpdateQueryTotalCount\n\nDeleteQueryTotalCount\n\nManagementQueryTotalCount\n\nDDLQueryTotalCount\n\nCopyQueryTotalCount\n\nUndefinedQueryTotalCount\n\nStatements failed count since the node was started:\n\nSelectQueryFailedCount\n\nInsertQueryFailedCount\n\nUpdateQueryFailedCount\n\nDeleteQueryFailedCount\n\nManagementQueryFailedCount\n\nDDLQueryFailedCount\n\nCopyQueryFailedCount\n\nUndefinedQueryFailedCount\n\nThe sum of the durations, in milliseconds, since the node was started, of all statement executions grouped by type:\n\nSelectQuerySumOfDurations\n\nInsertQuerySumOfDurations\n\nUpdateQuerySumOfDurations\n\nDeleteQuerySumOfDurations\n\nManagementQuerySumOfDurations\n\nDDLQuerySumOfDurations\n\nCopyQuerySumOfDurations\n\nUndefinedQuerySumOfDurations\n\nNodeStatus MBean\n\nThe NodeStatus JMX MBean exposes the status of the current node as boolean values.\n\nNodeStatus can be accessed using the JMX MBean object name io.crate.monitoring:type=NodeStatus and the following attributes:\n\nReady\n\nDefines if the node is able to process SQL statements.\n\nNodeInfo MXBean\n\nThe NodeInfo JMX MXBean exposes information about the current node.\n\nNodeInfo can be accessed using the JMX MXBean object name io.crate.monitoring:type=NodeInfo and the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nNodeId\n\n\t\n\nProvides the unique identifier of the node in the cluster.\n\n\n\n\nNodeName\n\n\t\n\nProvides the human friendly name of the node.\n\n\n\n\nClusterStateVersion\n\n\t\n\nProvides the version of the current applied cluster state.\n\n\n\n\nShardStats\n\n\t\n\nStatistics about the number of shards located on the node.\n\n\n\n\nShardInfo\n\n\t\n\nDetailed information about the shards located on the node.\n\nShardStats returns a CompositeData object containing statistics about the number of shards located on the node with the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nTotal\n\n\t\n\nThe number of shards located on the node.\n\n\n\n\nPrimaries\n\n\t\n\nThe number of primary shards located on the node.\n\n\n\n\nReplicas\n\n\t\n\nThe number of replica shards located on the node.\n\n\n\n\nUnassigned\n\n\t\n\nThe number of unassigned shards in the cluster. If the node is the elected master node in the cluster, this will show the total number of unassigned shards in the cluster, otherwise 0.\n\nShardInfo returns an Array of CompositeData objects containing detailed information about the shards located on the node with the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nId\n\n\t\n\nThe shard id. This shard id is managed by the system, ranging from 0 up to the number of configured shards of the table.\n\n\n\n\nTable\n\n\t\n\nThe name of the table this shard belongs to.\n\n\n\n\nPartitionIdent\n\n\t\n\nThe partition ident of a partitioned table. Empty for non-partitioned tables.\n\n\n\n\nRoutingState\n\n\t\n\nThe current state of the shard in the routing table. Possible states are:\n\nUNASSIGNED\n\nINITIALIZING\n\nSTARTED\n\nRELOCATING\n\n\n\n\nState\n\n\t\n\nThe current state of the shard. Possible states are:\n\nCREATED\n\nRECOVERING\n\nPOST_RECOVERY\n\nSTARTED\n\nRELOCATED\n\nCLOSED\n\nINITIALIZING\n\nUNASSIGNED\n\n\n\n\nSize\n\n\t\n\nThe estimated cumulated size in bytes of all files of this shard.\n\nConnections MBean\n\nThe Connections MBean exposes information about any open connections to a CrateDB node.\n\nIt can be accessed using the io.crate.monitoring:type=Connections object name and has the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nHttpOpen\n\n\t\n\nThe number of currently established connections via HTTP\n\n\n\n\nHttpTotal\n\n\t\n\nThe number of total connections established via HTTP over the life time of a node\n\n\n\n\nPsqlOpen\n\n\t\n\nThe number of currently established connections via the PostgreSQL protocol\n\n\n\n\nPsqlTotal\n\n\t\n\nThe number of total connections established via the PostgreSQL protocol over the life time of a node\n\n\n\n\nTransportOpen\n\n\t\n\nThe number of currently established connections via the transport protocol\n\nThreadPools MXBean\n\nThe ThreadPools MXBean exposes statistical information about the used thread pools of a CrateDB node.\n\nIt can be accessed using the io.crate.monitoring:type=ThreadPools object name and has following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nGeneric\n\n\t\n\nThread pool statistics of the generic thread pool.\n\n\n\n\nSearch\n\n\t\n\nThread pool statistics of the search thread pool used by read statements on user generated tables.\n\n\n\n\nWrite\n\n\t\n\nThread pool statistics of the write thread pool used for writing and deleting data.\n\n\n\n\nManagement\n\n\t\n\nThread pool statistics of the management thread pool used by management tasks like stats collecting, repository information, shard allocations, etc.\n\n\n\n\nFlush\n\n\t\n\nThread pool statistics of the flush thread pool used for fsyncing to disk and merging segments in the storage engine.\n\n\n\n\nRefresh\n\n\t\n\nThread pool statistics of the refresh thread pool used for automatic and on-demand refreshing of tables\n\n\n\n\nSnapshot\n\n\t\n\nThread pool statistics of the snapshot thread pool used for creating and restoring snapshots.\n\n\n\n\nForceMerge\n\n\t\n\nThread pool statistics of the force_merge thread pool used when running an optimize statement.\n\n\n\n\nListener\n\n\t\n\nThread pool statistics of the listener thread pool used on client nodes for asynchronous result listeners.\n\n\n\n\nGet\n\n\t\n\nThread pool statistics of the get thread pool used when querying sys.nodes or sys.shards.\n\n\n\n\nFetchShardStarted\n\n\t\n\nThread pool statistics of the fetch_shard_started thread pool used on shard allocation .\n\n\n\n\nFetchShardStore\n\n\t\n\nThread pool statistics of the fetch_shard_store used on shard replication.\n\nEach of them returns a CompositeData object containing detailed statistics of each thread pool with the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\npoolSize\n\n\t\n\nThe current number of threads in the pool.\n\n\n\n\nlargestPoolSize\n\n\t\n\nThe largest number of threads that have ever simultaneously been in the pool.\n\n\n\n\nqueueSize\n\n\t\n\nThe current number of tasks in the queue.\n\n\n\n\nactive\n\n\t\n\nThe approximate number of threads that are actively executing tasks.\n\n\n\n\ncompleted\n\n\t\n\nThe approximate total number of tasks that have completed execution.\n\n\n\n\nrejected\n\n\t\n\nThe number of rejected executions.\n\nCircuitBreakers MXBean\n\nThe CircuitBreaker MXBean exposes statistical information about all available circuit breakers of a CrateDB node.\n\nIt can be accessed using the io.crate.monitoring:type=CircuitBreakers object name and has following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nParent\n\n\t\n\nStatistics of the parent circuit breaker containing summarized counters across all circuit breakers.\n\n\n\n\nQuery\n\n\t\n\nStatistics of the query circuit breaker used to account memory usage of SQL execution including intermediate states e.g. on aggregation and resulting rows.\n\n\n\n\nJobsLog\n\n\t\n\nStatistics of the jobs_log circuit breaker used to account memory usage of the sys.jobs_log table.\n\n\n\n\nOperationsLog\n\n\t\n\nStatistics of the operations_log circuit breaker used to account memory usage of the sys.operations_log table.\n\n\n\n\nFieldData\n\n\t\n\nStatistics of the field_data circuit breaker used for estimating the amount of memory a field will require to be loaded into memory.\n\n\n\n\nInFlightRequests\n\n\t\n\nStatistics of the in_flight_requests circuit breaker used to account memory usage of all incoming requests on transport or HTTP level.\n\n\n\n\nRequest\n\n\t\n\nStatistics of the request circuit breaker used to account memory usage of per-request data structure.\n\nEach of them returns a CompositeData object containing detailed statistics of each circuit breaker with the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nname\n\n\t\n\nThe circuit breaker name this statistic belongs to.\n\n\n\n\nused\n\n\t\n\nThe currently accounted used memory estimations.\n\n\n\n\nlimit\n\n\t\n\nThe configured limit when to trip.\n\n\n\n\ntrippedCount\n\n\t\n\nThe total number of occurred trips.\n\nExposing JMX via HTTP\n\nThe JMX metrics and a readiness endpoint can be exposed via HTTP (e.g. to be used by Prometheus) by using the Crate JMX HTTP Exporter Java agent. See the README in the Crate JMX HTTP Exporter repository for more information."
  },
  {
    "title": "Jobs management — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/jobs-management.html",
    "html": "5.6\nJobs management\n\nEach executed SQL statement results in a corresponding job. Jobs that are currently executing are logged in the system table sys.jobs (see Jobs, operations, and logs).\n\nTo obtain the UUID of a job, stats needs to be enabled (see Collecting stats). Job logging can be disabled by setting the queue size to zero.\n\nKilling an active job forces CrateDB to stop its execution on the cluster immediately. There are two different SQL commands available for killing jobs.\n\nThe KILL ALL statement stops every single job on each node that is running. It returns the total number of contexts of all jobs that have been killed. A job can have contexts on multiple nodes.\n\ncr> kill all;\nKILL OK, ... rows affected (... sec)\n\n\nKILL job_id kills one single job with the specified job_id. Like KILL ALL it returns the total number of contexts of that job killed on all nodes.\n\ncr> kill '175011ce-9bbc-45f2-a86a-5b7f993a93a6';\nKILL OK, ... rows affected (... sec)\n\n\nSee KILL for detailed syntax information on KILL statements."
  },
  {
    "title": "Authentication — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/auth/index.html",
    "html": "5.6\nAuthentication\n\nTable of contents\n\nAuthentication Methods\nTrust method\nPassword authentication method\nClient certificate authentication method\nHost-Based Authentication (HBA)\nAuthentication against CrateDB\nAuthenticating as a superuser\nAuthenticating to Admin UI\nNode-to-node communication"
  },
  {
    "title": "Cluster-wide settings — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/config/cluster.html",
    "html": "5.6\nCluster-wide settings\n\nAll current applied cluster settings can be read by querying the sys.cluster.settings column. Most cluster settings can be changed at runtime. This is documented at each setting.\n\nTable of contents\n\nNon-runtime cluster-wide settings\n\nCollecting stats\n\nShard limits\n\nUsage data collector\n\nGraceful stop\n\nBulk operations\n\nDiscovery\n\nUnicast host discovery\n\nDiscovery via DNS\n\nDiscovery on Amazon EC2\n\nRouting allocation\n\nShard balancing\n\nAttribute-based shard allocation\n\nCluster-wide attribute awareness\n\nCluster-wide attribute filtering\n\nDisk-based shard allocation\n\nRecovery\n\nMemory management\n\nQuery circuit breaker\n\nRequest circuit breaker\n\nAccounting circuit breaker\n\nStats circuit breakers\n\nTotal circuit breaker\n\nThread pools\n\nSettings for fixed thread pools\n\nOverload Protection\n\nMetadata\n\nMetadata gateway\n\nLogical Replication\n\nNon-runtime cluster-wide settings\n\nCluster wide settings which cannot be changed at runtime need to be specified in the configuration of each node in the cluster.\n\nCaution\n\nCluster settings specified via node configurations are required to be exactly the same on every node in the cluster for proper operation of the cluster.\n\nCollecting stats\nstats.enabled\nDefault: true\nRuntime: yes\n\nA boolean indicating whether or not to collect statistical information about the cluster.\n\nCaution\n\nThe collection of statistical information incurs a slight performance penalty, as details about every job and operation across the cluster will cause data to be inserted into the corresponding system tables.\n\nstats.jobs_log_size\nDefault: 10000\nRuntime: yes\n\nThe maximum number of job records kept to be kept in the sys.jobs_log table on each node.\n\nA job record corresponds to a single SQL statement to be executed on the cluster. These records are used for performance analytics. A larger job log produces more comprehensive stats, but uses more RAM.\n\nOlder job records are deleted as newer records are added, once the limit is reached.\n\nSetting this value to 0 disables collecting job information.\n\nstats.jobs_log_expiration\nDefault: 0s (disabled)\nRuntime: yes\n\nThe job record expiry time in seconds.\n\nJob records in the sys.jobs_log table are periodically cleared if they are older than the expiry time. This setting overrides stats.jobs_log_size.\n\nIf the value is set to 0, time based log entry eviction is disabled.\n\nNote\n\nIf both the stats.operations_log_size and stats.operations_log_expiration settings are disabled, jobs will not be recorded.\n\nstats.jobs_log_filter\nDefault: true (Include everything)\nRuntime: yes\n\nAn expression to determine if a job should be recorded into sys.jobs_log. The expression must evaluate to a boolean. If it evaluates to true the statement will show up in sys.jobs_log until it’s evicted due to one of the other rules. (expiration or size limit reached).\n\nThe expression may reference all columns contained in sys.jobs_log. A common use case is to include only jobs that took a certain amount of time to execute:\n\ncr> SET GLOBAL \"stats.jobs_log_filter\" = $$ended - started > '5 minutes'::interval$$;\n\nstats.jobs_log_persistent_filter\nDefault: false (Include nothing)\nRuntime: yes\n\nAn expression to determine if a job should also be recorded to the regular CrateDB log. Entries that match this filter will be logged under the StatementLog logger with the INFO level.\n\nThis is similar to stats.jobs_log_filter except that these entries are persisted to the log file. This should be used with caution and shouldn’t be set to an expression that matches many queries as the logging operation will block on IO and can therefore affect performance.\n\nA common use case is to use this for slow query logging.\n\nstats.operations_log_size\nDefault: 10000\nRuntime: yes\n\nThe maximum number of operations records to be kept in the sys.operations_log table on each node.\n\nA job consists of one or more individual operations. Operations records are used for performance analytics. A larger operations log produces more comprehensive stats, but uses more RAM.\n\nOlder operations records are deleted as newer records are added, once the limit is reached.\n\nSetting this value to 0 disables collecting operations information.\n\nstats.operations_log_expiration\nDefault: 0s (disabled)\nRuntime: yes\n\nEntries of sys.operations_log are cleared by a periodically job when they are older than the specified expire time. This setting overrides stats.operations_log_size. If the value is set to 0 the time based log entry eviction is disabled.\n\nNote\n\nIf both settings stats.operations_log_size and stats.operations_log_expiration are disabled, no job information will be collected.\n\nstats.service.interval\nDefault: 24h\nRuntime: yes\n\nDefines the refresh interval to refresh tables statistics used to produce optimal query execution plans.\n\nThis field expects a time value either as a bigint or double precision or alternatively as a string literal with a time suffix (ms, s, m, h, d, w).\n\nIf the value provided is 0 then the refresh is disabled.\n\nCaution\n\nUsing a very small value can cause a high load on the cluster.\n\nstats.service.max_bytes_per_sec\nDefault: 40mb\nRuntime: yes\n\nSpecifies the maximum number of bytes per second that can be read on data nodes to collect statistics. If this is set to a positive number, the underlying I/O operations of the ANALYZE statement are throttled.\n\nIf the value provided is 0 then the throttling is disabled.\n\nShard limits\ncluster.max_shards_per_node\nDefault: 1000\nRuntime: yes\n\nThe maximum amount of shards per node.\n\nAny operations that would result in the creation of additional shard copies that would exceed this limit are rejected.\n\nFor example. If you have 999 shards in the current cluster and you try to create a new table, the create table operation will fail.\n\nSimilarly, if a write operation would lead to the creation of a new partition, the statement will fail.\n\nEach shard on a node requires some memory and increases the size of the cluster state. Having too many shards per node will impact the clusters stability and it is therefore discouraged to raise the limit above 1000.\n\nNote\n\nThe maximum amount of shards per node setting is also used for the Maximum shards per node check.\n\nUsage data collector\n\nThe settings of the Usage Data Collector are read-only and cannot be set during runtime. Please refer to Usage Data Collector to get further information about its usage.\n\nudc.enabled\nDefault: true\nRuntime: no\n\ntrue: Enables the Usage Data Collector.\n\nfalse: Disables the Usage Data Collector.\n\nudc.initial_delay\nDefault: 10m\nRuntime: no\n\nThe delay for first ping after start-up.\n\nThis field expects a time value either as a bigint or double precision or alternatively as a string literal with a time suffix (ms, s, m, h, d, w).\n\nudc.interval\nDefault: 24h\nRuntime: no\n\nThe interval a UDC ping is sent.\n\nThis field expects a time value either as a bigint or double precision or alternatively as a string literal with a time suffix (ms, s, m, h, d, w).\n\nudc.url\nDefault: https://udc.crate.io\nRuntime: no\n\nThe URL the ping is sent to.\n\nGraceful stop\n\nBy default, when the CrateDB process stops it simply shuts down, possibly making some shards unavailable which leads to a red cluster state and lets some queries fail that required the now unavailable shards. In order to safely shutdown a CrateDB node, the graceful stop procedure can be used.\n\nThe following cluster settings can be used to change the shutdown behaviour of nodes of the cluster:\n\ncluster.graceful_stop.min_availability\nDefault: primaries\nRuntime: yes\nAllowed values: none | primaries | full\n\nnone: No minimum data availability is required. The node may shut down even if records are missing after shutdown.\n\nprimaries: At least all primary shards need to be available after the node has shut down. Replicas may be missing.\n\nfull: All records and all replicas need to be available after the node has shut down. Data availability is full.\n\nNote\n\nThis option is ignored if there is only 1 node in a cluster!\n\ncluster.graceful_stop.timeout\nDefault: 2h\nRuntime: yes\n\nDefines the maximum waiting time in milliseconds for the reallocation process to finish. The force setting will define the behaviour when the shutdown process runs into this timeout.\n\nThe timeout expects a time value either as a bigint or double precision or alternatively as a string literal with a time suffix (ms, s, m, h, d, w).\n\ncluster.graceful_stop.force\nDefault: false\nRuntime: yes\n\nDefines whether graceful stop should force stopping of the node if it runs into the timeout which is specified with the cluster.graceful_stop.timeout setting.\n\nBulk operations\n\nSQL DML Statements involving a huge amount of rows like COPY FROM, INSERT or UPDATE can take an enormous amount of time and resources. The following settings change the behaviour of those queries.\n\nbulk.request_timeout\nDefault: 1m\nRuntime: yes\n\nDefines the timeout of internal shard-based requests involved in the execution of SQL DML Statements over a huge amount of rows.\n\nDiscovery\n\nData sharding and work splitting are at the core of CrateDB. This is how we manage to execute very fast queries over incredibly large datasets. In order for multiple CrateDB nodes to work together a cluster needs to be formed. The process of finding other nodes with which to form a cluster is called discovery. Discovery runs when a CrateDB node starts and when a node is not able to reach the master node and continues until a master node is found or a new master node is elected.\n\ndiscovery.seed_hosts\nDefault: 127.0.0.1\nRuntime: no\n\nIn order to form a cluster with CrateDB instances running on other nodes a list of seed master-eligible nodes needs to be provided. This setting should normally contain the addresses of all the master-eligible nodes in the cluster. In order to seed the discovery process the nodes listed here must be live and contactable. This setting contains either an array of hosts or a comma-delimited string. By default a node will bind to the available loopback and scan for local ports between 4300 and 4400 to try to connect to other nodes running on the same server. This default behaviour provides local auto clustering without any configuration. Each value should be in the form of host:port or host (where port defaults to the setting transport.tcp.port).\n\nNote\n\nIPv6 hosts must be bracketed.\n\ncluster.initial_master_nodes\nDefault: not set\nRuntime: no\n\nContains a list of node names, full-qualified hostnames or IP addresses of the master-eligible nodes which will vote in the very first election of a cluster that’s bootstrapping for the first time. By default this is not set, meaning it expects this node to join an already formed cluster. In development mode, with no discovery settings configured, this step is performed by the nodes themselves, but this auto-bootstrapping is designed to aim development and is not safe for production. In production you must explicitly list the names or IP addresses of the master-eligible nodes whose votes should be counted in the very first election.\n\ndiscovery.type\nDefault: zen\nRuntime: no\nAllowed values: zen | single-node\n\nSpecifies whether CrateDB should form a multiple-node cluster. By default, CrateDB discovers other nodes when forming a cluster and allows other nodes to join the cluster later. If discovery.type is set to single-node, CrateDB forms a single-node cluster and the node won’t join any other clusters. This can be useful for testing. It is not recommend to use this for production setups. The single-node mode also skips bootstrap checks.\n\nCaution\n\nIf a node is started without any initial_master_nodes or a discovery_type set to single-node (e.g., the default configuration), it will never join a cluster even if the configuration is subsequently changed.\n\nIt is possible to force the node to forget its current cluster state by using the crate-node CLI tool. However, be aware that this may result in data loss.\n\nUnicast host discovery\n\nAs described above, CrateDB has built-in support for statically specifying a list of addresses that will act as the seed nodes in the discovery process using the discovery.seed_hosts setting.\n\nCrateDB also has support for several different mechanisms of seed nodes discovery. Currently there are two other discovery types: via DNS and via EC2 API.\n\nWhen a node starts up with one of these discovery types enabled, it performs a lookup using the settings for the specified mechanism listed below. The hosts and ports retrieved from the mechanism will be used to generate a list of unicast hosts for node discovery.\n\nThe same lookup is also performed by all nodes in a cluster whenever the master is re-elected (see Cluster Meta Data).\n\ndiscovery.seed_providers\nDefault: not set\nRuntime: no\nAllowed values: srv, ec2\n\nSee also: Discovery.\n\nDiscovery via DNS\n\nCrate has built-in support for discovery via DNS. To enable DNS discovery the discovery.seed_providers setting needs to be set to srv.\n\nThe order of the unicast hosts is defined by the priority, weight and name of each host defined in the SRV record. For example:\n\n_crate._srv.example.com. 3600 IN SRV 2 20 4300 crate1.example.com.\n_crate._srv.example.com. 3600 IN SRV 1 10 4300 crate2.example.com.\n_crate._srv.example.com. 3600 IN SRV 2 10 4300 crate3.example.com.\n\n\nwould result in a list of discovery nodes ordered like:\n\ncrate2.example.com:4300, crate3.example.com:4300, crate1.example.com:4300\n\ndiscovery.srv.query\nRuntime: no\n\nThe DNS query that is used to look up SRV records, usually in the format _service._protocol.fqdn If not set, the service discovery will not be able to look up any SRV records.\n\ndiscovery.srv.resolver\nRuntime: no\n\nThe hostname or IP of the DNS server used to resolve DNS records. If this is not set, or the specified hostname/IP is not resolvable, the default (system) resolver is used.\n\nOptionally a custom port can be specified using the format hostname:port.\n\nDiscovery on Amazon EC2\n\nCrateDB has built-in support for discovery via the EC2 API. To enable EC2 discovery the discovery.seed_providers settings needs to be set to ec2.\n\ndiscovery.ec2.access_key\nRuntime: no\n\nThe access key ID to identify the API calls.\n\ndiscovery.ec2.secret_key\nRuntime: no\n\nThe secret key to identify the API calls.\n\nFollowing settings control the discovery:\n\ndiscovery.ec2.groups\nRuntime: no\n\nA list of security groups; either by ID or name. Only instances with the given group will be used for unicast host discovery.\n\ndiscovery.ec2.any_group\nDefault: true\nRuntime: no\n\nDefines whether all (false) or just any (true) security group must be present for the instance to be used for discovery.\n\ndiscovery.ec2.host_type\nDefault: private_ip\nRuntime: no\nAllowed values: private_ip, public_ip, private_dns, public_dns\n\nDefines via which host type to communicate with other instances.\n\ndiscovery.ec2.availability_zones\nRuntime: no\n\nA list of availability zones. Only instances within the given availability zone will be used for unicast host discovery.\n\ndiscovery.ec2.tag.<name>\nRuntime: no\n\nEC2 instances for discovery can also be filtered by tags using the discovery.ec2.tag. prefix plus the tag name.\n\nE.g. to filter instances that have the environment tags with the value dev your setting will look like: discovery.ec2.tag.environment: dev.\n\ndiscovery.ec2.endpoint\nRuntime: no\n\nIf you have your own compatible implementation of the EC2 API service you can set the endpoint that should be used.\n\nRouting allocation\ncluster.routing.allocation.enable\nDefault: all\nRuntime: yes\nAllowed values: all | none | primaries | new_primaries\n\nall allows all shard allocations, the cluster can allocate all kinds of shards.\n\nnone allows no shard allocations at all. No shard will be moved or created.\n\nprimaries only primaries can be moved or created. This includes existing primary shards.\n\nnew_primaries allows allocations for new primary shards only. This means that for example a newly added node will not allocate any replicas. However it is still possible to allocate new primary shards for new indices. Whenever you want to perform a zero downtime upgrade of your cluster you need to set this value before gracefully stopping the first node and reset it to all after starting the last updated node.\n\nNote\n\nThis allocation setting has no effect on the recovery of primary shards! Even when cluster.routing.allocation.enable is set to none, nodes will recover their unassigned local primary shards immediately after restart.\n\ncluster.routing.rebalance.enable\nDefault: all\nRuntime: yes\nAllowed values: all | none | primaries | replicas\n\nEnables or disables rebalancing for different types of shards:\n\nall allows shard rebalancing for all types of shards.\n\nnone disables shard rebalancing for any types.\n\nprimaries allows shard rebalancing only for primary shards.\n\nreplicas allows shard rebalancing only for replica shards.\n\ncluster.routing.allocation.allow_rebalance\nDefault: indices_all_active\nRuntime: yes\nAllowed values: always | indices_primary_active | indices_all_active\n\nDefines when rebalancing will happen based on the total state of all the indices shards in the cluster.\n\nDefaults to indices_all_active to reduce chatter during initial recovery.\n\ncluster.routing.allocation.cluster_concurrent_rebalance\nDefault: 2\nRuntime: yes\n\nDefines how many concurrent rebalancing tasks are allowed across all nodes.\n\ncluster.routing.allocation.node_initial_primaries_recoveries\nDefault: 4\nRuntime: yes\n\nDefines how many concurrent primary shard recoveries are allowed on a node.\n\nSince primary recoveries use data that is already on disk (as opposed to inter-node recoveries), recovery should be fast and so this setting can be higher than node_concurrent_recoveries.\n\ncluster.routing.allocation.node_concurrent_recoveries\nDefault: 2\nRuntime: yes\n\nDefines how many concurrent recoveries are allowed on a node.\n\nShard balancing\n\nYou can configure how CrateDB attempts to balance shards across a cluster by specifying one or more property weights. CrateDB will consider a cluster to be balanced when no further allowed action can bring the weighted properties of each node closer together.\n\nNote\n\nBalancing may be restricted by other settings (e.g., attribute-based and disk-based shard allocation).\n\ncluster.routing.allocation.balance.shard\nDefault: 0.45f\nRuntime: yes\n\nDefines the weight factor for shards allocated on a node (float). Raising this raises the tendency to equalize the number of shards across all nodes in the cluster.\n\ncluster.routing.allocation.balance.index\nDefault: 0.55f\nRuntime: yes\n\nDefines a factor to the number of shards per index allocated on a specific node (float). Increasing this value raises the tendency to equalize the number of shards per index across all nodes in the cluster.\n\ncluster.routing.allocation.balance.threshold\nDefault: 1.0f\nRuntime: yes\n\nMinimal optimization value of operations that should be performed (non negative float). Increasing this value will cause the cluster to be less aggressive about optimising the shard balance.\n\nAttribute-based shard allocation\n\nYou can control how shards are allocated to specific nodes by setting custom attributes on each node (e.g., server rack ID or node availability zone). After doing this, you can define cluster-wide attribute awareness and then configure cluster-wide attribute filtering.\n\nSee Also\n\nFor an in-depth example of using custom node attributes, check out the multi-zone setup how-to guide.\n\nCluster-wide attribute awareness\n\nTo make use of custom attributes for attribute-based shard allocation, you must configure cluster-wide attribute awareness.\n\ncluster.routing.allocation.awareness.attributes\nRuntime: no\n\nYou may define custom node attributes which can then be used to do awareness based on the allocation of a shard and its replicas.\n\nFor example, let’s say we want to use an attribute named rack_id. We start two nodes with node.attr.rack_id set to rack_one. Then we create a single table with five shards and one replica. The table will be fully deployed on the current nodes (five shards and one replica each, making a total of 10 shards).\n\nNow, if we start two more nodes with node.attr.rack_id set to rack_two, CrateDB will relocate shards to even out the number of shards across the nodes. However, a shard and its replica will not be allocated to nodes sharing the same rack_id value.\n\nThe awareness.attributes setting supports using several values.\n\ncluster.routing.allocation.awareness.force.*.values\nRuntime: no\n\nAttributes on which shard allocation will be forced. Here, * is a placeholder for the awareness attribute, which can be configured using the cluster.routing.allocation.awareness.attributes setting.\n\nFor example, let’s say we configured forced shard allocation for an awareness attribute named zone with values set to zone1, zone2. Start two nodes with node.attr.zone set to zone1. Then, create a table with five shards and one replica. The table will be created, but only five shards will be allocated (with no replicas). The replicas will only be allocated when we start one or more nodes with node.attr.zone set to zone2.\n\nCluster-wide attribute filtering\n\nTo control how CrateDB uses custom attributes for attribute-based shard allocation, you must configure cluster-wide attribute filtering.\n\nNote\n\nCrateDB will retroactively enforce filter definitions. If a new filter would prevent newly created matching shards from being allocated to a node, CrateDB would also move any existing matching shards away from that node.\n\ncluster.routing.allocation.include.*\nRuntime: yes\n\nOnly allocate shards on nodes where at least one of the specified values matches the attribute.\n\nFor example:\n\ncluster.routing.allocation.include.zone: \"zone1,zone2\"`\n\ncluster.routing.allocation.exclude.*\nRuntime: yes\n\nOnly allocate shards on nodes where none of the specified values matches the attribute.\n\nFor example:\n\ncluster.routing.allocation.exclude.zone: \"zone1\"\n\ncluster.routing.allocation.require.*\nRuntime: yes\n\nUsed to specify a number of rules, which all of them must match for a node in order to allocate a shard on it.\n\nDisk-based shard allocation\ncluster.routing.allocation.disk.threshold_enabled\nDefault: true\nRuntime: yes\n\nPrevent shard allocation on nodes depending of the disk usage.\n\ncluster.routing.allocation.disk.watermark.low\nDefault: 85%\nRuntime: yes\n\nDefines the lower disk threshold limit for shard allocations. New shards will not be allocated on nodes with disk usage greater than this value. It can also be set to an absolute bytes value (like e.g. 500mb) to prevent the cluster from allocating new shards on node with less free disk space than this value.\n\ncluster.routing.allocation.disk.watermark.high\nDefault: 90%\nRuntime: yes\n\nDefines the higher disk threshold limit for shard allocations. The cluster will attempt to relocate existing shards to another node if the disk usage on a node rises above this value. It can also be set to an absolute bytes value (like e.g. 500mb) to relocate shards from nodes with less free disk space than this value.\n\ncluster.routing.allocation.disk.watermark.flood_stage\nDefault: 95%\nRuntime: yes\n\nDefines the threshold on which CrateDB enforces a read-only block on every index that has at least one shard allocated on a node with at least one disk exceeding the flood stage.\n\nNote\n\nblocks.read_only_allow_delete setting is automatically reset to FALSE for the tables if the disk space is freed and the threshold is undershot.\n\ncluster.routing.allocation.disk.watermark settings may be defined as percentages or bytes values. However, it is not possible to mix the value types.\n\nBy default, the cluster will retrieve information about the disk usage of the nodes every 30 seconds. This can also be changed by setting the cluster.info.update.interval setting.\n\nNote\n\nThe watermark settings are also used for the Routing allocation disk watermark low and Routing allocation disk watermark high node check.\n\nSetting cluster.routing.allocation.disk.threshold_enabled to false will disable the allocation decider, but the node checks will still be active and warn users about running low on disk space.\n\ncluster.routing.allocation.total_shards_per_node\nDefault: -1\nRuntime: yes\n\nLimits the number of shards that can be allocated per node. A value of -1 means unlimited.\n\nSetting this to 1000, for example, will prevent CrateDB from assigning more than 1000 shards per node. A node with 1000 shards would be excluded from allocation decisions and CrateDB would attempt to allocate shards to other nodes, or leave shards unassigned if no suitable node can be found.\n\nRecovery\nindices.recovery.max_bytes_per_sec\nDefault: 40mb\nRuntime: yes\n\nSpecifies the maximum number of bytes that can be transferred during shard recovery per seconds. Limiting can be disabled by setting it to 0. This setting allows to control the network usage of the recovery process. Higher values may result in higher network utilization, but also faster recovery process.\n\nindices.recovery.retry_delay_state_sync\nDefault: 500ms\nRuntime: yes\n\nDefines the time to wait after an issue caused by cluster state syncing before retrying to recover.\n\nindices.recovery.retry_delay_network\nDefault: 5s\nRuntime: yes\n\nDefines the time to wait after an issue caused by the network before retrying to recover.\n\nindices.recovery.internal_action_timeout\nDefault: 15m\nRuntime: yes\n\nDefines the timeout for internal requests made as part of the recovery.\n\nindices.recovery.internal_action_long_timeout\nDefault: 30m\nRuntime: yes\n\nDefines the timeout for internal requests made as part of the recovery that are expected to take a long time. Defaults to twice internal_action_timeout.\n\nindices.recovery.recovery_activity_timeout\nDefault: 30m\nRuntime: yes\n\nRecoveries that don’t show any activity for more then this interval will fail. Defaults to internal_action_long_timeout.\n\nindices.recovery.max_concurrent_file_chunks\nDefault: 2\nRuntime: yes\n\nControls the number of file chunk requests that can be sent in parallel per recovery. As multiple recoveries are already running in parallel, controlled by cluster.routing.allocation.node_concurrent_recoveries, increasing this expert-level setting might only help in situations where peer recovery of a single shard is not reaching the total inbound and outbound peer recovery traffic as configured by indices.recovery.max_bytes_per_sec, but is CPU-bound instead, typically when using transport-level security or compression.\n\nMemory management\nmemory.allocation.type\nDefault: on-heap\nRuntime: yes\n\nSupported values are on-heap and off-heap. This influences if memory is preferably allocated in the heap space or in the off-heap/direct memory region.\n\nSetting this to off-heap doesn’t imply that the heap won’t be used anymore. Most allocations will still happen in the heap space but some operations will be allowed to utilize off heap buffers.\n\nWarning\n\nUsing off-heap is considered experimental.\n\nmemory.operation_limit\nDefault: 0\nRuntime: yes\n\nDefault value for the memory.operation_limit session setting. Changing the cluster setting will only affect new sessions, not existing sessions.\n\nQuery circuit breaker\n\nThe Query circuit breaker will keep track of the used memory during the execution of a query. If a query consumes too much memory or if the cluster is already near its memory limit it will terminate the query to ensure the cluster keeps working.\n\nindices.breaker.query.limit\nDefault: 60%\nRuntime: yes\n\nSpecifies the limit for the query breaker. Provided values can either be absolute values (interpreted as a number of bytes), byte sizes (like 1mb) or percentage of the heap size (like 12%). A value of -1 disables breaking the circuit while still accounting memory usage.\n\nRequest circuit breaker\n\nThe request circuit breaker allows an estimation of required heap memory per request. If a single request exceeds the specified amount of memory, an exception is raised.\n\nindices.breaker.request.limit\nDefault: 60%\nRuntime: yes\n\nSpecifies the JVM heap limit for the request circuit breaker.\n\nAccounting circuit breaker\n\nTracks things that are held in memory independent of queries. For example the memory used by Lucene for segments.\n\nindices.breaker.accounting.limit\nDefault: 100%\nRuntime: yes\n\nSpecifies the JVM heap limit for the accounting circuit breaker\n\nCaution\n\nThis setting is deprecated and will be removed in a future release.\n\nStats circuit breakers\n\nSettings that control the behaviour of the stats circuit breaker. There are two breakers in place, one for the jobs log and one for the operations log. For each of them, the breaker limit can be set.\n\nstats.breaker.log.jobs.limit\nDefault: 5%\nRuntime: yes\n\nThe maximum memory that can be used from CRATE_HEAP_SIZE for the sys.jobs_log table on each node.\n\nWhen this memory limit is reached, the job log circuit breaker logs an error message and clears the sys.jobs_log table completely.\n\nstats.breaker.log.operations.limit\nDefault: 5%\nRuntime: yes\n\nThe maximum memory that can be used from CRATE_HEAP_SIZE for the sys.operations_log table on each node.\n\nWhen this memory limit is reached, the operations log circuit breaker logs an error message and clears the sys.operations_log table completely.\n\nTotal circuit breaker\nindices.breaker.total.limit\nDefault: 95%\nRuntime: yes\n\nThe maximum memory that can be used by all aforementioned circuit breakers together.\n\nEven if an individual circuit breaker doesn’t hit its individual limit, queries might still get aborted if several circuit breakers together would hit the memory limit configured in indices.breaker.total.limit.\n\nThread pools\n\nEvery node uses a number of thread pools to schedule operations, each pool is dedicated to specific operations. The most important pools are:\n\nwrite: Used for write operations like index, update or delete. The type defaults to fixed.\n\nsearch: Used for read operations like SELECT statements. The type defaults to fixed.\n\nget: Used for some specific read operations. For example on tables like sys.shards or sys.nodes. The type defaults to fixed.\n\nrefresh: Used for refresh operations. The type defaults to scaling.\n\ngeneric: For internal tasks like cluster state management. The type defaults to scaling.\n\nlogical_replication: For logical replication operations. The type defaults to fixed.\n\nIn addition to those pools, there are also netty worker threads which are used to process network requests and many CPU bound actions like query analysis and optimization.\n\nThe thread pool settings are expert settings which you generally shouldn’t need to touch. They are dynamically sized depending on the number of available CPU cores. If you’re running multiple services on the same machine you instead should change the processors setting.\n\nIncreasing the number of threads for a pool can result in degraded performance due to increased context switching and higher memory footprint.\n\nIf you observe idle CPU cores increasing the thread pool size is rarely the right course of action, instead it can be a sign that:\n\nOperations are blocked on disk IO. Increasing the thread pool size could result in more operations getting queued and blocked on disk IO without increasing throughput but decreasing it due to more memory pressure and additional garbage collection activity.\n\nIndividual operations running single threaded. Not all tasks required to process a SQL statement can be further subdivided and processed in parallel, but many operations default to use one thread per shard. Because of this, you can consider increasing the number of shards of a table to increase the parallelism of a single individual statement and increase CPU core utilization. As an alternative you can try increasing the concurrency on the client side, to have CrateDB process more SQL statements in parallel.\n\nthread_pool.<name>.type\nRuntime: no\nAllowed values: fixed | scaling\n\nfixed holds a fixed size of threads to handle the requests. It also has a queue for pending requests if no threads are available.\n\nscaling ensures that a thread pool holds a dynamic number of threads that are proportional to the workload.\n\nSettings for fixed thread pools\n\nIf the type of a thread pool is set to fixed there are a few optional settings.\n\nthread_pool.<name>.size\nRuntime: no\n\nNumber of threads. The default size of the different thread pools depend on the number of available CPU cores.\n\nthread_pool.<name>.queue_size\nDefault write: 200\nDefault search: 1000\nDefault get: 100\nRuntime: no\n\nSize of the queue for pending requests. A value of -1 sets it to unbounded. If you have burst workloads followed by periods of inactivity it can make sense to increase the queue_size to allow a node to buffer more queries before rejecting new operations. But be aware, increasing the queue size if you have sustained workloads will only increase the system’s memory consumption and likely degrade performance.\n\nOverload Protection\n\nOverload protection settings control how many resources operations like INSERT INTO FROM QUERY or COPY can use.\n\nThe values here serve as a starting point for an algorithm that dynamically adapts the effective concurrency limit based on the round-trip time of requests. Whenever one of these settings is updated, the previously calculated effective concurrency is reset.\n\nChanging settings will only effect new operations, already running operations will continue with the previous settings.\n\noverload_protection.dml.initial_concurrency\nDefault: 5\nRuntime: yes\n\nThe initial number of concurrent operations allowed per target node.\n\noverload_protection.dml.min_concurrency\nDefault: 1\nRuntime: yes\n\nThe minimum number of concurrent operations allowed per target node.\n\noverload_protection.dml.max_concurrency\nDefault: 2000\nRuntime: yes\n\nThe maximum number of concurrent operations allowed per target node.\n\noverload_protection.dml.queue_size\nDefault: 200\nRuntime: yes\n\nHow many operations are allowed to queue up.\n\nMetadata\ncluster.info.update.interval\nDefault: 30s\nRuntime: yes\n\nDefines how often the cluster collect metadata information (e.g. disk usages etc.) if no concrete event is triggered.\n\nMetadata gateway\n\nThe following settings can be used to configure the behavior of the metadata gateway.\n\ngateway.expected_nodes\nDefault: -1\nRuntime: no\n\nThe setting gateway.expected_nodes defines the total number of nodes expected in the cluster. It is evaluated together with gateway.recover_after_nodes to decide if the cluster can start with recovery.\n\nCaution\n\nThis setting is deprecated and will be removed in a future version. Use gateway.expected_data_nodes instead.\n\ngateway.expected_data_nodes\nDefault: -1\nRuntime: no\n\nThe setting gateway.expected_data_nodes defines the total number of data nodes expected in the cluster. It is evaluated together with gateway.recover_after_data_nodes to decide if the cluster can start with recovery.\n\ngateway.recover_after_time\nDefault: 5m\nRuntime: no\n\nThe gateway.recover_after_time setting defines the time to wait for the number of nodes set in gateway.expected_data_nodes (or gateway.expected_nodes) to become available, before starting the recovery, once the number of nodes defined in gateway.recover_after_data_nodes (or gateway.recover_after_nodes) has already been reached. This setting is ignored if gateway.expected_data_nodes or gateway.expected_nodes are set to 0 or 1. It also has no effect if gateway.recover_after_data_nodes is set equal to gateway.expected_data_nodes (or gateway.recover_after_nodes is set equal to gateway.expected_nodes). The cluster also proceeds to immediate recovery, and the default 5 minutes waiting time does not apply, if neither this setting nor expected_nodes and expected_data_nodes are explicitly set.\n\ngateway.recover_after_nodes\nDefault: -1\nRuntime: no\n\nThe gateway.recover_after_nodes setting defines the number of nodes that need to join the cluster before the cluster state recovery can start. If this setting is -1 and gateway.expected_nodes is set, all nodes will need to be started before the cluster state recovery can start. Please note that proceeding with recovery when not all nodes are available could trigger the promotion of shards and the creation of new replicas, generating disk and network load, which may be unnecessary. You can use a combination of this setting with gateway.recovery_after_time to mitigate this risk.\n\nCaution\n\nThis setting is deprecated and will be removed in CrateDB 5.0. Use gateway.recover_after_data_nodes instead.\n\ngateway.recover_after_data_nodes\nDefault: -1\nRuntime: no\n\nThe gateway.recover_after_data_nodes setting defines the number of data nodes that need to be started before the cluster state recovery can start. If this setting is -1 and gateway.expected_data_nodes is set, all data nodes will need to be started before the cluster state recovery can start. Please note that proceeding with recovery when not all data nodes are available could trigger the promotion of shards and the creation of new replicas, generating disk and network load, which may be unnecessary. You can use a combination of this setting with gateway.recovery_after_time to mitigate this risk.\n\nLogical Replication\n\nReplication process can be configured by the following settings. Settings are dynamic and can be changed in runtime.\n\nreplication.logical.ops_batch_size\nDefault: 50000\nMin value: 16\nRuntime: yes\n\nMaximum number of operations to replicate from the publisher cluster per poll. Represents a number to advance a sequence.\n\nreplication.logical.reads_poll_duration\nDefault: 50\nRuntime: yes\n\nThe maximum time (in milliseconds) to wait for changes per poll operation. When a subscriber makes another one request to a publisher, it has reads_poll_duration milliseconds to harvest changes from the publisher.\n\nreplication.logical.recovery.chunk_size\nDefault: 1MB\nMin value: 1KB\nMax value: 1GB\nRuntime: yes\n\nChunk size to transfer files during the initial recovery of a replicating table.\n\nreplication.logical.recovery.max_concurrent_file_chunks\nDefault: 2\nMin value: 1\nMax value: 5\nRuntime: yes\n\nControls the number of file chunk requests that can be sent in parallel between clusters during the recovery."
  },
  {
    "title": "Information schema — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/information-schema.html",
    "html": "5.6\nInformation schema\n\ninformation_schema is a special schema that contains virtual tables which are read-only and can be queried to get information about the state of the cluster.\n\nTable of contents\n\nAccess\n\nVirtual tables\n\ntables\n\nsettings\n\nviews\n\ncolumns\n\ntable_constraints\n\nkey_column_usage\n\ntable_partitions\n\nroutines\n\nschemata\n\nsql_features\n\ncharacter_sets\n\nAccess\n\nWhen the user management is enabled, accessing the information_schema is open to all users and it does not require any privileges.\n\nHowever, being able to query information_schema tables will not allow the user to retrieve all the rows in the table, as it can contain information related to tables over which the connected user does not have any privileges. The only rows that will be returned will be the ones the user is allowed to access.\n\nFor example, if the user john has any privilege on the doc.books table but no privilege at all on doc.locations, when john issues a SELECT * FROM information_schema.tables statement, the tables information related to the doc.locations table will not be returned.\n\nVirtual tables\ntables\n\nThe information_schema.tables virtual table can be queried to get a list of all available tables and views and their settings, such as number of shards or number of replicas.\n\ncr> SELECT table_schema, table_name, table_type, number_of_shards, number_of_replicas\n... FROM information_schema.tables\n... ORDER BY table_schema ASC, table_name ASC;\n+--------------------+-------------------------+------------+------------------+--------------------+\n| table_schema       | table_name              | table_type | number_of_shards | number_of_replicas |\n+--------------------+-------------------------+------------+------------------+--------------------+\n| doc                | galaxies                | VIEW       |             NULL | NULL               |\n| doc                | locations               | BASE TABLE |                2 | 0                  |\n| doc                | partitioned_table       | BASE TABLE |                4 | 0-1                |\n| doc                | quotes                  | BASE TABLE |                2 | 0                  |\n| information_schema | character_sets          | BASE TABLE |             NULL | NULL               |\n| information_schema | columns                 | BASE TABLE |             NULL | NULL               |\n| information_schema | key_column_usage        | BASE TABLE |             NULL | NULL               |\n| information_schema | referential_constraints | BASE TABLE |             NULL | NULL               |\n| information_schema | routines                | BASE TABLE |             NULL | NULL               |\n| information_schema | schemata                | BASE TABLE |             NULL | NULL               |\n| information_schema | sql_features            | BASE TABLE |             NULL | NULL               |\n| information_schema | table_constraints       | BASE TABLE |             NULL | NULL               |\n| information_schema | table_partitions        | BASE TABLE |             NULL | NULL               |\n| information_schema | tables                  | BASE TABLE |             NULL | NULL               |\n| information_schema | views                   | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_am                   | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_attrdef              | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_attribute            | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_class                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_constraint           | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_cursors              | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_database             | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_depend               | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_description          | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_enum                 | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_event_trigger        | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_index                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_indexes              | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_locks                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_namespace            | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_proc                 | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_publication          | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_publication_tables   | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_range                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_roles                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_settings             | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_shdescription        | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_stats                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_subscription         | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_subscription_rel     | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_tables               | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_tablespace           | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_type                 | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_views                | BASE TABLE |             NULL | NULL               |\n| sys                | allocations             | BASE TABLE |             NULL | NULL               |\n| sys                | checks                  | BASE TABLE |             NULL | NULL               |\n| sys                | cluster                 | BASE TABLE |             NULL | NULL               |\n| sys                | health                  | BASE TABLE |             NULL | NULL               |\n| sys                | jobs                    | BASE TABLE |             NULL | NULL               |\n| sys                | jobs_log                | BASE TABLE |             NULL | NULL               |\n| sys                | jobs_metrics            | BASE TABLE |             NULL | NULL               |\n| sys                | node_checks             | BASE TABLE |             NULL | NULL               |\n| sys                | nodes                   | BASE TABLE |             NULL | NULL               |\n| sys                | operations              | BASE TABLE |             NULL | NULL               |\n| sys                | operations_log          | BASE TABLE |             NULL | NULL               |\n| sys                | privileges              | BASE TABLE |             NULL | NULL               |\n| sys                | repositories            | BASE TABLE |             NULL | NULL               |\n| sys                | roles                   | BASE TABLE |             NULL | NULL               |\n| sys                | segments                | BASE TABLE |             NULL | NULL               |\n| sys                | shards                  | BASE TABLE |             NULL | NULL               |\n| sys                | snapshot_restore        | BASE TABLE |             NULL | NULL               |\n| sys                | snapshots               | BASE TABLE |             NULL | NULL               |\n| sys                | summits                 | BASE TABLE |             NULL | NULL               |\n| sys                | users                   | BASE TABLE |             NULL | NULL               |\n+--------------------+-------------------------+------------+------------------+--------------------+\nSELECT 64 rows in set (... sec)\n\n\nThe table also contains additional information such as the specified routing column and partition columns:\n\ncr> SELECT table_name, clustered_by, partitioned_by\n... FROM information_schema.tables\n... WHERE table_schema = 'doc'\n... ORDER BY table_schema ASC, table_name ASC;\n+-------------------+--------------+----------------+\n| table_name        | clustered_by | partitioned_by |\n+-------------------+--------------+----------------+\n| galaxies          | NULL         | NULL           |\n| locations         | id           | NULL           |\n| partitioned_table | _id          | [\"date\"]       |\n| quotes            | id           | NULL           |\n+-------------------+--------------+----------------+\nSELECT 4 rows in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nDescription\n\n\t\n\nData Type\n\n\n\n\nblobs_path\n\n\t\n\nThe data path of the blob table\n\n\t\n\nTEXT\n\n\n\n\nclosed\n\n\t\n\nThe state of the table\n\n\t\n\nBOOLEAN\n\n\n\n\nclustered_by\n\n\t\n\nThe routing column used to cluster the table\n\n\t\n\nTEXT\n\n\n\n\ncolumn_policy\n\n\t\n\nDefines whether the table uses a STRICT or a DYNAMIC Column policy\n\n\t\n\nTEXT\n\n\n\n\nnumber_of_replicas\n\n\t\n\nThe number of replicas the table currently has\n\n\t\n\nINTEGER\n\n\n\n\nnumber_of_shards\n\n\t\n\nThe number of shards the table is currently distributed across\n\n\t\n\nINTEGER\n\n\n\n\npartitioned_by\n\n\t\n\nThe partition columns (used to partition the table)\n\n\t\n\nTEXT\n\n\n\n\nreference_generation\n\n\t\n\nSpecifies how values in the self-referencing column are generated\n\n\t\n\nTEXT\n\n\n\n\nrouting_hash_function\n\n\t\n\nThe name of the hash function used for internal routing\n\n\t\n\nTEXT\n\n\n\n\nself_referencing_column_name\n\n\t\n\nThe name of the column that uniquely identifies each row (always _id)\n\n\t\n\nTEXT\n\n\n\n\nsettings\n\n\t\n\nWITH\n\n\t\n\nOBJECT\n\n\n\n\ntable_catalog\n\n\t\n\nRefers to the table_schema\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nThe name of the table\n\n\t\n\nTEXT\n\n\n\n\ntable_schema\n\n\t\n\nThe name of the schema the table belongs to\n\n\t\n\nTEXT\n\n\n\n\ntable_type\n\n\t\n\nThe type of the table (BASE TABLE for tables, VIEW for views)\n\n\t\n\nTEXT\n\n\n\n\nversion\n\n\t\n\nA collection of version numbers relevant to the table\n\n\t\n\nOBJECT\n\nsettings\n\nTable settings specify configuration parameters for tables. Some settings can be set during Cluster runtime and others are only applied on cluster restart.\n\nThis list of table settings in WITH shows detailed information of each parameter.\n\nTable parameters can be applied with CREATE TABLE on creation of a table. With ALTER TABLE they can be set on already existing tables.\n\nThe following statement creates a new table and sets the refresh interval of shards to 500 ms and sets the shard allocation for primary shards only:\n\ncr> create table parameterized_table (id integer, content text)\n... with (\"refresh_interval\"=500, \"routing.allocation.enable\"='primaries');\nCREATE OK, 1 row affected (... sec)\n\n\nThe settings can be verified by querying information_schema.tables:\n\ncr> select settings['routing']['allocation']['enable'] as alloc_enable,\n...   settings['refresh_interval'] as refresh_interval\n... from information_schema.tables\n... where table_name='parameterized_table';\n+--------------+------------------+\n| alloc_enable | refresh_interval |\n+--------------+------------------+\n| primaries    |              500 |\n+--------------+------------------+\nSELECT 1 row in set (... sec)\n\n\nOn existing tables this needs to be done with ALTER TABLE statement:\n\ncr> alter table parameterized_table\n... set (\"routing.allocation.enable\"='none');\nALTER OK, -1 rows affected (... sec)\n\nviews\n\nThe table information_schema.views contains the name, definition and options of all available views.\n\ncr> SELECT table_schema, table_name, view_definition\n... FROM information_schema.views\n... ORDER BY table_schema ASC, table_name ASC;\n+--------------+------------+-------------------------+\n| table_schema | table_name | view_definition         |\n+--------------+------------+-------------------------+\n| doc          | galaxies   | SELECT                  |\n|              |            |   \"id\"                  |\n|              |            | , \"name\"                |\n|              |            | , \"description\"         |\n|              |            | FROM \"locations\"        |\n|              |            | WHERE \"kind\" = 'Galaxy' |\n+--------------+------------+-------------------------+\nSELECT 1 row in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nDescription\n\n\t\n\nData Type\n\n\n\n\ntable_catalog\n\n\t\n\nThe catalog of the table of the view (refers to table_schema)\n\n\t\n\nTEXT\n\n\n\n\ntable_schema\n\n\t\n\nThe schema of the table of the view\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nThe name of the table of the view\n\n\t\n\nTEXT\n\n\n\n\nview_definition\n\n\t\n\nThe SELECT statement that defines the view\n\n\t\n\nTEXT\n\n\n\n\ncheck_option\n\n\t\n\nNot applicable for CrateDB, always return NONE\n\n\t\n\nTEXT\n\n\n\n\nis_updatable\n\n\t\n\nWhether the view is updatable. Not applicable for CrateDB, always returns FALSE\n\n\t\n\nBOOLEAN\n\n\n\n\nowner\n\n\t\n\nThe user that created the view\n\n\t\n\nTEXT\n\nNote\n\nIf you drop the table of a view, the view will still exist and show up in the information_schema.tables and information_schema.views tables.\n\ncolumns\n\nThis table can be queried to get a list of all available columns of all tables and views and their definition like data type and ordinal position inside the table:\n\ncr> select table_name, column_name, ordinal_position as pos, data_type\n... from information_schema.columns\n... where table_schema = 'doc' and table_name not like 'my_table%'\n... order by table_name asc, column_name asc;\n+-------------------+--------------------------------+-----+--------------------------+\n| table_name        | column_name                    | pos | data_type                |\n+-------------------+--------------------------------+-----+--------------------------+\n| locations         | date                           |   3 | timestamp with time zone |\n| locations         | description                    |   6 | text                     |\n| locations         | id                             |   1 | integer                  |\n| locations         | information                    |  11 | object_array             |\n| locations         | information['evolution_level'] |  13 | smallint                 |\n| locations         | information['population']      |  12 | bigint                   |\n| locations         | inhabitants                    |   7 | object                   |\n| locations         | inhabitants['description']     |   9 | text                     |\n| locations         | inhabitants['interests']       |   8 | text_array               |\n| locations         | inhabitants['name']            |  10 | text                     |\n| locations         | kind                           |   4 | text                     |\n| locations         | landmarks                      |  14 | text_array               |\n| locations         | name                           |   2 | text                     |\n| locations         | position                       |   5 | integer                  |\n| partitioned_table | date                           |   3 | timestamp with time zone |\n| partitioned_table | id                             |   1 | bigint                   |\n| partitioned_table | title                          |   2 | text                     |\n| quotes            | id                             |   1 | integer                  |\n| quotes            | quote                          |   2 | text                     |\n+-------------------+--------------------------------+-----+--------------------------+\nSELECT 19 rows in set (... sec)\n\n\nYou can even query this table’s own columns (attention: this might lead to infinite recursion of your mind, beware!):\n\ncr> select column_name, data_type, ordinal_position\n... from information_schema.columns\n... where table_schema = 'information_schema'\n... and table_name = 'columns' order by column_name asc;\n+--------------------------+------------+------------------+\n| column_name              | data_type  | ordinal_position |\n+--------------------------+------------+------------------+\n| character_maximum_length | integer    |                1 |\n| character_octet_length   | integer    |                2 |\n| character_set_catalog    | text       |                3 |\n| character_set_name       | text       |                4 |\n| character_set_schema     | text       |                5 |\n| check_action             | integer    |                6 |\n| check_references         | text       |                7 |\n| collation_catalog        | text       |                8 |\n| collation_name           | text       |                9 |\n| collation_schema         | text       |               10 |\n| column_default           | text       |               11 |\n| column_details           | object     |               12 |\n| column_details['name']   | text       |               13 |\n| column_details['path']   | text_array |               14 |\n| column_name              | text       |               15 |\n| data_type                | text       |               16 |\n| datetime_precision       | integer    |               17 |\n| domain_catalog           | text       |               18 |\n| domain_name              | text       |               19 |\n| domain_schema            | text       |               20 |\n| generation_expression    | text       |               21 |\n| identity_cycle           | boolean    |               22 |\n| identity_generation      | text       |               23 |\n| identity_increment       | text       |               24 |\n| identity_maximum         | text       |               25 |\n| identity_minimum         | text       |               26 |\n| identity_start           | text       |               27 |\n| interval_precision       | integer    |               28 |\n| interval_type            | text       |               29 |\n| is_generated             | text       |               30 |\n| is_identity              | boolean    |               31 |\n| is_nullable              | boolean    |               32 |\n| numeric_precision        | integer    |               33 |\n| numeric_precision_radix  | integer    |               34 |\n| numeric_scale            | integer    |               35 |\n| ordinal_position         | integer    |               36 |\n| table_catalog            | text       |               37 |\n| table_name               | text       |               38 |\n| table_schema             | text       |               39 |\n| udt_catalog              | text       |               40 |\n| udt_name                 | text       |               41 |\n| udt_schema               | text       |               42 |\n+--------------------------+------------+------------------+\nSELECT 42 rows in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nDescription\n\n\t\n\nData Type\n\n\n\n\ntable_catalog\n\n\t\n\nRefers to the table_schema\n\n\t\n\nTEXT\n\n\n\n\ntable_schema\n\n\t\n\nSchema name containing the table\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nTable Name\n\n\t\n\nTEXT\n\n\n\n\ncolumn_name\n\n\t\n\nColumn Name For fields in object columns this is not an identifier but a path and therefore must not be double quoted when programmatically obtained.\n\n\t\n\nTEXT\n\n\n\n\nordinal_position\n\n\t\n\nThe position of the column within the table\n\n\t\n\nINTEGER\n\n\n\n\nis_nullable\n\n\t\n\nWhether the column is nullable\n\n\t\n\nBOOLEAN\n\n\n\n\ndata_type\n\n\t\n\nThe data type of the column\n\nFor further information see Data types\n\n\t\n\nTEXT\n\n\n\n\ncolumn_default\n\n\t\n\nThe default expression of the column\n\n\t\n\nTEXT\n\n\n\n\ncharacter_maximum_length\n\n\t\n\nIf the data type is a character type then return the declared length limit; otherwise NULL.\n\n\t\n\nINTEGER\n\n\n\n\ncharacter_octet_length\n\n\t\n\nNot implemented (always returns NULL)\n\nPlease refer to TEXT type\n\n\t\n\nINTEGER\n\n\n\n\nnumeric_precision\n\n\t\n\nIndicates the number of significant digits for a numeric data_type. For all other data types this column is NULL.\n\n\t\n\nINTEGER\n\n\n\n\nnumeric_precision_radix\n\n\t\n\nIndicates in which base the value in the column numeric_precision for a numeric data_type is exposed. This can either be 2 (binary) or 10 (decimal). For all other data types this column is NULL.\n\n\t\n\nINTEGER\n\n\n\n\nnumeric_scale\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nINTEGER\n\n\n\n\ndatetime_precision\n\n\t\n\nContains the fractional seconds precision for a timestamp data_type. For all other data types this column is null.\n\n\t\n\nINTEGER\n\n\n\n\ninterval_type\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ninterval_precision\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nINTEGER\n\n\n\n\ncharacter_set_catalog\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncharacter_set_schema\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncharacter_set_name\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncollation_catalog\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncollation_schema\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncollation_name\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ndomain_catalog\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ndomain_schema\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ndomain_name\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nudt_catalog\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nudt_schema\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nudt_name\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncheck_references\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncheck_action\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nINTEGER\n\n\n\n\ngeneration_expression\n\n\t\n\nThe expression used to generate ad column. If the column is not generated NULL is returned.\n\n\t\n\nTEXT\n\n\n\n\nis_generated\n\n\t\n\nReturns ALWAYS or NEVER wether the column is generated or not.\n\n\t\n\nTEXT\n\n\n\n\nis_identity\n\n\t\n\nNot implemented (always returns false)\n\n\t\n\nBOOLEAN\n\n\n\n\nidentity_cycle\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nBOOLEAN\n\n\n\n\nidentity_generation\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nidentity_increment\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nidentity_maximum\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nidentity_minimum\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nidentity_start\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\ntable_constraints\n\nThis table can be queried to get a list of all defined table constraints, their type, name and which table they are defined in.\n\nNote\n\nCurrently only PRIMARY_KEY constraints are supported.\n\ncr> select table_schema, table_name, constraint_name, constraint_type as type\n... from information_schema.table_constraints\n... where table_name = 'tables'\n...   or table_name = 'quotes'\n...   or table_name = 'documents'\n...   or table_name = 'tbl'\n... order by table_schema desc, table_name asc limit 10;\n+--------------------+------------+------------------------+-------------+\n| table_schema       | table_name | constraint_name        | type        |\n+--------------------+------------+------------------------+-------------+\n| information_schema | tables     | tables_pk              | PRIMARY KEY |\n| doc                | quotes     | quotes_pk              | PRIMARY KEY |\n| doc                | quotes     | doc_quotes_id_not_null | CHECK       |\n| doc                | tbl        | doc_tbl_col_not_null   | CHECK       |\n+--------------------+------------+------------------------+-------------+\nSELECT 4 rows in set (... sec)\n\nkey_column_usage\n\nThis table may be queried to retrieve primary key information from all user tables:\n\ncr> select constraint_name, table_name, column_name, ordinal_position\n... from information_schema.key_column_usage\n... where table_name = 'students'\n+-----------------+------------+-------------+------------------+\n| constraint_name | table_name | column_name | ordinal_position |\n+-----------------+------------+-------------+------------------+\n| students_pk     | students   | id          |                1 |\n| students_pk     | students   | department  |                2 |\n+-----------------+------------+-------------+------------------+\nSELECT 2 rows in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nDescription\n\n\t\n\nData Type\n\n\n\n\nconstraint_catalog\n\n\t\n\nRefers to table_catalog\n\n\t\n\nTEXT\n\n\n\n\nconstraint_schema\n\n\t\n\nRefers to table_schema\n\n\t\n\nTEXT\n\n\n\n\nconstraint_name\n\n\t\n\nName of the constraint\n\n\t\n\nTEXT\n\n\n\n\ntable_catalog\n\n\t\n\nRefers to table_schema\n\n\t\n\nTEXT\n\n\n\n\ntable_schema\n\n\t\n\nName of the schema that contains the table that contains the constraint\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nName of the table that contains the constraint\n\n\t\n\nTEXT\n\n\n\n\ncolumn_name\n\n\t\n\nName of the column that contains the constraint\n\n\t\n\nTEXT\n\n\n\n\nordinal_position\n\n\t\n\nPosition of the column within the constraint (starts with 1)\n\n\t\n\nINTEGER\n\ntable_partitions\n\nThis table can be queried to get information about all partitioned tables, Each partition of a table is represented as one row. The row contains the information table name, schema name, partition ident, and the values of the partition. values is a key-value object with the partition column (or columns) as key(s) and the corresponding value as value(s).\n\ncr> insert into a_partitioned_table (id, content) values (1, 'content_a');\nINSERT OK, 1 row affected (... sec)\n\ncr> alter table a_partitioned_table set (number_of_shards=5);\nALTER OK, -1 rows affected (... sec)\n\ncr> insert into a_partitioned_table (id, content) values (2, 'content_b');\nINSERT OK, 1 row affected (... sec)\n\n\nThe following example shows a table where the column content of table a_partitioned_table has been used to partition the table. The table has two partitions. The partitions are introduced when data is inserted where content is content_a, and content_b.:\n\ncr> select table_name, table_schema as schema, partition_ident, \"values\"\n... from information_schema.table_partitions\n... order by table_name, partition_ident;\n+---------------------+--------+--------------------+--------------------------+\n| table_name          | schema | partition_ident    | values                   |\n+---------------------+--------+--------------------+--------------------------+\n| a_partitioned_table | doc    | 04566rreehimst2vc4 | {\"content\": \"content_a\"} |\n| a_partitioned_table | doc    | 04566rreehimst2vc8 | {\"content\": \"content_b\"} |\n+---------------------+--------+--------------------+--------------------------+\nSELECT 2 rows in set (... sec)\n\n\nThe second partition has been created after the number of shards for future partitions have been changed on the partitioned table, so they show 5 instead of 4:\n\ncr> select table_name, partition_ident,\n... number_of_shards, number_of_replicas\n... from information_schema.table_partitions\n... order by table_name, partition_ident;\n+---------------------+--------------------+------------------+--------------------+\n| table_name          | partition_ident    | number_of_shards | number_of_replicas |\n+---------------------+--------------------+------------------+--------------------+\n| a_partitioned_table | 04566rreehimst2vc4 |                4 | 0-1                |\n| a_partitioned_table | 04566rreehimst2vc8 |                5 | 0-1                |\n+---------------------+--------------------+------------------+--------------------+\nSELECT 2 rows in set (... sec)\n\nroutines\n\nThe routines table contains tokenizers, token-filters, char-filters, custom analyzers created by CREATE ANALYZER statements (see Creating a custom analyzer), and functions created by CREATE FUNCTION statements:\n\ncr> select routine_name, routine_type\n... from information_schema.routines\n... group by routine_name, routine_type\n... order by routine_name asc limit 5;\n+----------------------+--------------+\n| routine_name         | routine_type |\n+----------------------+--------------+\n| PathHierarchy        | TOKENIZER    |\n| apostrophe           | TOKEN_FILTER |\n| arabic               | ANALYZER     |\n| arabic_normalization | TOKEN_FILTER |\n| arabic_stem          | TOKEN_FILTER |\n+----------------------+--------------+\nSELECT 5 rows in set (... sec)\n\n\nFor example you can use this table to list existing tokenizers like this:\n\ncr> select routine_name\n... from information_schema.routines\n... where routine_type='TOKENIZER'\n... order by routine_name asc limit 10;\n+----------------+\n| routine_name   |\n+----------------+\n| PathHierarchy  |\n| char_group     |\n| classic        |\n| edge_ngram     |\n| keyword        |\n| letter         |\n| lowercase      |\n| ngram          |\n| path_hierarchy |\n| pattern        |\n+----------------+\nSELECT 10 rows in set (... sec)\n\n\nOr get an overview of how many routines and routine types are available:\n\ncr> select count(*), routine_type\n... from information_schema.routines\n... group by routine_type\n... order by routine_type;\n+----------+--------------+\n| count(*) | routine_type |\n+----------+--------------+\n|       45 | ANALYZER     |\n|        3 | CHAR_FILTER  |\n|       16 | TOKENIZER    |\n|       61 | TOKEN_FILTER |\n+----------+--------------+\nSELECT 4 rows in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nData Type\n\n\n\n\nroutine_name\n\n\t\n\nTEXT\n\n\n\n\nroutine_type\n\n\t\n\nTEXT\n\n\n\n\nroutine_body\n\n\t\n\nTEXT\n\n\n\n\nroutine_schema\n\n\t\n\nTEXT\n\n\n\n\ndata_type\n\n\t\n\nTEXT\n\n\n\n\nis_deterministic\n\n\t\n\nBOOLEAN\n\n\n\n\nroutine_definition\n\n\t\n\nTEXT\n\n\n\n\nspecific_name\n\n\t\n\nTEXT\n\nroutine_name\n\nName of the routine (might be duplicated in case of overloading)\n\nroutine_type\n\nType of the routine. Can be FUNCTION, ANALYZER, CHAR_FILTER, TOKEN_FILTER or TOKEN_FILTER.\n\nroutine_schema\n\nThe schema where the routine was defined. If it doesn’t apply, then NULL.\n\nroutine_body\n\nThe language used for the routine implementation. If it doesn’t apply, then NULL.\n\ndata_type\n\nThe return type of the function. If it doesn’t apply, then NULL.\n\nis_deterministic\n\nIf the routine is deterministic then True, else False (NULL if it doesn’t apply).\n\nroutine_definition\n\nThe function definition (NULL if it doesn’t apply).\n\nspecific_name\n\nUsed to uniquely identify the function in a schema, even if the function is overloaded. Currently the specific name contains the types of the function arguments. As the format might change in the future, it should be only used to compare it to other instances of specific_name.\n\nschemata\n\nThe schemata table lists all existing schemas. The blob, information_schema, and sys schemas are always available. The doc schema is available after the first user table is created.\n\ncr> select schema_name from information_schema.schemata order by schema_name;\n+--------------------+\n| schema_name        |\n+--------------------+\n| blob               |\n| doc                |\n| information_schema |\n| pg_catalog         |\n| sys                |\n+--------------------+\nSELECT 5 rows in set (... sec)\n\nsql_features\n\nThe sql_features table outlines supported and unsupported SQL features of CrateDB based to the current SQL standard (see SQL standard compliance):\n\ncr> select feature_name, is_supported, sub_feature_id, sub_feature_name\n... from information_schema.sql_features\n... where feature_id='F501';\n+--------------------------------+--------------+----------------+--------------------+\n| feature_name                   | is_supported | sub_feature_id | sub_feature_name   |\n+--------------------------------+--------------+----------------+--------------------+\n| Features and conformance views | FALSE        |                |                    |\n| Features and conformance views | TRUE         | 1              | SQL_FEATURES view  |\n| Features and conformance views | FALSE        | 2              | SQL_SIZING view    |\n| Features and conformance views | FALSE        | 3              | SQL_LANGUAGES view |\n+--------------------------------+--------------+----------------+--------------------+\nSELECT 4 rows in set (... sec)\n\n\nName\n\n\t\n\nData Type\n\n\t\n\nNullable\n\n\n\n\nfeature_id\n\n\t\n\nTEXT\n\n\t\n\nNO\n\n\n\n\nfeature_name\n\n\t\n\nTEXT\n\n\t\n\nNO\n\n\n\n\nsub_feature_id\n\n\t\n\nTEXT\n\n\t\n\nNO\n\n\n\n\nsub_feature_name\n\n\t\n\nTEXT\n\n\t\n\nNO\n\n\n\n\nis_supported\n\n\t\n\nTEXT\n\n\t\n\nNO\n\n\n\n\nis_verified_by\n\n\t\n\nTEXT\n\n\t\n\nYES\n\n\n\n\ncomments\n\n\t\n\nTEXT\n\n\t\n\nYES\n\nfeature_id\n\nIdentifier of the feature\n\nfeature_name\n\nDescriptive name of the feature by the Standard\n\nsub_feature_id\n\nIdentifier of the sub feature; If it has zero-length, this is a feature\n\nsub_feature_name\n\nDescriptive name of the sub feature by the Standard; If it has zero-length, this is a feature\n\nis_supported\n\nYES if the feature is fully supported by the current version of CrateDB, NO if not\n\nis_verified_by\n\nIdentifies the conformance test used to verify the claim;\n\nAlways NULL since the CrateDB development group does not perform formal testing of feature conformance\n\ncomments\n\nEither NULL or shows a comment about the supported status of the feature\n\ncharacter_sets\n\nThe character_sets table identifies the character sets available in the current database.\n\nIn CrateDB there is always a single entry listing UTF8:\n\ncr> SELECT character_set_name, character_repertoire FROM information_schema.character_sets;\n+--------------------+----------------------+\n| character_set_name | character_repertoire |\n+--------------------+----------------------+\n| UTF8               | UCS                  |\n+--------------------+----------------------+\nSELECT 1 row in set (... sec)\n\n\nColumn Name\n\n\t\n\nReturn Type\n\n\t\n\nDescription\n\n\n\n\ncharacter_set_catalog\n\n\t\n\nTEXT\n\n\t\n\nNot implemented, this column is always null.\n\n\n\n\ncharacter_set_schema\n\n\t\n\nTEXT\n\n\t\n\nNot implemented, this column is always null.\n\n\n\n\ncharacter_set_name\n\n\t\n\nTEXT\n\n\t\n\nName of the character set\n\n\n\n\ncharacter_repertoire\n\n\t\n\nTEXT\n\n\t\n\nCharacter repertoire\n\n\n\n\nform_of_use\n\n\t\n\nTEXT\n\n\t\n\nCharacter encoding form, same as character_set_name\n\n\n\n\ndefault_collate_catalog\n\n\t\n\nTEXT\n\n\t\n\nName of the database containing the default collation (Always crate)\n\n\n\n\ndefault_collate_schema\n\n\t\n\nTEXT\n\n\t\n\nName of the schema containing the default collation (Always NULL)\n\n\n\n\ndefault_collate_name\n\n\t\n\nTEXT\n\n\t\n\nName of the default collation (Always NULL)"
  },
  {
    "title": "Optimistic Concurrency Control — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/occ.html",
    "html": "5.6\nOptimistic Concurrency Control\n\nTable of contents\n\nIntroduction\n\nOptimistic update\n\nOptimistic delete\n\nKnown limitations\n\nIntroduction\n\nEven though CrateDB does not support transactions, Optimistic Concurrency Control can be achieved by using the internal system columns _seq_no and _primary_term.\n\nEvery new primary shard row has an initial sequence number of 0. This value is increased by 1 on every insert, delete or update operation the primary shard executes. The primary term will be incremented when a shard is promoted to primary so the user can know if they are executing an update against the most up to date cluster configuration.\n\nIt’s possible to fetch the _seq_no and _primary_term by selecting them:\n\ncr> SELECT id, type, _seq_no, _primary_term FROM sensors ORDER BY 1;\n+-----+-------+---------+---------------+\n| id  | type  | _seq_no | _primary_term |\n+-----+-------+---------+---------------+\n| ID1 | DHT11 |       0 |             1 |\n| ID2 | DHT21 |       0 |             1 |\n+-----+-------+---------+---------------+\nSELECT 2 rows in set (... sec)\n\n\nThese _seq_no and _primary_term values can now be used on updates and deletes.\n\nNote\n\nOptimistic concurrency control only works using the = operator, checking for the exact _seq_no and _primary_term your update or delete is based on.\n\nOptimistic update\n\nQuerying for the correct _seq_no and _primary_term ensures that no concurrent update and cluster configuration change has taken place:\n\ncr> UPDATE sensors SET last_verification = '2020-01-10 09:40'\n... WHERE\n...   id = 'ID1'\n...   AND \"_seq_no\" = 0\n...   AND \"_primary_term\" = 1;\nUPDATE OK, 1 row affected (... sec)\n\n\nUpdating a row with a wrong or outdated sequence number or primary term will not execute the update and results in 0 affected rows:\n\ncr> UPDATE sensors SET last_verification = '2020-01-10 09:40'\n... WHERE\n...   id = 'ID1'\n...   AND \"_seq_no\" = 42\n...   AND \"_primary_term\" = 5;\nUPDATE OK, 0 rows affected (... sec)\n\nOptimistic delete\n\nThe same can be done when deleting a row:\n\ncr> DELETE FROM sensors WHERE id = 'ID2'\n...   AND \"_seq_no\" = 0\n...   AND \"_primary_term\" = 1;\nDELETE OK, 1 row affected (... sec)\n\nKnown limitations\n\nThe _seq_no and _primary_term columns can only be used when specifying the whole primary key in a query. For example, the query below is not possible with the database schema used for testing, because type is not declared as a primary key:\n\ncr> DELETE FROM sensors WHERE type = 'DHT11'\n...   AND \"_seq_no\" = 3\n...   AND \"_primary_term\" = 1;\nUnsupportedFeatureException[\"_seq_no\" and \"_primary_term\" columns can only be used\ntogether in the WHERE clause with equals comparisons and if there are also equals\ncomparisons on primary key columns]\n\n\nIn order to use the optimistic concurrency control mechanism, both the _seq_no and _primary_term columns need to be specified. It is not possible to only specify one of them. For example, the query below will result in an error:\n\ncr> DELETE FROM sensors WHERE id = 'ID1' AND \"_seq_no\" = 3;\nVersioningValidationException[\"_seq_no\" and \"_primary_term\" columns can only be used\ntogether in the WHERE clause with equals comparisons and if there are also equals\ncomparisons on primary key columns]\n\n\nNote\n\nBoth DELETE and UPDATE commands will return a row count of 0, if the given required version does not match the actual version of the relevant row."
  },
  {
    "title": "Blobs — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/blobs.html",
    "html": "5.6\nBlobs\n\nCrateDB includes support to store binary large objects. By utilizing CrateDB’s cluster features the files can be replicated and sharded just like regular data.\n\nTable of contents\n\nCreating a table for blobs\n\nCustom location for storing blob data\n\nGlobal by configuration\n\nPer blob table setting\n\nList\n\nAltering a blob table\n\nDeleting a blob table\n\nUsing blob tables\n\nUploading\n\nDownloading\n\nDeleting\n\nCreating a table for blobs\n\nBefore adding blobs a blob table must be created. Blob tables can be sharded. This makes it possible to distribute binaries over multiple nodes. Lets use the CrateDB shell crash to issue the SQL statement:\n\nsh$ crash -c \"create blob table myblobs clustered into 3 shards with (number_of_replicas=0)\"\nCREATE OK, 1 row affected (... sec)\n\n\nNow CrateDB is configured to allow blobs to be management under the /_blobs/myblobs endpoint.\n\nCustom location for storing blob data\n\nIt is possible to define a custom directory path for storing blob data which can be completely different than the normal data path. Best use case for this is storing normal data on a fast SSD and blob data on a large cheap spinning disk.\n\nThe custom blob data path can be set either globally by configuration or while creating a blob table. The path can be either absolute or relative and must be creatable/writable by the user CrateDB is running as. A relative path value is relative to CRATE_HOME.\n\nBlob data will be stored under this path with the following layout:\n\n/<blobs.path>/nodes/<NODE_NO>/indices/<INDEX_UUID>/<SHARD_ID>/blobs\n\nGlobal by configuration\n\nJust uncomment or add following entry at the CrateDB configuration in order to define a custom path globally for all blob tables:\n\nblobs.path: /path/to/blob/data\n\n\nAlso see Configuration.\n\nPer blob table setting\n\nIt is also possible to define a custom blob data path per table instead of global by configuration. Also per table setting take precedence over the configuration setting.\n\nSee CREATE BLOB TABLE for details.\n\nCreating a blob table with a custom blob data path:\n\nsh$ crash -c \"create blob table myblobs clustered into 3 shards with (blobs_path='/tmp/crate_blob_data')\" # doctest: +SKIP\nCREATE OK, 1 row affected (... sec)\n\nList\n\nTo list all blobs inside a blob table a SELECT statement can be used:\n\nsh$ crash -c \"select digest, last_modified from blob.myblobs\"\n+------------------------------------------+---------------+\n| digest                                   | last_modified |\n+------------------------------------------+---------------+\n| 4a756ca07e9487f482465a99e8286abc86ba4dc7 | ...           |\n+------------------------------------------+---------------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nTo query blob tables it is necessary to always specify the schema name blob.\n\nAltering a blob table\n\nThe number of replicas a blob table has can be changed using the ALTER BLOB TABLE clause:\n\nsh$ crash -c \"alter blob table myblobs set (number_of_replicas=0)\"\nALTER OK, -1 rows affected (... sec)\n\nDeleting a blob table\n\nBlob tables can be deleted similar to normal tables:\n\nsh$ crash -c \"drop blob table myblobs\"\nDROP OK, 1 row affected (... sec)\n\nUsing blob tables\n\nThe usage of Blob Tables is only supported using the HTTP/HTTPS protocol. This section describes how binaries can be stored, fetched and deleted.\n\nNote\n\nFor the reason of internal optimization any successful request could lead to a 307 Temporary Redirect response.\n\nUploading\n\nTo upload a blob the SHA1 hash of the blob has to be known upfront since this has to be used as the ID of the new blob. For this example we use a fancy Python one-liner to compute the SHA hash:\n\nsh$ python3 -c 'import hashlib;print(hashlib.sha1(\"contents\".encode(\"utf-8\")).hexdigest())'\n4a756ca07e9487f482465a99e8286abc86ba4dc7\n\n\nThe blob can now be uploaded by issuing a PUT request:\n\nsh$ curl -isSX PUT '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7' -d 'contents'\nHTTP/1.1 201 Created\ncontent-length: 0\n\n\nIf a blob already exists with the given hash a 409 Conflict is returned:\n\nsh$ curl -isSX PUT '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7' -d 'contents'\nHTTP/1.1 409 Conflict\ncontent-length: 0\n\nDownloading\n\nTo download a blob simply use a GET request:\n\nsh$ curl -sS '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7'\ncontents\n\n\nIf the blob doesn’t exist a 404 Not Found error is returned:\n\nsh$ curl -isS '127.0.0.1:4200/_blobs/myblobs/e5fa44f2b31c1fb553b6021e7360d07d5d91ff5e'\nHTTP/1.1 404 Not Found\ncontent-length: 0\n\n\nTo determine if a blob exists without downloading it, a HEAD request can be used:\n\nsh$ curl -sS -I '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7'\nHTTP/1.1 200 OK\ncontent-length: 8\naccept-ranges: bytes\nexpires: Thu, 31 Dec 2037 23:59:59 GMT\ncache-control: max-age=315360000\n\n\nNote\n\nThe cache headers for blobs are static and basically allows clients to cache the response forever since the blob is immutable.\n\nDeleting\n\nTo delete a blob simply use a DELETE request:\n\nsh$ curl -isS -XDELETE '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7'\nHTTP/1.1 204 No Content\n\n\nIf the blob doesn’t exist a 404 Not Found error is returned:\n\nsh$ curl -isS -XDELETE '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7'\nHTTP/1.1 404 Not Found\ncontent-length: 0\n"
  },
  {
    "title": "User-defined functions — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/user-defined-functions.html",
    "html": "5.6\nUser-defined functions\n\nTable of contents\n\nCREATE OR REPLACE\n\nSupported types\n\nOverloading\n\nDeterminism\n\nDROP FUNCTION\n\nSupported languages\n\nJavaScript\n\nJavaScript supported types\n\nWorking with NUMBERS\n\nCREATE OR REPLACE\n\nCrateDB supports user-defined functions. See CREATE FUNCTION for a full syntax description.\n\nCREATE FUNCTION defines a new function:\n\ncr> CREATE FUNCTION my_subtract_function(integer, integer)\n... RETURNS integer\n... LANGUAGE JAVASCRIPT\n... AS 'function my_subtract_function(a, b) { return a - b; }';\nCREATE OK, 1 row affected  (... sec)\n\ncr> SELECT doc.my_subtract_function(3, 1) AS col;\n+-----+\n| col |\n+-----+\n|   2 |\n+-----+\nSELECT 1 row in set (... sec)\n\n\nCREATE OR REPLACE FUNCTION will either create a new function or replace an existing function definition:\n\ncr> CREATE OR REPLACE FUNCTION log10(bigint)\n... RETURNS double precision\n... LANGUAGE JAVASCRIPT\n... AS 'function log10(a) {return Math.log(a)/Math.log(10); }';\nCREATE OK, 1 row affected  (... sec)\n\ncr> SELECT doc.log10(10) AS col;\n+-----+\n| col |\n+-----+\n| 1.0 |\n+-----+\nSELECT 1 row in set (... sec)\n\n\nIt is possible to use named function arguments in the function signature. For example, the calculate_distance function signature has two geo_point arguments named start and end:\n\ncr> CREATE OR REPLACE FUNCTION calculate_distance(\"start\" geo_point, \"end\" geo_point)\n... RETURNS real\n... LANGUAGE JAVASCRIPT\n... AS 'function calculate_distance(start, end) {\n...       return Math.sqrt(\n...            Math.pow(end[0] - start[0], 2),\n...            Math.pow(end[1] - start[1], 2));\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\n\nNote\n\nArgument names are used for query documentation purposes only. You cannot reference arguments by name in the function body.\n\nOptionally, a schema-qualified function name can be defined. If you omit the schema, the current session schema is used:\n\ncr> CREATE OR REPLACE FUNCTION my_schema.log10(bigint)\n... RETURNS double precision\n... LANGUAGE JAVASCRIPT\n... AS 'function log10(a) { return Math.log(a)/Math.log(10); }';\nCREATE OK, 1 row affected  (... sec)\n\n\nNote\n\nIn order to improve the PostgreSQL server compatibility CrateDB allows the creation of user defined functions against the pg_catalog schema. However, the creation of user defined functions against the read-only System information and Information schema schemas is prohibited.\n\nSupported types\n\nFunction arguments and return values can be any of the supported data types. The values passed into a function must strictly correspond to the specified argument data types.\n\nNote\n\nThe value returned by the function will be casted to the return type provided in the definition if required. An exception will be thrown if the cast is not successful.\n\nOverloading\n\nWithin a specific schema, you can overload functions by defining functions with the same name but a different set of arguments:\n\ncr> CREATE FUNCTION my_schema.my_multiply(integer, integer)\n... RETURNS integer\n... LANGUAGE JAVASCRIPT\n... AS 'function my_multiply(a, b) { return a * b; }';\nCREATE OK, 1 row affected  (... sec)\n\n\nThis would overload the my_multiply function with different argument types:\n\ncr> CREATE FUNCTION my_schema.my_multiply(bigint, bigint)\n... RETURNS bigint\n... LANGUAGE JAVASCRIPT\n... AS 'function my_multiply(a, b) { return a * b; }';\nCREATE OK, 1 row affected  (... sec)\n\n\nThis would overload the my_multiply function with more arguments:\n\ncr> CREATE FUNCTION my_schema.my_multiply(bigint, bigint, bigint)\n... RETURNS bigint\n... LANGUAGE JAVASCRIPT\n... AS 'function my_multiply(a, b, c) { return a * b * c; }';\nCREATE OK, 1 row affected  (... sec)\n\n\nCaution\n\nIt is considered bad practice to create functions that have the same name as the CrateDB built-in functions.\n\nNote\n\nIf you call a function without a schema name, CrateDB will look it up in the built-in functions first and only then in the user-defined functions available in the search_path.\n\nTherefore a built-in function with the same name as a user-defined function will hide the latter, even if it contains a different set of arguments. However, such functions can still be called if the schema name is explicitly provided.\n\nDeterminism\n\nCaution\n\nUser-defined functions need to be deterministic, meaning that they must always return the same result value when called with the same argument values, because CrateDB might cache the returned values and reuse the value if the function is called multiple times with the same arguments.\n\nDROP FUNCTION\n\nFunctions can be dropped like this:\n\ncr> DROP FUNCTION doc.log10(bigint);\nDROP OK, 1 row affected  (... sec)\n\n\nAdding IF EXISTS prevents from raising an error if the function doesn’t exist:\n\ncr> DROP FUNCTION IF EXISTS doc.log10(integer);\nDROP OK, 1 row affected  (... sec)\n\n\nOptionally, argument names can be specified within the drop statement:\n\ncr> DROP FUNCTION IF EXISTS doc.calculate_distance(start_point geo_point, end_point geo_point);\nDROP OK, 1 row affected  (... sec)\n\n\nOptionally, you can provide a schema:\n\ncr> DROP FUNCTION my_schema.log10(bigint);\nDROP OK, 1 row affected  (... sec)\n\nSupported languages\n\nCurrently, CrateDB only supports JavaScript for user-defined functions.\n\nJavaScript\n\nThe user defined function JavaScript is compatible with the ECMAScript 2019 specification.\n\nCrateDB uses the GraalVM JavaScript engine as a JavaScript (ECMAScript) language execution runtime. The GraalVM JavaScript engine is a Java application that works on the stock Java Virtual Machines (VMs). The interoperability between Java code (host language) and JavaScript user-defined functions (guest language) is guaranteed by the GraalVM Polyglot API.\n\nPlease note: CrateDB does not use the GraalVM JIT compiler as optimizing compiler. However, the stock host Java VM JIT compilers can JIT-compile, optimize, and execute the GraalVM JavaScript codebase to a certain extent.\n\nThe execution context for guest JavaScript is created with restricted privileges to allow for the safe execution of less trusted guest language code. The guest language application context for each user-defined function is created with default access modifiers, so any access to managed resources is denied. The only exception is the host language interoperability configuration which explicitly allows access to Java lists and arrays. Please refer to GraalVM Security Guide for more detailed information.\n\nAlso, even though user-defined functions implemented with ECMA-compliant JavaScript, objects that are normally accessible with a web browser (e.g. window, console, and so on) are not available.\n\nNote\n\nGraalVM treats objects provided to JavaScript user-defined functions as close as possible to their respective counterparts and therefore by default only a subset of prototype functions are available in user-defined functions. For CrateDB 4.6 and earlier the object prototype was disabled.\n\nPlease refer to the GraalVM JavaScript Compatibility FAQ to learn more about the compatibility.\n\nJavaScript supported types\n\nJavaScript functions can handle all CrateDB data types. However, for some return types the function output must correspond to the certain format.\n\nIf a function requires geo_point as a return type, then the JavaScript function must return a double precision array of size 2, WKT string or GeoJson object.\n\nHere is an example of a JavaScript function returning a double array:\n\ncr> CREATE FUNCTION rotate_point(point geo_point, angle real)\n... RETURNS geo_point\n... LANGUAGE JAVASCRIPT\n... AS 'function rotate_point(point, angle) {\n...       var cos = Math.cos(angle);\n...       var sin = Math.sin(angle);\n...       var x = cos * point[0] - sin * point[1];\n...       var y = sin * point[0] + cos * point[1];\n...       return [x, y];\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\n\nBelow is an example of a JavaScript function returning a WKT string, which will be cast to geo_point:\n\ncr> CREATE FUNCTION symmetric_point(point geo_point)\n... RETURNS geo_point\n... LANGUAGE JAVASCRIPT\n... AS 'function symmetric_point (point, angle) {\n...       var x = - point[0],\n...           y = - point[1];\n...       return \"POINT (\\\" + x + \\\", \\\" + y +\\\")\";\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\n\nSimilarly, if the function specifies the geo_shape return data type, then the JavaScript function should return a GeoJson object or WKT string:\n\ncr> CREATE FUNCTION line(\"start\" array(double precision), \"end\" array(double precision))\n... RETURNS object\n... LANGUAGE JAVASCRIPT\n... AS 'function line(start, end) {\n...        return { \"type\": \"LineString\", \"coordinates\" : [start_point, end_point] };\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\n\nNote\n\nIf the return value of the JavaScript function is undefined, it is converted to NULL.\n\nWorking with NUMBERS\n\nThe JavaScript engine interprets numbers as java.lang.Double, java.lang.Long, or java.lang.Integer, depending on the computation performed. In most cases, this is not an issue, since the return type of the JavaScript function will be cast to the return type specified in the CREATE FUNCTION statement, although cast might result in a loss of precision.\n\nHowever, when you try to cast DOUBLE PRECISION to TIMESTAMP WITH TIME ZONE, it will be interpreted as UTC seconds and will result in a wrong value:\n\ncr> CREATE FUNCTION utc(bigint, bigint, bigint)\n... RETURNS TIMESTAMP WITH TIME ZONE\n... LANGUAGE JAVASCRIPT\n... AS 'function utc(year, month, day) {\n...       return Date.UTC(year, month, day, 0, 0, 0);\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\ncr> SELECT date_format(utc(2016,04,6)) as epoque;\n+------------------------------+\n| epoque                       |\n+------------------------------+\n| 48314-07-22T00:00:00.000000Z |\n+------------------------------+\nSELECT 1 row in set (... sec)\n\n\nTo avoid this behavior, the numeric value should be divided by 1000 before it is returned:\n\ncr> CREATE FUNCTION utc(bigint, bigint, bigint)\n... RETURNS TIMESTAMP WITH TIME ZONE\n... LANGUAGE JAVASCRIPT\n... AS 'function utc(year, month, day) {\n...       return Date.UTC(year, month, day, 0, 0, 0)/1000;\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\ncr> SELECT date_format(utc(2016,04,6)) as epoque;\n+-----------------------------+\n| epoque                      |\n+-----------------------------+\n| 2016-05-06T00:00:00.000000Z |\n+-----------------------------+\nSELECT 1 row in set (... sec)\n"
  },
  {
    "title": "Querying — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/dql/index.html",
    "html": "5.6\nQuerying\n\nThis section provides an overview of how to query CrateDB.\n\nSee Also\n\nGeneral use: Data definition\n\nGeneral use: Data manipulation\n\nTable of contents\n\nSelecting data\nIntroduction\nFROM clause\nJoins\nDISTINCT clause\nWHERE clause\nComparison operators\nArray comparisons\nEXISTS\nContainer data types\nAggregation\nWindow functions\nGROUP BY\nWITH Queries (Common Table Expressions)\nJoins\nCross joins\nInner joins\nOuter joins\nJoin conditions\nAvailable join algorithms\nLimitations\nUnion\nUnion All\nUnion Distinct\nUnion of object types\nUnion of different types\nRefresh\nIntroduction\nMultiple Table Refresh\nPartition Refresh\nFulltext search\nMATCH Predicate\nUsage\nSearching On Multiple Columns\nNegative Search\nFilter By _score\nGeo search\nIntroduction\nMATCH predicate\nExact queries"
  },
  {
    "title": "Session settings — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/config/session.html",
    "html": "5.6\nSession settings\n\nTable of contents\n\nUsage\n\nSupported session settings\n\nSession settings only apply to the currently connected client session.\n\nUsage\n\nTo configure a modifiable session setting, use SET, for example:\n\nSET search_path TO myschema, doc;\n\n\nTo retrieve the current value of a session setting, use SHOW e.g:\n\nSHOW search_path;\n\n\nBesides using SHOW, it is also possible to use the current_setting scalar function.\n\nSupported session settings\nsearch_path\nDefault: pg_catalog, doc\nModifiable: yes\n\nThe list of schemas to be searched when a relation is referenced without a schema.\n\nCrateDB will try to resolve an unqualified relation name against the configured search_path by iterating over the configured schemas in the order they were declared. The first matching relation in the search_path is used. CrateDB will report an error if there is no match.\n\nNote\n\nThis setting mirrors the PostgreSQL search_path setting.\n\nSome PostgreSQL clients require access to various tables in the pg_catalog schema. Usually, this is to extract information about built-in data types or functions.\n\nCrateDB implements the system pg_catalog schema and it automatically includes it in the search_path before the configured schemas, unless it is already explicitly in the schema configuration.\n\napplication_name\nDefault: null\nModifiable: yes\n\nAn arbitrary application name that can be set to identify an application that connects to a CrateDB node.\n\nSome clients set this implicitly to their client name.\n\nstatement_timeout\nDefault: '0'\nModifiable: yes\n\nThe maximum duration of any statement before it gets cancelled. If 0 (the default), queries are allowed to run infinitely and don’t get cancelled automatically.\n\nThe value is an INTERVAL with a maximum of 2147483647 milliseconds. That’s roughly 24 days.\n\nmemory.operation_limit\nDefault: 0\nModifiable: yes\n\nThis is an experimental expert setting defining the maximal amount of memory in bytes that an individual operation can consume before triggering an error.\n\n0 means unlimited. In that case only the global circuit breaker limits apply.\n\nThere is no 1:1 mapping from SQL statement to operation. Some SQL statements have no corresponding operation. Other SQL statements can have more than one operation. You can use the sys.operations view to get some insights, but keep in mind that both, operations which are used to execute a query, and their name could change with any release, including hotfix releases.\n\nenable_hashjoin\nDefault: true\nModifiable: yes\n\nAn experimental setting which enables CrateDB to consider whether a JOIN operation should be evaluated using the HashJoin implementation instead of the Nested-Loops implementation.\n\nNote\n\nIt is not always possible or efficient to use the HashJoin implementation. Having this setting enabled, will only add the option of considering it, it will not guarantee it. See also the available join algorithms for more insights on this topic.\n\nerror_on_unknown_object_key\nDefault: true\nModifiable: yes\n\nThis setting controls the behaviour of querying unknown object keys to dynamic objects. CrateDB will throw an error by default if any of the queried object keys are unknown or will return a null if the setting is set to false.\n\ndatestyle\nDefault: ISO\nModifiable: yes\n\nShows the display format for date and time values. Only the ISO style is supported. Optionally provided pattern conventions for the order of date parts (Day, Month, Year) are ignored.\n\nNote\n\nThe session setting currently has no effect in CrateDB and exists for compatibility with PostgreSQL. Trying to set this to a date format style other than ISO will raise an exception.\n\nmax_index_keys\nDefault: 32\nModifiable: no\n\nShows the maximum number of index keys.\n\nNote\n\nThe session setting has no effect in CrateDB and exists for compatibility with PostgreSQL.\n\nmax_identifier_length\nDefault: 255\nModifiable: no\n\nShows the maximum length of identifiers in bytes.\n\nserver_version_num\nDefault: 100500\nModifiable: no\n\nShows the emulated PostgreSQL server version.\n\nserver_version\nDefault: 10.5\nModifiable: no\n\nShows the emulated PostgreSQL server version.\n\nstandard_conforming_strings\nDefault: on\nModifiable: no\n\nCauses '...' strings to treat backslashes literally.\n\noptimizer\nDefault: true\nModifiable: yes\n\nThis setting indicates whether a query optimizer rule is activated. The name of the query optimizer rule has to be provided as a suffix as part of the setting e.g. SET optimizer_rewrite_collect_to_get = false.\n\nNote\n\nThe optimizer setting is for advanced use only and can significantly impact the performance behavior of the queries.\n\noptimizer_eliminate_cross_join\nDefault: true\nModifiable: yes\n\nThis setting indicates if the cross join elimination rule of the optimizer rule is activated.\n\nWarning\n\nExperimental session settings might be removed in the future even in minor feature releases."
  },
  {
    "title": "Version 5.4.0 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.4.0.html",
    "html": "5.6\nVersion 5.4.0\n\nReleased on 2023-07-11.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.4.0.\n\nWe recommend that you upgrade to the latest 5.3 release before moving to 5.4.0.\n\nA rolling upgrade from 5.3.x to 5.4.0 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nKnown Issues\n\nBreaking Changes\n\nDeprecations\n\nChanges\n\nSQL Statements\n\nSQL Standard and PostgreSQL Compatibility\n\nScalar and Aggregation Functions\n\nPerformance and Resilience Improvements\n\nData Types\n\nAdministration and Operations\n\nKnown Issues\n\nVersion 5.0.0 introduced a regression which can cause some JOIN queries to return no results when the query optimizer re-orders the joined tables. As a workaround, users should apply the following session settings before running such queries to prevent the query optimizer from re-ordering them and therefore produce the correct results:\n\nOn CrateDB >= 5.4.0:\n\nSET optimizer_reorder_hash_join = false\nSET optimizer_reorder_nested_loop_join = false\n\n\nOn CrateDB < 5.4.0 (this will disable the hash-join algorithm which may lead to poor performance):\n\nSET enable_hashjoin = false\n\nBreaking Changes\n\nImplemented several changes to the pg_catalog tables, involving the addition and removal of columns, and modification of column data types, to align with PostgreSQL version 14:\n\npg_attribute\n\nAdded: atthasmissing, attmissingval\n\nType Changed: spcacl from OBJECT[] to STRING[]\n\npg_class\n\nAdded: relrewrite\n\nRemoved: relhasoids, relhaspkey\n\nType Changed: relacl from OBJECT[] to STRING[]\n\npg_constraint\n\nAdded: conparentid\n\nRemoved: consrc\n\nType Changed: conbin from OBJECT to STRING\n\npg_index\n\nAdded: indnkeyatts\n\npg_namespace\n\nType Changed: nspacl from OBJECT[] to STRING[]\n\npg_proc\n\nAdded: prokind, prosqlbody, prosupport\n\nRemoved: proisagg, proiswindow, protransform\n\nType Changed: proargdefaults from OBJECT[] to STRING\n\npg_type\n\nAdded: typacl, typalign, typanalyze, typdefaultbin, typmodin, typmodout, typstorage, typsubscript\n\nRaise an exception if duplicate columns are detected on named index column definition instead of silently ignoring them.\n\nAdjusted allowed array index range to be from Integer.MIN_VALUE to Integer.MAX_VALUE. The behavior is now also consistent between subscripts on array literals and on columns, and between index literals and index expressions. That means something like tags[-1] will now return NULL just like ARRAY['AUT', 'GER'][-1] or ARRAY['AUT', 'GER'][1 - 5] did.\n\nDeprecations\n\nNone\n\nChanges\nSQL Statements\n\nExtended the EXPLAIN statement output to include the estimated row count in the output of the execution plan. The statement also has now options for ANALYZE and COSTS to have better control on the generated output plan.\n\nSQL Standard and PostgreSQL Compatibility\n\nBumped the version of PostgreSQL wire protocol to 14 since 10 has been deprecated.\n\nAdded any_value as an alias to the arbitrary aggregation function, for compliance with the SQL2023 standard. Extended the aggregations to support any type.\n\nChanged literal INTERVAL data type to do normalization up to day units, and comply with PostgreSQL behavior, e.g.:\n\ncr> SELECT INTERVAL '1 month 42 days 126 hours 512 mins 7123 secs';\n+------------------------------+\n| 'P1M47DT16H30M43S'::interval |\n+------------------------------+\n| 1 mon 47 days 16:30:43       |\n+------------------------------+\n\n\nAdded attgenerated column to pg_catalog.pg_attribute table which returns '' (empty string) for normal columns and 's' for generated columns.\n\nAdded the pg_catalog.pg_cursors table to expose open cursors.\n\nAdded the standard_conforming_strings read-only session setting for improved compatibility with PostgreSQL clients.\n\nAllow casts in both forms: CAST(<literal or parameter> AS <datatype>) and <literal or parameter>::<datatype> for LIMIT and OFFSET clauses,\n\ne.g.:\n\nSELECT * FROM test OFFSET CAST(? AS long) LIMIT '20'::int\n\n\nAdded support for ORDER BY, MAX, MIN and comparison operators on expressions of type INTERVAL.\n\nAdded support for setting session settings via a \"options\" property in the startup message for PostgreSQL wire protocol clients.\n\nAn example for JDBC:\n\nProperties props = new Properties();\nprops.setProperty(\"options\", \"-c statement_timeout=90000\");\nConnection conn = DriverManager.getConnection(url, props);\n\n\nAdded support for underscores in numeric literals. Example:\n\nSELECT 1_000_000;\n\n\nAdded support for updating arrays by elements, e.g.:\n\nUPDATE t SET a[1] = 2 WHERE id = 1;\n\n\nArray comparisons like = ANY will now automatically unnest the array argument to the required dimensions.\n\nAn example:\n\ncr> SELECT 1 = ANY([ [1, 2], [3, 4] ]);   -- automatic unnesting\nTrue\n\ncr> SELECT [1] = ANY([ [1, 2], [3, 4] ]); -- no unnesting\nFalse\n\nScalar and Aggregation Functions\n\nAdded support for AVG() aggregation on INTERVAL data type.\n\nAdded a array_unnest scalar function.\n\nAdded a btrim scalar function.\n\nAdded array_set scalar function.\n\nPerformance and Resilience Improvements\n\nImproved the partition filtering logic to also narrow partitions if the partition is based on a generated column using the date_bin scalar.\n\nImproved COPY FROM retry logic to retry with a delay which increases exponentially on temporary network timeout and general network errors.\n\nData Types\n\nAdded support to disable column storage for numeric data types, timestamp and timestamp with timezone.\n\nAdministration and Operations\n\nAdded optimizer rules for reordering of joins for hash and nested-loop joins. This allows now to control the join-reordering and disable it, if desired, with session settings:\n\nSET optimizer_reorder_hash_join = false\nSET optimizer_reorder_nested_loop_join = false\n\n\nNote that these settings are experimental, and may change in the future.\n\nAdded a statement_timeout session setting and cluster setting that allows to set a timeout for queries.\n\nThe severity of the node checks on the metadata gateway recovery settings has been lowered from HIGH to MEDIUM as leaving these to default or suboptimal values does not translate into data corruption or loss.\n\nAdded the ability to set a storage_class for S3 repositories."
  },
  {
    "title": "Version 5.4.1 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.4.1.html",
    "html": "5.6\nVersion 5.4.1\n\nReleased on 2023-08-04.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.4.1.\n\nWe recommend that you upgrade to the latest 5.3 release before moving to 5.4.1.\n\nBefore upgrading, you should back up your data.\n\nWarning\n\nA rolling upgrade from 5.3.x to 5.4.1 is not supported if your cluster contains view definitions. If you want to do a rolling upgrade, skip 5.4.1 and upgrade to 5.4.2.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nKnown Issues\n\nFixes\n\nSee the Version 5.4.0 release notes for a full list of changes in the 5.4 series.\n\nKnown Issues\n\nVersion 5.0.0 introduced a regression which can cause some JOIN queries to return no results when the query optimizer re-orders the joined tables. As a workaround, users should apply the following session settings before running such queries to prevent the query optimizer from re-ordering them and therefore produce the correct results:\n\nOn CrateDB >= 5.4.0:\n\nSET optimizer_reorder_hash_join = false\nSET optimizer_reorder_nested_loop_join = false\n\n\nOn CrateDB < 5.4.0 (this will disable the hash-join algorithm which may lead to poor performance):\n\nSET enable_hashjoin = false\n\nFixes\n\nUpdated the bundled OpenJDK to 20.0.2+9\n\nAllowed user to RESET cluster settings with archived. prefix.\n\nFixed an issue that could lead to a class_cast_exception when using a subscript expression on an aliased expression that shadows another column. For example:\n\nSELECT a['b'] FROM (SELECT UNNEST(a) AS a FROM tbl) t;\n\n\nFixed an issue that caused SET SESSION statements to fail for settings that are both session settings and global settings, like statement_timeout.\n\nFixed an issue that caused a object_column = {} query to not match if the object column doesn’t have any child columns.\n\nFixed an issue that caused queries against views to evaluate the view definition with the current search path. This could cause issues if the search path diverged. The fix only applies to newly created views.\n\nFixed a Failed to validate IP error that occurred when using the any_value aggregation on a column of type IP.\n\nImproved error message when passing settings with WITH clause on a CREATE SUBSCRIPTION statement, which is not currently supported.\n\nFixed an issue that resulted in hiding errors with the connection URL of CREATE SUBSCRIPTION, when executed from a non-superuser but with AL privileges.\n\nAllowed trailing / without a database name for connection URL of CREATE SUBSCRIPTION.\n\nFixed a NullPointerException which could happen if using a cross join on a sub-query, where the sub-query was executed using a Fetch operator. An example query:\n\nSELECT\n  *\nFROM\n  (SELECT a FROM tbl1 ORDER BY b DESC LIMIT 1) i,\n  tbl2\nWHERE\n  c >= 50;\n\n\nFixed a NullPointerException which was thrown, instead of using default no compression behavior, when compression parameter of COPY TO statement is set to null.\n\nFixed IndexOutOfBoundsException caused by an IS [NOT] NULL filter on a sub-column of an object or object array in a WHERE clause, e.g.\n\nCREATE TABLE test (o1 ARRAY(OBJECT AS (col INT)), o2 OBJECT);\nSELECT * FROM test WHERE o1[1]['col'] IS NULL;\n=> IndexOutOfBoundsException[Index: 1 Size: 1]\nSELECT * FROM test AS T WHERE T.o2['unknown_col'] IS NOT NULL;\n=> IndexOutOfBoundsException[Index: 1 Size: 1]\n\n\nFixed an issue which caused INSERT INTO statements to skip generated expression validation for partitioned columns.\n\nFixed an issue which caused arrays in IGNORED objects to be converted to nulls.\n\nFixed an issue that caused LIKE/ILIKE operators on INDEX OFF columns to return empty results.\n\nFixed an issue that caused LIKE ANY/ILIKE ANY operators to accept only left arguments as patterns. Both arguments can now be patterns, e.g.\n\nSELECT 'a' LIKE ANY(['a%']);\n+-------+\n| false |\n+-------+\n| FALSE | -- true will be returned after the fix\n+-------+\n\n\nFixed an issue which caused INSERT INTO ... SELECT ... statements to leave behind empty partitions if NULL or CHECK constraint on partitioned by column failed.\n\nFixed an issue which caused errors on querying information_schema tables when the query of a VIEW is erroneous due to changes made to the underlying tables/views. Also, added a comment to view definition in pg_catalog.pg_views and information_schema.views tables to denote that a VIEW’s query is erroneous.\n\nFixed SQLParseException caused by querying an unknown key from an object column of a table that is aliased and with the session setting error_on_unknown_object_key, set to false, e.g.\n\nCREATE TABLE test (o OBJECT);\nSELECT T.o['unknown'] from (SELECT * FROM test) AS T;\n=> SQLParseException[Couldn't create execution plan from logical plan because of: Couldn't find o['unknown'] in SourceSymbols{inputs={}, nonDeterministicFunctions={}}\n"
  },
  {
    "title": "Version 5.4.3 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.4.3.html",
    "html": "5.6\nVersion 5.4.3\n\nReleased on 2023-09-05.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.4.3.\n\nWe recommend that you upgrade to the latest 5.3 release before moving to 5.4.3.\n\nA rolling upgrade from 5.3.x to 5.4.3 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nFixes\n\nSee the Version 5.4.0 release notes for a full list of changes in the 5.4 series.\n\nFixes\n\nFixed a regression introduced with CrateDB 5.0.0 which can cause that some nested join queries using the hash-join algorithm will return empty results after table statistics have been collected and the join order is optimized."
  },
  {
    "title": "Version 5.4.4 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.4.4.html",
    "html": "5.6\nVersion 5.4.4\n\nReleased on 2023-10-13.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.4.4.\n\nWe recommend that you upgrade to the latest 5.3 release before moving to 5.4.4.\n\nA rolling upgrade from 5.3.x to 5.4.4 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nFixes\n\nSee the Version 5.4.0 release notes for a full list of changes in the 5.4 series.\n\nFixes\n\nFixed an issue that led to file not found errors when trying to restore a snapshot that was taken after a table had been swapped. A new snapshot must be taken to apply the fix and solve the issue.\n\nFixed an issue that caused nested accesses to ignored objects with unknown object keys from returning non nulls. e.g.:\n\nSELECT o['a']['unknown'] FROM t;\n\n\nreturning the value for o['a'] instead of a null.\n\nFixed an issue that caused the output object column of UNION on object columns to contain only the sub-columns of the object column from the right hand side. With this fix, the output object column will contain the merged sub-columns of the respective objects of both relations.\n\nFixed an issue that caused IS NULL predicate to be ineffective on sub-columns of ignored object types.\n\nFixed a regression introduced with CrateDB 5.3.1 which caused a NullPointerException when creating a repository on S3 with authentication provided by IAM role attached to the EC2 instance running the CrateDB node.\n\nFixed an issue which led to skipping duplicate column check, in CREATE TABLE statements, if duplicate columns have the same type.\n\nFixed an issue with missing validation on INSERT statement, allowing to specify duplicate target columns.\n\nFixed an issue that caused UPDATE statements, invoked immediately after a DELETE statement, which empties a table, to show an error.\n\nFixed an issue that caused CASE expressions to throw UnsupportedFeatureException when null values are returned for one or more conditions. e.g.:\n\nSELECT CASE col1\n  WHEN 'value1' THEN 1\n  WHEN 'value2' THEN NULL\n  WHEN 'value3' THEN 3\n  ELSE NULL\nEND\nFROM (SELECT 'value1' AS col1) a;\n\n\nFixed an issue that caused column policy of object types created by CREATE TABLE AS to be overridden to strict.\n\nFixed an issue that caused RETURNING clause, referring to non-deterministic GENERATED or DEFAULT columns, return a value, not equal to the actually persisted one.\n\nFixed an issue that caused storing inconsistent values on primary and replica for GENERATED or DEFAULT columns, using non-deterministic functions, returning different values even within the scope of a single query. e.g.:: gen_random_text_uuid and random. Functions like NOW or CURRENT_TIMESTAMP are unaffected by this issue.\n\nFixed an issue that caused privileges checks to be bypassed when using scalar sub-selects in various clauses of a query: SELECT, WHERE, HAVING, etc.\n\nFixed an issue that caused any operators such as != any, not like any or range operators like < any to throw NullPointerExceptions or return invalid results when the left hand side argument is a column and the right hand side argument array contains at least one null.\n\nFixed an issue that caused (I)LIKE ANY operators to fail with an exception when comparing non-string expressions.\n\nFixed an issue that caused IS NULL operators to raise an error if the column had the column store disabled."
  },
  {
    "title": "Version 5.4.2 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.4.2.html",
    "html": "5.6\nVersion 5.4.2\n\nReleased on 2023-08-11.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.4.2.\n\nWe recommend that you upgrade to the latest 5.3 release before moving to 5.4.2.\n\nA rolling upgrade from 5.3.x to 5.4.2 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nKnown Issues\n\nFixes\n\nSee the Version 5.4.0 release notes for a full list of changes in the 5.4 series.\n\nKnown Issues\n\nVersion 5.0.0 introduced a regression which can cause some JOIN queries to return no results when the query optimizer re-orders the joined tables. As a workaround, users should apply the following session settings before running such queries to prevent the query optimizer from re-ordering them and therefore produce the correct results:\n\nOn CrateDB >= 5.4.0:\n\nSET optimizer_reorder_hash_join = false\nSET optimizer_reorder_nested_loop_join = false\n\n\nOn CrateDB < 5.4.0 (this will disable the hash-join algorithm which may lead to poor performance):\n\nSET enable_hashjoin = false\n\nFixes\n\nFixed an issue that led to serialization errors on a rolling upgrade to 5.4.1 if the cluster contained user defined views."
  },
  {
    "title": "Version 5.4.5 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.4.5.html",
    "html": "5.6\nVersion 5.4.5\n\nReleased on 2023-10-26.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.4.5.\n\nWe recommend that you upgrade to the latest 5.3 release before moving to 5.4.5.\n\nA rolling upgrade from 5.3.x to 5.4.5 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nFixes\n\nSee the Version 5.4.0 release notes for a full list of changes in the 5.4 series.\n\nFixes\n\nFixed an issue that led to Received resultset tuples, but no field structure for them errors in clients using the PostgreSQL wire protocol, if a query was triggered after another query was suspended and left unconsumed.\n\nFixed an issue that led to Couldn't create execution plan from logical plan .. errors when trying to use a correlated join in the WHERE clause of a query with a join. For example:\n\nSELECT\n    n.nspname AS schema,\n    t.typname AS typename,\n    t.oid::int4 AS typeid\nFROM\n    pg_type t\n    LEFT JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace\nWHERE\n    EXISTS (\n        SELECT 1 FROM pg_catalog.pg_type el WHERE el.oid = t.typelem);\n\n\nFixed an issue that prevented namespaces from showing up in pg_catalog.pg_namespace if a user had privileges on a table within a schema, but no privileges on the schema itself.\n\nFixed an issue that caused UNION to throw SQLParseExceptions instead of AmbiguousColumnExceptions, when 2 or more columns are assigned the same name. e.g.:\n\nSELECT a FROM (SELECT a, b AS a FROM t UNION SELECT 1, 1) t2;\n-- selecting 'a' from 't2' is ambiguous since there are 'a' and 'b AS a'\n\n\nFixed a regression introduced with CrateDB Version 5.3.0 that may cause INSERT INTO ... ON CONFLICT .. queries to fail with unexpected errors or even updating wrong sub-columns when using a sub-column expression in the ON CONFLICT clause.\n\nFixed an issue that caused UNION to return wrong results or throw SQLParseException when the output columns had identical names which were from tables that were aliased. e.g.:\n\nSELECT * FROM (SELECT t1.a, t2.a FROM t AS t1, t AS t2) t3 UNION SELECT 1, 1;\n\n\nwhere t1.a and t2.a are from aliased tables that also have identical names, a.\n\nFixed a regression, introduced in Version 4.2.0 which caused wrong HTTP error code to be returned, when some internal issue occurred during the creation of execution plan for a query, e.g.: shards of a table involved in the query become unavailable.\n\nFixed an issue that caused the properties of a INDEX using clause within a type definition for ARRAY(GEO_SHAPE) in a CREATE TABLE statement to be ignored."
  },
  {
    "title": "Version 5.4.6 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.4.6.html",
    "html": "5.6\nVersion 5.4.6\n\nReleased on 2023-11-30.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.4.6.\n\nWe recommend that you upgrade to the latest 5.3 release before moving to 5.4.6.\n\nA rolling upgrade from 5.3.x to 5.4.6 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nFixes\n\nSee the Version 5.4.0 release notes for a full list of changes in the 5.4 series.\n\nFixes\n\nFixed an issue that caused queries with a NOT expression in the WHERE clause to fail evaluating NULL correctly.\n\nFixed an issue that caused the value for generated primary key columns to evaluate to NULL in INSERT INTO .. ON CONFLICT statements if the column wasn’t part of the target column list.\n\nCreating a table that uses a table-function as part of a default expression or generated expression now results in an error on table creation, instead of never inserting records due to runtime failures.\n\nImproved the error message when using COPY FROM with wait_for_completion=false and RETURN SUMMARY. It now reports that the combination is not supported instead of running into a ClassCastException.\n\nFixed an issue that caused queries with a NOT (a AND b) expression in the WHERE clause to not evaluate correctly with NULL values.\n\nFixed an issue that caused queries with a NOT or != on a CASE expression containing a nullable column to exclude NULL entries.\n\nFixed an issue that caused the hash-join operator generate invalid hashes which lead to a broken join operation when there were more than two relations involved e.g.::\n\nSELECT * FROM t1, t2, t3 WHERE t3.c = t1.a AND t3.c = t2.b AND t1.a = t2.b;\n\n\nwould generate the logical plan::\n\nHashJoin[(t3.c = t2.b AND t1.a = t2.b)]\n  ├ HashJoin[(t3.c = t1.a)]\n  │  ├ Collect[doc.t3 | [c] | true]\n  │  └ Collect[doc.t1 | [a] | true]\n  └ Collect[doc.t2 | [b] | true]\n\n\nThe hash-symbol generation for the join t3.c = t2.b AND t1.a = t2.b was broken and would not join the data."
  },
  {
    "title": "Version 5.4.7 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.4.7.html",
    "html": "5.6\nVersion 5.4.7\n\nReleased on 2023-12-21.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.4.7.\n\nWe recommend that you upgrade to the latest 5.3 release before moving to 5.4.7.\n\nA rolling upgrade from 5.3.x to 5.4.7 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nSecurity Fixes\n\nPackaging Changes\n\nSee the Version 5.4.0 release notes for a full list of changes in the 5.4 series.\n\nSecurity Fixes\n\nThe HTTP transport will not trust any X-Real-IP header by default anymore. This prevents a client from spoofing its IP address by setting these headers and thus bypassing IP based authentication with is enabled by default for the crate superuser. To keep allowing the X-Real-IP header to be trusted, you have to explicitly enable it via the auth.trust.http_support_x_real_ip node setting.\n\nPackaging Changes\n\nThe RPM and DEB packages changed slightly to unify the build process. The most important change is that the crate service no longer automatically starts after package installation, to allow changing the configuration first.\n\nOther than that, the structure is now:\n\nbin, jdk and lib are installed into /usr/share/crate. In the RPM package this used to be in /opt/crate.\n\nThe home directory of the crate user is /usr/share/crate\n\nchanges, notice, license are in /usr/share/doc/crate\n\nservice file is in /usr/lib/systemd/system\n\nThe crate.yml configuration file is in /etc/crate/\n\nThe default environment configuration file at RPM packages changed to /etc/default/crate to be consistent with the DEB package. The old location at /etc/sysconfig/crate is not supported anymore.\n\nIf you haven’t made any significant configuration changes the new packages should keep working out of the box.\n\nImportant for Debian and Ubuntu users: There is now a new repository.\n\nYou’ll have to update the repository configuration to install CrateDB newer than 5.5.1.\n\nThis new repository keeps old CrateDB versions in the Package index and also contains packages for ARM64."
  },
  {
    "title": "Version 5.4.8 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.4.8.html",
    "html": "5.6\nVersion 5.4.8\n\nReleased on 2024-01-29.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.4.8.\n\nWe recommend that you upgrade to the latest 5.3 release before moving to 5.4.8.\n\nA rolling upgrade from 5.3.x to 5.4.8 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of Contents\n\nSecurity Fixes\n\nSee the Version 5.4.0 release notes for a full list of changes in the 5.4 series.\n\nSecurity Fixes\n\nFixed a security issue where any CrateDB user could read/import the content of any file on the host system, the CrateDB process user has read access to, by using the COPY FROM command with a file URI. This access is now restricted to the crate superuser only. See CVE-2024-24565 for more details. (Thanks to @Tu0Laj1 for reporting this issue)"
  },
  {
    "title": "Version 5.5.5 - Unreleased — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.5.5.html",
    "html": "5.6\nVersion 5.5.5 - Unreleased\n\nNote\n\nIn development. 5.5.5 isn’t released yet. These are the release notes for the upcoming release.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.5.5.\n\nWe recommend that you upgrade to the latest 5.4 release before moving to 5.5.5.\n\nA rolling upgrade from 5.4.x to 5.5.5 is supported.\n\nBefore upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of contents\n\nFixes\n\nSee the Version 5.5.0 release notes for a full list of changes in the 5.5 series.\n\nFixes\n\nNone"
  },
  {
    "title": "Version 5.6.1 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.6.1.html",
    "html": "5.6\nVersion 5.6.1\n\nReleased on 2024-01-29.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.6.1.\n\nWe recommend that you upgrade to the latest 5.5 release before moving to 5.6.1.\n\nA rolling upgrade from 5.5.x to 5.6.1 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of contents\n\nSecurity Fixes\n\nFixes\n\nSee the Version 5.6.0 release notes for a full list of changes in the 5.6 series.\n\nSecurity Fixes\n\nFixed a security issue where any CrateDB user could read/import the content of any file on the host system, the CrateDB process user has read access to, by using the COPY FROM command with a file URI. This access is now restricted to the crate superuser only. See CVE-2024-24565 for more details. (Thanks to @Tu0Laj1 for reporting this issue)\n\nFixes\n\nAdded a workaround for a change in JDK 21.0.2 which caused many operations to get stuck.\n\nFixed an issue that led to errors when privileges are defined for users, when performing a rolling upgrade of a cluster from a version before Version 5.6.0 to Version 5.6.0.\n\nFixed an issue that caused SELECT statements with WHERE clause having an equality condition on a primary key to return NULL when selecting an object sub-column of ARRAY(OBJECT) type.\n\nFixed an issue that caused failure of a statement, mixing correlated subquery and sub-select. An example:\n\nCREATE TABLE tbl(x INT);\nINSERT INTO tbl(x) VALUES (1);\nSELECT (\n   SELECT x FROM tbl\n      WHERE t.x = tbl.x\n    AND\n      tbl.x IN (SELECT generate_series from generate_series(1, 1))\n) FROM tbl t;\n"
  },
  {
    "title": "Resiliency — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/concepts/resiliency.html",
    "html": "5.6\nResiliency\n\nDistributed systems are tricky. All sorts of things can go wrong that are beyond your control. The network can go away, disks can fail, hosts can be terminated unexpectedly. CrateDB tries very hard to cope with these sorts of issues while maintaining availability, consistency, and durability.\n\nHowever, as with any distributed system, sometimes, rarely, things can go wrong.\n\nThankfully, for most use-cases, if you follow best practices, you are extremely unlikely to experience resiliency issues with CrateDB.\n\nSee Also\n\nAppendix: Resiliency Issues\n\nTable of contents\n\nMonitoring cluster status\n\nStorage and consistency\n\nDeployment strategies\n\nMonitoring cluster status\n\nThe Admin UI in CrateDB has a status indicator which can be used to determine the stability and health of a cluster.\n\nA green status indicates that all shards have been replicated, are available, and are not being relocated. This is the lowest risk status for a cluster. The status will turn yellow when there is an elevated risk of encountering issues, due to a network failure or the failure of a node in the cluster.\n\nThe status is updated every few seconds (variable on your cluster ping configuration).\n\nStorage and consistency\n\nCode that expects the behavior of an ACID compliant database like MySQL may not always work as expected with CrateDB.\n\nCrateDB does not support ACID transactions, but instead has atomic operations and eventual consistency at the row level. See also Clustering.\n\nEventual consistency is the trade-off that CrateDB makes in exchange for high-availability that can tolerate most hardware and network failures. So you may observe data from different cluster nodes temporarily falling very briefly out-of-sync with each other, although over time they will become consistent.\n\nFor example, you know a row has been written as soon as you get the INSERT OK message. But that row might not be read back by a subsequent SELECT on a different node until after a table refresh (which typically occurs within one second).\n\nYour applications should be designed to work this storage and consistency model.\n\nDeployment strategies\n\nWhen deploying CrateDB you should carefully weigh your need for high-availability and disaster recovery against operational complexity and expense.\n\nWhich strategy you pick is going to depend on the specifics of your situation.\n\nHere are some considerations:\n\nCrateDB is designed to scale horizontally. Make sure that your machines are fit for purpose, i.e. use SSDs, increase RAM up to 64 GB, and use multiple CPU cores when you can. But if you want to dynamically increase (or decrease) the capacity of your cluster, add (or remove) nodes.\n\nIf availability is a concern, you can add nodes across multiple zones (e.g. different data centers or geographical regions). The more available your CrateDB cluster is, the more likely it is to withstand external failures like a zone going down.\n\nIf data durability or read performance is a concern, you can increase the number of table replicas. More table replicas means a smaller chance of permanent data loss due to hardware failures, in exchange for the use of more disk space and more intra-cluster network traffic.\n\nIf disaster recovery is important, you can take regular snapshots and store those snapshots in cold storage. This safeguards data that has already been successfully written and replicated across the cluster.\n\nCrateDB works well as part of a data pipeline, especially if you’re working with high-volume data. If you have a message queue in front of CrateDB, you can configure it with backups and replay the data flow for a specific timeframe. This can be used to recover from issues that affect your data before it has been successfully written and replicated across the cluster.\n\nIndeed, this is the generally recommended way to recover from any of the rare consistency or data-loss issues you might encounter when CrateDB experiences network or hardware failures (see next section)."
  },
  {
    "title": "Data definition — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/ddl/index.html",
    "html": "5.6\nData definition\n\nThis section provides an overview of how to create tables and perform other data-definition related operations with CrateDB.\n\nSee Also\n\nGeneral use: Data manipulation\n\nGeneral use: Querying\n\nTable of contents\n\nCreating tables\nTable definition\nTable configuration\nData types\nOverview\nPrimitive types\nContainer types\nFLOAT_VECTOR\nGeographic types\nType casting\nPostgreSQL compatibility\nSystem columns\nGenerated columns\nGeneration expressions\nLast modified dates\nPartitioning\nConstraints\nPRIMARY KEY\nNULL\nNOT NULL\nCHECK\nStorage\nColumn store\nPartitioned tables\nIntroduction\nCreation\nInformation schema\nInsert\nUpdate\nDelete\nQuerying\nAlter\nLimitations\nConsistency notes related to concurrent DML statement\nSharding\nIntroduction\nNumber of shards\nRouting\nReplication\nTable configuration\nShard recovery\nUnderreplication\nShard allocation filtering\nSettings\nSpecial attributes\nColumn policy\nstrict\ndynamic\nFulltext indices\nIndex definition\nDisable indexing\nPlain index (default)\nCreating a custom analyzer\nExtending a built-in analyzer\nFulltext analyzers\nOverview\nBuilt-in analyzers\nBuilt-in tokenizers\nBuilt-in token filters\nBuilt-in char filter\nShow Create Table\nViews\nCreating views\nQuerying views\nDropping views\nAltering tables\nUpdating parameters\nAdding columns\nRenaming columns\nClosing and opening tables\nRenaming tables\nReroute shards"
  },
  {
    "title": "Built-in functions and operators — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/builtins/index.html",
    "html": "5.6\nBuilt-in functions and operators\n\nThis chapter provides an overview of built-in functions and operators.\n\nTable of contents\n\nScalar functions\nString functions\nDate and time functions\nGeo functions\nMathematical functions\nRegular expression functions\nArray functions\nObject functions\nConditional functions and expressions\nSystem information functions\nSpecial functions\nAggregation\nAggregate expressions\nAggregate functions\nLimitations\nArithmetic operators\nBit operators\nTable functions\nScalar functions\nempty_row( )\nunnest( array [ array , ] )\npg_catalog.generate_series(start, stop, [step])\npg_catalog.generate_subscripts(array, dim, [reverse])\nregexp_matches(source, pattern [, flags])\npg_catalog.pg_get_keywords()\ninformation_schema._pg_expandarray(array)\nComparison operators\nBasic operators\nWHERE clause operators\nArray comparisons\nIN (value [, ...])\nANY/SOME (array expression)\nALL (array_expression)\nSubquery expressions\nIN (subquery)\nANY/SOME (subquery)\nALL (subquery)\nWindow functions\nWindow function call\nWindow definition\nGeneral-purpose window functions\nAggregate window functions"
  },
  {
    "title": "Data manipulation — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/dml.html",
    "html": "5.6\nData manipulation\n\nThis section provides an overview of how to manipulate data (e.g., inserting rows) with CrateDB.\n\nSee Also\n\nGeneral use: Data definition\n\nGeneral use: Querying\n\nTable of contents\n\nInserting data\n\nInserting data by query\n\nUpserts (ON CONFLICT DO UPDATE SET)\n\nUpdating data\n\nDeleting data\n\nImport and export\n\nImporting data\n\nExample\n\nDetailed error reporting\n\nExporting data\n\nInserting data\n\nInserting data to CrateDB is done by using the SQL INSERT statement.\n\nNote\n\nThe column list is always ordered based on the column position in the CREATE TABLE statement of the table. If the insert columns are omitted, the values in the VALUES clauses must correspond to the table columns in that order.\n\nInserting a row:\n\ncr> insert into locations (id, date, description, kind, name, position)\n... values (\n...   '14',\n...   '2013-09-12T21:43:59.000Z',\n...   'Blagulon Kappa is the planet to which the police are native.',\n...   'Planet',\n...   'Blagulon Kappa',\n...   7\n... );\nINSERT OK, 1 row affected (... sec)\n\n\nWhen inserting rows with the VALUES clause all data is validated in terms of data types compatibility and compliance with defined constraints, and if there are any issues an error message is returned and no rows are inserted.\n\nInserting multiple rows at once (aka. bulk insert) can be done by defining multiple values for the INSERT statement:\n\ncr> insert into locations (id, date, description, kind, name, position) values\n... (\n...   '16',\n...   '2013-09-14T21:43:59.000Z',\n...   'Blagulon Kappa II is the planet to which the police are native.',\n...   'Planet',\n...   'Blagulon Kappa II',\n...   19\n... ),\n... (\n...   '17',\n...   '2013-09-13T16:43:59.000Z',\n...   'Brontitall is a planet with a warm, rich atmosphere and no mountains.',\n...   'Planet',\n...   'Brontitall',\n...   10\n... );\nINSERT OK, 2 rows affected (... sec)\n\n\nWhen inserting into tables containing Generated columns or Base Columns having the Default clause specified, their values can be safely omitted. They are generated upon insert:\n\ncr> CREATE TABLE debit_card (\n...   owner text,\n...   num_part1 integer,\n...   num_part2 integer,\n...   check_sum integer GENERATED ALWAYS AS ((num_part1 + num_part2) * 42),\n...   \"user\" text DEFAULT 'crate'\n... );\nCREATE OK, 1 row affected (... sec)\n\ncr> insert into debit_card (owner, num_part1, num_part2) values\n... ('Zaphod Beeblebrox', 1234, 5678);\nINSERT OK, 1 row affected (... sec)\n\ncr> select * from debit_card;\n+-------------------+-----------+-----------+-----------+-------+\n| owner             | num_part1 | num_part2 | check_sum | user  |\n+-------------------+-----------+-----------+-----------+-------+\n| Zaphod Beeblebrox |      1234 |      5678 |    290304 | crate |\n+-------------------+-----------+-----------+-----------+-------+\nSELECT 1 row in set (... sec)\n\n\nFor Generated columns, if the value is given, it is validated against the generation clause of the column and the currently inserted row:\n\ncr> insert into debit_card (owner, num_part1, num_part2, check_sum) values\n... ('Arthur Dent', 9876, 5432, 642935);\nSQLParseException[Given value 642935 for generated column check_sum does not match calculation ((num_part1 + num_part2) * 42) = 642936]\n\nInserting data by query\n\nIt is possible to insert data using a query instead of values. Column data types of source and target table can differ as long as the values are castable. This gives the opportunity to restructure the tables data, renaming a field, changing a field’s data type or convert a normal table into a partitioned one.\n\nCaution\n\nWhen inserting data from a query, there is no error message returned when rows fail to be inserted, they are instead skipped, and the number of rows affected is decreased to reflect the actual number of rows for which the operation succeeded.\n\nExample of changing a field’s data type, in this case, changing the position data type from integer to smallint:\n\ncr> create table locations2 (\n...     id text primary key,\n...     name text,\n...     date timestamp with time zone,\n...     kind text,\n...     position smallint,\n...     description text\n... ) clustered by (id) into 2 shards with (number_of_replicas = 0);\nCREATE OK, 1 row affected (... sec)\n\ncr> insert into locations2 (id, name, date, kind, position, description)\n... (\n...     select id, name, date, kind, position, description\n...     from locations\n...     where position < 10\n... );\nINSERT OK, 14 rows affected (... sec)\n\n\nExample of creating a new partitioned table out of the locations table with data partitioned by year:\n\ncr> create table locations_parted (\n...     id text primary key,\n...     name text,\n...     year text primary key,\n...     date timestamp with time zone,\n...     kind text,\n...     position integer\n... ) clustered by (id) into 2 shards\n... partitioned by (year) with (number_of_replicas = 0);\nCREATE OK, 1 row affected (... sec)\n\ncr> insert into locations_parted (id, name, year, date, kind, position)\n... (\n...     select\n...         id,\n...         name,\n...         date_format('%Y', date),\n...         date,\n...         kind,\n...         position\n...     from locations\n... );\nINSERT OK, 16 rows affected (... sec)\n\n\nResulting partitions of the last insert by query:\n\ncr> select table_name, partition_ident, values, number_of_shards, number_of_replicas\n... from information_schema.table_partitions\n... where table_name = 'locations_parted'\n... order by partition_ident;\n+------------------+-----------------+------------------+------------------+--------------------+\n| table_name       | partition_ident | values           | number_of_shards | number_of_replicas |\n+------------------+-----------------+------------------+------------------+--------------------+\n| locations_parted | 042j2e9n74      | {\"year\": \"1979\"} |                2 |                  0 |\n| locations_parted | 042j4c1h6c      | {\"year\": \"2013\"} |                2 |                  0 |\n+------------------+-----------------+------------------+------------------+--------------------+\nSELECT 2 rows in set (... sec)\n\n\nNote\n\nlimit, offset and order by are not supported inside the query statement.\n\nUpserts (ON CONFLICT DO UPDATE SET)\n\nThe ON CONFLICT DO UPDATE SET clause is used to update the existing row if inserting is not possible because of a duplicate-key conflict if a document with the same PRIMARY KEY already exists. This is type of operation is commonly referred to as an upsert, being a combination of “update” and “insert”.\n\ncr> SELECT\n...     name,\n...     visits,\n...     extract(year from last_visit) AS last_visit\n... FROM uservisits ORDER BY NAME;\n+----------+--------+------------+\n| name     | visits | last_visit |\n+----------+--------+------------+\n| Ford     |      1 | 2013       |\n| Trillian |      3 | 2013       |\n+----------+--------+------------+\nSELECT 2 rows in set (... sec)\n\ncr> INSERT INTO uservisits (id, name, visits, last_visit) VALUES\n... (\n...     0,\n...     'Ford',\n...     1,\n...     '2015-01-12'\n... ) ON CONFLICT (id) DO UPDATE SET\n...     visits = visits + 1;\nINSERT OK, 1 row affected (... sec)\n\ncr> SELECT\n...     name,\n...     visits,\n...     extract(year from last_visit) AS last_visit\n... FROM uservisits WHERE id = 0;\n+------+--------+------------+\n| name | visits | last_visit |\n+------+--------+------------+\n| Ford |      2 | 2013       |\n+------+--------+------------+\nSELECT 1 row in set (... sec)\n\n\nIt’s possible to refer to values which would be inserted if no duplicate-key conflict occurred, by using the special excluded table. This table is especially useful in multiple-row inserts, to refer to the current rows values:\n\ncr> INSERT INTO uservisits (id, name, visits, last_visit) VALUES\n... (\n...     0,\n...     'Ford',\n...     2,\n...     '2016-01-13'\n... ),\n... (\n...     1,\n...     'Trillian',\n...     5,\n...     '2016-01-15'\n... ) ON CONFLICT (id) DO UPDATE SET\n...     visits = visits + excluded.visits,\n...     last_visit = excluded.last_visit;\nINSERT OK, 2 rows affected (... sec)\n\ncr> SELECT\n...     name,\n...     visits,\n...     extract(year from last_visit) AS last_visit\n... FROM uservisits ORDER BY name;\n+----------+--------+------------+\n| name     | visits | last_visit |\n+----------+--------+------------+\n| Ford     |      4 | 2016       |\n| Trillian |      8 | 2016       |\n+----------+--------+------------+\nSELECT 2 rows in set (... sec)\n\n\nThis can also be done when using a query instead of values:\n\ncr> CREATE TABLE uservisits2 (\n...   id integer primary key,\n...   name text,\n...   visits integer,\n...   last_visit timestamp with time zone\n... ) CLUSTERED BY (id) INTO 2 SHARDS WITH (number_of_replicas = 0);\nCREATE OK, 1 row affected (... sec)\n\ncr> INSERT INTO uservisits2 (id, name, visits, last_visit)\n... (\n...     SELECT id, name, visits, last_visit\n...     FROM uservisits\n... );\nINSERT OK, 2 rows affected (... sec)\n\ncr> INSERT INTO uservisits2 (id, name, visits, last_visit)\n... (\n...     SELECT id, name, visits, last_visit\n...     FROM uservisits\n... ) ON CONFLICT (id) DO UPDATE SET\n...     visits = visits + excluded.visits,\n...     last_visit = excluded.last_visit;\nINSERT OK, 2 rows affected (... sec)\n\ncr> SELECT\n...     name,\n...     visits,\n...     extract(year from last_visit) AS last_visit\n... FROM uservisits ORDER BY name;\n+----------+--------+------------+\n| name     | visits | last_visit |\n+----------+--------+------------+\n| Ford     |      4 | 2016       |\n| Trillian |      8 | 2016       |\n+----------+--------+------------+\nSELECT 2 rows in set (... sec)\n\n\nSee Also\n\nSQL syntax: ON CONFLICT DO UPDATE SET\n\nUpdating data\n\nIn order to update documents in CrateDB the SQL UPDATE statement can be used:\n\ncr> update locations set description = 'Updated description'\n... where name = 'Bartledan';\nUPDATE OK, 1 row affected (... sec)\n\n\nUpdating nested objects is also supported:\n\ncr> update locations set inhabitants['name'] = 'Human' where name = 'Bartledan';\nUPDATE OK, 1 row affected (... sec)\n\n\nIt’s also possible to reference a column within the expression, for example to increment a number like this:\n\ncr> update locations set position = position + 1 where position < 3;\nUPDATE OK, 6 rows affected (... sec)\n\n\nNote\n\nIf the same documents are updated concurrently an VersionConflictException might occur. CrateDB contains a retry logic that tries to resolve the conflict automatically.\n\nDeleting data\n\nDeleting rows in CrateDB is done using the SQL DELETE statement:\n\ncr> delete from locations where position > 3;\nDELETE OK, ... rows affected (... sec)\n\nImport and export\nImporting data\n\nUsing the COPY FROM statement, CrateDB nodes can import data from local files or files that are available over the network.\n\nThe supported data formats are JSON and CSV. The format is inferred from the file extension, if possible. Alternatively the format can also be provided as an option (see WITH). If the format is not provided and cannot be inferred from the file extension, it will be processed as JSON.\n\nJSON files must contain a single JSON object per line.\n\nExample JSON data:\n\n{\"id\": 1, \"quote\": \"Don't panic\"}\n{\"id\": 2, \"quote\": \"Ford, you're turning into a penguin. Stop it.\"}\n\n\nCSV files must contain a header with comma-separated values, which will be added as columns.\n\nExample CSV data:\n\nid,quote\n1,\"Don't panic\"\n2,\"Ford, you're turning into a penguin. Stop it.\"\n\n\nNote\n\nThe COPY FROM statement will convert and validate your data.\n\nValues for generated columns will be computed if the data does not contain them, otherwise they will be imported and validated\n\nFurthermore, column names in your data are considered case sensitive (as if they were quoted in a SQL statement).\n\nFor further information, including how to import data to Partitioned tables, take a look at the COPY FROM reference.\n\nExample\n\nHere’s an example statement:\n\ncr> COPY quotes FROM 'file:///tmp/import_data/quotes.json';\nCOPY OK, 3 rows affected (... sec)\n\n\nThis statement imports data from the /tmp/import_data/quotes.json file into a table named quotes.\n\nNote\n\nThe file you specify must be available on one of the CrateDB nodes. This statement will not work with files that are local to your client.\n\nFor the above statement, every node in the cluster will attempt to import data from a file located at /tmp/import_data/quotes.json relative to the crate process (i.e., if you are running CrateDB inside a container, the file must also be inside the container).\n\nIf you want to import data from a file that on your local computer using COPY FROM, you must first transfer the file to one of the CrateDB nodes.\n\nConsult the COPY FROM reference for additional information.\n\nIf you want to import all files inside the /tmp/import_data directory on every CrateDB node, you can use a wildcard, like so:\n\ncr> COPY quotes FROM '/tmp/import_data/*' WITH (bulk_size = 4);\nCOPY OK, 3 rows affected (... sec)\n\n\nThis wildcard can also be used to only match certain files in a directory:\n\ncr> COPY quotes FROM '/tmp/import_data/qu*.json';\nCOPY OK, 3 rows affected (... sec)\n\nDetailed error reporting\n\nIf the RETURN_SUMMARY clause is specified, a result set containing information about failures and successfully imported records is returned.\n\ncr> COPY locations FROM '/tmp/import_data/locations_with_failure/locations*.json' RETURN SUMMARY;\n +--...--+----------...--------+---------------+-------------+--------------------...-------------------------------------+\n | node  | uri                 | success_count | error_count | errors                                                     |\n +--...--+----------...--------+---------------+-------------+--------------------...-------------------------------------+\n | {...} | .../locations1.json |             6 |           0 | {}                                                         |\n | {...} | .../locations2.json |             5 |           2 | {\"Cannot cast value...{\"count\": ..., \"line_numbers\": ...}} |\n +--...--+----------...--------+---------------+-------------+--------------------...-------------------------------------+\n COPY 2 rows in set (... sec)\n\n\nIf an error happens while processing the URI in general, the error_count and success_count columns will contains NULL values to indicate that no records were processed.\n\ncr> COPY locations FROM '/tmp/import_data/not-existing.json' RETURN SUMMARY;\n +--...--+-----------...---------+---------------+-------------+------------------------...------------------------+\n | node  | uri                   | success_count | error_count | errors                                            |\n +--...--+-----------...---------+---------------+-------------+------------------------...------------------------+\n | {...} | .../not-existing.json |          NULL |        NULL | {\"...not-existing.json (...)\": {\"count\": 1, ...}} |\n +--...--+-----------...---------+---------------+-------------+------------------------...------------------------+\nCOPY 1 row in set (... sec)\n\n\nSee COPY FROM for more information.\n\nExporting data\n\nData can be exported using the COPY TO statement. Data is exported in a distributed way, meaning each node will export its own data.\n\nReplicated data is not exported. So every row of an exported table is stored only once.\n\nThis example shows how to export a given table into files named after the table and shard ID with gzip compression:\n\ncr> REFRESH TABLE quotes;\nREFRESH OK...\n\ncr> COPY quotes TO DIRECTORY '/tmp/' with (compression='gzip');\nCOPY OK, 3 rows affected ...\n\n\nInstead of exporting a whole table, rows can be filtered by an optional WHERE clause condition. This is useful if only a subset of the data needs to be exported:\n\ncr> COPY quotes WHERE match(quote_ft, 'time') TO DIRECTORY '/tmp/' WITH (compression='gzip');\nCOPY OK, 2 rows affected ...\n\n\nFor further details see COPY TO."
  },
  {
    "title": "Version 5.5.4 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.5.4.html",
    "html": "5.6\nVersion 5.5.4\n\nReleased on 2024-01-29.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.5.4.\n\nWe recommend that you upgrade to the latest 5.4 release before moving to 5.5.4.\n\nA rolling upgrade from 5.4.x to 5.5.4 is supported.\n\nBefore upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of contents\n\nSecurity Fixes\n\nFixes\n\nSee the Version 5.5.0 release notes for a full list of changes in the 5.5 series.\n\nSecurity Fixes\n\nFixed a security issue where any CrateDB user could read/import the content of any file on the host system, the CrateDB process user has read access to, by using the COPY FROM command with a file URI. This access is now restricted to the crate superuser only. See CVE-2024-24565 for more details. (Thanks to @Tu0Laj1 for reporting this issue)\n\nFixes\n\nFixed an issue that caused SELECT statements with WHERE clause having an equality condition on a primary key to return NULL when selecting an object sub-column of ARRAY(OBJECT) type.\n\nFixed an issue that caused failure of a statement, mixing correlated subquery and sub-select. An example:\n\nCREATE TABLE tbl(x INT);\nINSERT INTO tbl(x) VALUES (1);\nSELECT (\n   SELECT x FROM tbl\n      WHERE t.x = tbl.x\n    AND\n      tbl.x IN (SELECT generate_series from generate_series(1, 1))\n) FROM tbl t;\n"
  },
  {
    "title": "Version 5.6.0 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.6.0.html",
    "html": "5.6\nVersion 5.6.0\n\nReleased on 2024-01-22.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.6.0.\n\nWe recommend that you upgrade to the latest 5.5 release before moving to 5.6.0.\n\nA rolling upgrade from 5.5.x to 5.6.0 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of contents\n\nBreaking Changes\n\nDeprecations\n\nChanges\n\nSQL Statements\n\nSQL Standard and PostgreSQL Compatibility\n\nData Types\n\nScalar and Aggregation Functions\n\nPerformance and Resilience Improvements\n\nAdministration and Operations\n\nUser Interface\n\nBreaking Changes\n\nWhen restoring a snapshot, USERS and PRIVILEGES keywords, used to restore user management metadata has been replaced by USERMANAGEMENT, which dictates that all users and roles of the database together with their privileges are restored. Restoring USERS or PRIVILEGES separately is not possible anymore.\n\nDeprecations\n\nUSERS and PRIVILEGES keywords, used when restoring a snapshot, in order to restore users and privileges metadata respectively, have been deprecated. They have been replaced by USERMANAGEMENT and their behavior has been modified, please see Breaking Changes for details.\n\nChanges\nSQL Statements\n\nAdded support for explicit NULL column constraint definitions in CREATE TABLE statements.\n\nAdded support for named PRIMARY KEY constraint declaration.\n\nExtended the EXPLAIN statement to support the VERBOSE option.\n\nAdded support for ALTER TABLE RENAME COLUMN statement.\n\nAdded support for CREATE ROLE statement. For details see Administration and Operations.\n\nAdded support for ALTER ROLE statement, which is identical to ALTER USER statement.\n\nAdded support for DROP ROLE statement, which is identical to DROP USER statement.\n\nSQL Standard and PostgreSQL Compatibility\n\nAdded a unknown type for serialization via the PostgreSQL wire protocol and to the pg_catalog.pg_type table. This should resolve compatibility issues with npgsql >= 8.0.\n\nAdded an empty pg_catalog.pg_depend table.\n\nChanged pg_catalog.pg_roles table to be properly populated, as previously it was always returning 0 rows.\n\nAdded support of optional ESCAPE parameter to LIKE and ILIKE operators.\n\nData Types\n\nIntroduced the BKD-tree-based indexing strategy for geo_shape.\n\nScalar and Aggregation Functions\n\nUpdated the tdigest library which results in the percentile aggregation function to behave differently in some cases. For example, the following query used to return 4.5 but will now return 5.0:\n\nSELECT percentile(x, 0.5) FROM generate_series(0, 9, 1) AS t (x) ;\n\nPerformance and Resilience Improvements\n\nReduced the amount of disk reads necessary for ANALYZE operations.\n\nImproved filter push-down for left/right outer joins when the joins are nested e.g.:\n\nSELECT * FROM (SELECT * FROM a LEFT JOIN b ON a.a = b.b LEFT JOIN c ON b.b = c.c) t WHERE b > 1;\n\n\nNow, the above query will result in the following logical plan\n\nNestedLoopJoin[LEFT | (b = c)] (rows=unknown)\n  ├ HashJoin[(a = b)] (rows=unknown)\n  │  ├ Collect[doc.a | [a] | true] (rows=unknown)\n  │  └ Collect[doc.b | [b] | (b > 1)] (rows=unknown)\n  └ Collect[doc.c | [c] | true] (rows=unknown)\n\nAdministration and Operations\n\nAdded database roles, which can be used to group privileges, and can be granted to users or other roles, thus enabling privileges inheritance.\n\nAllowed un-indexed columns or columns without doc-values to be queryable.\n\nAdded the new options schema_rename_pattern, schema_rename_replacement, table_rename_pattern and table_rename_replacement to RESTORE SNAPSHOT to allow renaming tables during restore.\n\nAdded sys.roles table which contains all database roles defined in the cluster.\n\nAdded granted_roles column to sys.users table which lists the roles granted to a user, together with the user that granted each role.\n\nUser Interface\n\nUpdated to Admin UI 1.24.7, which fixed a minor grammar issue on the Spanish version of the Help page, and optimized images with oxipng."
  },
  {
    "title": "Version 5.6.3 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.6.3.html",
    "html": "5.6\nVersion 5.6.3\n\nReleased on 2023-03-22.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.6.3.\n\nWe recommend that you upgrade to the latest 5.5 release before moving to 5.6.3.\n\nA rolling upgrade from 5.5.x to 5.6.3 is supported.\n\nBefore upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of contents\n\nFixes\n\nSee the Version 5.6.0 release notes for a full list of changes in the 5.6 series.\n\nFixes\n\nFixed an issue in filter-push-down for joins which prevented constant join conditions to be pushed-down in hash-joins.\n\nFixed an issue that could cause SELECT statements on virtual tables or views and a WHERE clause to run with an expensive execution plan and not utilize the table’s indices due to the merge_filter_and_collect optimizer rule not being applied.\n\nFixed an issue that could cause SELECT statements to ignore a WHERE clause if it involved views or virtual tables, and unused columns containing a window function.\n\nFixed an issue that could cause SHOW CREATE TABLE on a missing table to mention similarly named tables, despite the user not having any permissions on those tables.\n\nFixed an issue that caused wrong results to be returned for queries for which the WHERE clause includes a PK column and a non-PK column under an OR operator, e.g.:\n\nSELECT * FROM tbl WHERE pk_col = 1 OR other_col = 'foo'\n\n\nFixed several issues with PostgreSQL style INTERVALS:\n\nDuplicate definitions now raise an error:\n\nSELECT '1 year 2 years'::INTERVAL\n\n\nUnits next to values are now supported without whitespace separation:\n\nSELECT '1year 3day'::INTERVAL\n\n\nWeeks were ignored if days were also present in the string:\n\nSELECT '2 weeks 3 days'::INTERVAL\n\n\nMade the parsing of the interval units strict. Before using units like yearrr was allowed, now it raises an error.\n\nMade the interval units case-insensitive. The following used to raise an error:\n\nSELECT '2 WEEKS'::INTERVAL\n\n\nFixed an issue that caused CrateDB to fail to notify client applications connecting via PostgreSQL Wire Protocol that standard_conforming_strings is set to on which caused the clients to treat all query strings as non standard conforming.\n\nFixed NullPointerException thrown when joining tables with USING clause which contains columns not existing in either or both tables.\n\nFixed an issue that caused an inner join with a WHERE clause containing a CASE expression to return invalid results.\n\nFixed an issue that caused an inner join query to throw an UnsupportedFeatureException when the join condition contains columns from more than one table and the WHERE clause contains an expression better suited as the join condition e.g.:\n\nSELECT * FROM t1 INNER JOIN t2 ON (t1.a = t2.b + t1.a) WHERE t1.a = t2.b + 1\n"
  },
  {
    "title": "Environment variables — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/config/environment.html",
    "html": "5.6\nEnvironment variables\n\nCrateDB can be configured with some environment variables.\n\nThere are many different ways to set environment variables, depending on how CrateDB is being deployed.\n\nHere is a trivial example:\n\nsh$ export CRATE_HOME=/tmp/crate\nsh$ ./bin/crate\n\n\nHere, we set CRATE_HOME to /tmp/crate, export it so that sub-processes of the shell have access, and then start CrateDB.\n\nCrateDB supports two kinds of environment variables:\n\nApplication variables\n\nJava Virtual Machine (JVM) variables\n\nWe use “application” here to distinguish between CrateDB running as a Java application and the JVM itself, which runs CrateDB.\n\nTable of contents\n\nApplication variables\n\nJVM variables\n\nGeneral\n\nApplication variables\nCRATE_HOME: directory path\n\nThe home directory of the CrateDB installation.\n\nThis directory is used as the root for the configuration directory, data directory, log directory, and so on.\n\nIf you have installed CrateDB from a package, this variable should be set for you.\n\nIf you are installing manually, in most cases, this should be set to the directory from which you would normally execute bin/crate, i.e. the root directory of the basic installation.\n\nJVM variables\nGeneral\nCRATE_JAVA_OPTS: Java options\n\nThe Java options to use when running CrateDB.\n\nFor example, you could change the stack size like this:\n\nCRATE_JAVA_OPTS=-Xss500k\n\n\nSee Also\n\nFor more information about Java options, consult the documentation for Microsoft Windows or Unix-like operating systems.\n\nCRATE_HEAP_SIZE: size\n\nThe Java heap size, i.e. the amount of memory that can be used.\n\nYou can set the heap size to four gigabytes like this:\n\nCRATE_HEAP_SIZE=4g\n\n\nUse g for gigabytes or m for megabytes.\n\nSee Also\n\nAppropriate memory configuration is important for optimal performance.\n\nCRATE_HEAP_DUMP_PATH: file or directory path (default: varies)\n\nThe directory to be used for heap dumps in the case of a crash.\n\nIf a directory path is configured, new heap dumps will be written to that directory every time CrateDB crashes.\n\nIf a file path is configured (i.e. the last node of the path is non-existent or exists and is a file) CrateDB will overwrite that file with a heap dump every time it crashes.\n\nDefault values are as follows:\n\nFor a basic installation, the process working directory\n\nIf you have installed a CrateDB Linux package, /var/lib/crate\n\nWhen running CrateDB on Docker, /data/data\n\nWarning\n\nMake sure there is enough disk space available for heap dumps."
  },
  {
    "title": "Node-specific settings — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/config/node.html",
    "html": "5.6\nNode-specific settings\n\nTable of contents\n\nBasics\n\nNode types\n\nGeneral\n\nNetworking\n\nHosts\n\nPorts\n\nAdvanced TCP settings\n\nTransport settings\n\nPaths\n\nPlug-ins\n\nCPU\n\nMemory\n\nGarbage collection\n\nAuthentication\n\nTrust authentication\n\nHost-based authentication\n\nHBA entries\n\nSecured communications (SSL/TLS)\n\nCross-origin resource sharing (CORS)\n\nBlobs\n\nRepositories\n\nQueries\n\nLegacy\n\nJavaScript language\n\nCustom attributes\n\nBasics\ncluster.name\nDefault: crate\nRuntime: no\n\nThe name of the CrateDB cluster the node should join to.\n\nnode.name\nRuntime: no\n\nThe name of the node. If no name is configured a random one will be generated.\n\nNote\n\nNode names must be unique in a CrateDB cluster.\n\nnode.store.allow_mmap\nDefault: true\nRuntime: no\n\nThe setting indicates whether or not memory-mapping is allowed.\n\nNode types\n\nCrateDB supports different types of nodes.\n\nThe following settings can be used to differentiate nodes upon startup:\n\nnode.master\nDefault: true\nRuntime: no\n\nWhether or not this node is able to get elected as master node in the cluster.\n\nnode.data\nDefault: true\nRuntime: no\n\nWhether or not this node will store data.\n\nUsing different combinations of these two settings, you can create four different types of node. Each type of node is differentiated by what types of load it will handle.\n\nTabulating the truth values for node.master and node.data produces a truth table outlining the four different types of node:\n\n\t\n\nMaster\n\n\t\n\nNo master\n\n\n\n\nData\n\n\t\n\nHandle all loads.\n\n\t\n\nHandles client requests and query execution.\n\n\n\n\nNo data\n\n\t\n\nHandles cluster management.\n\n\t\n\nHandles client requests.\n\nNodes marked as node.master will only handle cluster management if they are elected as the cluster master. All other loads are shared equally.\n\nGeneral\nnode.sql.read_only\nDefault: false\nRuntime: no\n\nIf set to true, the node will only allow SQL statements which are resulting in read operations.\n\nstatement_timeout\nDefault: 0\nRuntime: yes\n\nThe maximum duration of any statement before it gets cancelled.\n\nThis value is used as default value for the statement_timeout session setting\n\nIf 0 queries are allowed to run infinitely and don’t get cancelled automatically.\n\nNote\n\nUpdating this setting won’t affect existing sessions, it will only take effect for new sessions.\n\nNetworking\nHosts\nnetwork.host\nDefault: _local_\nRuntime: no\n\nThe IP address CrateDB will bind itself to. This setting sets both the network.bind_host and network.publish_host values.\n\nnetwork.bind_host\nDefault: _local_\nRuntime: no\n\nThis setting determines to which address CrateDB should bind itself to.\n\nnetwork.publish_host\nDefault: _local_\nRuntime: no\n\nThis setting is used by a CrateDB node to publish its own address to the rest of the cluster.\n\nTip\n\nApart from IPv4 and IPv6 addresses there are some special values that can be used for all above settings:\n\n_local_\n\n\t\n\nAny loopback addresses on the system, for example 127.0.0.1.\n\n\n\n\n_site_\n\n\t\n\nAny site-local addresses on the system, for example 192.168.0.1.\n\n\n\n\n_global_\n\n\t\n\nAny globally-scoped addresses on the system, for example 8.8.8.8.\n\n\n\n\n_[INTERFACE]_\n\n\t\n\nAddresses of a network interface, for example _en0_.\n\nPorts\nhttp.port\nRuntime: no\n\nThis defines the TCP port range to which the CrateDB HTTP service will be bound to. It defaults to 4200-4300. Always the first free port in this range is used. If this is set to an integer value it is considered as an explicit single port.\n\nThe HTTP protocol is used for the REST endpoint which is used by all clients except the Java client.\n\nhttp.publish_port\nRuntime: no\n\nThe port HTTP clients should use to communicate with the node. It is necessary to define this setting if the bound HTTP port (http.port) of the node is not directly reachable from outside, e.g. running it behind a firewall or inside a Docker container.\n\ntransport.tcp.port\nRuntime: no\n\nThis defines the TCP port range to which the CrateDB transport service will be bound to. It defaults to 4300-4400. Always the first free port in this range is used. If this is set to an integer value it is considered as an explicit single port.\n\nThe transport protocol is used for internal node-to-node communication.\n\ntransport.publish_port\nRuntime: no\n\nThe port that the node publishes to the cluster for its own discovery. It is necessary to define this setting when the bound tranport port (transport.tcp.port) of the node is not directly reachable from outside, e.g. running it behind a firewall or inside a Docker container.\n\npsql.port\nRuntime: no\n\nThis defines the TCP port range to which the CrateDB Postgres service will be bound to. It defaults to 5432-5532. Always the first free port in this range is used. If this is set to an integer value it is considered as an explicit single port.\n\nAdvanced TCP settings\n\nAny interface that uses TCP (Postgres wire, HTTP & Transport protocols) shares the following settings:\n\nnetwork.tcp.no_delay\nDefault: true\nRuntime: no\n\nEnable or disable the Nagle’s algorithm for buffering TCP packets. Buffering is disabled by default.\n\nnetwork.tcp.keep_alive\nDefault: true\nRuntime: no\n\nConfigures the SO_KEEPALIVE option for sockets, which determines whether they send TCP keepalive probes.\n\nnetwork.tcp.reuse_address\nDefault: true on non-windows machines and false otherwise\nRuntime: no\n\nConfigures the SO_REUSEADDRS option for sockets, which determines whether they should reuse the address.\n\nnetwork.tcp.send_buffer_size\nDefault: -1\nRuntime: no\n\nThe size of the TCP send buffer (SO_SNDBUF socket option). By default not explicitly set.\n\nnetwork.tcp.receive_buffer_size\nDefault: -1\nRuntime: no\n\nThe size of the TCP receive buffer (SO_RCVBUF socket option). By default not explicitly set.\n\nNote\n\nEach setting in this section has its counterpart for HTTP and transport. To provide a protocol specific setting, remove network prefix and use either http or transport instead. For example, no_delay can be configured as http.tcp.no_delay and transport.tcp.no_delay. Please note, that PG interface takes its settings from transport.\n\nTransport settings\ntransport.connect_timeout\nDefault: 30s\nRuntime: no\n\nThe connect timeout for initiating a new connection.\n\ntransport.compress\nDefault: false\nRuntime: no\n\nSet to true to enable compression (DEFLATE) between all nodes.\n\ntransport.ping_schedule\nDefault: -1\nRuntime: no\n\nSchedule a regular application-level ping message to ensure that transport connections between nodes are kept alive. Defaults to -1 (disabled). It is preferable to correctly configure TCP keep-alives instead of using this feature, because TCP keep-alives apply to all kinds of long-lived connections and not just to transport connections.\n\nPaths\n\nNote\n\nRelative paths are relative to CRATE_HOME. Absolute paths override this behavior.\n\npath.conf\nDefault: config\nRuntime: no\n\nFilesystem path to the directory containing the configuration files crate.yml and log4j2.properties.\n\npath.data\nDefault: data\nRuntime: no\n\nFilesystem path to the directory where this CrateDB node stores its data (table data and cluster metadata).\n\nMultiple paths can be set by using a comma separated list and each of these paths will hold full shards (instead of striping data across them). For example:\n\npath.data: /path/to/data1,/path/to/data2\n\n\nWhen CrateDB finds striped shards at the provided locations (from CrateDB <0.55.0), these shards will be migrated automatically on startup.\n\npath.logs\nDefault: logs\nRuntime: no\n\nFilesystem path to a directory where log files should be stored.\n\nCan be used as a variable inside log4j2.properties.\n\nFor example:\n\nappender:\n  file:\n    file: ${path.logs}/${cluster.name}.log\n\npath.repo\nRuntime: no\n\nA list of filesystem or UNC paths where repositories of type fs may be stored.\n\nWithout this setting a CrateDB user could write snapshot files to any directory that is writable by the CrateDB process. To safeguard against this security issue, the possible paths have to be whitelisted here.\n\nSee also location setting of repository type fs.\n\nSee Also\n\nblobs.path\n\nPlug-ins\nplugin.mandatory\nRuntime: no\n\nA list of plug-ins that are required for a node to startup.\n\nIf any plug-in listed here is missing, the CrateDB node will fail to start.\n\nCPU\nprocessors\nRuntime: no\n\nThe number of processors is used to set the size of the thread pools CrateDB is using appropriately. If not set explicitly, CrateDB will infer the number from the available processors on the system.\n\nIn environments where the CPU amount can be restricted (like Docker) or when multiple CrateDB instances are running on the same hardware, the inferred number might be too high. In such a case, it is recommended to set the value explicitly.\n\nMemory\nbootstrap.memory_lock\nDefault: false\nRuntime: no\n\nCrateDB performs poorly when the JVM starts swapping: you should ensure that it never swaps. If set to true, CrateDB will use the mlockall system call on startup to ensure that the memory pages of the CrateDB process are locked into RAM.\n\nGarbage collection\n\nCrateDB logs if JVM garbage collection on different memory pools takes too long. The following settings can be used to adjust these timeouts:\n\nmonitor.jvm.gc.collector.young.warn\nDefault: 1000ms\nRuntime: no\n\nCrateDB will log a warning message if it takes more than the configured timespan to collect the Eden Space (heap).\n\nmonitor.jvm.gc.collector.young.info\nDefault: 700ms\nRuntime: no\n\nCrateDB will log an info message if it takes more than the configured timespan to collect the Eden Space (heap).\n\nmonitor.jvm.gc.collector.young.debug\nDefault: 400ms\nRuntime: no\n\nCrateDB will log a debug message if it takes more than the configured timespan to collect the Eden Space (heap).\n\nmonitor.jvm.gc.collector.old.warn\nDefault: 10000ms\nRuntime: no\n\nCrateDB will log a warning message if it takes more than the configured timespan to collect the Old Gen / Tenured Gen (heap).\n\nmonitor.jvm.gc.collector.old.info\nDefault: 5000ms\nRuntime: no\n\nCrateDB will log an info message if it takes more than the configured timespan to collect the Old Gen / Tenured Gen (heap).\n\nmonitor.jvm.gc.collector.old.debug\nDefault: 2000ms\nRuntime: no\n\nCrateDB will log a debug message if it takes more than the configured timespan to collect the Old Gen / Tenured Gen (heap).\n\nAuthentication\nTrust authentication\nauth.trust.http_default_user\nDefault: crate\nRuntime: no\n\nThe default user that should be used for authentication when clients connect to CrateDB via HTTP protocol and they do not specify a user via the Authorization request header.\n\nauth.trust.http_support_x_real_ip\nDefault: false\nRuntime: no\n\nIf enabled, the HTTP transport will trust the X-Real-IP header sent by the client to determine the client’s IP address. This is useful when CrateDB is running behind a reverse proxy or load-balancer. For improved security, any _local_ IP address (127.0.0.1 and ::1) defined in this header will be ignored.\n\nWarning\n\nEnabling this setting can be a security risk, as it allows clients to impersonate other clients by sending a fake X-Real-IP header.\n\nHost-based authentication\n\nAuthentication settings (auth.host_based.*) are node settings, which means that their values apply only to the node where they are applied and different nodes may have different authentication settings.\n\nauth.host_based.enabled\nDefault: false\nRuntime: no\n\nSetting to enable or disable Host Based Authentication (HBA). It is disabled by default.\n\nHBA entries\n\nThe auth.host_based.config. setting is a group setting that can have zero, one or multiple groups that are defined by their group key (${order}) and their fields (user, address, method, protocol, ssl).\n\n${order}:\nAn identifier that is used as a natural order key when looking up the host\nbased configuration entries. For example, an order key of a will be\nlooked up before an order key of b. This key guarantees that the entry\nlookup order will remain independent from the insertion order of the\nentries.\n\nThe Host-Based Authentication (HBA) setting is a list of predicates that users can specify to restrict or allow access to CrateDB.\n\nThe meaning of the fields of the are as follows:\n\nauth.host_based.config.${order}.user\nRuntime: no\nSpecifies an existing CrateDB username, only crate user (superuser) is\navailable. If no user is specified in the entry, then all existing users\ncan have access.\nauth.host_based.config.${order}.address\nRuntime: no\nThe client machine addresses that the client matches, and which are allowed\nto authenticate. This field may contain an IPv4 address, an IPv6 address or\nan IPv4 CIDR mask. For example: 127.0.0.1 or 127.0.0.1/32. It also\nmay contain a hostname or the special _local_ notation which will match\nboth IPv4 and IPv6 connections from localhost. A hostname specification\nthat starts with a dot (.) matches a suffix of the actual hostname.\nSo .crate.io would match foo.crate.io but not just crate.io. If no address\nis specified in the entry, then access to CrateDB is open for all hosts.\nauth.host_based.config.${order}.method\nRuntime: no\nThe authentication method to use when a connection matches this entry.\nValid values are trust, cert, and password. If no method is\nspecified, the trust method is used by default.\nSee Trust method, Client certificate authentication method and Password authentication method for more\ninformation about these methods.\nauth.host_based.config.${order}.protocol\nRuntime: no\nSpecifies the protocol for which the authentication entry should be used.\nIf no protocol is specified, then this entry will be valid for all\nprotocols that rely on host based authentication see Trust method).\nauth.host_based.config.${order}.ssl\nDefault: optional\nRuntime: no\nSpecifies whether the client must use SSL/TLS to connect to the cluster.\nIf set to on then the client must be connected through SSL/TLS\notherwise is not authenticated. If set to off then the client must\nnot be connected via SSL/TLS otherwise is not authenticated. Finally\noptional, which is the value when the option is completely skipped,\nmeans that the client can be authenticated regardless of SSL/TLS is used\nor not.\n\nExample of config groups:\n\nauth.host_based.config:\n  entry_a:\n    user: crate\n    address: 127.16.0.0/16\n  entry_b:\n    method: trust\n  entry_3:\n    user: crate\n    address: 172.16.0.0/16\n    method: trust\n    protocol: pg\n    ssl: on\n\nSecured communications (SSL/TLS)\n\nSecured communications via SSL allows you to encrypt traffic between CrateDB nodes and clients connecting to them. Connections are secured using Transport Layer Security (TLS).\n\nssl.http.enabled\nDefault: false\nRuntime: no\n\nSet this to true to enable secure communication between the CrateDB node and the client through SSL via the HTTPS protocol.\n\nssl.psql.enabled\nDefault: false\nRuntime: no\n\nSet this to true to enable secure communication between the CrateDB node and the client through SSL via the PostgreSQL wire protocol.\n\nssl.transport.mode\nDefault: legacy\nRuntime: no\n\nFor communication between nodes, choose:\n\noff\n\nSSL cannot be used\n\nlegacy\n\nSSL is not used. If HBA is enabled, transport connections won’t be verified Any reachable host can establish a connection.\n\non\n\nSSL must be used\n\nssl.keystore_filepath\nRuntime: no\n\nThe full path to the node keystore file.\n\nssl.keystore_password\nRuntime: no\n\nThe password used to decrypt the keystore file defined with ssl.keystore_filepath.\n\nssl.keystore_key_password\nRuntime: no\n\nThe password entered at the end of the keytool -genkey command.\n\nNote\n\nOptionally trusted CA certificates can be stored separately from the node’s keystore into a truststore for CA certificates.\n\nssl.truststore_filepath\nRuntime: no\n\nThe full path to the node truststore file. If not defined, then only a keystore will be used.\n\nssl.truststore_password\nRuntime: no\n\nThe password used to decrypt the truststore file defined with ssl.truststore_filepath.\n\nssl.resource_poll_interval\nDefault: 5m\nRuntime: no\n\nThe frequency at which SSL files such as keystore and truststore are polled for changes.\n\nCross-origin resource sharing (CORS)\n\nMany browsers support the same-origin policy which requires web applications to explicitly allow requests across origins. The cross-origin resource sharing settings in CrateDB allow for configuring these.\n\nhttp.cors.enabled\nDefault: false\nRuntime: no\n\nEnable or disable cross-origin resource sharing.\n\nhttp.cors.allow-origin\nDefault: <empty>\nRuntime: no\n\nDefine allowed origins of a request. * allows any origin (which can be a substantial security risk) and by prepending a / the string will be treated as a regular expression. For example /https?:\\/\\/crate.io/ will allow requests from https://crate.io and https://crate.io. This setting disallows any origin by default.\n\nhttp.cors.max-age\nDefault: 1728000 (20 days)\nRuntime: no\n\nMax cache age of a preflight request in seconds.\n\nhttp.cors.allow-methods\nDefault: OPTIONS, HEAD, GET, POST, PUT, DELETE\nRuntime: no\n\nAllowed HTTP methods.\n\nhttp.cors.allow-headers\nDefault: X-Requested-With, Content-Type, Content-Length\nRuntime: no\n\nAllowed HTTP headers.\n\nhttp.cors.allow-credentials\nDefault: false\nRuntime: no\n\nAdd the Access-Control-Allow-Credentials header to responses.\n\nBlobs\nblobs.path\nRuntime: no\n\nPath to a filesystem directory where to store blob data allocated for this node.\n\nBy default blobs will be stored under the same path as normal data. A relative path value is interpreted as relative to CRATE_HOME.\n\nRepositories\n\nRepositories are used to backup a CrateDB cluster.\n\nrepositories.url.allowed_urls\nRuntime: no\n\nThis setting only applies to repositories of type url.\n\nWith this setting a list of urls can be specified which are allowed to be used if a repository of type url is created.\n\nWildcards are supported in the host, path, query and fragment parts.\n\nThis setting is a security measure to prevent access to arbitrary resources.\n\nIn addition, the supported protocols can be restricted using the repositories.url.supported_protocols setting.\n\nrepositories.url.supported_protocols\nDefault: http, https, ftp, file and jar\nRuntime: no\n\nA list of protocols that are supported by repositories of type url.\n\nThe jar protocol is used to access the contents of jar files. For more info, see the java JarURLConnection documentation.\n\nSee also the path.repo Setting.\n\nQueries\nindices.query.bool.max_clause_count\nDefault: 8192\nRuntime: no\n\nThis setting limits the number of boolean clauses that can be generated by != ANY(), LIKE ANY(), ILIKE ANY(), NOT LIKE ANY() and NOT ILIKE ANY() operators on arrays in order to prevent users from executing queries that may result in heavy memory consumption causing nodes to crash with OutOfMemory exceptions. Throws TooManyClauses errors when the limit is exceeded.\n\nNote\n\nYou can avoid TooManyClauses errors by increasing this setting. The number of boolean clauses used can be larger than the elements of the array .\n\nLegacy\nlegacy.table_function_column_naming\nDefault: false\nRuntime: no\n\nSince CrateDB 5.0.0, if the table function is not aliased and is returning a single base data typed column, the table function name is used as the column name. This setting can be set in order to use the naming convention prior to 5.0.0.\n\nThe following table functions are affected by this setting:\n\nunnest\n\nregexp_matches\n\ngenerate_series\n\nWhen the setting is set and a single column is expected to be returned, the returned column will be named col1, groups, or col1 respectively.\n\nNote\n\nBeware that if not all nodes in the cluster are consistently set or unset, the behaviour will depend on the node handling the query.\n\nJavaScript language\nlang.js.enabled\nDefault: true\nRuntime: no\n\nSetting to enable or disable JavaScript UDF support.\n\nCustom attributes\n\nThe node.attr namespace is a bag of custom attributes. Custom attributes can be used to control shard allocation.\n\nYou can create any attribute you want under this namespace, like node.attr.key: value. These attributes use the node.attr namespace to distinguish them from core node attribute like node.name.\n\nCustom attributes are not validated by CrateDB, unlike core node attributes."
  },
  {
    "title": "SQL Standard Compliance — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/appendices/compliance.html",
    "html": "3.3\nSQL Standard Compliance\n\nThis section provides a list of features that CrateDB supports and to what extent it conforms to the current SQL standard ISO/IEC 9075 “Database Language SQL”.\n\nThis list is approximate and features that are listed as supported might be nonconforming in their implementation. However, the main reference documentation of CrateDB always contains the most accurate information about what CrateDB supports, what they are and how to use them."
  },
  {
    "title": "Storage and consistency — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/concepts/storage-consistency.html",
    "html": "5.6\nStorage and consistency\n\nThis document provides an overview on how CrateDB stores and distributes state across the cluster and what consistency and durability guarantees are provided.\n\nNote\n\nSince CrateDB heavily relies on Elasticsearch and Lucene for storage and cluster consensus, concepts shown here might look familiar to Elasticsearch users, since the implementation is actually reused from the Elasticsearch code.\n\nTable of contents\n\nData storage\n\nAtomicity at document level\n\nDurability\n\nAddressing documents\n\nConsistency\n\nCluster meta data\n\nData storage\n\nEvery table in CrateDB is sharded, which means that tables are divided and distributed across the nodes of a cluster. Each shard in CrateDB is a Lucene index broken down into segments getting stored on the filesystem. Physically the files reside under one of the configured data directories of a node.\n\nLucene only appends data to segment files, which means that data written to the disc will never be mutated. This makes it easy for replication and recovery, since syncing a shard is simply a matter of fetching data from a specific marker.\n\nAn arbitrary number of replica shards can be configured per table. Every operational replica holds a full synchronized copy of the primary shard.\n\nWith read operations, there is no difference between executing the operation on the primary shard or on any of the replicas. CrateDB randomly assigns a shard when routing an operation. It is possible to configure this behavior if required, see our best practice guide on multi zone setups for more details.\n\nWrite operations are handled differently than reads. Such operations are synchronous over all active replicas with the following flow:\n\nThe primary shard and the active replicas are looked up in the cluster state for the given operation. The primary shard and a quorum of the configured replicas need to be available for this step to succeed.\n\nThe operation is routed to the according primary shard for execution.\n\nThe operation gets executed on the primary shard\n\nIf the operation succeeds on the primary, the operation gets executed on all replicas in parallel.\n\nAfter all replica operations finish the operation result gets returned to the caller.\n\nShould any replica shard fail to write the data or times out in step 5, it’s immediately considered as unavailable.\n\nAtomicity at document level\n\nEach row of a table in CrateDB is a semi structured document which can be nested arbitrarily deep through the use of object and array types.\n\nOperations on documents are atomic. Meaning that a write operation on a document either succeeds as a whole or has no effect at all. This is always the case, regardless of the nesting depth or size of the document.\n\nCrateDB does not provide transactions. Since every document in CrateDB has a version number assigned, which gets increased every time a change occurs, patterns like Optimistic Concurrency Control can help to work around that limitation.\n\nDurability\n\nEach shard has a WAL also known as translog. It guarantees that operations on documents are persisted to disk without having to issue a Lucene-Commit for every write operation. When the translog gets flushed all data is written to the persistent index storage of Lucene and the translog gets cleared.\n\nIn case of an unclean shutdown of a shard, the transactions in the translog are getting replayed upon startup to ensure that all executed operations are permanent.\n\nThe translog is also directly transferred when a newly allocated replica initializes itself from the primary shard. There is no need to flush segments to disc just for replica recovery purposes.\n\nAddressing documents\n\nEvery document has an internal identifier. By default this identifier is derived from the primary key. Documents living in tables without a primary key are assigned a unique auto-generated ID automatically when created.\n\nEach document is routed to one specific shard according to the routing column. All rows that have the same routing column row value are stored in the same shard. The routing column can be specified with the CLUSTERED clause when creating the table. If a primary key has been defined, it will be used as the default routing column, otherwise the internal document ID is used.\n\nWhile transparent to the user, internally there are two ways how CrateDB accesses documents:\n\nget\n\nDirect access by identifier. Only applicable if the routing key and the identifier can be computed from the given query specification. (e.g: the full primary key is defined in the where clause).\n\nThis is the most efficient way to access a document, since only a single shard gets accessed and only a simple index lookup on the _id field has to be done.\n\nsearch\n\nQuery by matching against fields of documents across all candidate shards of the table.\n\nConsistency\n\nCrateDB is eventual consistent for search operations. Search operations are performed on shared IndexReaders which besides other functionality, provide caching and reverse lookup capabilities for shards. An IndexReader is always bound to the Lucene segment it was started from, which means it has to be refreshed in order to see new changes, this is done on a time based manner, but can also be done manually (see refresh). Therefore a search only sees a change if the according IndexReader was refreshed after that change occurred.\n\nIf a query specification results in a get operation, changes are visible immediately. This is achieved by looking up the document in the translog first, which will always have the most recent version of the document. The common update and fetch use-case is therefore possible. If a client updates a row and that row is looked up by its primary key after that update the changes will always be visible, since the information will be retrieved directly from the translog.\n\nNote\n\nDirty reads can occur if the primary shard becomes isolated. The primary will only realize it is isolated once it tries to communicate with its replicas or the master. At that point, a write operation is already committed into the primary and can be read by a concurrent read operation. In order to minimise the window of opportunity for this phenomena, the CrateDB nodes communicate with the master every second (by default) and once they realise no master is known, they will start rejecting write operations.\n\nEvery replica shard is updated synchronously with its primary and always carries the same information. Therefore it does not matter if the primary or a replica shard is accessed in terms of consistency. Only the refresh of the IndexReader affects consistency.\n\nCaution\n\nSome outage conditions can affect these consistency claims. See the resiliency documentation for details.\n\nCluster meta data\n\nCluster meta data is held in the so called “Cluster State”, which contains the following information:\n\nTables schemas.\n\nPrimary and replica shard locations. Basically just a mapping from shard number to the storage node.\n\nStatus of each shard, which tells if a shard is currently ready for use or has any other state like “initializing”, “recovering” or cannot be assigned at all.\n\nInformation about discovered nodes and their status.\n\nConfiguration information.\n\nEvery node has its own copy of the cluster state. However there is only one node allowed to change the cluster state at runtime. This node is called the “master” node and gets auto-elected. The “master” node has no special configuration at all, all nodes are master-eligible by default, and any master-eligible node can be elected as the master. There is also an automatic re-election if the current master node goes down for some reason.\n\nNote\n\nTo avoid a scenario where two masters could be elected due to network partitioning, CrateDB automatically defines a quorum of nodes with which it is possible to elect a master. For details on how this works and further information see Master Node Election.\n\nTo explain the flow of events for any cluster state change, here is an example flow for an ALTER TABLE statement which changes the schema of a table:\n\nA node in the cluster receives the ALTER TABLE request.\n\nThe node sends out a request to the current master node to change the table definition.\n\nThe master node applies the changes locally to the cluster state and sends out a notification to all affected nodes about the change.\n\nThe nodes apply the change, so that they are now in sync with the master.\n\nEvery node might take some local action depending on the type of cluster state change."
  },
  {
    "title": "PostgreSQL Wire Protocol — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/interfaces/postgres.html",
    "html": "3.3\nPostgreSQL Wire Protocol\n\nTable of Contents\n\nIntroduction\n\nServer Compatibility and Implementation Status\n\nStart-Up\n\nSSL Support\n\nAuthentication\n\nParameterStatus\n\nDatabase selection\n\nQuery Modes\n\nSimple Query\n\nExtended Query\n\nCopy Operations\n\nFunction Call\n\nCanceling Requests\n\npg_catalog\n\npg_type\n\nShow Transaction Isolation\n\nBEGIN/COMMIT Statements\n\nClient Compatibility\n\nJDBC\n\nLimitations\n\nConnection Failover and Load Balancing\n\nImplementation Differences\n\nCOPY\n\nObjects\n\nType Casts\n\nArrays\n\nDeclaration of Arrays\n\nAccessing Arrays\n\nText Search Functions and Operators\n\nExpression Evaluation\n\nIntroduction\n\nCrateDB contains support for the PostgreSQL wire protocol v3.\n\nIf a node is started with postgres support enabled it will bind to port 5432 by default. To use a custom port, set the corresponding Ports in the Configuration.\n\nHowever, even though connecting PostgreSQL tools and client libraries is supported, the actual SQL statements have to be supported by CrateDB’s SQL dialect. A notable difference is that CrateDB doesn’t support transactions, which is why clients should generally enable autocommit.\n\nNote\n\nIn order to use setFetchSize in JDBC it is possible to set autocommit to false.\n\nThe client will utilize the fetchSize on SELECT statements and only load up to fetchSize rows into memory.\n\nSee PostgreSQL JDBC Query docs <https://jdbc.postgresql.org/documentation/head/query.html> for more information.\n\nWrite operations will still behave as if autocommit was enabled and commit or rollback calls are ignored.\n\nServer Compatibility and Implementation Status\n\nCrateDB emulates PostgreSQL server version 10.5.\n\nStart-Up\nSSL Support\n\nSSL support is only available in the Enterprise Edition. If enterprise is disabled, all SSLRequests are answered with N, indicating to the client that SSL is not available.\n\nSee Also\n\nSecured Communications (SSL/TLS)\n\nAuthentication\n\nIf the Enterprise Edition is enabled, authentication methods can be configured using Host Based Authentication (HBA).\n\nIf the enterprise functionality is disabled, all “start up” requests are answered with an AuthenticationOK response.\n\nParameterStatus\n\nAfter the authentication has succeeded the server has the possibility to send multiple ParameterStatus messages to the client. These are used to communicate information like server_version (emulates PostgreSQL 9.5) or server_encoding.\n\nCrateDB also sends a message containing the crate_version parameter. This contains the current CrateDB version number.\n\nThis information is useful for clients to detect that they’re connecting to CrateDB instead of a PostgreSQL instance.\n\nDatabase selection\n\nSince CrateDB uses schemas instead of databases, the database parameter sets the default schema name for future queries. If no schema is specified, the schema doc will be used as default. Additionally, the only supported charset is UTF8.\n\nQuery Modes\nSimple Query\n\nThe Simple Query protocol mode is fully implemented.\n\nExtended Query\n\nThe Extended Query protocol mode is implemented with the following limitations:\n\nThe ParameterDescription message works for the most common use cases except for DDL statements.\n\nTo optimize the execution of bulk operations the execution of statements is delayed until the Sync message is received\n\nCopy Operations\n\nCrateDB does not support the COPY sub-protocol.\n\nFunction Call\n\nThe function call sub-protocol is not supported since it’s a legacy feature.\n\nCanceling Requests\n\nOperations can be cancelled using the KILL statement, hence the CancelRequest message is unsupported. Consequently, the server won’t send a BackendKeyData message during connection initialization.\n\npg_catalog\n\nFor improved compatibility, the pg_catalog schema is implemented containing following tables:\n\npg_type\n\npg_database\n\npg_class\n\npg_namespace\n\npg_attribute\n\npg_attrdef\n\npg_index\n\npg_constraint\n\npg_description\n\npg_type\n\nSome clients require the pg_catalog.pg_type in order to be able to stream arrays or other non-primitive types.\n\nFor compatibility reasons there is a trimmed down pg_type table available in CrateDB:\n\ncr> select oid, typname, typarray, typelem, typlen from pg_catalog.pg_type order by oid;\n+------+--------------+----------+---------+--------+\n|  oid | typname      | typarray | typelem | typlen |\n+------+--------------+----------+---------+--------+\n|   16 | bool         |     1000 |       0 |      1 |\n|   18 | char         |     1002 |       0 |      1 |\n|   20 | int8         |     1016 |       0 |      8 |\n|   21 | int2         |     1005 |       0 |      2 |\n|   23 | int4         |     1007 |       0 |      4 |\n|  114 | json         |      199 |       0 |     -1 |\n|  199 | _json        |        0 |     114 |     -1 |\n|  700 | float4       |     1021 |       0 |      4 |\n|  701 | float8       |     1022 |       0 |      8 |\n| 1000 | _bool        |        0 |      16 |     -1 |\n| 1002 | _char        |        0 |      18 |     -1 |\n| 1005 | _int2        |        0 |      21 |     -1 |\n| 1007 | _int4        |        0 |      23 |     -1 |\n| 1015 | _varchar     |        0 |    1043 |     -1 |\n| 1016 | _int8        |        0 |      20 |     -1 |\n| 1021 | _float4      |        0 |     700 |     -1 |\n| 1022 | _float8      |        0 |     701 |     -1 |\n| 1043 | varchar      |     1015 |       0 |     -1 |\n| 1184 | timestamptz  |     1185 |       0 |      8 |\n| 1185 | _timestamptz |        0 |    1184 |     -1 |\n+------+--------------+----------+---------+--------+\nSELECT 20 rows in set (... sec)\n\n\nNote\n\nThis is just a snapshot of the table. Check table information_schema.columns to get information for all supported columns.\n\nShow Transaction Isolation\n\nFor compatibility with JDBC the SHOW TRANSACTION ISOLATION LEVEL statement is implemented:\n\ncr> show transaction isolation level;\n+-----------------------+\n| transaction_isolation |\n+-----------------------+\n| read uncommitted      |\n+-----------------------+\nSHOW 1 row in set (... sec)\n\nBEGIN/COMMIT Statements\n\nFor compatibility with clients that use the Postgres wire protocol, such as the Golang lib/pq and pgx drivers, the full PostgreSQL syntax of the BEGIN and COMMIT statements is implemented, for example:\n\ncr> BEGIN TRANSACTION ISOLATION LEVEL READ UNCOMMITTED,\n...                   READ ONLY,\n...                   NOT DEFERRABLE;\nBEGIN OK, 0 rows affected  (... sec)\n\ncr> COMMIT\nCOMMIT OK, 0 rows affected  (... sec)\n\n\nSince CrateDB does not support transactions, both the COMMIT and BEGIN statement and any of its parameters are ignored.\n\nClient Compatibility\nJDBC\n\npgjdbc JDBC drivers version 9.4.1209 and above are compatible.\n\nLimitations\n\nreflection methods like conn.getMetaData().getTables(...) won’t work since the required tables are unavailable in CrateDB.\n\nAs a workaround it’s possible to use SHOW TABLES or query the information_schema tables manually using SELECT statements.\n\nOBJECT and GEO_SHAPE columns can be streamed as JSON but require pgjdbc version 9.4.1210 or newer.\n\nMultidimensional arrays will be streamed as JSON encoded string to avoid a protocol limitation where all sub-arrays are required to have the same length.\n\nThe behavior of PreparedStatement.executeBatch in error cases depends on in which stage an error occurs: A BatchUpdateException is thrown if no processing has been done yet, whereas single operations failing after the processing started are indicated by an EXECUTE_FAILED (-3) return value.\n\nTransaction limitations as described above.\n\nHaving escape processing enabled could prevent the usage of Object Literals in case an object key’s starting character clashes with a JDBC escape keyword (see also JDBC escape syntax). Currently, disabling escape processing will remedy this, but prevent the Extended Query API from working due to a bug at pgjdbc.\n\nConnection Failover and Load Balancing\n\nConnection failover and load balancing is supported as described here: PostgreSQL JDBC connection failover.\n\nNote\n\nIt is not recommended to use the targetServerType parameter since CrateDB has no concept of master-replica nodes.\n\nImplementation Differences\n\nThe PostgreSQL Wire Protocol makes it easy to use many PostgreSQL compatible tools and libraries directly with CrateDB. However, many of these tools assume that they are talking to PostgreSQL specifically, and thus rely on SQL extensions and idioms that are unique to PostgreSQL. Because of this, some tools or libraries may not work with other SQL databases such as CrateDB.\n\nCrateDB’s SQL query engine enables real-time search & aggregations for online analytic processing (OLAP) and business intelligence (BI) with the benefit of the ability to scale horizontally. The use-cases of CrateDB are different than those of PostgreSQL, as CrateDB’s specialized storage schema and query execution engine address different requirements (see High Level Architecture).\n\nThe listed features below cover the main differences in implementation and dialect between CrateDB and PostgreSQL. A detailed comparison between CrateDB’s SQL dialect and standard SQL is defined in Compatibility.\n\nCOPY\n\nCrateDB does not support the distinct sub-protocol that is used to serve COPY operations and provides another implementation for transferring bulk data using the COPY FROM and COPY TO statements.\n\nObjects\n\nThe definition of structured values by using JSON types, composite types or HSTORE are not supported. CrateDB alternatively allows the definition of nested documents (of type object) that store fieldscontaining any CrateDB supported data type, including nested object types.\n\nType Casts\n\nCrateDB accepts the Type Conversion syntax for conversion of one data type to another (see Value Expressions).\n\nArrays\nDeclaration of Arrays\n\nWhile multidimensional arrays in PostgreSQL must have matching extends for each dimension, CrateDB allows different length nested arrays as this example shows:\n\ncr> select [[1,2,3],[1,2]] from sys.cluster;\n+---------------------+\n| [[1, 2, 3], [1, 2]] |\n+---------------------+\n| [[1, 2, 3], [1, 2]] |\n+---------------------+\nSELECT 1 row in set (... sec)\n\nAccessing Arrays\n\nFetching arbitrary rectangular slices of an array using lower-bound:upper-bound expression (see Arrays) in the array subscript is not supported.\n\nText Search Functions and Operators\n\nThe functions and operators provided by PostgreSQL for full-text search (see PostgreSQL Fulltext Search) are not compatible with those provided by CrateDB. For more information about the built-in full-text search in CrateDB refer to Fulltext Search.\n\nIf you are missing features, functions or dialect improvements and have a great use case for it, let us know on Github. We’re always improving and extending CrateDB, and we love to hear feedback.\n\nExpression Evaluation\n\nUnlike PostgreSQL, expressions are not evaluated if the query results in 0 rows either because of the table is empty or by a not matching where clause."
  },
  {
    "title": "Community Edition — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/editions/community.html",
    "html": "3.3\nCommunity Edition\n\nThe community edition (CE) of CrateDB is a distribution that excludes enterprise modules which are licensed under a more restrictive license. The CE can be used under the terms of the Apache License version 2 without further restrictions.\n\nThe Enterprise Features are not available within the CE.\n\nThe community edition distribution must be built from source:\n\n$ git clone https://github.com/crate/crate\n$ cd crate\n$ git submodule update --init\n$ [git checkout <tag> ]\n$ ./gradlew clean communityEditionDistTar\n\n\nThe built tarball will be available under app/build/distributions."
  },
  {
    "title": "Version 5.6.2 — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.6.2.html",
    "html": "5.6\nVersion 5.6.2\n\nReleased on 2024-02-15.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.6.2.\n\nWe recommend that you upgrade to the latest 5.5 release before moving to 5.6.2.\n\nA rolling upgrade from 5.5.x to 5.6.2 is supported. Before upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of contents\n\nFixes\n\nSee the Version 5.6.0 release notes for a full list of changes in the 5.6 series.\n\nFixes\n\nFixed an issue that would cause no results being returned when filtering on a PRIMARY KEY column, wrapped with a CAST, e.g.:\n\nCREATE TABLE tbl(a INTEGER, PRIMARY KEY(a), b INTEGER);\nINSERT INTO tbl(a) VALUES (1, 11);\nREFRESH TABLE tbl;\nSELECT * FROM tbl WHERE CAST(t.a AS BOOLEAN)= true;\n\n\nSame issue, previously, would cause no rows to be updated or deleted, e.g.\n\nUPDATE tbl SET b = b + 1 WHERE CAST(t.a AS BOOLEAN)= true;\nDELETE FROM tbl WHERE CAST(t.a AS BOOLEAN)= true;\n\n\nFixed an issue that can cause errors during a rolling upgrade to >= Version 5.6.0 if one of the following command is executed on relations with existing privileges:\n\nDROP TABLE\n\nDROP VIEW\n\nRENAME TABLE\n\nSWAP TABLE\n\nThese commands are now rejected until all nodes are upgraded.\n\nFixed an issue that caused NullPointerException to be thrown when attempting to rename a table on cluster which has been upgraded from versions < Version 5.6.0 to a version >= Version 5.6.0, and there were users and privileges defined.\n\nFixed trim, ltrim, rtrim, and btrim scalar functions to return NULL`, instead of the original string, when the ``trimmingText argument is NULL, complying with PostgreSQL behaviour for these functions.\n\nFixed a regression introduced in 5.6.0 that caused concat_ws returning the wrong result when used on a column with NULL values in the WHERE-clause combined with a NOT-predicate. An example:\n\nSELECT * FROM t1 WHERE NOT CONCAT_WS(true, column_with_null_value, false);\n\n\nFixed a bug (present since at least Version 5.2.0) where columns cast to a numeric type with a non-default precision could return the unscaled value in a multi-node cluster\n\nFixed an issue that caused SELECT statements with WHERE clause having primary keys under NOT predicate to return invalid results.\n\nFixed an issue that caused SELECT statements with WHERE clause having NOT predicate whose argument consists of NULLABLE scalar functions with NULL argument that could evaluate to NULL to return invalid results. An example\n\nSELECT * FROM t WHERE (col % NULL) != 1;\n\n\nA NULLABLE function in this context means a function returning NULL if and only if the input is a NULL.\n\nFixed a race condition that could lead to ShardCollectContext already added errors when making a query after a table had been idle without any accesses for a while.\n\nFixed an issue when resolving relations. When resolving an unqualified name (no explicit schema), it first exhausted the search path looking for tables before moving on to views. Now it will correctly look for both table and view in each element of the search path before moving onto the next.\n\nFor example, with a search path set to a, b, a query on tbl will now look for:\n\ntable a.tbl\n\nview a.tbl\n\ntable b.tbl\n\nview b.tbl\n\nInstead of:\n\ntable a.tbl\n\ntable b.tbl\n\nview a.tbl\n\nview b.tbl"
  },
  {
    "title": "Version 5.6.4 - Unreleased — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/5.6.4.html",
    "html": "5.6\nVersion 5.6.4 - Unreleased\n\nNote\n\nIn development. 5.6.4 isn’t released yet. These are the release notes for the upcoming release.\n\nNote\n\nIf you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher before you upgrade to 5.6.4.\n\nWe recommend that you upgrade to the latest 5.5 release before moving to 5.6.4.\n\nA rolling upgrade from 5.5.x to 5.6.4 is supported.\n\nBefore upgrading, you should back up your data.\n\nWarning\n\nTables that were created before CrateDB 4.x will not function with 5.x and must be recreated before moving to 5.x.x.\n\nYou can recreate tables using COPY TO and COPY FROM or by inserting the data into a new table.\n\nTable of contents\n\nFixes\n\nSee the Version 5.6.0 release notes for a full list of changes in the 5.6 series.\n\nFixes\n\nChanged has_database_privilege([user,] database, privilege text) and has_schema_privilege([user,] schema, privilege text) functions to be registered under pg_catalog schema, to be compatible with PostgreSQL behaviour."
  },
  {
    "title": "Release Notes — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/index.html",
    "html": "5.6\nRelease Notes\n\nInformation about individual CrateDB releases, typically including upgrade information and changelog.\n\nNote\n\nThe latest stable, testing, and nightly version is always available from the CrateDB download page.\n\nOlder version of CrateDB are available in the release archives.\n\nVersions\n5.x\n5.6.x\nVersion 5.6.4 - Unreleased\nVersion 5.6.3\nVersion 5.6.2\nVersion 5.6.1\nVersion 5.6.0\n5.5.x\nVersion 5.5.5 - Unreleased\nVersion 5.5.4\nVersion 5.5.3\nVersion 5.5.2\nVersion 5.5.1\nVersion 5.5.0\n5.4.x\nVersion 5.4.8\nVersion 5.4.7\nVersion 5.4.6\nVersion 5.4.5\nVersion 5.4.4\nVersion 5.4.3\nVersion 5.4.2\nVersion 5.4.1\nVersion 5.4.0\n5.3.x\nVersion 5.3.9\nVersion 5.3.8\nVersion 5.3.7\nVersion 5.3.6\nVersion 5.3.5\nVersion 5.3.4\nVersion 5.3.3\nVersion 5.3.2\nVersion 5.3.1\nVersion 5.3.0\n5.2.x\nVersion 5.2.11\nVersion 5.2.10\nVersion 5.2.9\nVersion 5.2.8\nVersion 5.2.7\nVersion 5.2.6\nVersion 5.2.5\nVersion 5.2.4\nVersion 5.2.3\nVersion 5.2.2\nVersion 5.2.1\nVersion 5.2.0\n5.1.x\nVersion 5.1.4\nVersion 5.1.3\nVersion 5.1.2\nVersion 5.1.1\nVersion 5.1.0\n5.0.x\nVersion 5.0.3\nVersion 5.0.2\nVersion 5.0.1\nVersion 5.0.0\n4.x\n4.8.x\nVersion 4.8.4\nVersion 4.8.3\nVersion 4.8.2\nVersion 4.8.1\nVersion 4.8.0\n4.7.x\nVersion 4.7.3\nVersion 4.7.2\nVersion 4.7.1\nVersion 4.7.0\n4.6.x\nVersion 4.6.8\nVersion 4.6.7\nVersion 4.6.6\nVersion 4.6.5\nVersion 4.6.4\nVersion 4.6.3\nVersion 4.6.2\nVersion 4.6.1\nVersion 4.6.0\n4.5.x\nVersion 4.5.5\nVersion 4.5.4\nVersion 4.5.3\nVersion 4.5.2\nVersion 4.5.1\nVersion 4.5.0\n4.4.x\nVersion 4.4.3\nVersion 4.4.2\nVersion 4.4.1\nVersion 4.4.0\n4.3.x\nVersion 4.3.4\nVersion 4.3.3\nVersion 4.3.2\nVersion 4.3.1\nVersion 4.3.0\n4.2.x\nVersion 4.2.7\nVersion 4.2.6\nVersion 4.2.5\nVersion 4.2.4\nVersion 4.2.3\nVersion 4.2.2\nVersion 4.2.1\nVersion 4.2.0\n4.1.x\nVersion 4.1.8\nVersion 4.1.7\nVersion 4.1.6\nVersion 4.1.5\nVersion 4.1.4\nVersion 4.1.3\nVersion 4.1.2\nVersion 4.1.1\nVersion 4.1.0\n4.0.x\nVersion 4.0.12\nVersion 4.0.11\nVersion 4.0.10\nVersion 4.0.9\nVersion 4.0.8\nVersion 4.0.7\nVersion 4.0.6\nVersion 4.0.5\nVersion 4.0.4\nVersion 4.0.3\nVersion 4.0.2\nVersion 4.0.1\nVersion 4.0.0\n3.x\n3.3.x\nVersion 3.3.6\nVersion 3.3.5\nVersion 3.3.4\nVersion 3.3.3\nVersion 3.3.2\nVersion 3.3.1\nVersion 3.3.0\n3.2.x\nVersion 3.2.8\nVersion 3.2.7\nVersion 3.2.6\nVersion 3.2.5\nVersion 3.2.4\nVersion 3.2.3\nVersion 3.2.2\nVersion 3.2.1\nVersion 3.2.0\n3.1.x\nVersion 3.1.6\nVersion 3.1.5\nVersion 3.1.4\nVersion 3.1.3\nVersion 3.1.2\nVersion 3.1.1\nVersion 3.1.0\n3.0.x\nVersion 3.0.7\nVersion 3.0.6\nVersion 3.0.5\nVersion 3.0.4\nVersion 3.0.3\nVersion 3.0.2\nVersion 3.0.1\nVersion 3.0.0\n2.x\n2.3.x\nVersion 2.3.11\nVersion 2.3.10\nVersion 2.3.9\nVersion 2.3.8\nVersion 2.3.7\nVersion 2.3.6\nVersion 2.3.5\nVersion 2.3.4\nVersion 2.3.3\nVersion 2.3.2\nVersion 2.3.1\nVersion 2.3.0\n2.2.x\nVersion 2.2.7\nVersion 2.2.6\nVersion 2.2.5\nVersion 2.2.4\nVersion 2.2.3\nVersion 2.2.2\nVersion 2.2.1\nVersion 2.2.0\n2.1.x\nVersion 2.1.10\nVersion 2.1.9\nVersion 2.1.8\nVersion 2.1.7\nVersion 2.1.6\nVersion 2.1.5\nVersion 2.1.4\nVersion 2.1.3\nVersion 2.1.2\nVersion 2.1.1\nVersion 2.1.0\n2.0.x\nVersion 2.0.7\nVersion 2.0.6\nVersion 2.0.5\nVersion 2.0.4\nVersion 2.0.3\nVersion 2.0.2\nVersion 2.0.1\nVersion 2.0.0\n1.x\n1.2.x\nVersion 1.2.0\n1.1.x\nVersion 1.1.6\nVersion 1.1.5\nVersion 1.1.4\nVersion 1.1.3\nVersion 1.1.2\nVersion 1.1.1\nVersion 1.1.0\n1.0.x\nVersion 1.0.6\nVersion 1.0.5\nVersion 1.0.4\nVersion 1.0.3\nVersion 1.0.2\nVersion 1.0.1\nVersion 1.0.0\nOlder Versions\n\nFor older versions, see the 0.57.8 CHANGES.txt file."
  },
  {
    "title": "HTTP endpoint — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/interfaces/http.html",
    "html": "5.6\nHTTP endpoint\n\nCrateDB provides a HTTP Endpoint that can be used to submit SQL queries. The endpoint is accessible under <servername:port>/_sql.\n\nSQL statements are sent to the _sql endpoint in json format, whereby the statement is sent as value associated to the key stmt.\n\nSee Also\n\nData manipulation\n\nA simple SELECT statement can be submitted like this:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' \\\n... -d '{\"stmt\":\"select name, position from locations order by id limit 2\"}'\n{\n  \"cols\": [\n    \"name\",\n    \"position\"\n  ],\n  \"rows\": [\n    [\n      \"North West Ripple\",\n      1\n    ],\n    [\n      \"Outer Eastern Rim\",\n      2\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nNote\n\nWe’re using a simple command line invocation of curl here so you can see how to run this by hand in the terminal. For the rest of the examples in this document, we use here documents (i.e. EOF) for multi line readability.\n\nTable of contents\n\nParameter substitution\n\nDefault schema\n\nColumn types\n\nAvailable data types\n\nBulk operations\n\nError handling\n\nError codes\n\nBulk errors\n\nParameter substitution\n\nIn addition to the stmt key the request body may also contain an args key which can be used for SQL parameter substitution.\n\nThe SQL statement has to be changed to use placeholders where the values should be inserted. Placeholders can either be numbered (in the form of $1, $2, etc.) or unnumbered using a question mark ?.\n\nThe placeholders will then be substituted with values from an array that is expected under the args key:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\":\n...     \"select date,position from locations\n...     where date <= \\$1 and position < \\$2 order by position\",\n...   \"args\": [\"1979-10-12\", 3]\n... }\n... EOF\n{\n  \"cols\": [\n    \"date\",\n    \"position\"\n  ],\n  \"rows\": [\n    [\n      308534400000,\n      1\n    ],\n    [\n      308534400000,\n      2\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nNote\n\nIn this example the placeholders start with an backslash due to shell escaping.\n\nWarning\n\nParameter substitution must not be used within subscript notation.\n\nFor example, column[?] is not allowed.\n\nThe same query using question marks as placeholders looks like this:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\":\n...     \"select date,position from locations\n...     where date <= ? and position < ? order by position\",\n...   \"args\": [\"1979-10-12\", 3]\n... }\n... EOF\n{\n  \"cols\": [\n    \"date\",\n    \"position\"\n  ],\n  \"rows\": [\n    [\n      308534400000,\n      1\n    ],\n    [\n      308534400000,\n      2\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nNote\n\nWith some queries the row count is not ascertainable. In this cases rowcount is -1.\n\nDefault schema\n\nIt is possible to set a default schema while querying the CrateDB cluster via _sql end point. In such case the HTTP request should contain the Default-Schema header with the specified schema name:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' \\\n... -H 'Default-Schema: doc' -d@- <<- EOF\n... {\n...   \"stmt\":\"select name, position from locations order by id limit 2\"\n... }\n... EOF\n{\n  \"cols\": [\n    \"name\",\n    \"position\"\n  ],\n  \"rows\": [\n    [\n      \"North West Ripple\",\n      1\n    ],\n    [\n      \"Outer Eastern Rim\",\n      2\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nIf the schema name is not specified in the header, the default doc schema will be used instead.\n\nColumn types\n\nCrateDB can respond a list col_types with the data type ID of every responded column. This way one can know what exact data type a column is holding.\n\nIn order to get the list of column data types, a types query parameter must be passed to the request:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql?types' -d@- <<- EOF\n... {\n...   \"stmt\":\n...     \"select date, position from locations\n...      where date <= \\$1 and position < \\$2 order by position\",\n...   \"args\": [\"1979-10-12\", 3]\n... }\n... EOF\n{\n  \"cols\": [\n    \"date\",\n    \"position\"\n  ],\n  \"col_types\": [\n    11,\n    9\n  ],\n  \"rows\": [\n    [\n      308534400000,\n      1\n    ],\n    [\n      308534400000,\n      2\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nThe Array collection data type is displayed as a list where the first value is the collection type and the second is the inner type. The inner type could also be a collection.\n\nExample of JSON representation of a column list of (String, Integer[]):\n\n\"column_types\": [ 4, [ 100, 9 ] ]\n\nAvailable data types\n\nIDs of all currently available data types:\n\nID\n\n\t\n\nData type\n\n\n\n\n0\n\n\t\n\nNULL\n\n\n\n\n1\n\n\t\n\nNot supported\n\n\n\n\n2\n\n\t\n\nCHAR\n\n\n\n\n3\n\n\t\n\nBOOLEAN\n\n\n\n\n4\n\n\t\n\nTEXT\n\n\n\n\n5\n\n\t\n\nIP\n\n\n\n\n6\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\n7\n\n\t\n\nREAL\n\n\n\n\n8\n\n\t\n\nSMALLINT\n\n\n\n\n9\n\n\t\n\nINTEGER\n\n\n\n\n10\n\n\t\n\nBIGINT\n\n\n\n\n11\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\n12\n\n\t\n\nOBJECT\n\n\n\n\n13\n\n\t\n\nGEO_POINT\n\n\n\n\n14\n\n\t\n\nGEO_SHAPE\n\n\n\n\n15\n\n\t\n\nTIMESTAMP WITHOUT TIME ZONE\n\n\n\n\n16\n\n\t\n\nUnchecked object\n\n\n\n\n17\n\n\t\n\nINTERVAL\n\n\n\n\n19\n\n\t\n\nREGPROC\n\n\n\n\n20\n\n\t\n\nTIME\n\n\n\n\n21\n\n\t\n\nOIDVECTOR\n\n\n\n\n22\n\n\t\n\nNUMERIC\n\n\n\n\n23\n\n\t\n\nREGCLASS\n\n\n\n\n24\n\n\t\n\nDATE\n\n\n\n\n25\n\n\t\n\nBIT\n\n\n\n\n26\n\n\t\n\nJSON\n\n\n\n\n27\n\n\t\n\nCHARACTER\n\n\n\n\n28\n\n\t\n\nFLOAT VECTOR\n\n\n\n\n100\n\n\t\n\nARRAY\n\nBulk operations\n\nThe REST endpoint allows to issue bulk operations which are executed as single calls on the back-end site. It can be compared to prepared statement.\n\nA bulk operation can be expressed simply as an SQL statement.\n\nSupported bulk SQL statements are:\n\nInsert\n\nUpdate\n\nDelete\n\nInstead of the args (Parameter substitution) key, use the key bulk_args. This allows to specify a list of lists, containing all the records which shall be processed. The inner lists need to match the specified columns.\n\nThe bulk response contains a results array, with a row count for each bulk operation. Those results are in the same order as the issued operations of the bulk operation.\n\nThe following example describes how to issue an insert bulk operation and insert three records at once:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\": \"INSERT INTO locations (id, name, kind, description)\n...           VALUES (?, ?, ?, ?)\",\n...   \"bulk_args\": [\n...     [1337, \"Earth\", \"Planet\", \"An awesome place to spend some time on.\"],\n...     [1338, \"Sun\", \"Star\", \"An extraordinarily hot place.\"],\n...     [1339, \"Titan\", \"Moon\", \"Titan, where it rains fossil fuels.\"]\n...   ]\n... }\n... EOF\n{\n  \"cols\": [],\n  \"duration\": ...,\n  \"results\": [\n    {\n      \"rowcount\": 1\n    },\n    {\n      \"rowcount\": 1\n    },\n    {\n      \"rowcount\": 1\n    }\n  ]\n}\n\nError handling\n\nQueries that are invalid or cannot be satisfied will result in an error response. The response will contain an error code, an error message and in some cases additional arguments that are specific to the error code.\n\nClient libraries should use the error code to translate the error into an appropriate exception:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\":\"select name, position from foo.locations\"\n... }\n... EOF\n{\n  \"error\": {\n    \"message\": \"SchemaUnknownException[Schema 'foo' unknown]\",\n    \"code\": 4045\n  }\n}\n\n\nTo get more insight into what exactly went wrong an additional error_trace GET parameter can be specified to return the stack trace:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql?error_trace=true' -d@- <<- EOF\n... {\n...   \"stmt\":\"select name, position from foo.locations\"\n... }\n... EOF\n{\n  \"error\": {\n    \"message\": \"SchemaUnknownException[Schema 'foo' unknown]\",\n    \"code\": 4045\n  },\n  \"error_trace\": \"...\"\n}\n\n\nNote\n\nThis parameter is intended for CrateDB developers or for users requesting support for CrateDB. Client libraries shouldn’t make use of this option and not include the stack trace.\n\nError codes\n\nCode\n\n\t\n\nError\n\n\n\n\n4000\n\n\t\n\nThe statement contains an invalid syntax or unsupported SQL statement\n\n\n\n\n4001\n\n\t\n\nThe statement contains an invalid analyzer definition.\n\n\n\n\n4002\n\n\t\n\nThe name of the relation is invalid.\n\n\n\n\n4003\n\n\t\n\nField type validation failed\n\n\n\n\n4004\n\n\t\n\nPossible feature not supported (yet)\n\n\n\n\n4005\n\n\t\n\nAlter table using a table alias is not supported.\n\n\n\n\n4006\n\n\t\n\nThe used column alias is ambiguous.\n\n\n\n\n4007\n\n\t\n\nThe operation is not supported on this relation, as it is not accessible.\n\n\n\n\n4008\n\n\t\n\nThe name of the column is invalid.\n\n\n\n\n4009\n\n\t\n\nCrateDB License is expired. (Deprecated.)\n\n\n\n\n4010\n\n\t\n\nUser is not authorized to perform the SQL statement.\n\n\n\n\n4011\n\n\t\n\nMissing privilege for user.\n\n\n\n\n4031\n\n\t\n\nOnly read operations are allowed on this node.\n\n\n\n\n4041\n\n\t\n\nUnknown relation.\n\n\n\n\n4042\n\n\t\n\nUnknown analyzer.\n\n\n\n\n4043\n\n\t\n\nUnknown column.\n\n\n\n\n4044\n\n\t\n\nUnknown type.\n\n\n\n\n4045\n\n\t\n\nUnknown schema.\n\n\n\n\n4046\n\n\t\n\nUnknown Partition.\n\n\n\n\n4047\n\n\t\n\nUnknown Repository.\n\n\n\n\n4048\n\n\t\n\nUnknown Snapshot.\n\n\n\n\n4049\n\n\t\n\nUnknown user-defined function.\n\n\n\n\n40410\n\n\t\n\nUnknown user.\n\n\n\n\n4091\n\n\t\n\nA document with the same primary key exists already.\n\n\n\n\n4092\n\n\t\n\nA VersionConflict. Might be thrown if an attempt was made to update the same document concurrently.\n\n\n\n\n4093\n\n\t\n\nA relation with the same name exists already.\n\n\n\n\n4094\n\n\t\n\nThe used table alias contains tables with different schema.\n\n\n\n\n4095\n\n\t\n\nA repository with the same name exists already.\n\n\n\n\n4096\n\n\t\n\nA snapshot with the same name already exists in the repository.\n\n\n\n\n4097\n\n\t\n\nA partition for the same values already exists in this table.\n\n\n\n\n4098\n\n\t\n\nA user-defined function with the same signature already exists.\n\n\n\n\n4099\n\n\t\n\nA user with the same name already exists.\n\n\n\n\n5000\n\n\t\n\nUnhandled server error.\n\n\n\n\n5001\n\n\t\n\nThe execution of one or more tasks failed.\n\n\n\n\n5002\n\n\t\n\nOne or more shards are not available.\n\n\n\n\n5003\n\n\t\n\nThe query failed on one or more shards\n\n\n\n\n5004\n\n\t\n\nCreating a snapshot failed\n\n\n\n\n5030\n\n\t\n\nThe query was killed by a kill statement\n\nBulk errors\n\nIf a bulk operation fails, the resulting row count will be -2 and the resulting object may contain an error_message depending on the resulting error:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\": \"INSERT into locations (name, id) values (?,?)\",\n...   \"bulk_args\": [\n...     [\"Mars\", 1341],\n...     [\"Sun\", 1341]\n...   ]\n... }\n... EOF\n{\n  \"cols\": [],\n  \"duration\": ...,\n  \"results\": [\n    {\n      \"rowcount\": 1\n    },\n    {\n      \"rowcount\": -2\n    }\n  ]\n}\n\n\nNote\n\nEvery bulk operation will be executed, independent if one of the operation fails."
  },
  {
    "title": "SQL Statements — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/index.html",
    "html": "5.6\nSQL Statements\n\nTable of contents\n\nALTER CLUSTER\nALTER PUBLICATION\nALTER TABLE\nALTER ROLE\nALTER USER\nANALYZE\nBEGIN\nCLOSE\nCOMMIT\nCOPY FROM\nCOPY TO\nCREATE ANALYZER\nCREATE BLOB TABLE\nCREATE FUNCTION\nCREATE PUBLICATION\nCREATE REPOSITORY\nCREATE SNAPSHOT\nCREATE SUBSCRIPTION\nCREATE TABLE\nCREATE TABLE AS\nCREATE ROLE\nCREATE USER\nCREATE VIEW\nDEALLOCATE\nDECLARE\nDELETE\nDENY\nDISCARD\nDROP ANALYZER\nDROP FUNCTION\nDROP PUBLICATION\nDROP REPOSITORY\nDROP SNAPSHOT\nDROP SUBSCRIPTION\nDROP TABLE\nDROP ROLE\nDROP USER\nDROP VIEW\nEND\nEXPLAIN\nFETCH\nGRANT\nINSERT\nKILL\nOPTIMIZE\nREFRESH\nRESTORE SNAPSHOT\nREVOKE\nSELECT\nSET and RESET\nSET LICENSE\nSET AND RESET SESSION AUTHORIZATION\nSET TRANSACTION\nSHOW (session settings)\nSHOW COLUMNS\nSHOW CREATE TABLE\nSHOW SCHEMAS\nSHOW TABLES\nSTART TRANSACTION\nUPDATE\nVALUES\nWITH"
  },
  {
    "title": "General SQL — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/general/index.html",
    "html": "5.6\nGeneral SQL\n\nTable of contents\n\nConstraints\nValue expressions\nLexical structure"
  },
  {
    "title": "Usage Data Collector — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/udc.html",
    "html": "5.6\nUsage Data Collector\n\nThe CrateDB Usage Data Collector (UDC) is a sub-system that gathers usage data, reporting it to the UDC server at https://udc.crate.io. It is easy to disable, and does not collect any data that is confidential. For more information about what is being sent, see below.\n\nCrateDB uses this information as a form of automatic, effortless feedback from the community. We want to verify that we are doing the right thing by matching download statistics with usage statistics. After each release, we can see if there is a larger retention span of the server software.\n\nThe data collected is clearly stated here. If any future versions of this system collect additional data, we will clearly announce those changes.\n\nCrateDB is concerned about your privacy. We do not disclose any personally identifiable information.\n\nTable of contents\n\nTechnical information\n\nAdmin UI tracking\n\nConfiguration\n\nHow to disable UDC\n\nBy configuration\n\nBy system property\n\nTechnical information\n\nTo gather good statistics about CrateDB usage, UDC collects this information:\n\nName\n\n\t\n\nDescription\n\n\n\n\nKernel Version\n\n\t\n\nThe build number, and if there are any modifications to the kernel.\n\n\n\n\nCluster Id\n\n\t\n\nA randomized globally unique ID created every time the whole cluster is restarted.\n\n\n\n\nMaster\n\n\t\n\nBoolean whether the current node is master.\n\n\n\n\nPing Count\n\n\t\n\nUDC holds an internal counter per node which is incremented for every ping, and reset on every restart of the a node.\n\n\n\n\nCrateDB Version\n\n\t\n\nThe CrateDB version.\n\n\n\n\nJava Version\n\n\t\n\nThe Java version CrateDB is currently running with.\n\n\n\n\nHardware Address\n\n\t\n\nMAC address to uniquely identify instances behind firewalls.\n\n\n\n\nProcessor count\n\n\t\n\nNumber of available CPUs as reported by Runtime.availableProcessors\n\n\n\n\nEnterprise\n\n\t\n\nIdentifies whether the Enterprise Edition is used. 1\n\nAfter startup, UDC waits for 10 minutes before sending the first ping. It does this for two reasons; first, we don’t want the startup to be slower because of UDC, and secondly, we want to keep pings from automatic tests to a minimum. By default, UDC is sending pings every 24 hours. The ping to the UDC servers is done with a HTTP GET.\n\nAdmin UI tracking\n\nSince Admin UI v0.16.0 we are tracking the user ID along with the cluster ID to know how many active users are currently using CrateDB.\n\nConfiguration\n\nThe Usage Data Collector can be configured by adapting the crate.yml configuration file or adding a system property setting. Refer to Usage data collector to see how these settings can be accessed and how they are configured.\n\nHow to disable UDC\n\nBelow are two ways you can disable UDC. However we hope you support us offering the open source edition, and leave UDC on, so we learn how many people use CrateDB.\n\nBy configuration\n\nJust add following to your crate.yml configuration file:\n\nudc.enabled:  false\n\nBy system property\n\nIf you do not want to make any change to the jars or to the configuration, a system property setting like this will also make sure that UDC is never activated:\n\n-Cudc.enabled=false\n\n1\n\nThe “CrateDB Enterprise Edition” has been dissolved starting with CrateDB 4.5.0, see also Farewell to the CrateDB Enterprise License."
  },
  {
    "title": "Cloud discovery — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/discovery.html",
    "html": "5.6\nCloud discovery\n\nTable of contents\n\nAmazon EC2 discovery\n\nAmazon EC2 discovery\n\nCrateDB has native discovery support when running a cluster with Amazon Web Services (AWS). The discovery mechanism uses the Amazon EC2 API to generate the list of hosts for the unicast host discovery (see Unicast host discovery).\n\nThere is a best practice how to configure and run a CrateDB cluster on Amazon EC2."
  },
  {
    "title": "Snapshots — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/snapshots.html",
    "html": "5.6\nSnapshots\n\nTable of contents\n\nSnapshot\n\nCreating a repository\n\nCreating a snapshot\n\nRestore\n\nRestore data granularity\n\nCleanup\n\nDropping snapshots\n\nDropping repositories\n\nSnapshot\n\nIn CrateDB, backups are called Snapshots. They represent the state of the tables in a CrateDB cluster at the time the Snapshot was created. A Snapshot is always stored in a Repository which has to be created first.\n\nCaution\n\nYou cannot snapshot BLOB tables.\n\nCreating a repository\n\nRepositories are used to store, manage and restore snapshots.\n\nThey are created using the CREATE REPOSITORY statement:\n\ncr> CREATE REPOSITORY where_my_snapshots_go TYPE fs\n... WITH (location='repo_path', compress=true);\nCREATE OK, 1 row affected (... sec)\n\n\nRepositories are uniquely identified by their name. Every repository has a specific type which determines how snapshots are stored.\n\nCrateDB supports different repository types, see Types.\n\nThe creation of a repository configures it inside the CrateDB cluster. In general no data is written, no snapshots inside repositories changed or deleted. This way you can tell the CrateDB cluster about existing repositories which already contain snapshots.\n\nCreating a repository with the same name will result in an error:\n\ncr> CREATE REPOSITORY where_my_snapshots_go TYPE fs\n... WITH (location='another_repo_path', compress=false);\nRepositoryAlreadyExistsException[Repository 'where_my_snapshots_go' already exists]\n\nCreating a snapshot\n\nSnapshots are created inside a repository and can contain any number of tables. The CREATE SNAPSHOT statement is used to create a snapshots:\n\ncr> CREATE SNAPSHOT where_my_snapshots_go.snapshot1 ALL\n... WITH (wait_for_completion=true, ignore_unavailable=true);\nCREATE OK, 1 row affected (... sec)\n\n\nA snapshot is referenced by the name of the repository and the snapshot name, separated by a dot. If ALL is used, all user created tables of the cluster (except blob tables) are stored inside the snapshot.\n\nIt’s possible to only save a specific subset of tables in the snapshot by listing them explicitly:\n\ncr> CREATE SNAPSHOT where_my_snapshots_go.snapshot2 TABLE quotes, doc.locations\n... WITH (wait_for_completion=true);\nCREATE OK, 1 row affected (... sec)\n\n\nEven single partition of Partitioned tables can be selected for backup. This is especially useful if old partitions need to be deleted but it should be possible to restore them if needed:\n\ncr> CREATE SNAPSHOT where_my_snapshots_go.snapshot3 TABLE\n...    locations,\n...    parted_table PARTITION (date='1970-01-01')\n... WITH (wait_for_completion=true);\nCREATE OK, 1 row affected (... sec)\n\n\nSnapshots are incremental. Snapshots of the same cluster created later only store data not already contained in the repository.\n\nAll examples above are used with the argument wait_for_completion set to true. As described in the CREATE REPOSITORY reference documentation, by doing this, the statement will only respond (successfully or not) when the snapshot is fully created. Otherwise the snapshot will be created in the background and the statement will immediately respond as successful. The status of a created snapshot can be retrieved by querying the sys.snapshots system table.\n\nRestore\n\nCaution\n\nIf you are restoring a snapshot into a newer version of CrateDB, be sure to check the Release Notes for upgrade instructions.\n\nCaution\n\nIf you try to restore a table that already exists, CrateDB will return an error. However, if you try to restore metadata or cluster settings that already exist, they will be overwritten.\n\nOnce a snapshot is created, it can be used to restore its tables to the state when the snapshot was created.\n\nTo get basic information about snapshots the sys.snapshots table can be queried:\n\ncr> SELECT repository, name, state, concrete_indices\n... FROM sys.snapshots\n... ORDER BY repository, name;\n+-----------------------+-----------+---------+--------------------...-+\n| repository            | name      | state   | concrete_indices       |\n+-----------------------+-----------+---------+--------------------...-+\n| where_my_snapshots_go | snapshot1 | SUCCESS | [...]                  |\n| where_my_snapshots_go | snapshot2 | SUCCESS | [...]                  |\n| where_my_snapshots_go | snapshot3 | SUCCESS | [...]                  |\n+-----------------------+-----------+---------+--------------------...-+\nSELECT 3 rows in set (... sec)\n\n\nTo restore a table from a snapshot we have to drop it beforehand:\n\ncr> DROP TABLE quotes;\nDROP OK, 1 row affected (... sec)\n\n\nRestoring a snapshot using the RESTORE SNAPSHOT statement.:\n\ncr> RESTORE SNAPSHOT where_my_snapshots_go.snapshot2\n... TABLE quotes\n... WITH (wait_for_completion=true);\nRESTORE OK, 1 row affected (... sec)\n\n\nIn this case only the quotes table from snapshot where_my_snapshots_go.snapshot2 is restored.\n\nIt’s not possible to restore tables that exist in the current cluster:\n\ncr> RESTORE SNAPSHOT where_my_snapshots_go.snapshot2 TABLE quotes;\nRelationAlreadyExists[Relation 'doc.quotes' already exists.]\n\n\nSingle partitions can be either imported into an existing partitioned table the partition belongs to.\n\nTo monitor the progress of RESTORE SNAPSHOT operations please query the sys.snapshot_restore table.\n\ncr> RESTORE SNAPSHOT where_my_snapshots_go.snapshot3 TABLE\n...    parted_table PARTITION (date='1970-01-01')\n... WITH (wait_for_completion=true);\nRESTORE OK, 1 row affected (... sec)\n\n\nOr if no matching partition table exists, it will be implicitly created during restore.\n\nCaution\n\nThis is only possible with CrateDB version 0.55.5 or greater!\n\nSnapshots of single partitions that have been created with earlier versions of CrateDB may be restored, but lead to orphaned partitions!\n\nWhen using CrateDB prior to 0.55.5 you will have to create the table schema first before restoring.\n\ncr> RESTORE SNAPSHOT where_my_snapshots_go.snapshot3 TABLE\n...    parted_table PARTITION (date=0)\n... WITH (wait_for_completion=true);\nRESTORE OK, 1 row affected (... sec)\n\nRestore data granularity\n\nYou are not limited to only being able to restore individual tables (or table partitions). For example:\n\nYou can use ALL instead of listing all tables to restore the whole snapshot, including all metadata and settings.\n\nYou can use TABLES to restore all tables but no metadata or settings. On the other hand, you can use METADATA to restore everything but tables.\n\nYou can use USERMANAGEMENT to restore database users, roles and their privileges.\n\nSee the RESTORE SNAPSHOT documentation for all possible options.\n\nCleanup\nDropping snapshots\n\nDropping a snapshot deletes all files inside the repository that are only referenced by this snapshot. Due to its incremental nature this might be very few files (e.g. for intermediate snapshots). Snapshots are dropped using the DROP SNAPSHOT command:\n\ncr> DROP SNAPSHOT where_my_snapshots_go.snapshot3;\nDROP OK, 1 row affected (... sec)\n\nDropping repositories\n\nIf a repository is not needed anymore, it can be dropped using the DROP REPOSITORY statement:\n\ncr> DROP REPOSITORY \"OldRepository\";\nDROP OK, 1 row affected (... sec)\n\n\nThis statement, like CREATE REPOSITORY, does not manipulate repository contents but only deletes stored configuration for this repository in the cluster state, so it’s not accessible any more."
  },
  {
    "title": "Logical replication — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/logical-replication.html",
    "html": "5.6\nLogical replication\n\nTable of contents\n\nPublication\n\nSubscription\n\nSecurity\n\nMonitoring\n\nLogical replication is a method of data replication across multiple clusters. CrateDB uses a publish and subscribe model where subscribers pull data from the publications of the publisher they subscribed to.\n\nReplicated tables on a subscriber can again be published further to other clusters and thus chaining subscriptions is possible.\n\nNote\n\nA replicated index on a subscriber is read-only.\n\nLogical replication is useful for the following use cases:\n\nConsolidating data from multiple clusters into a single one for aggregated reports.\n\nEnsure high availability if one cluster becomes unavailable.\n\nReplicating between different compatible versions of CrateDB. Replicating tables created on a cluster with higher major/minor version to a cluster with lower major/minor version is not supported.\n\nSee Also\n\nreplication.logical.ops_batch_size replication.logical.reads_poll_duration replication.logical.recovery.chunk_size replication.logical.recovery.max_concurrent_file_chunks\n\nPublication\n\nA publication is the upstream side of logical replication and it’s created on the cluster which acts as a data source.\n\nEach table can be added to multiple publications if needed. Publications can only contain tables. All operation types (INSERT, UPDATE, DELETE and schema changes) are replicated.\n\nEvery publication can have multiple subscribers.\n\nA publication is created using the CREATE PUBLICATION command. The individual tables can be added or removed dynamically using ALTER PUBLICATION. Publications can be removed using the DROP PUBLICATION command.\n\nCaution\n\nThe publishing cluster must have soft_deletes.enabled set to true so that a subscribing cluster can catch up with all changes made during replication pauses caused by network issues or explicitly done by a user.\n\nAlso, soft_deletes.retention_lease.period should be greater than or equal to replication.logical.reads_poll_duration.\n\nSubscription\n\nA subscription is the downstream side of logical replication. A subscription defines the connection to another database and set of publications to which it wants to subscribe. By default, the subscription creation triggers the replication process on the subscriber cluster. The subscriber cluster behaves in the same way as any other CrateDB cluster and can be used as a publisher for other clusters by defining its own publications.\n\nA cluster can have multiple subscriptions. It is also possible for a cluster to have both subscriptions and publications. A cluster cannot subscribe to locally already existing tables, therefore it is not possible to setup a bi-directional replication (both sides subscribing to ALL TABLES leads to a cluster trying to replicate its own tables from another cluster). However, two clusters still can cross-subscribe to each other if one cluster subscribes to locally non-existing tables of another cluster and vice versa.\n\nA subscription is added using the CREATE SUBSCRIPTION command and can be removed using the DROP SUBSCRIPTION command. A subscription starts replicating on its creation and stops on its removal (if no failure happen in-between).\n\nPublished tables must not exist on the subscriber. A cluster cannot subscribe to a table on another cluster if it exists already on its side, therefore it’s not possible to drop and re-create a subscription without starting from scratch i.e removing all replicated tables.\n\nOnly regular tables (including partitions) may be the target of a replication. For example, you can not replicate system tables or views.\n\nThe tables are matched between the publisher and the subscriber using the fully qualified table name. Replication to differently-named tables on the subscriber is not supported.\n\nSecurity\n\nTo create, alter or drop a publication, a user must have the AL privilege on the cluster. Only the owner (the user who created the publication) or a superuser is allowed to ALTER or DROP a publication. To add tables to a publication, the user must have DQL, DML, and DDL privileges on the table. When a user creates a publication that publishes all tables automatically, only those tables where the user has DQL, DML, and DDL privileges will be published. The user a subscriber uses to connect to the publisher must have DQL privileges on the published tables. Tables, included into a publication but not available for a subscriber due to lack of DQL privilege, will not be replicated.\n\nTo create or drop a subscription, a user must have the AL privilege on the cluster. Only the owner (the user who created the subscription) or a superuser is allowed to DROP a subscription.\n\nCaution\n\nA network setup that allows the two clusters to communicate is a pre-requisite for a working publication/subscription setup. See HBA.\n\nMonitoring\n\nAll publications are listed in the pg_publication table. More details for a publication are available in the pg_publication_tables table. It lists the replicated tables for a specific publication.\n\nAll subscriptions are listed in the pg_subscription table. More details for a subscription are available in the pg_subscription_rel table. The table contains detailed information about the replication state per table, including error messages if there was an error."
  },
  {
    "title": "Secured communications (SSL/TLS) — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/ssl.html",
    "html": "5.6\nSecured communications (SSL/TLS)\n\nYou can encrypt the internal communication between CrateDB nodes and the external communication with HTTP and PostgreSQL clients. When you configure encryption, CrateDB secures connections using Transport Layer Security (TLS).\n\nYou can enable SSL on a per-protocol basis:\n\nIf you enable SSL for HTTP, all connections will require HTTPS.\n\nBy default, if you enable SSL for the PostgreSQL wire protocol, clients can negotiate on a per-connection basis whether to use SSL. However, you can enforce SSL via Host-Based Authentication.\n\nIf you enable SSL for the CrateDB transport protocol (used for intra-node communication), nodes only accept SSL connections (ssl.transport.mode set to on).\n\nTip\n\nYou can use on SSL mode to configure a multi-zone cluster to ensure encryption for nodes communicating between zones. Please note, that SSL has to be on in all nodes as communication is point-2-point, and intra-zone communication will also be encrypted.\n\nTable of contents\n\nSSL/TLS configuration\n\nConfiguring the Keystore\n\nConfiguring a separate Truststore\n\nConnecting to a CrateDB node using HTTPS\n\nConnect to a CrateDB node using the Admin UI\n\nConnect to a CrateDB node using Crash\n\nConnect to a CrateDB node using REST\n\nConnecting to a CrateDB node using the PostgreSQL wire protocol with SSL/TLS\n\nConnect to a CrateDB node using JDBC\n\nConnect to a CrateDB node using psql\n\nSetting up a Keystore/Truststore with a certificate chain\n\nGenerate Keystore with a private key\n\nGenerate a certificate signing request\n\nOptional: Use a self-signed certificate to act as a Certificate Authority (CA)\n\nGenerate a self-signed certificate\n\nGenerate a signed cert\n\nImport the CA certificate into the Keystore\n\nImport CA into Truststore\n\nImport the signed certificate\n\nConfiguring CrateDB\n\nSSL/TLS configuration\n\nTo enable SSL a keystore and a few configuration changes are necessary. These changes need to be made in the crate.yml file on each node that should have secure communications enabled.\n\nSkip to Generate Keystore with a private key for a step-by-step instruction on how to create a keystore.\n\nOnce the keystore (and optional truststore) is created, continue with the following steps:\n\nSet ssl.psql.enabled or ssl.http.enabled to true.\n\nSet ssl.transport.mode to on.\n\nConfiguring the Keystore\n\n(Optional) Configuring a separate Truststore\n\nNote\n\nCrateDB monitors SSL files such as keystore and truststore that are configured as values of the node settings. If any of these files are updated CrateDB dynamically reloads them. The polling frequency of the files is set via the ssl.resource_poll_interval setting.\n\nConfiguring the Keystore\n\nSSL/TLS needs a keystore. The keystore holds the node certificate(s) which should be signed by a certificate authority (CA). A third-party CA or your organization’s existing CA can be used.\n\nWhen a client connects to a node using SSL/TLS, the client receives the certificate provided by the node and will determine if the node’s certificate is valid, trusted, and matches the hostname or IP address it is trying to connect to.\n\nCaution\n\nTechnically, it’s possible to disable CA checks for certificates on the client. It is strongly recommended however to use certificates signed by an official CA or by a private CA (company PKI) that is also known to the client. This will help to ensure that establishing trust is as painless as possible.\n\nSee Generate Keystore with a private key for information about how to create a keystore.\n\nOnce the keystore is prepared, define the absolute file path to the keystore .jks file on the node using ssl.keystore_filepath setting.\n\nNote\n\nMake sure that the keystore file has the right permissions and is accessible by the system user crate.\n\nAlso, define the password needed to decrypt the keystore by using the ssl.keystore_password setting.\n\nUse ssl.keystore_key_password setting to define the key password used when creating the Keystore.\n\nFor a full list of the settings needed to configure SSL/TLS, refer to SSL configuration reference.\n\nConfiguring a separate Truststore\n\nTrusted CA certificates can be stored in a node’s keystore or a separate truststore can be used to store them.\n\nIf you want to use a separate truststore, create a node truststore and import the CA certificate(s) you want to trust. Once the truststore is prepared, define the absolute file path of the truststore .jks file on the node using the ssl.truststore_filepath setting.\n\nNote\n\nMake sure that the truststore file has the right permissions and is accessible by the system user crate.\n\nAlso define the password needed to decrypt the keystore by using the ssl.truststore_password setting.\n\nFor a full list of the settings needed to configure SSL/TLS, refer to SSL configuration reference.\n\nConnecting to a CrateDB node using HTTPS\nConnect to a CrateDB node using the Admin UI\n\nCrate’s HTTP endpoint remains unchanged. When you have turned on secure communication, it will use HTTPS instead of plain HTTP. Simply point your browser to the same URL you used before but changing the protocol to HTTPS:\n\nFor example, https://localhost:4200 becomes https://localhost:4200. If you have not configured the CrateDB node’s keystore with a signed certificate from a Certificate Authority (CA), then you will get something like the following: NET::ERR_CERT_AUTHORITY_INVALID. You either need to get your certificate signed from one of the CAs included in your browser or import your owned certificates into the browser. A third option is storing an exception for the CrateDB node certification in your browser after verifying that this is indeed a certificate you trust.\n\nConnect to a CrateDB node using Crash\n\nYou can connect to a CrateDB node using a secure communication:\n\ncrash --hosts https://localhost:4200\n\n\nTo validate the provided certificates, please see the options --verify-ssl and --key-file.\n\nConnect to a CrateDB node using REST\n\nIssue your REST requests to the node using the https:// protocol. You may have to configure your client to validate the received certificate accordingly.\n\nConnecting to a CrateDB node using the PostgreSQL wire protocol with SSL/TLS\nConnect to a CrateDB node using JDBC\n\nJDBC needs to validate the CrateDB node’s identity by checking that the node certificate is signed by a trusted authority. If the certificate is signed by a certificate authority (CA) that is known to the Java runtime, there is nothing further to do (as Java comes with copies of the most common CA’s certificates).\n\nIf you have a certificate that is signed by a CA not known to the Java runtime, you need to configure a truststore which contains the node’s certificate and provide the path to the truststore file along with the password when starting your Java application:\n\njava -Djavax.net.ssl.trustStore=mystore -Djavax.net.ssl.trustStorePassword=mypassword com.mycompany.MyApp\n\n\nIn case you face any issues extra debugging information is available by adding -Djavax.net.debug=ssl to your command line.\n\nLast but not least, the connection parameter ssl=true must be added to the connection URL so that the JDBC driver will try and establish an SSL connection.\n\nFor further information, visit JDBC SSL documentation.\n\nConnect to a CrateDB node using psql\n\nBy default, psql attempts to use SSL if available on the node. For further information including the different SSL modes please visit the PSQL documentation.\n\nSetting up a Keystore/Truststore with a certificate chain\n\nIn case you need to setup a Keystore or a Truststore, here are the commands to get you started. All the commands use a validity of 36500 days (about 100 years). You might want to use less.\n\nGenerate Keystore with a private key\n\nThe first step is to create a Keystore with a private key using the RSA algorithm. The “first and last name” is the common name (CN) which should overlap with the URL the service it is used with.\n\nCommand:\n\nkeytool -keystore keystore -genkey -keyalg RSA -alias server -validity 36500\n\n\nOutput:\n\nEnter keystore password:\nRe-enter new password:\nWhat is your first and last name?\n  [Unknown]:  ssl.crate.io\nWhat is the name of your organizational unit?\n  [Unknown]:  Cryptography Department\nWhat is the name of your organization?\n  [Unknown]:  Crate.io GmbH\nWhat is the name of your City or Locality?\n  [Unknown]:  Berlin\nWhat is the name of your State or Province?\n  [Unknown]:  Berlin\nWhat is the two-letter country code for this unit?\n  [Unknown]:  DE\nIs CN=ssl.crate.io, OU=Cryptography Department, O=Crate.io GmbH, L=Berlin, ST=Berlin, C=DE correct?\n  [no]:  yes\n\nEnter key password for <server>\n    (RETURN if same as keystore password):\nRe-enter new password:\n\nGenerate a certificate signing request\n\nTo establish trust for this key, we need to sign it. This is done by generating a certificate signing request.\n\nIf you have access to a certificate authority (CA), you can skip the next steps and get the signed certificate from the CA using the signing request which we will generate with the command below. If you don’t have access to a CA, then follow the optional steps after this step to establish your own CA.\n\nCommand:\n\nkeytool -keystore keystore -certreq -alias server -keyalg RSA -file server.csr\n\n\nOutput:\n\nEnter keystore password:\nEnter key password for <server>\n\nOptional: Use a self-signed certificate to act as a Certificate Authority (CA)\n\nNote\n\nOnly follow these optional steps if you want to create your own Certificate Authority (CA). Otherwise, please request a signed certificate from one of the CAs bundled with Java.\n\nGenerate a self-signed certificate\n\nIf you don’t get your certificate signed from one of the official CAs, you might want to create your own CA with a self-signed certificate. The common name (CN) should overlap with the CN of the server key generated in the first step. For example, ssl.crate.io overlaps with *.crate.io.\n\nNote\n\nIn this step by step guide it is shown how to create a server certificate. If you want to create a client certificate the steps are almost the same with the exception of providing a common name that is equivalent to the crate username as described in client certificate authentication method.\n\nCommand:\n\nopenssl req -x509 -sha256 -nodes -days 36500 -newkey rsa:2048 \\\n    -keyout rootCA.key -out rootCA.crt\n\n\nOutput:\n\nGenerating a 2048 bit RSA private key\n.......................................................................+++\n.............................................................+++\nwriting new private key to 'rootCA.key'\n-----\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n-----\nCountry Name (2 letter code) [AU]:AT\nState or Province Name (full name) [Some-State]:Vorarlberg\nLocality Name (eg, city) []:Dornbirn\nOrganization Name (eg, company) [Internet Widgits Pty Ltd]:Crate.io\nOrganizational Unit Name (eg, section) []:Cryptography Department\nCommon Name (e.g. server FQDN or YOUR name) []:*.crate.io\nEmail Address []:info@crate.io\n\nGenerate a signed cert\n\nIn order that the server can prove itself to have a valid and trusted domain it is required that the server certificate contains subjectAltName.\n\nCreate a file called ssl.ext with the following content. In section [alt_names] list valid domain names of the server:\n\nauthorityKeyIdentifier=keyid,issuer\nbasicConstraints=CA:FALSE\nkeyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment\nsubjectAltName = @alt_names\n\n[alt_names]\nDNS.1 = www.example.com\n\n\nNow you can generate a signed cert from our certificate signing request.\n\nCommand:\n\nopenssl x509 -req -in server.csr -CA rootCA.crt -CAkey rootCA.key \\\n    -CAcreateserial -out server.crt -sha256 -days 36500 -extfile ssl.ext\n\n\nOutput:\n\nSignature ok\nsubject=/C=DE/ST=Berlin/L=Berlin/O=Crate.io GmbH/OU=Cryptography Department/CN=ssl.crate.io\nGetting CA Private Key\n\nImport the CA certificate into the Keystore\n\nThe CA needs to be imported to the Keystore for the certificate chain to be available when we import our signed certificate.\n\nCommand:\n\nkeytool -import -keystore keystore -file rootCA.crt -alias theCARoot\n\n\nOutput:\n\nEnter keystore password:\nOwner: EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT\nIssuer: EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT\nSerial number: f13562ec6184401e\nValid from: Mon Jun 12 13:09:17 CEST 2017 until: Wed May 19 13:09:17 CEST 2117\nCertificate fingerprints:\n     MD5:  BB:A1:79:53:FE:71:EC:61:2A:19:81:E8:0E:E8:C9:81\n     SHA1: 96:66:C1:01:49:17:D1:19:FB:DB:83:86:50:3D:3D:AD:DA:F7:C6:A9\n     SHA256: 69:82:C5:24:9A:A1:AE:DF:80:29:7A:26:92:C1:A5:9F:AF:7D:03:56:CC:C3:E9:73:3B:FD:85:66:35:D6:8A:9B\n     Signature algorithm name: SHA256withRSA\n     Version: 3\n\nExtensions:\n\n#1: ObjectId: 2.5.29.35 Criticality=false\nAuthorityKeyIdentifier [\nKeyIdentifier [\n0000: CD 29 4E 07 3D C3 7C D0   16 45 FB 0A CE 8D B4 98  .)N.=....E......\n0010: B7 A8 4C 79                                        ..Ly\n]\n[EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT]\nSerialNumber: [    f13562ec 6184401e]\n]\n\n#2: ObjectId: 2.5.29.19 Criticality=false\nBasicConstraints:[\n  CA:true\n  PathLen:2147483647\n]\n\n#3: ObjectId: 2.5.29.14 Criticality=false\nSubjectKeyIdentifier [\nKeyIdentifier [\n0000: CD 29 4E 07 3D C3 7C D0   16 45 FB 0A CE 8D B4 98  .)N.=....E......\n0010: B7 A8 4C 79                                        ..Ly\n]\n]\n\nTrust this certificate? [no]:  yes\nCertificate was added to keystore\n\nImport CA into Truststore\n\nIf we are using our own CA, we should also import the certificate to the Truststore, such that it is available for clients which want to verify signatures.\n\nCommand:\n\nkeytool -import -keystore truststore -file rootCA.crt -alias theCARoot\n\n\nOutput:\n\nEnter keystore password:\nRe-enter new password:\nOwner: EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT\nIssuer: EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT\nSerial number: f13562ec6184401e\nValid from: Mon Jun 12 13:09:17 CEST 2017 until: Wed May 19 13:09:17 CEST 2117\nCertificate fingerprints:\n     MD5:  BB:A1:79:53:FE:71:EC:61:2A:19:81:E8:0E:E8:C9:81\n     SHA1: 96:66:C1:01:49:17:D1:19:FB:DB:83:86:50:3D:3D:AD:DA:F7:C6:A9\n     SHA256: 69:82:C5:24:9A:A1:AE:DF:80:29:7A:26:92:C1:A5:9F:AF:7D:03:56:CC:C3:E9:73:3B:FD:85:66:35:D6:8A:9B\n     Signature algorithm name: SHA256withRSA\n     Version: 3\n\nExtensions:\n\n#1: ObjectId: 2.5.29.35 Criticality=false\nAuthorityKeyIdentifier [\nKeyIdentifier [\n0000: CD 29 4E 07 3D C3 7C D0   16 45 FB 0A CE 8D B4 98  .)N.=....E......\n0010: B7 A8 4C 79                                        ..Ly\n]\n[EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT]\nSerialNumber: [    f13562ec 6184401e]\n]\n\n#2: ObjectId: 2.5.29.19 Criticality=false\nBasicConstraints:[\n  CA:true\n  PathLen:2147483647\n]\n\n#3: ObjectId: 2.5.29.14 Criticality=false\nSubjectKeyIdentifier [\nKeyIdentifier [\n0000: CD 29 4E 07 3D C3 7C D0   16 45 FB 0A CE 8D B4 98  .)N.=....E......\n0010: B7 A8 4C 79                                        ..Ly\n]\n]\n\nTrust this certificate? [no]:  yes\nCertificate was added to keystore\n\nImport the signed certificate\n\nNow we have a signed certificate, signed by either from a official CA or from our own CA. Let’s import it to the Keystore.\n\nCommand:\n\nkeytool -import -keystore keystore -file server.crt -alias server\n\n\nOutput:\n\nEnter keystore password:\nEnter key password for <server>\nCertificate reply was installed in keystore\n\nConfiguring CrateDB\n\nFinally, you want to supply the Keystore/Truststore in the CrateDB configuration, see Secured communications (SSL/TLS)."
  },
  {
    "title": "Optimization — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/optimization.html",
    "html": "5.6\nOptimization\n\nTable of contents\n\nIntroduction\n\nMultiple table optimization\n\nPartition optimization\n\nIntroduction\n\nIn CrateDB every table (or if partitioned every partition) consists of segments. When inserting/deleting/updating data new segments are created following as an append-only strategy, which gives the advantage of fast writes but on the other hand can result into a big number of segments. As the number of segments increases the read operations become slower since more segments need to be visited. Moreover each segment consumes file handles, memory and CPU. CrateDB solves this problem by merging segments automatically in the background. Small segments are merged into bigger segments, which, in turn, are merged into even bigger segments. Furthermore any deleted rows and documents are not copied to the new bigger segment during this process.\n\nIf required one or more tables or table partitions can be optimized explicitly in order to improve performance. A few parameters can also be configured for the optimization process, like the max number of segments you wish to have when optimization is completed, or if you only wish to merge segments with deleted data, etc. See OPTIMIZE for detailed description of parameters.\n\ncr> OPTIMIZE table locations;\nOPTIMIZE OK, 1 row affected (... sec)\n\n\nNote\n\nSystem tables cannot be optimized.\n\nMultiple table optimization\n\nIf needed, multiple tables can be defined comma-separated in a single SQL request. The result message is printed if the request on every given table is completed.\n\ncr> OPTIMIZE TABLE locations, parted_table;\nOPTIMIZE OK, 2 rows affected (... sec)\n\n\nNote\n\nIf one or more tables or partitions do not exist, none of the given tables/partitions are optimized and an error is returned. The error returns only the first non-existent table/partition.\n\nPartition optimization\n\nAdditionally it is possible to define a specific PARTITION of a partitioned table which should be optimized (see Partitioned tables).\n\nBy using the PARTITION clause in the optimize statement a separate request for a given partition can be performed. That means that only specific partitions of a partitioned table are optimized. For further details on how to create an optimize request on partitioned tables see the SQL syntax and its synopsis (see OPTIMIZE).\n\ncr> OPTIMIZE TABLE parted_table PARTITION (day='2014-04-08');\nOPTIMIZE OK, 1 row affected (... sec)\n\n\nIn case the PARTITION clause is omitted all partitions will be optimized. If a table has many partitions this should be avoided due to performance reasons."
  },
  {
    "title": "Privileges — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/privileges.html",
    "html": "5.6\nPrivileges\n\nTo execute statements, a user needs to have the required privileges.\n\nTable of contents\n\nIntroduction\n\nPrivilege Classes\n\nPrivilege types\n\nDQL\n\nDML\n\nDDL\n\nAL\n\nHierarchical inheritance of privileges\n\nBehavior of GRANT, DENY and REVOKE\n\nGRANT\n\nDENY\n\nREVOKE\n\nList privileges\n\nRoles inheritance\n\nIntroduction\n\nInheritance\n\nPrivileges resolution\n\nGRANT\n\nREVOKE\n\nIntroduction\n\nCrateDB has a superuser (crate) which has the privilege to do anything. The privileges of other users and roles have to be managed using the GRANT, DENY or REVOKE statements.\n\nThe privileges that can be granted, denied or revoked are:\n\nDQL\n\nDML\n\nDDL\n\nAL\n\nSkip to Privilege types for details.\n\nPrivilege Classes\n\nThe privileges can be granted on different classes:\n\nCLUSTER\n\nSCHEMA\n\nTABLE and VIEW\n\nSkip to Hierarchical inheritance of privileges for details.\n\nA user with AL on level CLUSTER can grant privileges they have themselves to other users or roles as well.\n\nPrivilege types\nDQL\n\nGranting Data Query Language (DQL) privilege to a user or role, indicates that this user/role is allowed to execute SELECT, SHOW, REFRESH and COPY TO statements, as well as using the available user-defined functions, on the object for which the privilege applies.\n\nDML\n\nGranting Data Manipulation Language (DML) privilege to a user or role, indicates that this user/role is allowed to execute INSERT, COPY FROM, UPDATE and DELETE statements, on the object for which the privilege applies.\n\nDDL\n\nGranting Data Definition Language (DDL) privilege to a user or role, indicates that this user/role is allowed to execute the following statements on objects for which the privilege applies:\n\nCREATE TABLE\n\nDROP TABLE\n\nCREATE VIEW\n\nDROP VIEW\n\nCREATE FUNCTION\n\nDROP FUNCTION\n\nCREATE REPOSITORY\n\nDROP REPOSITORY\n\nCREATE SNAPSHOT\n\nDROP SNAPSHOT\n\nRESTORE SNAPSHOT\n\nALTER TABLE\n\nAL\n\nGranting Administration Language (AL) privilege to a user or role, enables the user/role to execute the following statements:\n\nCREATE USER/ROLE\n\nDROP USER/ROLE\n\nSET GLOBAL\n\nAll statements enabled via the AL privilege operate on a cluster level. So granting this on a schema or table level will have no effect.\n\nHierarchical inheritance of privileges\n\nPrivileges can be managed on three different levels, namely: CLUSTER, SCHEMA, and TABLE/VIEW.\n\nWhen a privilege is assigned on a certain level, the privilege will propagate down the hierarchy. Privileges defined on a lower level will always override those from a higher level:\n\n  cluster\n    ||\n  schema\n   /  \\\ntable view\n\n\nThis statement will grant DQL privilege to user riley on all the tables and functions of the doc schema:\n\ncr> GRANT DQL ON SCHEMA doc TO riley;\nGRANT OK, 1 row affected (... sec)\n\n\nThis statement will deny DQL privilege to user riley on the doc schema table doc.accounting. However, riley will still have DQL privilege on all the other tables of the doc schema:\n\ncr> DENY DQL ON TABLE doc.accounting TO riley;\nDENY OK, 1 row affected (... sec)\n\n\nNote\n\nIn CrateDB, schemas are just namespaces that are created and dropped implicitly. Therefore, when GRANT, DENY or REVOKE are invoked on a schema level, CrateDB takes the schema name provided without further validation.\n\nPrivileges can be managed on all schemas and tables of the cluster, except the information_schema.\n\nViews are on the same hierarchy with tables, i.e. a privilege on a view is gained through a GRANT on either the view itself, the schema the view belongs to, or a cluster-wide privilege. Privileges on relations which are referenced in the view do not grant any privileges on the view itself. On the contrary, even if the user/role does not have any privileges on a view’s referenced relations but on the view itself, the user/role can still access the relations through the view. For example:\n\ncr> CREATE VIEW first_customer as SELECT * from doc.accounting ORDER BY id LIMIT 1\nCREATE OK, 1 row affected (... sec)\n\n\nPreviously we had issued a DENY for user riley on doc.accounting but we can still access it through the view because we have access to it through the doc schema:\n\ncr> SELECT id from first_customer;\n+----+\n| id |\n+----+\n|  1 |\n+----+\nSELECT 1 row in set (... sec)\n\n\nSee Also\n\nViews: Privileges\n\nBehavior of GRANT, DENY and REVOKE\n\nNote\n\nYou can only grant, deny, or revoke privileges for an existing user or role. You must first create a user/role and then configure privileges.\n\nGRANT\n\nTo grant a privilege to an existing user or role on the whole cluster, we use the GRANT SQL statement, for example:\n\ncr> GRANT DML TO wolfgang;\nGRANT OK, 1 row affected (... sec)\n\n\nDQL privilege can be granted on the sys schema to user wolfgang, like this:\n\ncr> GRANT DQL ON SCHEMA sys TO wolfgang;\nGRANT OK, 1 row affected (... sec)\n\n\nThe following statement will grant all privileges on table doc.books to user wolfgang:\n\ncr> GRANT ALL PRIVILEGES ON TABLE doc.books TO wolfgang;\nGRANT OK, 4 rows affected (... sec)\n\n\nUsing “ALL PRIVILEGES” is a shortcut to grant all the currently grantable privileges to a user or role.\n\nNote\n\nIf no schema is specified in the table ident, the table will be looked up in the current schema.\n\nIf a user/role with the name specified in the SQL statement does not exist the statement returns an error:\n\ncr> GRANT DQL TO layla;\nRoleUnknownException[Role 'layla' does not exist]\n\n\nTo grant ALL PRIVILEGES to user will on the cluster, we can use the following syntax:\n\ncr> GRANT ALL PRIVILEGES TO will;\nGRANT OK, 4 rows affected (... sec)\n\n\nUsing ALL PRIVILEGES is a shortcut to grant all the currently grantable privileges to a user or role, namely DQL, DML and DDL.\n\nPrivileges can be granted to multiple users/roles in the same statement, like so:\n\ncr> GRANT DDL ON TABLE doc.books TO wolfgang, will;\nGRANT OK, 1 row affected (... sec)\n\nDENY\n\nTo deny a privilege to an existing user or role on the whole cluster, use the DENY SQL statement, for example:\n\ncr> DENY DDL TO will;\nDENY OK, 1 row affected (... sec)\n\n\nDQL privilege can be denied on the sys schema to user wolfgang like this:\n\ncr> DENY DQL ON SCHEMA sys TO wolfgang;\nDENY OK, 1 row affected (... sec)\n\n\nThe following statement will deny DQL privilege on table doc.books to user wolfgang:\n\ncr> DENY DQL ON TABLE doc.books TO wolfgang;\nDENY OK, 1 row affected (... sec)\n\n\nDENY ALL or DENY ALL PRIVILEGES will deny all privileges to a user or role, on the cluster it can be used like this:\n\ncr> DENY ALL TO will;\nDENY OK, 3 rows affected (... sec)\n\nREVOKE\n\nTo revoke a privilege that was previously granted or denied to a user or role use the REVOKE SQL statement, for example the DQL privilege that was previously denied to user wolfgang on the sys schema, can be revoked like this:\n\ncr> REVOKE DQL ON SCHEMA sys FROM wolfgang;\nREVOKE OK, 1 row affected (... sec)\n\n\nThe privileges that were granted and denied to user wolfgang on doc.books can be revoked like this:\n\ncr> REVOKE ALL ON TABLE doc.books FROM wolfgang;\nREVOKE OK, 4 rows affected (... sec)\n\n\nThe privileges that were granted to user will on the cluster can be revoked like this:\n\ncr> REVOKE ALL FROM will;\nREVOKE OK, 4 rows affected (... sec)\n\n\nNote\n\nThe REVOKE statement can remove only privileges that have been granted or denied through the GRANT or DENY statements. If the privilege on a specific object was not explicitly granted, the REVOKE statement has no effect. The effect of the REVOKE statement will be reflected in the row count.\n\nNote\n\nWhen a privilege is revoked from a user or role, it can still be active for that user/role, if the user/role inherits it, from another role.\n\nList privileges\n\nCrateDB exposes the privileges of users and roles of the database through the sys.privileges system table.\n\nBy querying the sys.privileges table you can get all information regarding the existing privileges. E.g.:\n\ncr> SELECT * FROM sys.privileges order by grantee, class, ident;\n+---------+----------+---------+----------------+-------+------+\n| class   | grantee  | grantor | ident          | state | type |\n+---------+----------+---------+----------------+-------+------+\n| SCHEMA  | riley    | crate   | doc            | GRANT | DQL  |\n| TABLE   | riley    | crate   | doc.accounting | DENY  | DQL  |\n| TABLE   | will     | crate   | doc.books      | GRANT | DDL  |\n| CLUSTER | wolfgang | crate   | NULL           | GRANT | DML  |\n+---------+----------+---------+----------------+-------+------+\nSELECT 4 rows in set (... sec)\n\nRoles inheritance\nIntroduction\n\nYou can grant, or revoke roles for an existing user or role. This allows to group granted or denied privileges and inherit them to other users or roles.\n\nYou must first create usesr and roles and then grant roles to other roles or users. You can configure the privileges of each role before or after granting roles to other roles or users.\n\nNote\n\nRoles can be granted to other roles or users, but users (roles which can also login to the database) cannot be granted to other roles or users.\n\nNote\n\nSuperuser crate cannot be granted to other users or roles, and roles cannot be granted to it.\n\nInheritance\n\nThe inheritance can span multiple levels, so you can have role_a which is granted to role_b, which in turn is granted to role_c, and so on. Each role can be granted to multiple other roles and each role or user can be granted multiple other roles. Cycles cannot be created, for example:\n\ncr> GRANT role_a TO role_b;\nGRANT OK, 1 row affected (... sec)\n\ncr> GRANT role_b TO role_c;\nGRANT OK, 1 row affected (... sec)\n\ncr> GRANT role_c TO role_a;\nSQLParseException[Cannot grant role role_c to role_a, role_a is a parent role of role_c and a cycle will be created]\n\nPrivileges resolution\n\nWhen a user executes a statement, the privileges mechanism will check first if the user has been granted the required privileges, if not, it will check if the roles which this user has been granted have those privileges and if not, it will continue checking the roles granted to those parent roles of the user and so on. For example:\n\ncr> GRANT role_a TO role_b;\nGRANT OK, 1 row affected (... sec)\n\ncr> GRANT role_b TO role_c;\nGRANT OK, 1 row affected (... sec)\n\ncr> GRANT DQL ON TABLE sys.users TO role_a;\nGRANT OK, 1 row affected (... sec)\n\ncr> GRANT role_c TO john;\nGRANT OK, 1 row affected (... sec)\n\n\nUser john is able to query sys.users, as even though he lacks DQL privilege on the table, he is granted role_c which in turn is granted role_b which is granted role_a, and role has the DQL privilege on sys.users.\n\nKeep in mind that DENY has precedence over GRANT. If a role has been both granted and denied a privilege (directly or through role inheritance), then DENY will take effect. For example, GRANT is inherited from a role and DENY directly set on the user:\n\ncr> GRANT DQL ON TABLE sys.users TO role_a;\nGRANT OK, 1 row affected (... sec)\n\ncr> GRANT role_a TO john\nGRANT OK, 1 row affected (... sec)\n\ncr> DENY DQL ON TABLE sys.users TO john\nDENY OK, 1 row affected (... sec)\n\n\nUser john cannot query sys.users.\n\nAnother example with DENY in effect, inherited from a role:\n\ncr> GRANT DQL ON TABLE sys.users TO role_a;\nGRANT OK, 1 row affected (... sec)\n\ncr> DENY DQL ON TABLE sys.users TO role_b;\nDENY OK, 1 row affected (... sec)\n\ncr> GRANT role_a, role_b TO john;\nGRANT OK, 2 rows affected (... sec)\n\n\nUser john cannot query sys.users.\n\nGRANT\n\nTo grant an existing role to an existing user or role on the whole cluster, we use the GRANT SQL statement, for example:\n\ncr> GRANT role_dql TO wolfgang;\nGRANT OK, 1 row affected (... sec)\n\n\nDML privilege can be granted on the sys schema to role role_dml, so, by inheritance, to user wolfgang as well, like this:\n\ncr> GRANT DQL ON SCHEMA sys TO role_dql;\nGRANT OK, 1 row affected (... sec)\n\n\nThe following statements will grant all privileges on table doc.books to role role_all_on_books, and by inheritance to user wolfgang as well:\n\ncr> GRANT role_all_on_books TO wolfgang;\nGRANT OK, 1 row affected (... sec)\n\ncr> GRANT ALL PRIVILEGES ON TABLE doc.books TO role_all_on_books;\nGRANT OK, 4 rows affected (... sec)\n\n\nIf a role with the name specified in the SQL statement does not exist the statement returns an error:\n\ncr> GRANT DDL TO role_ddl;\nRoleUnknownException[Role 'role_ddl' does not exist]\n\n\nMultiple roles can be granted to multiple users/roles in the same statement, like so:\n\ncr> GRANT role_dql, role_all_on_books TO layla, will;\nGRANT OK, 4 rows affected (... sec)\n\n\nNotice that 4 rows affected is returned, as in total there are 2 users, will and layla and each of them is granted two roles: role_dql and role_all_on_books.\n\nREVOKE\n\nTo revoke a role that was previously granted to a user or role use the REVOKE SQL statement. For example role role_dql which was previously granted to users wolfgang,``layla`` and will, can be revoked like this:\n\ncr> REVOKE role_dql FROM wolfgang, layla, will;\nREVOKE OK, 3 rows affected (... sec)\n\n\nIf a privilege is revoked from a role which is granted to other roles or users, the privilege is automatically revoked also for those roles and users, for example if we revoke privileges on table doc.books from role_all_on_books:\n\ncr> REVOKE ALL PRIVILEGES ON TABLE doc.books FROM role_all_on_books;\nREVOKE OK, 4 rows affected (... sec)\n\n\nuser wolfgang, who is granted the role role_all_on_books, also looses those privileges.\n\nIf a user is granted the same privilege by inheriting two different roles, when revoking one of the roles, the user still keeps the privilege. For example if user john gets granted `role_dql and role_dml:\n\ncr> GRANT DQL TO role_dql;\nGRANT OK, 1 row affected (... sec)\n\ncr> GRANT DQL, DML TO role_dml;\nGRANT OK, 2 rows affected (... sec)\n\ncr> GRANT role_dql, role_dml TO john;\nGRANT OK, 2 rows affected (... sec)\n\n\nand then we revoke role_dql from john:\n\ncr> REVOKE role_dql FROM john;\nREVOKE OK, 1 row affected (... sec)\n\n\njohn still has DQL privilege since it inherits it from role_dml which is still granted to him."
  },
  {
    "title": "Enterprise Features — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/editions/enterprise.html",
    "html": "3.3\nEnterprise Features\n\nTable of Contents\n\nFeature List\n\nTrial\n\nFeature List\n\nThe default distribution of CrateDB contains the following enterprise features:\n\nUser Management: manage multiple database users\n\nPrivileges: configure user privileges\n\nAuthentication: manage your database with authentication, and more\n\nSystem information functions: CURRENT_USER, USER, SESSION_USER\n\nSupport for JavaScript in UDF: write user-defined functions in JavaScript\n\nJMX Monitoring: monitor your query stats with JMX\n\nMQTT Ingestion Source: ingest data using MQTT without any 3rd party tools\n\nhyperloglog_distinct: distinct count aggregation using the HyperLoglog++ algorithm\n\nfirst_value(arg): first_value window function\n\nlast_value(arg): last_value window function\n\nnth_value(arg, number): nth_value window function\n\nThe CrateDB admin UI: shards browser, monitoring overview, privileges browser\n\nNote\n\nIt is also possible to build a Community Edition which won’t contain these features but which can be used without licensing restrictions.\n\nTrial\n\nYou may evaluate CrateDB including all enterprise features with a trial license. This trial license is active by default and is limited to 3 nodes. If you require more than 3 nodes you must request an enterprise license and configure CrateDB using the SET LICENSE statement.\n\nCaution\n\nIf you exceed the 3 nodes limitation your cluster will stop accepting queries. The functionality will be limited to:\n\nSET LICENSE\n\nSELECT (information_schema and sys schemas only)\n\nDECOMMISSION <nodeId | nodeName>\n\nIf you wish to use CrateDB without an enterprise license and without the 3 nodes limitation, you can switch to the Community Edition."
  },
  {
    "title": "HTTP Endpoint — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/interfaces/http.html",
    "html": "3.3\nHTTP Endpoint\n\nTable of Contents\n\nIntroduction\n\nParameter Substitution\n\nDefault Schema\n\nColumn Types\n\nBulk Operations\n\nError Handling\n\nBulk Errors\n\nIntroduction\n\nCrateDB provides a HTTP Endpoint that can be used to submit SQL queries. The endpoint is accessible under <servername:port>/_sql.\n\nSQL statements are sent to the _sql endpoint in json format, whereby the statement is sent as value associated to the key stmt.\n\nSee Also\n\nData Manipulation\n\nA simple SELECT statement can be submitted like this:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' \\\n... -d '{\"stmt\":\"select name, position from locations order by id limit 2\"}'\n{\n  \"cols\": [\n    \"name\",\n    \"position\"\n  ],\n  \"rows\": [\n    [\n      \"North West Ripple\",\n      1\n    ],\n    [\n      \"Arkintoofle Minor\",\n      3\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nNote\n\nWe’re using a simple command line invokation of curl here so you can see how to run this by hand in the terminal. For the rest of the examples in this document, we use here documents (i.e. EOF) for multiline readability.\n\nParameter Substitution\n\nIn addition to the stmt key the request body may also contain an args key which can be used for SQL parameter substitution.\n\nThe SQL statement has to be changed to use placeholders where the values should be inserted. Placeholders can either be numbered (in the form of $1, $2, etc.) or unnumbered using a question mark ?.\n\nThe placeholders will then be substituted with values from an array that is expected under the args key:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\":\n...     \"select date,position from locations\n...     where date <= \\$1 and position < \\$2 order by position\",\n...   \"args\": [\"1979-10-12\", 3]\n... }\n... EOF\n{\n  \"cols\": [\n    \"date\",\n    \"position\"\n  ],\n  \"rows\": [\n    [\n      308534400000,\n      1\n    ],\n    [\n      308534400000,\n      2\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nNote\n\nIn this example the placeholders start with an backslash due to shell escaping.\n\nWarning\n\nParameter substitution must not be used within subscript notation.\n\nFor example, column[?] is not allowed.\n\nThe same query using question marks as placeholders looks like this:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\":\n...     \"select date,position from locations\n...     where date <= ? and position < ? order by position\",\n...   \"args\": [\"1979-10-12\", 3]\n... }\n... EOF\n{\n  \"cols\": [\n    \"date\",\n    \"position\"\n  ],\n  \"rows\": [\n    [\n      308534400000,\n      1\n    ],\n    [\n      308534400000,\n      2\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nNote\n\nWith some queries the row count is not ascertainable. In this cases rowcount is -1.\n\nDefault Schema\n\nIt is possible to set a default schema while querying the CrateDB cluster via _sql end point. In such case the HTTP request should contain the Default-Schema header with the specified schema name:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' \\\n... -H 'Default-Schema: doc' -d@- <<- EOF\n... {\n...   \"stmt\":\"select name, position from locations order by id limit 2\"\n... }\n... EOF\n{\n  \"cols\": [\n    \"name\",\n    \"position\"\n  ],\n  \"rows\": [\n    [\n      \"North West Ripple\",\n      1\n    ],\n    [\n      \"Arkintoofle Minor\",\n      3\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nIf the schema name is not specified in the header, the default doc schema will be used instead.\n\nColumn Types\n\nCrateDB can respond a list col_types with the data type ID of every responded column. This way one can know what exact data type a column is holding.\n\nIn order to get the list of column data types, a types query parameter must be passed to the request:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql?types' -d@- <<- EOF\n... {\n...   \"stmt\":\n...     \"select date, position from locations\n...      where date <= \\$1 and position < \\$2 order by position\",\n...   \"args\": [\"1979-10-12\", 3]\n... }\n... EOF\n{\n  \"cols\": [\n    \"date\",\n    \"position\"\n  ],\n  \"col_types\": [\n    11,\n    9\n  ],\n  \"rows\": [\n    [\n      308534400000,\n      1\n    ],\n    [\n      308534400000,\n      2\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nCollection data types like Set or Array are displayed as a list where the first value is the collection type and the second is the inner type. Of course the inner type could also be a collection.\n\nExample of JSON representation of a column list of (String, Set<Integer[]>):\n\n\"column_types\": [ 4, [ 101, [ 100, 9 ] ] ]\n\n\nIDs of all currently available data types:\n\nID\n\n\t\n\nData Type\n\n\n\n\n0\n\n\t\n\nNull\n\n\n\n\n1\n\n\t\n\nNot Supported\n\n\n\n\n2\n\n\t\n\nByte\n\n\n\n\n3\n\n\t\n\nBoolean\n\n\n\n\n4\n\n\t\n\nString\n\n\n\n\n5\n\n\t\n\nIp\n\n\n\n\n6\n\n\t\n\nDouble\n\n\n\n\n7\n\n\t\n\nFloat\n\n\n\n\n8\n\n\t\n\nShort\n\n\n\n\n9\n\n\t\n\nInteger\n\n\n\n\n10\n\n\t\n\nLong\n\n\n\n\n11\n\n\t\n\nTimestamp\n\n\n\n\n12\n\n\t\n\nObject\n\n\n\n\n13\n\n\t\n\nGeoPoint (Double[])\n\n\n\n\n14\n\n\t\n\nGeoShape\n\n\n\n\n100\n\n\t\n\nArray\n\n\n\n\n101\n\n\t\n\nSet\n\nBulk Operations\n\nThe REST endpoint allows to issue bulk operations which are executed as single calls on the back-end site. It can be compared to prepared statement.\n\nA bulk operation can be expressed simply as an SQL statement.\n\nSupported bulk SQL statements are:\n\nInsert\n\nUpdate\n\nDelete\n\nInstead of the args (Parameter Substitution) key, use the key bulk_args. This allows to specify a list of lists, containing all the records which shall be processed. The inner lists need to match the specified columns.\n\nThe bulk response contains a results array, with a rowcount for each bulk operation. Those results are in the same order as the issued operations of the bulk operation.\n\nThe following example describes how to issue an insert bulk operation and insert three records at once:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\": \"INSERT INTO locations (id, name, kind, description)\n...           VALUES (?, ?, ?, ?)\",\n...   \"bulk_args\": [\n...     [1337, \"Earth\", \"Planet\", \"An awesome place to spend some time on.\"],\n...     [1338, \"Sun\", \"Star\", \"An extraordinarily hot place.\"],\n...     [1339, \"Titan\", \"Moon\", \"Titan, where it rains fossil fuels.\"]\n...   ]\n... }\n... EOF\n{\n  \"cols\": [],\n  \"duration\": ...,\n  \"results\": [\n    {\n      \"rowcount\": 1\n    },\n    {\n      \"rowcount\": 1\n    },\n    {\n      \"rowcount\": 1\n    }\n  ]\n}\n\nError Handling\n\nQueries that are invalid or cannot be satisfied will result in an error response. The response will contain an error code, an error message and in some cases additional arguments that are specific to the error code.\n\nClient libraries should use the error code to translate the error into an appropriate exception:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\":\"select name, position from foo.locations\"\n... }\n... EOF\n{\n  \"error\": {\n    \"message\": \"SQLActionException[SchemaUnknownException: Schema 'foo' unknown]\",\n    \"code\": 4045\n  }\n}\n\n\nTo get more insight into what exactly went wrong an additional error_trace GET parameter can be specified to return the stack trace:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql?error_trace=true' -d@- <<- EOF\n... {\n...   \"stmt\":\"select name, position from foo.locations\"\n... }\n... EOF\n{\n  \"error\": {\n    \"message\": \"SQLActionException[SchemaUnknownException: Schema 'foo' unknown]\",\n    \"code\": 4045\n  },\n  \"error_trace\": \"...\"\n}\n\n\nNote\n\nThis parameter is intended for CrateDB developers or for users requesting support for CrateDB. Client libraries shouldn’t make use of this option and not include the stacktrace.\n\nCurrently the defined error codes are:\n\nCode\n\n\t\n\nError\n\n\n\n\n4000\n\n\t\n\nThe statement contains an invalid syntax or unsupported SQL statement\n\n\n\n\n4001\n\n\t\n\nThe statement contains an invalid analyzer definition.\n\n\n\n\n4002\n\n\t\n\nThe name of the relation is invalid.\n\n\n\n\n4003\n\n\t\n\nField type validation failed\n\n\n\n\n4004\n\n\t\n\nPossible feature not supported (yet)\n\n\n\n\n4005\n\n\t\n\nAlter table using a table alias is not supported.\n\n\n\n\n4006\n\n\t\n\nThe used column alias is ambiguous.\n\n\n\n\n4007\n\n\t\n\nThe operation is not supported on this relation, as it is not accessible.\n\n\n\n\n4008\n\n\t\n\nThe name of the column is invalid.\n\n\n\n\n4009\n\n\t\n\nCrateDB License is expired.\n\n\n\n\n4010\n\n\t\n\nUser is not authorized to perform the SQL statement.\n\n\n\n\n4011\n\n\t\n\nMissing privilege for user.\n\n\n\n\n4031\n\n\t\n\nOnly read operations are allowed on this node.\n\n\n\n\n4041\n\n\t\n\nUnknown relation.\n\n\n\n\n4042\n\n\t\n\nUnknown analyzer.\n\n\n\n\n4043\n\n\t\n\nUnknown column.\n\n\n\n\n4044\n\n\t\n\nUnknown type.\n\n\n\n\n4045\n\n\t\n\nUnknown schema.\n\n\n\n\n4046\n\n\t\n\nUnknown Partition.\n\n\n\n\n4047\n\n\t\n\nUnknown Repository.\n\n\n\n\n4048\n\n\t\n\nUnknown Snapshot.\n\n\n\n\n4049\n\n\t\n\nUnknown user-defined function.\n\n\n\n\n40410\n\n\t\n\nUnknown user.\n\n\n\n\n4091\n\n\t\n\nA document with the same primary key exists already.\n\n\n\n\n4092\n\n\t\n\nA VersionConflict. Might be thrown if an attempt was made to update the same document concurrently.\n\n\n\n\n4093\n\n\t\n\nA relation with the same name exists already.\n\n\n\n\n4094\n\n\t\n\nThe used table alias contains tables with different schema.\n\n\n\n\n4095\n\n\t\n\nA repository with the same name exists already.\n\n\n\n\n4096\n\n\t\n\nA snapshot with the same name already exists in the repository.\n\n\n\n\n4097\n\n\t\n\nA partition for the same values already exists in this table.\n\n\n\n\n4098\n\n\t\n\nA user-defined function with the same signature already exists.\n\n\n\n\n4099\n\n\t\n\nA user with the same name already exists.\n\n\n\n\n5000\n\n\t\n\nUnhandled server error.\n\n\n\n\n5001\n\n\t\n\nThe execution of one or more tasks failed.\n\n\n\n\n5002\n\n\t\n\nOne or more shards are not available.\n\n\n\n\n5003\n\n\t\n\nThe query failed on one or more shards\n\n\n\n\n5004\n\n\t\n\nCreating a snapshot failed\n\n\n\n\n5030\n\n\t\n\nThe query was killed by a kill statement\n\nBulk Errors\n\nIf a bulk operation fails, the resulting rowcount will be -2 and the resulting object may contain an error_message depending on the resulting error:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\": \"INSERT into locations (name, id) values (?,?)\",\n...   \"bulk_args\": [\n...     [\"Mars\", 1341],\n...     [\"Sun\", 1341]\n...   ]\n... }\n... EOF\n{\n  \"cols\": [],\n  \"duration\": ...,\n  \"results\": [\n    {\n      \"rowcount\": 1\n    },\n    {\n      \"rowcount\": -2\n    }\n  ]\n}\n\n\nNote\n\nEvery bulk operation will be executed, independent if one of the operation fails."
  },
  {
    "title": "Users and roles management — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/user-management.html",
    "html": "5.6\nUsers and roles management\n\nUsers and roles account information is stored in the cluster metadata of CrateDB and supports the following statements to create, alter and drop users and roles:\n\nCREATE USER\n\nCREATE ROLE\n\nALTER USER or ALTER ROLE\n\nDROP USER or DROP ROLE\n\nThese statements are database management statements that can be invoked by superusers that already exist in the CrateDB cluster. The CREATE USER, CREATE ROLE, DROP USER and DROP ROLE statements can also be invoked by users with the AL privilege. ALTER USER or ALTER ROLE can be invoked by users to change their own password, without requiring any privilege.\n\nWhen CrateDB is started, the cluster contains one predefined superuser. This user is called crate. It is not possible to create any other superusers.\n\nThe definition of all users and roles, including hashes of their passwords, together with their privileges is backed up together with the cluster’s metadata when a snapshot is created, and it is restored when using the ALL, METADATA, or USERMANAGEMENT keywords with the:ref:sql-restore-snapshot command.\n\nTable of contents\n\nROLES\n\nCREATE ROLE\n\nALTER ROLE\n\nDROP ROLE\n\nList roles\n\nUSERS\n\nCREATE USER\n\nALTER USER\n\nDROP USER\n\nList users\n\nROLES\n\nRoles are entities that are not allowed to login, but can be assigned privileges and they can be granted to other roles, thus creating a role hierarchy, or directly to users. For example, a role myschema_dql_role can be granted with DQL privileges on schema myschema and afterwards the role can be granted to a user, which will automatically inherit those privileges from the myschema_dql_role. A role myschema_dml_role can be granted with DML privileges on schema myschema and can also be granted the role myschema_dql_role, thus gaining also DQL privileges. When myschema_dml_role is granted to a user, this user will automatically have both DQL and DML privileges on myschema.\n\nCREATE ROLE\n\nTo create a new role for the CrateDB database cluster use the CREATE ROLE SQL statement:\n\ncr> CREATE ROLE role_a;\nCREATE OK, 1 row affected (... sec)\n\n\nTip\n\nNewly created roles do not have any privileges. After creating a role, you should configure user privileges.\n\nFor example, to grant all privileges to the role_a user, run:\n\ncr> GRANT ALL PRIVILEGES TO role_a;\nGRANT OK, 4 rows affected (... sec)\n\n\nThe name parameter of the statement follows the principles of an identifier which means that it must be double-quoted if it contains special characters (e.g. whitespace) or if the case needs to be maintained:\n\ncr> CREATE ROLE \"Custom Role\";\nCREATE OK, 1 row affected (... sec)\n\n\nIf a role or user with the name specified in the SQL statement already exists the statement returns an error:\n\ncr> CREATE ROLE \"Custom Role\";\nRoleAlreadyExistsException[Role 'Custom Role' already exists]\n\nALTER ROLE\n\nALTER ROLE and ALTER USER SQL statements are not supported for roles, only for users.\n\nDROP ROLE\n\nTo remove an existing role from the CrateDB database cluster use the DROP ROLE or DROP USER SQL statement:\n\ncr> DROP ROLE role_c;\nDROP OK, 1 row affected (... sec)\n\ncr> DROP USER role_d;\nDROP OK, 1 row affected (... sec)\n\n\nIf a role with the name specified in the SQL statement does not exist, the statement returns an error:\n\ncr> DROP ROLE role_d;\nRoleUnknownException[Role 'role_d' does not exist]\n\nList roles\n\nCrateDB exposes database roles via the read-only Roles system table. The sys.roles table shows all roles in the cluster which can be used to group privileges.\n\nTo list all existing roles query the table:\n\ncr> SELECT name, granted_roles FROM sys.roles order by name;\n+--------+------------------------------------------+\n| name   | granted_roles                            |\n+--------+------------------------------------------+\n| role_a | []                                       |\n| role_b | [{\"grantor\": \"crate\", \"role\": \"role_c\"}] |\n| role_c | []                                       |\n+--------+------------------------------------------+\nSELECT 3 rows in set (... sec)\n\nUSERS\nCREATE USER\n\nTo create a new user for the CrateDB database cluster use the CREATE USER SQL statement:\n\ncr> CREATE USER user_a;\nCREATE OK, 1 row affected (... sec)\n\n\nTip\n\nNewly created users do not have any privileges. After creating a user, you should configure user privileges.\n\nFor example, to grant all privileges to the user_a user, run:\n\ncr> GRANT ALL PRIVILEGES TO user_a;\nGRANT OK, 4 rows affected (... sec)\n\n\nIt can be used to connect to the database cluster using available authentication methods. You can specify the user’s password in the WITH clause of the CREATE statement. This is required if you want to use the Password authentication method:\n\ncr> CREATE USER user_b WITH (password = 'a_secret_password');\nCREATE OK, 1 row affected (... sec)\n\n\nThe username parameter of the statement follows the principles of an identifier which means that it must be double-quoted if it contains special characters (e.g. whitespace) or if the case needs to be maintained:\n\ncr> CREATE USER \"Custom User\";\nCREATE OK, 1 row affected (... sec)\n\n\nIf a user with the username specified in the SQL statement already exists the statement returns an error:\n\ncr> CREATE USER \"Custom User\";\nRoleAlreadyExistsException[Role 'Custom User' already exists]\n\nALTER USER\n\nTo alter the password for an existing user from the CrateDB database cluster use the ALTER ROLE or ALTER USER SQL statements:\n\ncr> ALTER USER user_a SET (password = 'pass');\nALTER OK, 1 row affected (... sec)\n\n\nThe password can be reset (cleared) if specified as NULL:\n\ncr> ALTER USER user_a SET (password = NULL);\nALTER OK, 1 row affected (... sec)\n\n\nNote\n\nThe built-in superuser crate has no password and it is not possible to set a new password for this user.\n\nDROP USER\n\nTo remove an existing user from the CrateDB database cluster use the DROP ROLE or DROP USER SQL statements:\n\ncr> DROP USER user_c;\nDROP OK, 1 row affected (... sec)\n\ncr> DROP ROLE user_d;\nDROP OK, 1 row affected (... sec)\n\n\nIf a user with the username specified in the SQL statement does not exist the statement returns an error:\n\ncr> DROP USER user_d;\nRoleUnknownException[Role 'user_d' does not exist]\n\n\nNote\n\nIt is not possible to drop the built-in superuser crate.\n\nList users\n\nCrateDB exposes database users via the read-only Users system table. The sys.users table shows all users in the cluster which can be used for authentication. The initial superuser crate which is available for all CrateDB clusters is also part of that list.\n\nTo list all existing users query the table:\n\ncr> SELECT name, granted_roles, password, superuser FROM sys.users order by name;\n+--------+----------------------------------------------------------------------------------+----------+-----------+\n| name   | granted_roles                                                                    | password | superuser |\n+--------+----------------------------------------------------------------------------------+----------+-----------+\n| crate  | []                                                                               | NULL     | TRUE      |\n| user_a | [{\"grantor\": \"crate\", \"role\": \"role_a\"}, {\"grantor\": \"crate\", \"role\": \"role_b\"}] | NULL     | FALSE     |\n| user_b | []                                                                               | ******** | FALSE     |\n+--------+----------------------------------------------------------------------------------+----------+-----------+\nSELECT 3 rows in set (... sec)\n\n\nNote\n\nCrateDB also supports retrieving the current connected user using the system information functions: CURRENT_USER, USER and SESSION_USER."
  },
  {
    "title": "System information — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/system-information.html",
    "html": "5.6\nSystem information\n\nCrateDB provides the sys schema which contains virtual tables. These tables are read-only and can be queried to get statistical real-time information about the cluster, its nodes and their shards:\n\nTable of contents\n\nCluster\n\nCluster license\n\nlicense\n\nCluster settings\n\nNodes\n\nid\n\nname\n\nhostname\n\nrest_url\n\nattributes\n\nport\n\nload\n\nmem\n\nheap\n\nversion\n\ncluster_state_version\n\nfs\n\nthread_pools\n\nos\n\ncgroup limitations\n\nUptime limitations\n\nos_info\n\nnetwork\n\nconnections\n\nprocess\n\nNode checks\n\nAcknowledge failed checks\n\nDescription of checked node settings\n\nRecovery expected data nodes\n\nRecovery after data nodes\n\nRecovery after time\n\nRouting allocation disk watermark high\n\nRouting allocation disk watermark low\n\nMaximum shards per node\n\nShards\n\nTable schema\n\nExample\n\nSegments\n\nJobs, operations, and logs\n\nJobs\n\nTable schema\n\nJobs metrics\n\nsys.jobs_metrics Table schema\n\nClassification\n\nOperations\n\nTable schema\n\nLogs\n\nsys.jobs_log Table schema\n\nsys.operations_log Table schema\n\nCluster checks\n\nCurrent Checks\n\nNumber of partitions\n\nTables need to be recreated\n\nCrateDB table version compatibility scheme\n\nAvoiding reindex using partitioned tables\n\nHow to reindex\n\nLicense check\n\nHealth\n\nHealth definition\n\nRepositories\n\nSnapshots\n\nSnapshot Restore\n\nSummits\n\nUsers\n\nRoles\n\nPrivileges\n\nAllocations\n\nShard table permissions\n\nsys jobs tables permissions\n\npg_stats\n\npg_publication\n\npg_publication_tables\n\npg_subscription\n\npg_subscription_rel\n\nCluster\n\nBasic information about the CrateDB cluster can be retrieved from the sys.cluster table:\n\nName\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nA unique ID generated by the system.\n\n\t\n\nTEXT\n\n\n\n\nlicense\n\n\t\n\nThe current CrateDB license information. Always NULL. This exists for backward compatibility\n\n\t\n\nOBJECT\n\n\n\n\nname\n\n\t\n\nThe cluster name.\n\n\t\n\nTEXT\n\n\n\n\nmaster_node\n\n\t\n\nNode ID of the node which currently operates as master\n\n\t\n\nTEXT\n\n\n\n\nsettings\n\n\t\n\nThe cluster settings.\n\n\t\n\nOBJECT\n\nThe result has at most 1 row:\n\ncr> select name from sys.cluster;\n+-----------------+\n| name            |\n+-----------------+\n| Testing-CrateDB |\n+-----------------+\nSELECT 1 row in set (... sec)\n\nCluster license\n\nThe sys.cluster.license expression returns information about the currently registered license.\n\nNote\n\nLicenses were removed in CrateDB 4.5. Accordingly, these values are deprecated and return NULL in CrateDB 4.5 and higher.\n\nlicense\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nlicense\n\n\t\nThe current CrateDB license information\n\nor NULL on CrateDB CE.\n\n\t\n\nOBJECT\n\n\n\n\nlicense['expiry_date']\n\n\t\n\nThe Dates and times on which the license expires.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nlicense['issued_to']\n\n\t\n\nThe organisation for which the license is issued.\n\n\t\n\nTEXT\n\n\n\n\nlicense['max_nodes']\n\n\t\n\nThe maximum number of nodes the license is valid for.\n\n\t\n\nINTEGER\n\nCluster settings\n\nThe sys.cluster.settings expression returns information about the currently applied cluster settings.\n\ncr> select settings from sys.cluster;\n+-----------------------------------------------------...-+\n| settings                                                |\n+-----------------------------------------------------...-+\n| {\"bulk\": {...}, \"cluster\": {...}, \"gateway\": {...}, ... |\n+-----------------------------------------------------...-+\nSELECT 1 row in set (... sec)\n\ncr> select column_name, data_type from information_schema.columns\n... where column_name like 'settings%'\n... and table_name = 'cluster';\n+-----------------------------------------------------------------------------------+--------------+\n| column_name                                                                       | data_type    |\n+-----------------------------------------------------------------------------------+--------------+\n| settings                                                                          | object       |\n| settings['bulk']                                                                  | object       |\n| settings['bulk']['request_timeout']                                               | text         |\n| settings['cluster']                                                               | object       |\n| settings['cluster']['graceful_stop']                                              | object       |\n| settings['cluster']['graceful_stop']['force']                                     | boolean      |\n| settings['cluster']['graceful_stop']['min_availability']                          | text         |\n| settings['cluster']['graceful_stop']['timeout']                                   | text         |\n| settings['cluster']['info']                                                       | object       |\n| settings['cluster']['info']['update']                                             | object       |\n| settings['cluster']['info']['update']['interval']                                 | text         |\n| settings['cluster']['max_shards_per_node']                                        | integer      |\n| settings['cluster']['routing']                                                    | object       |\n| settings['cluster']['routing']['allocation']                                      | object       |\n| settings['cluster']['routing']['allocation']['allow_rebalance']                   | text         |\n| settings['cluster']['routing']['allocation']['balance']                           | object       |\n| settings['cluster']['routing']['allocation']['balance']['index']                  | real         |\n| settings['cluster']['routing']['allocation']['balance']['shard']                  | real         |\n| settings['cluster']['routing']['allocation']['balance']['threshold']              | real         |\n| settings['cluster']['routing']['allocation']['cluster_concurrent_rebalance']      | integer      |\n| settings['cluster']['routing']['allocation']['disk']                              | object       |\n| settings['cluster']['routing']['allocation']['disk']['threshold_enabled']         | boolean      |\n| settings['cluster']['routing']['allocation']['disk']['watermark']                 | object       |\n| settings['cluster']['routing']['allocation']['disk']['watermark']['flood_stage']  | text         |\n| settings['cluster']['routing']['allocation']['disk']['watermark']['high']         | text         |\n| settings['cluster']['routing']['allocation']['disk']['watermark']['low']          | text         |\n| settings['cluster']['routing']['allocation']['enable']                            | text         |\n| settings['cluster']['routing']['allocation']['exclude']                           | object       |\n| settings['cluster']['routing']['allocation']['exclude']['_host']                  | text         |\n| settings['cluster']['routing']['allocation']['exclude']['_id']                    | text         |\n| settings['cluster']['routing']['allocation']['exclude']['_ip']                    | text         |\n| settings['cluster']['routing']['allocation']['exclude']['_name']                  | text         |\n| settings['cluster']['routing']['allocation']['include']                           | object       |\n| settings['cluster']['routing']['allocation']['include']['_host']                  | text         |\n| settings['cluster']['routing']['allocation']['include']['_id']                    | text         |\n| settings['cluster']['routing']['allocation']['include']['_ip']                    | text         |\n| settings['cluster']['routing']['allocation']['include']['_name']                  | text         |\n| settings['cluster']['routing']['allocation']['node_concurrent_recoveries']        | integer      |\n| settings['cluster']['routing']['allocation']['node_initial_primaries_recoveries'] | integer      |\n| settings['cluster']['routing']['allocation']['require']                           | object       |\n| settings['cluster']['routing']['allocation']['require']['_host']                  | text         |\n| settings['cluster']['routing']['allocation']['require']['_id']                    | text         |\n| settings['cluster']['routing']['allocation']['require']['_ip']                    | text         |\n| settings['cluster']['routing']['allocation']['require']['_name']                  | text         |\n| settings['cluster']['routing']['allocation']['total_shards_per_node']             | integer      |\n| settings['cluster']['routing']['rebalance']                                       | object       |\n| settings['cluster']['routing']['rebalance']['enable']                             | text         |\n| settings['gateway']                                                               | object       |\n| settings['gateway']['expected_data_nodes']                                        | integer      |\n| settings['gateway']['expected_nodes']                                             | integer      |\n| settings['gateway']['recover_after_data_nodes']                                   | integer      |\n| settings['gateway']['recover_after_nodes']                                        | integer      |\n| settings['gateway']['recover_after_time']                                         | text         |\n| settings['indices']                                                               | object       |\n| settings['indices']['breaker']                                                    | object       |\n| settings['indices']['breaker']['query']                                           | object       |\n| settings['indices']['breaker']['query']['limit']                                  | text         |\n| settings['indices']['breaker']['request']                                         | object       |\n| settings['indices']['breaker']['request']['limit']                                | text         |\n| settings['indices']['breaker']['total']                                           | object       |\n| settings['indices']['breaker']['total']['limit']                                  | text         |\n| settings['indices']['recovery']                                                   | object       |\n| settings['indices']['recovery']['internal_action_long_timeout']                   | text         |\n| settings['indices']['recovery']['internal_action_timeout']                        | text         |\n| settings['indices']['recovery']['max_bytes_per_sec']                              | text         |\n| settings['indices']['recovery']['recovery_activity_timeout']                      | text         |\n| settings['indices']['recovery']['retry_delay_network']                            | text         |\n| settings['indices']['recovery']['retry_delay_state_sync']                         | text         |\n| settings['indices']['replication']                                                | object       |\n| settings['indices']['replication']['retry_timeout']                               | text         |\n| settings['logger']                                                                | object_array |\n| settings['logger']['level']                                                       | text_array   |\n| settings['logger']['name']                                                        | text_array   |\n| settings['memory']                                                                | object       |\n| settings['memory']['allocation']                                                  | object       |\n| settings['memory']['allocation']['type']                                          | text         |\n| settings['memory']['operation_limit']                                             | integer      |\n| settings['overload_protection']                                                   | object       |\n| settings['overload_protection']['dml']                                            | object       |\n| settings['overload_protection']['dml']['initial_concurrency']                     | integer      |\n| settings['overload_protection']['dml']['max_concurrency']                         | integer      |\n| settings['overload_protection']['dml']['min_concurrency']                         | integer      |\n| settings['overload_protection']['dml']['queue_size']                              | integer      |\n| settings['replication']                                                           | object       |\n| settings['replication']['logical']                                                | object       |\n| settings['replication']['logical']['ops_batch_size']                              | integer      |\n| settings['replication']['logical']['reads_poll_duration']                         | text         |\n| settings['replication']['logical']['recovery']                                    | object       |\n| settings['replication']['logical']['recovery']['chunk_size']                      | text         |\n| settings['replication']['logical']['recovery']['max_concurrent_file_chunks']      | integer      |\n| settings['statement_timeout']                                                     | text         |\n| settings['stats']                                                                 | object       |\n| settings['stats']['breaker']                                                      | object       |\n| settings['stats']['breaker']['log']                                               | object       |\n| settings['stats']['breaker']['log']['jobs']                                       | object       |\n| settings['stats']['breaker']['log']['jobs']['limit']                              | text         |\n| settings['stats']['breaker']['log']['operations']                                 | object       |\n| settings['stats']['breaker']['log']['operations']['limit']                        | text         |\n| settings['stats']['enabled']                                                      | boolean      |\n| settings['stats']['jobs_log_expiration']                                          | text         |\n| settings['stats']['jobs_log_filter']                                              | text         |\n| settings['stats']['jobs_log_persistent_filter']                                   | text         |\n| settings['stats']['jobs_log_size']                                                | integer      |\n| settings['stats']['operations_log_expiration']                                    | text         |\n| settings['stats']['operations_log_size']                                          | integer      |\n| settings['stats']['service']                                                      | object       |\n| settings['stats']['service']['interval']                                          | text         |\n| settings['stats']['service']['max_bytes_per_sec']                                 | text         |\n| settings['udc']                                                                   | object       |\n| settings['udc']['enabled']                                                        | boolean      |\n| settings['udc']['initial_delay']                                                  | text         |\n| settings['udc']['interval']                                                       | text         |\n| settings['udc']['url']                                                            | text         |\n+-----------------------------------------------------------------------------------+--------------+\nSELECT ... rows in set (... sec)\n\n\nFor further details, see the Cluster Settings configuration section.\n\nNodes\n\nTo get information about the nodes query for sys.nodes.\n\nThis table can be queried for one, multiple or all nodes within a cluster.\n\nThe table schema is as follows:\n\nid\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nA unique ID within the cluster generated by the system.\n\n\t\n\nTEXT\n\nname\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nname\n\n\t\n\nThe node name within a cluster. The system will choose a random name. You can also customize the node name, see Node-specific settings.\n\n\t\n\nTEXT\n\nhostname\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nhostname\n\n\t\n\nThe specified host name of the machine the node is running on.\n\n\t\n\nTEXT\n\nrest_url\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nrest_url\n\n\t\n\nFull HTTP(s) address where the REST API of the node is exposed, including schema, hostname (or IP) and port.\n\n\t\n\nTEXT\n\nattributes\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nattributes\n\n\t\n\nThe custom attributes set for the node, e.g. if node.attr.color is blue, and node.attr.location is east`, the value of this column would be: ``{color=blue, location=east}\n\n\t\n\nOBJECT\n\nport\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nport\n\n\t\n\nThe specified ports for both HTTP and binary transport interfaces. You can also customize the ports setting, see Ports.\n\n\t\n\nOBJECT\n\n\n\n\nport['http']\n\n\t\n\nCrateDB’s HTTP port.\n\n\t\n\nINTEGER\n\n\n\n\nport['transport']\n\n\t\n\nCrateDB’s binary transport port.\n\n\t\n\nINTEGER\n\n\n\n\nport['psql']\n\n\t\n\nThe PostgreSQL wire protocol port.\n\n\t\n\nINTEGER\n\nload\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nload\n\n\t\n\nSystem load statistics\n\n\t\n\nOBJECT\n\n\n\n\nload['1']\n\n\t\n\nAverage load over the last 1 minute.\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\nload['5']\n\n\t\n\nAverage load over the last 5 minutes.\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\nload['15']\n\n\t\n\nAverage load over the last 15 minutes.\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\nload['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the load probe.\n\n\t\n\nBIGINT\n\nmem\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nmem\n\n\t\n\nMemory utilization statistics of the host.\n\n\t\n\nOBJECT\n\n\n\n\nmem['used']\n\n\t\n\nCurrently used memory in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nmem['used_percent']\n\n\t\n\nCurrently used memory in percent of total.\n\n\t\n\nSMALLINT\n\n\n\n\nmem['free']\n\n\t\n\nCurrently available memory in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nmem['free_percent']\n\n\t\n\nCurrently available memory in percent of total.\n\n\t\n\nSMALLINT\n\n\n\n\nmem['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the memory probe.\n\n\t\n\nBIGINT\n\nheap\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nheap\n\n\t\n\nHeap memory utilization statistics.\n\n\t\n\nOBJECT\n\n\n\n\nheap['used']\n\n\t\n\nCurrently used heap memory in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nheap['max']\n\n\t\n\nMaximum available heap memory. You can specify the max heap memory CrateDB should use in the Configuration.\n\n\t\n\nBIGINT\n\n\n\n\nheap['free']\n\n\t\n\nCurrently available heap memory in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nheap['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the heap probe.\n\n\t\n\nBIGINT\n\nversion\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nversion\n\n\t\n\nCrateDB version information.\n\n\t\n\nOBJECT\n\n\n\n\nversion['number']\n\n\t\n\nVersion string in format \"major.minor.hotfix\"\n\n\t\n\nTEXT\n\n\n\n\nversion['build_hash']\n\n\t\n\nSHA hash of the GitHub commit which this build was built from.\n\n\t\n\nTEXT\n\n\n\n\nversion['build_snapshot']\n\n\t\n\nIndicates whether this build is a snapshot build.\n\n\t\n\nBOOLEAN\n\n\n\n\nversion['minimum_index_compatibility_version']\n\n\t\n\nIndicates the minimum compatible index version which is supported.\n\n\t\n\nTEXT\n\n\n\n\nversion['minimum_wire_compatibility_version']\n\n\t\n\nIndicates the minimum compatible wire protocol version which is supported.\n\n\t\n\nTEXT\n\ncluster_state_version\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\ncluster_state_version\n\n\t\n\nThe current version of the cluster state. The cluster state is an immutable structure and that is recreated when a change is published.\n\n\t\n\nBIGINT\n\nfs\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nfs\n\n\t\n\nUtilization statistics about the file system.\n\n\t\n\nOBJECT\n\n\n\n\nfs['total']\n\n\t\n\nAggregated usage statistic of all disks on the host.\n\n\t\n\nOBJECT\n\n\n\n\nfs['total']['size']\n\n\t\n\nTotal size of all disks in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['used']\n\n\t\n\nTotal used space of all disks in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['available']\n\n\t\n\nTotal available space of all disks in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['reads']\n\n\t\n\nTotal number of reads on all disks.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['bytes_read']\n\n\t\n\nTotal size of reads on all disks in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['writes']\n\n\t\n\nTotal number of writes on all disks.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['bytes_written']\n\n\t\n\nTotal size of writes on all disks in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['disks']\n\n\t\n\nUsage statistics of individual disks on the host.\n\n\t\n\nARRAY\n\n\n\n\nfs['disks']['dev']\n\n\t\n\nDevice name\n\n\t\n\nTEXT\n\n\n\n\nfs['disks']['size']\n\n\t\n\nTotal size of the disk in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['disks']['used']\n\n\t\n\nUsed space of the disk in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['disks']['available']\n\n\t\n\nAvailable space of the disk in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['data']\n\n\t\n\nInformation about data paths used by the node.\n\n\t\n\nARRAY\n\n\n\n\nfs['data']['dev']\n\n\t\n\nDevice name\n\n\t\n\nTEXT\n\n\n\n\nfs['data']['path']\n\n\t\n\nFile path where the data of the node resides.\n\n\t\n\nTEXT\n\nthread_pools\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nthread_pools\n\n\t\n\nUsage statistics of Java thread pools.\n\n\t\n\nARRAY\n\n\n\n\nthread_pools['name']\n\n\t\n\nName of the pool.\n\n\t\n\nTEXT\n\n\n\n\nthread_pools['active']\n\n\t\n\nNumber of currently running thread in the thread pool.\n\n\t\n\nINTEGER\n\n\n\n\nthread_pools['rejected']\n\n\t\n\nTotal number of rejected threads in the thread pool.\n\n\t\n\nBIGINT\n\n\n\n\nthread_pools['largest']\n\n\t\n\nLargest number of threads that have ever simultaneously been in the pool.\n\n\t\n\nINTEGER\n\n\n\n\nthread_pools['completed']\n\n\t\n\nTotal number of completed thread in the thread pool.\n\n\t\n\nBIGINT\n\n\n\n\nthread_pools['threads']\n\n\t\n\nSize of the thread pool.\n\n\t\n\nINTEGER\n\n\n\n\nthread_pools['queue']\n\n\t\n\nNumber of thread currently in the queue.\n\n\t\n\nINTEGER\n\nos\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nos\n\n\t\n\nOperating system stats\n\n\t\n\nOBJECT\n\n\n\n\nos['uptime']\n\n\t\n\nSystem uptime in milliseconds\n\nRequires allowing system calls on Windows and macOS. See notes in Uptime limitations.\n\n\t\n\nBIGINT\n\n\n\n\nos['timestamp']\n\n\t\n\nUNIX timestamp in millisecond resolution\n\n\t\n\nBIGINT\n\n\n\n\nos['cpu']\n\n\t\n\nInformation about CPU utilization\n\n\t\n\nOBJECT\n\n\n\n\nos['cpu']['used']\n\n\t\n\nSystem CPU usage as percentage\n\n\t\n\nSMALLINT\n\n\n\n\nos['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the OS probe.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']\n\n\t\n\nInformation about cgroups (Linux only)\n\n\t\n\nOBJECT\n\n\n\n\nos['cgroup']['cpuacct']\n\n\t\n\nInformation about CPU accounting\n\n\t\n\nOBJECT\n\n\n\n\nos['cgroup']['cpuacct']['control_group']\n\n\t\n\nThe path to the CPU accounting cgroup\n\n\t\n\nTEXT\n\n\n\n\nos['cgroup']['cpuacct']['usage_nanos']\n\n\t\n\nThe total CPU time (in nanoseconds) consumed by all tasks in this cgroup.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['cpu']\n\n\t\n\nInformation about the CPU subsystem\n\n\t\n\nOBJECT\n\n\n\n\nos['cgroup']['cpu']['control_group']\n\n\t\n\nThe path to the CPU cgroup\n\n\t\n\nTEXT\n\n\n\n\nos['cgroup']['cpu']['cfs_period_micros']\n\n\t\n\nThe period of time (in microseconds) the cgroup access to the CPU gets reallocated.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['cpu']['cfs_quota_micros']\n\n\t\n\nThe total amount of time (in microseconds) for which all tasks in the cgroup can run during one period (cfs_period_micros).\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['cpu']['num_elapsed_periods']\n\n\t\n\nThe nr. of period intervals (cfs_period_micros) that have elapsed.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['cpu']['num_times_throttled']\n\n\t\n\nThe nr. of times tasks in the cgroup have been throttled.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['cpu']['time_throttled_nanos']\n\n\t\n\nThe total time (in nanoseconds) for which tasks in the cgroup have been throttled.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['mem']\n\n\t\n\nInformation about memory resources used by tasks in a cgroup.\n\n\t\n\nOBJECT\n\n\n\n\nos['cgroup']['mem']['control_group']\n\n\t\n\nThe path to the memory cgroup\n\n\t\n\nTEXT\n\n\n\n\nos['cgroup']['mem']['usage_bytes']\n\n\t\n\nThe total current memory usage by processes in the cgroup.\n\n\t\n\nTEXT\n\n\n\n\nos['cgroup']['mem']['limit_bytes']\n\n\t\n\nThe max. amount of user memory in the cgroup.\n\n\t\n\nTEXT\n\nThe CPU information values are cached for 1s. They might differ from the actual values at query time. Use the probe timestamp to get the time of collection. When analyzing the CPU usage over time, always use os['probe_timestamp'] to calculate the time difference between 2 probes.\n\ncgroup limitations\n\nNote\n\ncgroup metrics only work if the stats are available from /sys/fs/cgroup/cpu and /sys/fs/cgroup/cpuacct.\n\nUptime limitations\n\nNote\n\nos[‘uptime’] required a system call when running CrateDB on Windows or macOS, however, system calls are not permitted by default. If you require this metric you need to allow system calls by setting bootstrap.seccomp to false. This setting must be set in the crate.yml or via command line argument and cannot be changed at runtime.\n\nos_info\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nos_info\n\n\t\n\nOperating system information\n\n\t\n\nOBJECT\n\n\n\n\nos_info['available_processors']\n\n\t\n\nNumber of processors that are available in the JVM. This is usually equal to the number of cores of the CPU.\n\n\t\n\nINTEGER\n\n\n\n\nos_info['name']\n\n\t\n\nName of the operating system (ex: Linux, Windows, macOS)\n\n\t\n\nTEXT\n\n\n\n\nos_info['arch']\n\n\t\n\nName of the JVM architecture (ex: amd64, x86)\n\n\t\n\nTEXT\n\n\n\n\nos_info['version']\n\n\t\n\nVersion of the operating system\n\n\t\n\nTEXT\n\n\n\n\nos_info['jvm']\n\n\t\n\nInformation about the JVM (Java Virtual Machine)\n\n\t\n\nOBJECT\n\n\n\n\nos_info['jvm']['version']\n\n\t\n\nThe JVM version\n\n\t\n\nTEXT\n\n\n\n\nos_info['jvm']['vm_name']\n\n\t\n\nThe name of the JVM (e.g. OpenJDK, Java HotSpot(TM) )\n\n\t\n\nTEXT\n\n\n\n\nos_info['jvm']['vm_vendor']\n\n\t\n\nThe vendor name of the JVM\n\n\t\n\nTEXT\n\n\n\n\nos_info['jvm']['vm_version']\n\n\t\n\nThe version of the JVM\n\n\t\n\nTEXT\n\nnetwork\n\nNetwork statistics are deprecated in CrateDB 2.3 and may completely be removed in subsequent versions. All BIGINT columns always return 0.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nnetwork\n\n\t\n\nStatistics about network activity on the host.\n\n\t\n\nOBJECT\n\n\n\n\nnetwork['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the network probe.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']\n\n\t\n\nTCP network activity on the host.\n\n\t\n\nOBJECT\n\n\n\n\nnetwork['tcp']['connections']\n\n\t\n\nInformation about TCP network connections.\n\n\t\n\nOBJECT\n\n\n\n\nnetwork['tpc']['connections']['initiated']\n\n\t\n\nTotal number of initiated TCP connections.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tpc']['connections']['accepted']\n\n\t\n\nTotal number of accepted TCP connections.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tpc']['connections']['curr_established']\n\n\t\n\nTotal number of currently established TCP connections.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']['connections']['dropped']\n\n\t\n\nTotal number of dropped TCP connections.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']['connections']['embryonic_dropped']\n\n\t\n\nTotal number of TCP connections that have been dropped before they were accepted.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']['packets']\n\n\t\n\nInformation about TCP packets.\n\n\t\n\nOBJECT\n\n\n\n\nnetwork['tpc']['packets']['sent']\n\n\t\n\nTotal number of TCP packets sent.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']['packets']['received']\n\n\t\n\nTotal number of TCP packets received.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tpc']['packets']['retransmitted']\n\n\t\n\nTotal number of TCP packets retransmitted due to an error.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']['packets']['errors_received']\n\n\t\n\nTotal number of TCP packets that contained checksum errors, had a bad offset, were dropped because of a lack of memory or were too short.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']]['packets']['rst_sent']\n\n\t\n\nTotal number of RST packets sent due to left unread data in queue when socket is closed. See tools.ietf.org.\n\n\t\n\nBIGINT\n\nconnections\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nhttp\n\n\t\n\nNumber of connections established via HTTP\n\n\t\n\nOBJECT\n\n\n\n\nhttp['open']\n\n\t\n\nThe currently open connections established via HTTP\n\n\t\n\nBIGINT\n\n\n\n\nhttp['total']\n\n\t\n\nThe total number of connections that have been established via HTTP over the life time of a CrateDB node\n\n\t\n\nBIGINT\n\n\n\n\npsql\n\n\t\n\nNumber of connections established via PostgreSQL protocol\n\n\t\n\nOBJECT\n\n\n\n\npsql['open']\n\n\t\n\nThe currently open connections established via PostgreSQL protocol\n\n\t\n\nBIGINT\n\n\n\n\npsql['total']\n\n\t\n\nThe total number of connections that have been established via PostgreSQL protocol over the life time of a CrateDB node\n\n\t\n\nBIGINT\n\n\n\n\ntransport\n\n\t\n\nNumber of connections established via Transport protocol\n\n\t\n\nOBJECT\n\n\n\n\ntransport['open']\n\n\t\n\nThe currently open connections established via Transport protocol\n\n\t\n\nBIGINT\n\nprocess\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nprocess\n\n\t\n\nStatistics about the CrateDB process.\n\n\t\n\nOBJECT\n\n\n\n\nprocess['open_file_descriptors']\n\n\t\n\nNumber of currently open file descriptors used by the CrateDB process.\n\n\t\n\nBIGINT\n\n\n\n\nprocess['max_open_file_descriptors']\n\n\t\n\nThe maximum number of open file descriptors CrateDB can use.\n\n\t\n\nBIGINT\n\n\n\n\nprocess['probe_timestamp']\n\n\t\n\nThe system UNIX timestamp at the moment of the probe collection.\n\n\t\n\nBIGINT\n\n\n\n\nprocess['cpu']\n\n\t\n\nInformation about the CPU usage of the CrateDB process.\n\n\t\n\nOBJECT\n\n\n\n\nprocess['cpu']['percent']\n\n\t\n\nThe CPU usage of the CrateDB JVM process given in percent.\n\n\t\n\nSMALLINT\n\nThe CPU information values are cached for 1s. They might differ from the actual values at query time. Use the probe timestamp to get the time of the collect. When analyzing the CPU usage over time, always use process['probe_timestamp'] to calculate the time difference between 2 probes.\n\nNote\n\nIf one of the queried nodes is not responding within three seconds it returns null every column except id and name. This behaviour could be used to detect hanging nodes.\n\nNode checks\n\nThe table sys.node_checks exposes a list of internal node checks and results of their validation.\n\nThe table schema is the following:\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe unique check ID.\n\n\t\n\nINTEGER\n\n\n\n\nnode_id\n\n\t\n\nThe unique node ID.\n\n\t\n\nTEXT\n\n\n\n\nseverity\n\n\t\n\nThe level of severity. The higher the value of the field the higher severity.\n\n\t\n\nINTEGER\n\n\n\n\ndescription\n\n\t\n\nThe description message for the setting check.\n\n\t\n\nTEXT\n\n\n\n\npassed\n\n\t\n\nThe flag determines whether the check for the setting has passed.\n\n\t\n\nBOOLEAN\n\n\n\n\nacknowledged\n\n\t\n\nThe flag determines whether the check for this setting has been acknowledged by the user in order to ignored the value of passed column. This column can be updated.\n\n\t\n\nBOOLEAN\n\nExample query:\n\ncr> select id, node_id, description from sys.node_checks order by id, node_id;\n+----+---------...-+--------------------------------------------------------------...-+\n| id | node_id     | description                                                      |\n+----+---------...-+--------------------------------------------------------------...-+\n|  1 | ...         | It has been detected that the 'gateway.expected_data_nodes' s... |\n|  2 | ...         | The cluster setting 'gateway.recover_after_data_nodes' (or th... |\n|  3 | ...         | If any of the \"expected data nodes\" recovery settings are set... |\n|  5 | ...         | The high disk watermark is exceeded on the node. The cluster ... |\n|  6 | ...         | The low disk watermark is exceeded on the node. The cluster w... |\n|  7 | ...         | The flood stage disk watermark is exceeded on the node. Table... |\n|  8 | ...         | The amount of shards on the node reached 90 % of the limit of... |\n+----+---------...-+--------------------------------------------------------------...-+\nSELECT 7 rows in set (... sec)\n\nAcknowledge failed checks\n\nIt is possible to acknowledge every check by updating the acknowledged column. By doing this, specially CrateDB’s built-in Admin UI won’t complain anymore about failing checks.\n\nImagine we’ve added a new node to our cluster, but as the gateway.expected_data_nodes column can only be set via config-file or command-line argument, the check for this setting will not pass on the already running nodes until the config-file or command-line argument on these nodes is updated and the nodes are restarted (which is not what we want on a healthy well running cluster).\n\nIn order to make the Admin UI accept a failing check (so the checks label goes green again), we must acknowledge this check by updating it’s acknowledged flag:\n\ncr> update sys.node_checks set acknowledged = true where id = 1;\nUPDATE OK, 1 row affected (... sec)\n\n\nCaution\n\nUpdates on this column are transient, so changed values are lost after the affected node is restarted.\n\nDescription of checked node settings\nRecovery expected data nodes\n\nThis check looks at the gateway.expected_data_nodes setting and checks if its value matches the actual number of data nodes present in the cluster. If the actual number of nodes is below the expected number, the warning is raised to indicate some nodes are down. If the actual number is greater, this is flagged to indicate the setting should be updated.\n\nNote\n\nFor backward compatibility, setting the deprecated gateway.expected_nodes instead is still supported. It counts all nodes, not only data-carrying nodes.\n\nRecovery after data nodes\n\nThis check looks at the gateway.recover_after_data_nodes setting and checks if its value is greater than half the configured expected number, but not greater than the configured expected number.\n\nNote\n\nFor backward compatibility, setting the deprecated gateway.recover_after_nodes instead is still supported.\n\n(E / 2) < R <= E\n\n\nHere, R is the number of recovery nodes and E is the number of expected (data) nodes.\n\nIf recovery is started when some nodes are down, CrateDB proceeds on the basis the nodes that are down may not be coming back, and it will create new replicas and rebalance shards as necessary. This is throttled, and it can be controlled with routing allocation settings, but depending on the context, you may prefer to delay recovery if the nodes are only down for a short period of time, so it is advisable to review the documentation around the settings involved and configure them carefully.\n\nRecovery after time\n\nIf gateway.recover_after_data_nodes is set, then gateway.recover_after_time must not be set to 0s, otherwise the gateway.recover_after_data_nodes setting wouldn’t have any effect.\n\nNote\n\nFor backward compatibility, setting the deprecated gateway.recover_after_nodes instead is still supported.\n\nRouting allocation disk watermark high\n\nThe check for the cluster.routing.allocation.disk.watermark.high setting verifies that the high watermark is not exceeded on the current node. The usage of each disk for configured CrateDB data paths is verified against the threshold setting. If one or more verification fails the check is marked as not passed.\n\nRouting allocation disk watermark low\n\nThe check for the cluster.routing.allocation.disk.watermark.low which controls the low watermark for the node disk usage. The check verifies that the low watermark is not exceeded on the current node. The verification is done against each disk for configured CrateDB data paths. The check is not passed if the verification for one or more disk fails.\n\nMaximum shards per node\n\nThe check verifies that the amount of shards on the current node is less than 90 percent of cluster.max_shards_per_node. Creating new tables or partitions which would push the number of shards beyond 100 % of the limit will be rejected.\n\nShards\n\nThe table sys.shards contains real-time statistics for all shards of all (non-system) tables.\n\nTable schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nnode\n\n\t\n\nInformation about the node the shard is located at.\n\n\t\n\nOBJECT\n\n\n\n\nnode['name']\n\n\t\n\nThe name of the node the shard is located at.\n\n\t\n\nTEXT\n\n\n\n\nnode['id']\n\n\t\n\nThe id of the node the shard is located at.\n\n\t\n\nTEXT\n\n\n\n\nblob_path\n\n\t\n\nPath to the directory which contains the blob files of the shard, or null if the shard is not a blob shard.\n\n\t\n\nTEXT\n\n\n\n\nid\n\n\t\n\nThe shard id. This shard id is managed by the system, ranging from 0 up to the number of configured shards of the table.\n\n\t\n\nINTEGER\n\n\n\n\nmin_lucene_version\n\n\t\n\nShows the oldest Lucene segment version used in this shard.\n\n\t\n\nTEXT\n\n\n\n\nnum_docs\n\n\t\n\nThe total amount of documents within a shard.\n\n\t\n\nBIGINT\n\n\n\n\noprhan_partition\n\n\t\n\nTrue if this shard belongs to an orphaned partition which doesn’t belong to any table anymore.\n\n\t\n\nBOOLEAN\n\n\n\n\npartition_ident\n\n\t\n\nThe partition ident of a partitioned table. Empty for non-partitioned tables.\n\n\t\n\nTEXT\n\n\n\n\npath\n\n\t\n\nPath to the shard directory on the filesystem. This directory contains state and index files.\n\n\t\n\nTEXT\n\n\n\n\nprimary\n\n\t\n\nIndicates if this shard is the primary shard.\n\n\t\n\nBOOLEAN\n\n\n\n\nrecovery\n\n\t\n\nRecovery statistics for a shard.\n\n\t\n\nOBJECT\n\n\n\n\nrecovery['files']\n\n\t\n\nFile recovery statistics\n\n\t\n\nOBJECT\n\n\n\n\nrecovery['files']['percent']\n\n\t\n\nPercentage of files already recovered.\n\n\t\n\nREAL\n\n\n\n\nrecovery['files']['recovered']\n\n\t\n\nNumber of files recovered in the shard. Includes both existing and reused files.\n\n\t\n\nINTEGER\n\n\n\n\nrecovery['files']['reused']\n\n\t\n\nTotal number of files reused from a local copy while recovering the shard.\n\n\t\n\nINTEGER\n\n\n\n\nrecovery['files']['used']\n\n\t\n\nTotal number of files in the shard.\n\n\t\n\nINTEGER\n\n\n\n\nrecovery['size']\n\n\t\n\nRecovery statistics for the shard in bytes\n\n\t\n\nOBJECT\n\n\n\n\nrecovery['size']['percent']\n\n\t\n\nPercentage of bytes already recovered\n\n\t\n\nREAL\n\n\n\n\nrecovery['size']['recovered']\n\n\t\n\nNumber of bytes recovered. Includes both existing and re-used bytes.\n\n\t\n\nBIGINT\n\n\n\n\nrecovery['size']['reused']\n\n\t\n\nNumber of bytes re-used from a local copy while recovering the shard.\n\n\t\n\nBIGINT\n\n\n\n\nrecovery['size']['used']\n\n\t\n\nTotal number of bytes in the shard.\n\n\t\n\nBIGINT\n\n\n\n\nrecovery['stage']\n\n\t\n\nRecovery stage:\n\ninit: Recovery has not started\n\nindex: Reading the Lucene index meta-data and copying bytes from source to destination\n\nstart: Starting the engine, opening the index for use\n\ntranslog: Replaying transaction log\n\nfinalize: Cleanup\n\ndone: Complete\n\n\t\n\nTEXT\n\n\n\n\nrecovery['total_time']\n\n\t\n\nReturns elapsed time from the start of the shard recovery.\n\n\t\n\nBIGINT\n\n\n\n\nrecovery['type']\n\n\t\n\nRecovery type:\n\ngateway\n\nsnapshot\n\nreplica\n\nrelocating\n\n\t\n\nTEXT\n\n\n\n\nrelocating_node\n\n\t\n\nThe id of the node to which the shard is getting relocated to.\n\n\t\n\nTEXT\n\n\n\n\nrouting_state\n\n\t\n\nThe current state of the shard in the routing table. Possible states are:\n\nUNASSIGNED\n\nINITIALIZING\n\nSTARTED\n\nRELOCATING\n\n\t\n\nTEXT\n\n\n\n\nschema_name\n\n\t\n\nThe schema name of the table the shard belongs to\n\n\t\n\nTEXT\n\n\n\n\nsize\n\n\t\n\nThe current size in bytes. This value is cached for a short period and may return slightly outdated values.\n\n\t\n\nBIGINT\n\n\n\n\nstate\n\n\t\n\nThe current state of the shard. Possible states are:\n\nCREATED\n\nRECOVERING\n\nPOST_RECOVERY\n\nSTARTED\n\nRELOCATED\n\nCLOSED\n\nINITIALIZING\n\nUNASSIGNED\n\n\t\n\nTEXT\n\n\n\n\nclosed\n\n\t\n\nThe state of the table associated with the shard.\n\n\t\n\nBOOLEAN\n\n\n\n\ntable_name\n\n\t\n\nThe name of the table this shard belongs to\n\n\t\n\nTEXT\n\n\n\n\nseq_no_stats\n\n\t\n\nContains information about internal sequence numbering and checkpoints for these sequence numbers.\n\n\t\n\nOBJECT\n\n\n\n\nseq_no_stats['max_seq_no']\n\n\t\n\nThe highest sequence number that has been issued so far on the shard.\n\n\t\n\nBIGINT\n\n\n\n\nseq_no_stats['local_checkpoint']\n\n\t\n\nThe highest sequence number for which all lower sequence number of been processed on this shard. Due to concurrent indexing this can be lower than max_seq_no.\n\n\t\n\nBIGINT\n\n\n\n\nseq_no_stats['global_checkpoint']\n\n\t\n\nThe highest sequence number for which the local shard can guarantee that all lower sequence numbers have been processed on all active shard copies.\n\n\t\n\nBIGINT\n\n\n\n\ntranslog_stats\n\n\t\n\nContains information for the translog of the shard.\n\n\t\n\nOBJECT\n\n\n\n\ntranslog_stats['size']\n\n\t\n\nThe current size of the translog file in bytes.\n\n\t\n\nBIGINT\n\n\n\n\ntranslog_stats['uncommitted_size']\n\n\t\n\nThe size in bytes of the translog that has not been committed to Lucene yet.\n\n\t\n\nBIGINT\n\n\n\n\ntranslog_stats['number_of_operations']\n\n\t\n\nThe number of operations recorded in the translog.\n\n\t\n\nINTEGER\n\n\n\n\ntranslog_stats['uncommitted_operations']\n\n\t\n\nThe number of operations in the translog which have not been committed to Lucene yet.\n\n\t\n\nINTEGER\n\n\n\n\nretention_leases\n\n\t\n\nVersioned collection of retention leases.\n\n\t\n\nOBJECT\n\n\n\n\nflush_stats\n\n\t\n\nFlush information. Shard relocation resets this information.\n\n\t\n\nOBJECT\n\n\n\n\nflush_stats['count']\n\n\t\n\nThe total amount of flush operations that happened on the shard.\n\n\t\n\nBIGINT\n\n\n\n\nflush_stats['periodic_count']\n\n\t\n\nThe number of periodic flushes. Each periodic flush also counts as a regular flush. A periodic flush can happen after writes depending on settings like the translog flush threshold.\n\n\t\n\nBIGINT\n\n\n\n\nflush_stats['total_time_ns']\n\n\t\n\nThe total time spent on flush operations on the shard.\n\n\t\n\nBIGINT\n\nNote\n\nThe sys.shards table is subject to Shard table permissions.\n\nExample\n\nFor example, you can query shards like this:\n\ncr> select schema_name as schema,\n...   table_name as t,\n...   id,\n...   partition_ident as p_i,\n...   num_docs as docs,\n...   primary,\n...   relocating_node as r_n,\n...   routing_state as r_state,\n...   state,\n...   orphan_partition as o_p\n... from sys.shards where table_name = 'locations' and id = 1;\n+--------+-----------+----+-----+------+---------+------+---------+---------+-------+\n| schema | t         | id | p_i | docs | primary | r_n  | r_state |  state  | o_p   |\n+--------+-----------+----+-----+------+---------+------+---------+---------+-------+\n| doc    | locations |  1 |     |    8 | TRUE    | NULL | STARTED | STARTED | FALSE |\n+--------+-----------+----+-----+------+---------+------+---------+---------+-------+\nSELECT 1 row in set (... sec)\n\nSegments\n\nThe sys.segments table contains information about the Lucene segments of the shards.\n\nThe segment information is useful to understand the behaviour of the underlying Lucene file structures for troubleshooting and performance optimization of shards.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nsegment_name\n\n\t\n\nName of the segment, derived from the segment generation and used internally to create file names in the directory of the shard.\n\n\t\n\nTEXT\n\n\n\n\nshard_id\n\n\t\n\nID of the effected shard.\n\n\t\n\nINTEGER\n\n\n\n\ntable_schema\n\n\t\n\nSchema name of the table of the shard.\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nTable name of the shard.\n\n\t\n\nTEXT\n\n\n\n\npartition_ident\n\n\t\n\nThe partition ident of a partitioned table. Empty for non-partitioned tables.\n\n\t\n\nTEXT\n\n\n\n\nnode\n\n\t\n\nInformation about the node the shard is located at.\n\n\t\n\nOBJECT\n\n\n\n\nnode['name']\n\n\t\n\nThe name of the node the shard is located at.\n\n\t\n\nTEXT\n\n\n\n\nnode['id']\n\n\t\n\nThe id of the node the shard is located at.\n\n\t\n\nTEXT\n\n\n\n\ngeneration\n\n\t\n\nGeneration number of the segment, increments for each segment written.\n\n\t\n\nLONG\n\n\n\n\nnum_docs\n\n\t\n\nNumber of non-deleted Lucene documents in this segment.\n\n\t\n\nINTEGER\n\n\n\n\ndeleted_docs\n\n\t\n\nNumber of deleted Lucene documents in this segment.\n\n\t\n\nINTEGER\n\n\n\n\nsize\n\n\t\n\nDisk space used by the segment in bytes.\n\n\t\n\nLONG\n\n\n\n\nmemory\n\n\t\n\nUnavailable starting from CrateDB 5.0. Always returns -1.\n\n\t\n\nLONG\n\n\n\n\ncommitted\n\n\t\n\nIndicates if the segments are synced to disk. Segments that are synced can survive a hard reboot.\n\n\t\n\nBOOLEAN\n\n\n\n\nprimary\n\n\t\n\nDescribes if this segment is part of a primary shard.\n\n\t\n\nBOOLEAN\n\n\n\n\nsearch\n\n\t\n\nIndicates if the segment is searchable. If false, the segment has most likely been written to disk but needs a refresh to be searchable.\n\n\t\n\nBOOLEAN\n\n\n\n\nversion\n\n\t\n\nVersion of Lucene used to write the segment.\n\n\t\n\nTEXT\n\n\n\n\ncompound\n\n\t\n\nIf true, Lucene merges all files from the segment into a single file to save file descriptors.\n\n\t\n\nBOOLEAN\n\n\n\n\nattributes\n\n\t\n\nContains information about whether high compression was enabled.\n\n\t\n\nOBJECT\n\nNote\n\nThe information in the sys.segments table is expensive to calculate and therefore this information should be retrieved with awareness that it can have performance implications on the cluster.\n\nNote\n\nThe sys.shards table is subject to Shard table permissions.\n\nJobs, operations, and logs\n\nTo let you inspect the activities currently taking place in a cluster, CrateDB provides system tables that let you track current cluster jobs and operations. See Jobs Table and Operations Table.\n\nJobs and operations that finished executing are additionally recorded in memory. There are two retention policies available to control how many records should be kept.\n\nOne option is to configure the maximum number of records which should be kept. Once the configured table size is reached, the older log records are deleted as newer records are added. This is configurable using stats.jobs_log_size and stats.operations_log_size.\n\nAnother option is to configure an expiration time for the records. In this case, the records in the logs tables are periodically cleared if they are older than the expiry time. This behaviour is configurable using stats.jobs_log_expiration and stats.operations_log_expiration.\n\nIn addition to these retention policies, there is a memory limit in place preventing these tables from taking up too much memory. The amount of memory that can be used to store the jobs can be configured using stats.breaker.log.jobs.limit and stats.breaker.log.operations.limit. If the memory limit is reached, an error message will be logged and the log table will be cleared completely.\n\nIt is also possible to define a filter which must match for jobs to be recorded after they finished executing. This can be useful to only record slow queries or queries that failed due to an error. This filter can be configured using the stats.jobs_log_filer setting.\n\nFurthermore, there is a second filter setting which also results in a log entry in the regular CrateDB log file for all finished jobs that match this filter. This can be configured using stats.jobs_log_persistent_filter. This could be used to create a persistent slow query log.\n\nJobs\n\nThe sys.jobs table is a constantly updated view of all jobs that are currently being executed in the cluster.\n\nTable schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe job UUID.\n\nThis job ID is generated by the system.\n\n\t\n\nTEXT\n\n\n\n\nnode\n\n\t\n\nInformation about the node that created the job.\n\n\t\n\nOBJECT\n\n\n\n\nnode['id']\n\n\t\n\nThe id of the node.\n\n\t\n\nTEXT\n\n\n\n\nnode['name']\n\n\t\n\nThe name of the node.\n\n\t\n\nTEXT\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the job started.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nstmt\n\n\t\n\nShows the data query or manipulation statement represented by this job.\n\n\t\n\nTEXT\n\n\n\n\nusername\n\n\t\n\nThe user who is executing the statement.\n\n\t\n\nTEXT\n\nThe field username corresponds to the SESSION_USER that is performing the query:\n\ncr> select stmt, username, started from sys.jobs where stmt like 'sel% from %jobs%';\n+---------------------------------------------------------------------------------+----------+-...-----+\n| stmt                                                                            | username | started |\n+---------------------------------------------------------------------------------+----------+-...-----+\n| select stmt, username, started from sys.jobs where stmt like 'sel% from %jobs%' | crate    | ...     |\n+---------------------------------------------------------------------------------+----------+-...-----+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nIf the user management module is not available, the username is given as crate.\n\nEvery request that queries data or manipulates data is considered a “job” if it is a valid query. Requests that are not valid queries (for example, a request that tries to query a non-existent table) will not show up as jobs.\n\nNote\n\nThe sys.jobs table is subject to sys jobs tables permissions.\n\nJobs metrics\n\nThe sys.jobs_metrics table provides an overview of the query latency in the cluster. Jobs metrics are not persisted across node restarts.\n\nThe metrics are aggregated for each node and each unique classification of the statements.\n\nNote\n\nIn order to reduce the memory requirements for these metrics, the times are statistically sampled and therefore may have slight inaccuracies. In addition, durations are only tracked up to 10 minutes. Statements taking longer than that are capped to 10 minutes.\n\nsys.jobs_metrics Table schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nnode\n\n\t\n\nAn object containing the id and name of the node on which the metrics have been sampled.\n\n\t\n\nOBJECT\n\n\n\n\nclassification\n\n\t\n\nAn object containing the statement classification.\n\n\t\n\nOBJECT\n\n\n\n\nclassification['type']\n\n\t\n\nThe general type of the statement. Types are: INSERT, SELECT, UPDATE, DELETE, COPY, DDL, and MANAGEMENT.\n\n\t\n\nTEXT\n\n\n\n\nclassification['labels']\n\n\t\n\nLabels are only available for certain statement types that can be classified more accurately than just by their type.\n\n\t\n\nTEXT_ARRAY\n\n\n\n\ntotal_count\n\n\t\n\nTotal number of queries executed\n\n\t\n\nBIGINT\n\n\n\n\nfailed_count\n\n\t\n\nTotal number of queries that failed to complete successfully.\n\n\t\n\nBIGINT\n\n\n\n\nsum_of_durations\n\n\t\n\nSum of durations in ms of all executed queries per statement type.\n\n\t\n\nBIGINT\n\n\n\n\nstdev\n\n\t\n\nThe standard deviation of the query latencies\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\nmean\n\n\t\n\nThe mean query latency in ms\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\nmax\n\n\t\n\nThe maximum query latency in ms\n\n\t\n\nBIGINT\n\n\n\n\nmin\n\n\t\n\nThe minimum query latency in ms\n\n\t\n\nBIGINT\n\n\n\n\npercentiles\n\n\t\n\nAn object containing different percentiles\n\n\t\n\nOBJECT\n\nClassification\n\nCertain statement types (such as SELECT statements) have additional labels in their classification. These labels are the names of the logical plan operators that are involved in the query.\n\nFor example, the following UNION statement:\n\nSELECT name FROM t1 where id = 1\nUNION ALL\nSELECT name FROM t2 where id < 2\n\n\nwould result in the following labels:\n\nUnion` for the UNION ALL\n\nGet for the left SELECT\n\nCollect for the right SELECT\n\nNote\n\nLabels may be subject to change as they only represent internal properties of the statement!\n\nOperations\n\nThe sys.operations table is a constantly updated view of all operations that are currently being executed in the cluster:\n\ncr> select node['name'], job_id, name, used_bytes from sys.operations\n... order by name limit 1;\n+--------------+--------...-+-----...-+------------+\n| node['name'] | job_id     | name    | used_bytes |\n+--------------+--------...-+-----...-+------------+\n| crate        | ...        | ...     | ...        |\n+--------------+--------...-+-----...-+------------+\nSELECT 1 row in set (... sec)\n\n\nAn operation is a node-specific sub-component of a job (for when a job involves multi-node processing). Jobs that do not require multi-node processing will not produce any operations.\n\nTable schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe operation UUID.\n\nThis operation ID is generated by the system.\n\n\t\n\nTEXT\n\n\n\n\njob_id\n\n\t\n\nThe job id this operation belongs to.\n\n\t\n\nTEXT\n\n\n\n\nname\n\n\t\n\nThe name of the operation.\n\n\t\n\nTEXT\n\n\n\n\nnode\n\n\t\n\nInformation about the node that created the operation.\n\n\t\n\nOBJECT\n\n\n\n\nnode['id']\n\n\t\n\nThe id of the node.\n\n\t\n\nTEXT\n\n\n\n\nnode['name']\n\n\t\n\nThe name of the node.\n\n\t\n\nTEXT\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the operation started.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nused_bytes\n\n\t\n\nCurrently loaded amount of data by the operation.\n\n\t\n\nBIGINT\n\nNote\n\nIn some cases, operations are generated for internal CrateDB work that does not directly correspond to a user request. These entries do not have corresponding entries in sys.jobs.\n\nLogs\n\nThe sys.jobs and sys.operations tables have corresponding log tables: sys.jobs_log and sys.operations_log.\n\nsys.jobs_log Table schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe job ID.\n\n\t\n\nTEXT\n\n\n\n\nended\n\n\t\n\nThe point in time when the job finished.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nerror\n\n\t\n\nIf the job encountered an error, this will hold the error message.\n\n\t\n\nTEXT\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the job started.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nstmt\n\n\t\n\nShows the data query or manipulation statement executed by the job.\n\n\t\n\nTEXT\n\n\n\n\nusername\n\n\t\n\nThe user who executed the statement.\n\n\t\n\nTEXT\n\n\n\n\nclassification\n\n\t\n\nAn object containing the statement classification.\n\n\t\n\nOBJECT\n\n\n\n\nclassification['type']\n\n\t\n\nThe general type of the statement. Types are: INSERT, SELECT, UPDATE, DELETE,``COPY``, DDL, and MANAGEMENT.\n\n\t\n\nTEXT\n\n\n\n\nclassification['labels']\n\n\t\n\nLabels are only available for certain statement types that can be classified more accurately than just by their type.\n\n\t\n\nTEXT_ARRAY\n\n\n\n\nnode\n\n\t\n\nInformation about the node that created the job.\n\n\t\n\nOBJECT\n\n\n\n\nnode['id']\n\n\t\n\nThe id of the node.\n\n\t\n\nTEXT\n\n\n\n\nnode['name']\n\n\t\n\nThe name of the node.\n\n\t\n\nTEXT\n\nNote\n\nYou can control which jobs are recorded using the stats.jobs_log_filter\n\nNote\n\nThe sys.jobs_log table is subject to sys jobs tables permissions.\n\nsys.operations_log Table schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe operation ID.\n\n\t\n\nTEXT\n\n\n\n\njob_id\n\n\t\n\nThe job id.\n\n\t\n\nTEXT\n\n\n\n\nended\n\n\t\n\nThe point in time when the operation finished.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nerror\n\n\t\n\nIf the operation encountered an error, this will hold the error message.\n\n\t\n\nTEXT\n\n\n\n\nname\n\n\t\n\nThe name of the operation.\n\n\t\n\nTEXT\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the operation started.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nused_bytes\n\n\t\n\nThe amount of data loaded by the operation.\n\n\t\n\nBIGINT\n\nAfter a job or operation finishes, the corresponding entry will be moved into the corresponding log table:\n\ncr> select id, stmt, username, started, ended, error\n... from sys.jobs_log order by ended desc limit 2;\n+-...+----------------------------------------------...-+----------+-...-----+-...---+-------+\n| id | stmt                                             | username | started | ended | error |\n+-...+----------------------------------------------...-+----------+-...-----+-...---+-------+\n| ...| select node['name'], ...                         | crate    | ...     | ...   |  NULL |\n| ...| select stmt, username, started from sys.jobs ... | crate    | ...     | ...   |  NULL |\n+-...+----------------------------------------------...-+----------+-...-----+-...---+-------+\nSELECT 2 rows in set (... sec)\n\n\nInvalid queries are also logged in the sys.jobs_log table, i.e. queries that never make it to the sys.jobs table because they could not be executed.\n\nThe log tables are bound by a fixed size (stats.jobs_log_size) or by an expiration time (stats.jobs_log_expiration)\n\nSee Collecting stats for information on how to configure logs.\n\nCaution\n\nIf you deactivate statistics tracking, the logs tables will be truncated.\n\nCluster checks\n\nThe table sys.checks exposes a list of internal cluster checks and results of their validation.\n\nThe sys.checks table looks like this:\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe unique check id.\n\n\t\n\nINTEGER\n\n\n\n\nseverity\n\n\t\n\nThe level of severity. The higher the value of the field the higher severity.\n\n\t\n\nINTEGER\n\n\n\n\ndescription\n\n\t\n\nThe description message for the setting check.\n\n\t\n\nTEXT\n\n\n\n\npassed\n\n\t\n\nThe flag determines whether the check for the setting has passed.\n\n\t\n\nBOOLEAN\n\nHere’s an example query:\n\ncr> select id, description from sys.checks order by id;\n+----+--------------------------------------------------------------...-+\n| id | description                                                      |\n+----+--------------------------------------------------------------...-+\n|  2 | The total number of partitions of one or more partitioned tab... |\n|  3 | The following tables need to be recreated for compatibility w... |\n+----+--------------------------------------------------------------...-+\nSELECT 2 rows in set (... sec)\n\n\nCluster checks are also indicated in the CrateDB admin console. When all cluster checks (and all Node checks) pass, the Checks icon will be green. Here’s what it looks like when some checks are failing at the CRITICAL severity level:\n\nCurrent Checks\nNumber of partitions\n\nThis check warns if any partitioned table has more than 1000 partitions to detect the usage of a high cardinality field for partitioning.\n\nTables need to be recreated\n\nWarning\n\nDo not attempt to upgrade your cluster to a newer major version if this cluster check is failing. Follow the instructions below to get this cluster check passing.\n\nThis check warns you if your cluster contains tables that you need to reindex before you can upgrade to a future major version of CrateDB.\n\nIf you try to upgrade to a later major CrateDB version without reindexing the tables, CrateDB will refuse to start.\n\nCrateDB table version compatibility scheme\n\nCrateDB maintains backward compatibility for tables created in majorVersion - 1:\n\nTable Origin\n\n\t\n\nCurrent Version\n\n\t\n\nCurrent Version\n\n\t\n\nCurrent Version\n\n\n\t\n\n3.x\n\n\t\n\n4.x\n\n\t\n\n5.x\n\n\n\n\n3.x\n\n\t\n\n✔️\n\n\t\n\n✔️\n\n\t\n\n❌\n\n\n\n\n4.x\n\n\t\n\n❌\n\n\t\n\n✔️\n\n\t\n\n✔️\n\n\n\n\n5.x\n\n\t\n\n❌\n\n\t\n\n❌\n\n\t\n\n✔️\n\nAvoiding reindex using partitioned tables\n\nReindexing tables is an expensive operation which can take a long time. If you are storing time series data for a certain retention period and intend to delete old data, it is possible to use the partitioned tables to avoid reindex operations.\n\nYou will have to use a partition column that denotes time. For example, if you have a retention period of nine months, you could partition a table by a month column. Then, every month, the system will create a new partition. This new partition is created using the active CrateDB version and is compatible with the next major CrateDB version. Now to achieve your goal of avoiding a reindex, you must manually delete any partition older than nine months. If you do that, then after nine months you rolled through all partitions and the remaining nine are compatible with the next major CrateDB version.\n\nHow to reindex\n\nUse SHOW CREATE TABLE to get the schema required to create an empty copy of the table to recreate:\n\ncr> SHOW CREATE TABLE rx.metrics;\n+-----------------------------------------------------+\n| SHOW CREATE TABLE rx.metrics                        |\n+-----------------------------------------------------+\n| CREATE TABLE IF NOT EXISTS \"rx\".\"metrics\" (         |\n|    \"id\" TEXT NOT NULL,                                       |\n|    \"temperature\" REAL,                              |\n|    PRIMARY KEY (\"id\")                               |\n| )                                                   |\n| CLUSTERED BY (\"id\") INTO 4 SHARDS                   |\n| WITH (                                              |\n|    \"allocation.max_retries\" = 5,                    |\n|    \"blocks.metadata\" = false,                       |\n|    \"blocks.read\" = false,                           |\n|    \"blocks.read_only\" = false,                      |\n|    \"blocks.read_only_allow_delete\" = false,         |\n|    \"blocks.write\" = false,                          |\n|    codec = 'default',                               |\n|    column_policy = 'strict',                        |\n|    \"mapping.total_fields.limit\" = 1000,             |\n|    max_ngram_diff = 1,                              |\n|    max_shingle_diff = 3,                            |\n|    number_of_replicas = '0-1',                      |\n|    \"routing.allocation.enable\" = 'all',             |\n|    \"routing.allocation.total_shards_per_node\" = -1, |\n|    \"store.type\" = 'fs',                             |\n|    \"translog.durability\" = 'REQUEST',               |\n|    \"translog.flush_threshold_size\" = 536870912,     |\n|    \"translog.sync_interval\" = 5000,                 |\n|    \"unassigned.node_left.delayed_timeout\" = 60000,  |\n|    \"write.wait_for_active_shards\" = '1'             |\n| )                                                   |\n+-----------------------------------------------------+\nSHOW 1 row in set (... sec)\n\n\nCreate a new temporary table, using the schema returned from SHOW CREATE TABLE:\n\ncr> CREATE TABLE rx.tmp_metrics (id TEXT PRIMARY KEY, temperature REAL);\nCREATE OK, 1 row affected (... sec)\n\n\nCopy the data:\n\ncr> INSERT INTO rx.tmp_metrics (id, temperature) (SELECT id, temperature FROM rx.metrics);\nINSERT OK, 2 rows affected (... sec)\n\n\nSwap the tables:\n\ncr> ALTER CLUSTER SWAP TABLE rx.tmp_metrics TO rx.metrics;\nALTER OK, 1 row affected  (... sec)\n\n\nConfirm the new your_table contains all data and has the new version:\n\ncr> SELECT count(*) FROM rx.metrics;\n+----------+\n| count(*) |\n+----------+\n|        2 |\n+----------+\nSELECT 1 row in set (... sec)\n\ncr> SELECT version['created'] FROM information_schema.tables\n... WHERE table_schema = 'rx' AND table_name = 'metrics';\n+--------------------+\n| version['created'] |\n+--------------------+\n| 5.6.4              |\n+--------------------+\nSELECT 1 row in set (... sec)\n\n\nDrop the old table, as it is now obsolete:\n\ncr> DROP TABLE rx.tmp_metrics;\nDROP OK, 1 row affected  (... sec)\n\n\nAfter you reindexed all tables, this cluster check will pass.\n\nNote\n\nSnapshots of your tables created prior to them being upgraded will not work with future versions of CrateDB. For this reason, you should create a new snapshot for each of your tables. (See Snapshots.)\n\nLicense check\n\nNote\n\nThis check was removed in version 4.5 because CrateDB no longer requires an enterprise license, see also Farewell to the CrateDB Enterprise License.\n\nThis check warns you when your license is close to expiration, is already expired, or if the cluster contains more nodes than allowed by your license. It will yield a MEDIUM alert when your license is valid for less than 15 days and a HIGH alert when your license is valid for less than a day. All other cases, like already expired or max-nodes-violation, it will result in a HIGH alert. We recommend that you request a new license when this check triggers, in order to avoid the situation where operations are rejected due to an invalid license.\n\nHealth\n\nThe sys.health table lists the health of each table and table partition. The health is computed by checking the states of the shard of each table/partition.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\ntable_name\n\n\t\n\nThe table name.\n\n\t\n\nTEXT\n\n\n\n\ntable_schema\n\n\t\n\nThe schema of the table.\n\n\t\n\nTEXT\n\n\n\n\npartition_ident\n\n\t\n\nThe ident of the partition. NULL for non-partitioned tables.\n\n\t\n\nTEXT\n\n\n\n\nhealth\n\n\t\n\nThe health label. Can be RED, YELLOW or GREEN.\n\n\t\n\nTEXT\n\n\n\n\nseverity\n\n\t\n\nThe health as a smallint value. Useful when ordering on health.\n\n\t\n\nSMALLINT\n\n\n\n\nmissing_shards\n\n\t\n\nThe number of not assigned or started shards.\n\n\t\n\nINTEGER\n\n\n\n\nunderreplicated_shards\n\n\t\n\nThe number of shards which are not fully replicated.\n\n\t\n\nINTEGER\n\nBoth missing_shards and underreplicated_shards might return -1 if the cluster is in an unhealthy state that prevents the exact number from being calculated. This could be the case when the cluster can’t elect a master, because there are not enough eligible nodes available.\n\ncr> select * from sys.health order by severity desc, table_name;\n+--------+----------------+-----------------+----------+------------+--------------+------------------------+\n| health | missing_shards | partition_ident | severity | table_name | table_schema | underreplicated_shards |\n+--------+----------------+-----------------+----------+------------+--------------+------------------------+\n| GREEN  |              0 |            NULL |        1 | locations  | doc          |                      0 |\n| GREEN  |              0 |            NULL |        1 | quotes     | doc          |                      0 |\n+--------+----------------+-----------------+----------+------------+--------------+------------------------+\nSELECT 2 rows in set (... sec)\n\n\nThe health with the highest severity will always define the health of the query scope.\n\nExample of getting a cluster health (health of all tables):\n\ncr> select health from sys.health order by severity desc limit 1;\n+--------+\n| health |\n+--------+\n| GREEN  |\n+--------+\nSELECT 1 row in set (... sec)\n\nHealth definition\n\nHealth\n\n\t\n\nDescription\n\n\n\n\nRED\n\n\t\n\nAt least one primary shard is missing (primary shard not started or unassigned).\n\n\n\n\nYELLOW\n\n\t\n\nAt least one shard is underreplicated (replica shard not started or unassigned).\n\n\n\n\nGREEN\n\n\t\n\nAll primary and replica shards have been started.\n\nNote\n\nThe sys.health table is subject to Shard table permissions as it will expose a summary of table shard states.\n\nRepositories\n\nThe table sys.repositories lists all configured repositories that can be used to create, manage and restore snapshots (see Snapshots).\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nname\n\n\t\n\nThe repository name\n\n\t\n\nTEXT\n\n\n\n\ntype\n\n\t\n\nThe type of the repository determining how and where the repository stores its snapshots.\n\n\t\n\nTEXT\n\n\n\n\nsettings\n\n\t\n\nThe configuration settings the repository has been created with. The specific settings depend on the repository type, see CREATE REPOSITORY.\n\n\t\n\nOBJECT\n\ncr> SELECT name, type, settings FROM sys.repositories\n... ORDER BY name;\n+---------+------+---------------------------------------------------...--+\n| name    | type | settings                                               |\n+---------+------+---------------------------------------------------...--+\n| my_repo | fs   | {\"compress\": \"true\", \"location\": \"repo_location\", ...} |\n+---------+------+---------------------------------------------------...--+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nSensitive user account information will be masked and thus not visible to the user.\n\nSnapshots\n\nThe table sys.snapshots lists all existing snapshots in all configured repositories (see Snapshots).\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nname\n\n\t\n\nThe name of the snapshot\n\n\t\n\nTEXT\n\n\n\n\nrepository\n\n\t\n\nThe name of the repository that contains this snapshot.\n\n\t\n\nTEXT\n\n\n\n\nconcrete_indices\n\n\t\n\nContains the names of all tables and partitions that are contained in this snapshot how they are represented as ES index names.\n\n\t\n\nARRAY(TEXT)\n\n\n\n\ntables\n\n\t\n\nContains the fully qualified names of all tables within the snapshot.\n\n\t\n\nARRAY(TEXT)\n\n\n\n\ntable_partitions\n\n\t\n\nContains the table schema, table name and partition values of partitioned tables within the snapshot.\n\n\t\n\nARRAY(OBJECT)\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the creation of the snapshot started. Changes made after that are not stored in this snapshot.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nfinished\n\n\t\n\nThe point in time when the snapshot creation finished.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nstate\n\n\t\n\nThe current state of the snapshot. One of: IN_PROGRESS, SUCCESS, PARTIAL, or FAILED.\n\n\t\n\nTEXT\n\n\n\n\nversion\n\n\t\n\nAn internal version this snapshot was created with.\n\n\t\n\nTEXT\n\n\n\n\nfailures\n\n\t\n\nA list of failures that occurred while taking the snapshot. If taking the snapshot was successful this is empty.\n\n\t\n\nARRAY(TEXT)\n\nSnapshot/Restore operates on a per-shard basis. Hence, the state column indicates whether all (SUCCESS), some (PARTIAL), or no shards(FAILED) have been backed up. PARTIAL snapshots are the result of some primaries becoming unavailable while taking the snapshot when there are no replicas at hand (cluster state is RED). If there are replicas of the (now unavailable) primaries (cluster state is YELLOW) the snapshot succeeds and all shards are included (state SUCCESS). Building on a PARTIAL snapshot will include all primaries again.\n\nWarning\n\nIn case of a PARTIAL state another snapshot should be created in order to guarantee a full backup! Only SUCCESS includes all shards.\n\nThe concrete_indices column contains the names of all Elasticsearch indices that were stored in the snapshot. A normal CrateDB table maps to one Elasticsearch index, a partitioned table maps to one Elasticsearch index per partition. The mapping follows the following pattern:\n\nCrateDB table / partition name\n\n\t\n\nconcrete_indices entry\n\n\n\n\ndoc.my_table\n\n\t\n\nmy_table\n\n\n\n\nmy_schema.my_table\n\n\t\n\nmy_schema.my_table\n\n\n\n\ndoc.parted_table (value=null)\n\n\t\n\n.partitioned.my_table.0400\n\n\n\n\nmy_schema.parted_table (value=null)\n\n\t\n\nmy_schema..partitioned.my_table.0400\n\ncr> SELECT \"repository\", name, state, concrete_indices\n... FROM sys.snapshots order by \"repository\", name;\n+------------+-------------+---------+-----------------...-+\n| repository | name        | state   | concrete_indices    |\n+------------+-------------+---------+-----------------...-+\n| my_repo    | my_snapshot | SUCCESS | [...]               |\n+------------+-------------+---------+-----------------...-+\nSELECT 1 row in set (... sec)\n\nSnapshot Restore\n\nThe sys.snapshot_restore table contains information about the current state of snapshot restore operations.\n\npg_stats schema\n\nName\n\n\t\n\nDescription\n\n\t\n\nType\n\n\n\n\nid\n\n\t\n\nThe UUID of the restore snapshot operation.\n\n\t\n\nTEXT\n\n\n\n\nrepository\n\n\t\n\nThe name of the repository that contains the snapshot.\n\n\t\n\nTEXT\n\n\n\n\nsnapshot\n\n\t\n\nThe name of the snapshot.\n\n\t\n\nTEXT\n\n\n\n\nstate\n\n\t\n\nThe current state of the snapshot restore operations. Possible states are: INIT, STARTED, SUCCESS, and FAILURE.\n\n\t\n\nTEXT\n\n\n\n\nshards['table_schema']\n\n\t\n\nThe schema name of the table of the shard.\n\n\t\n\nTEXT\n\n\n\n\nshards['table_name']\n\n\t\n\nThe table name of the shard.\n\n\t\n\nTEXT\n\n\n\n\nshards['partition_ident']\n\n\t\n\nThe identifier of the partition of the shard. NULL if the is not partitioned.\n\n\t\n\nTEXT\n\n\n\n\nshards['shard_id']\n\n\t\n\nThe ID of the shard.\n\n\t\n\nINTEGER\n\n\n\n\nshards['state']\n\n\t\n\nThe restore state of the shard. Possible states are: INIT, STARTED, SUCCESS, and FAILURE.\n\n\t\n\nTEXT\n\nTo get more information about the restoring snapshots and shards one can join the sys.snapshot_restore with sys.shards or sys.snapshots table.\n\nSummits\n\nThe sys.summits table contains the information about the mountains in the Alps higher than 2000m. The mountain names from the table are also used to generate random nodes names.\n\nUsers\n\nThe sys.users table contains all existing database users in the cluster.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nname\n\n\t\n\nThe name of the database user.\n\n\t\n\nTEXT\n\n\n\n\nsuperuser\n\n\t\n\nFlag to indicate whether the user is a superuser.\n\n\t\n\nBOOLEAN\n\n\n\n\npassword\n\n\t\n\n******** if there is a password set or NULL if there is not.\n\n\t\n\nTEXT\n\n\n\n\ngranted_roles\n\n\t\n\nA list of parent roles granted to the user\n\n\t\n\nARRAY\n\n\n\n\ngranted_roles[role]\n\n\t\n\nThe name of the role granted to the user\n\n\t\n\nTEXT\n\n\n\n\ngranted_roles[grantor]\n\n\t\n\nThe name of user who granted the role to the user\n\n\t\n\nTEXT\n\nRoles\n\nThe sys.roles table contains all existing database roles in the cluster.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nname\n\n\t\n\nThe name of the database user.\n\n\t\n\nTEXT\n\n\n\n\ngranted_roles\n\n\t\n\nA list of parent roles granted to the user\n\n\t\n\nARRAY\n\n\n\n\ngranted_roles[role]\n\n\t\n\nThe name of the role granted to the user\n\n\t\n\nTEXT\n\n\n\n\ngranted_roles[grantor]\n\n\t\n\nThe name of user who granted the role to the user\n\n\t\n\nTEXT\n\nPrivileges\n\nThe sys.privileges table contains all privileges for each user and role of the database.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nclass\n\n\t\n\nThe class on which the privilege applies\n\n\t\n\nTEXT\n\n\n\n\ngrantee\n\n\t\n\nThe name of the database user or role for which the privilege is granted or denied\n\n\t\n\nTEXT\n\n\n\n\ngrantor\n\n\t\n\nThe name of the database user who granted or denied the privilege\n\n\t\n\nTEXT\n\n\n\n\nident\n\n\t\n\nThe name of the database object on which the privilege applies\n\n\t\n\nTEXT\n\n\n\n\nstate\n\n\t\n\nEither GRANT or DENY, which indicates if the user or role has been granted or denied access to the specific database object\n\n\t\n\nARRAY\n\n\n\n\ntype\n\n\t\n\nThe type of access for the specific database object\n\n\t\n\nTEXT\n\nAllocations\n\nThe sys.allocations table contains information about shards and their allocation state. The table contains:\n\nshards that are unassigned and why they are unassigned\n\nshards that are assigned but cannot be moved or rebalanced and why they remain on their current node\n\nIt can help to identify problems if shard allocations behave different than expected, e.g. when a shard stays unassigned or a shard does not move off a node.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\ntable_schema\n\n\t\n\nSchema name of the table of the shard.\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nTable name of the shard.\n\n\t\n\nTEXT\n\n\n\n\npartition_ident\n\n\t\n\nIdentifier of the partition of the shard. NULL if the table is not partitioned.\n\n\t\n\nTEXT\n\n\n\n\nshard_id\n\n\t\n\nID of the effected shard.\n\n\t\n\nINTEGER\n\n\n\n\nnode_id\n\n\t\n\nID of the node on which the shard resides. NULL if the shard is unassigned.\n\n\t\n\nTEXT\n\n\n\n\nprimary\n\n\t\n\nWhether the shard is a primary shard.\n\n\t\n\nBOOLEAN\n\n\n\n\ncurrent_state\n\n\t\n\nCurrent state of the shard. Possible states are: UNASSIGNED, INITIALIZING, STARTED, RELOCATING\n\n\t\n\nTEXT\n\n\n\n\nexplanation\n\n\t\n\nExplanation why the shard cannot be allocated, moved or rebalanced.\n\n\t\n\nTEXT\n\n\n\n\ndecisions\n\n\t\n\nA list of decisions that describe in detail why the shard in the current state.\n\n\t\n\nARRAY\n\n\n\n\ndecisions['node_id']\n\n\t\n\nID of the node of the decision.\n\n\t\n\nTEXT\n\n\n\n\ndecisions['node_name']\n\n\t\n\nName of the node of the decision.\n\n\t\n\nTEXT\n\n\n\n\ndecisions['explanations']\n\n\t\n\nDetailed list of human readable explanations why the node decided whether to allocate or rebalance the shard. Returns NULL if there is no need to rebalance the shard.\n\n\t\n\nARRAY\n\nNote\n\nThe sys.allocations table is subject to Shard table permissions.\n\nShard table permissions\n\nAccessing tables that return shards (sys.shards, sys.allocations) is subjected to the same privileges constraints as the other tables. Namely, in order to query them, the connected user needs to have the DQL privilege on that particular table, either directly or inherited from the SCHEMA or CLUSTER (for more information on privileges inheritance see Hierarchical Inheritance of Privileges).\n\nHowever, being able to query shard returning system tables will not allow the user to retrieve all the rows in the table, as they may contain information related to tables, which the connected user does not have any privileges for. The only rows that will be returned will be the ones the user is allowed to access.\n\nFor example, if the user john has any privilege on the doc.books table but no privilege at all on doc.locations, when john issues a SELECT * FROM sys.shards statement, the shards information related to the doc.locations table will not be returned.\n\nsys jobs tables permissions\n\nAccessing sys.jobs and sys.jobs_log tables is subjected to the same privileges constraints as other tables. To query them, the current user needs to have the DQL privilege on that particular table, either directly or inherited from the SCHEMA or CLUSTER.\n\nA user that doesn’t have superuser privileges is allowed to retrieve only their own job logs entries, while a user with superuser privileges has access to all.\n\npg_stats\n\nThe pg_stats table in the pg_catalog system schema contains statistical data about the contents of the CrateDB cluster.\n\nEntries are periodically created or updated in the interval configured with the stats.service.interval setting.\n\nAlternatively the statistics can also be updated using the ANALYZE command.\n\nThe table contains 1 entry per column for each table in the cluster which has been analyzed.\n\npg_stats schema\n\nName\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\nschemaname\n\n\t\n\ntext\n\n\t\n\nName of the schema containing the table.\n\n\n\n\ntablename\n\n\t\n\ntext\n\n\t\n\nName of the table.\n\n\n\n\nattname\n\n\t\n\ntext\n\n\t\n\nName of the column.\n\n\n\n\ninherited\n\n\t\n\nbool\n\n\t\n\nAlways false in CrateDB; For compatibility with PostgreSQL.\n\n\n\n\nnull_frac\n\n\t\n\nreal\n\n\t\n\nFraction of column entries that are null.\n\n\n\n\navg_width\n\n\t\n\ninteger\n\n\t\n\nAverage size in bytes of column’s entries.\n\n\n\n\nn_distinct\n\n\t\n\nreal\n\n\t\n\nAn approximation of the number of distinct values in a column.\n\n\n\n\nmost_common_vals\n\n\t\n\nstring[]\n\n\t\n\nA list of the most common values in the column. null if no values seem. more common than others.\n\n\n\n\nmost_common_freqs\n\n\t\n\nreal[]\n\n\t\n\nA list of the frequencies of the most common values. The size of the array always matches most_common_vals. If most_common_vals is null this is null as well.\n\n\n\n\nhistogram_bounds\n\n\t\n\nstring[]\n\n\t\n\nA list of values that divide the column’s values into groups of approximately equal population. The values in most_common_vals, if present, are omitted from this histogram calculation.\n\n\n\n\ncorrelation\n\n\t\n\nreal\n\n\t\n\nAlways 0.0. This column exists for PostgreSQL compatibility.\n\n\n\n\nmost_common_elems\n\n\t\n\nstring[]\n\n\t\n\nAlways null. Exists for PostgreSQL compatibility.\n\n\n\n\nmost_common_elem_freqs\n\n\t\n\nreal[]\n\n\t\n\nAlways null. Exists for PostgreSQL compatibility.\n\n\n\n\nelem_count_histogram\n\n\t\n\nreal[]\n\n\t\n\nAlways null. Exists for PostgreSQL compatibility.\n\nNote\n\nNot all data types support creating statistics. So some columns may not show up in the table.\n\npg_publication\n\nThe pg_publication table in the pg_catalog system schema contains all publications created in the cluster.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\noid\n\n\t\n\nRow identifier.\n\n\t\n\nINTEGER\n\n\n\n\npubname\n\n\t\n\nName of the publication.\n\n\t\n\nTEXT\n\n\n\n\npubowner\n\n\t\n\noid of the owner of the publication.\n\n\t\n\nINTEGER\n\n\n\n\npuballtables\n\n\t\n\nWhether this publication includes all tables in the cluster, including tables created in the future.\n\n\t\n\nBOOLEAN\n\n\n\n\npubinsert\n\n\t\n\nWhether INSERT operations are replicated for tables in the publication. Always true.\n\n\t\n\nBOOLEAN\n\n\n\n\npubupdate\n\n\t\n\nWhether UPDATE operations are replicated for tables in the publication. Always true.\n\n\t\n\nBOOLEAN\n\n\n\n\npubdelete\n\n\t\n\nWhether DELETE operations are replicated for tables in the publication. Always true.\n\n\t\n\nBOOLEAN\n\npg_publication_tables\n\nThe pg_publication_tables table in the pg_catalog system schema contains tables replicated by a publication.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\npubname\n\n\t\n\nName of the publication.\n\n\t\n\nTEXT\n\n\n\n\nschemaname\n\n\t\n\nName of the schema containing table.\n\n\t\n\nTEXT\n\n\n\n\ntablename\n\n\t\n\nName of the table.\n\n\t\n\nTEXT\n\npg_subscription\n\nThe pg_subscription table in the pg_catalog system schema contains all subscriptions created in the cluster.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\noid\n\n\t\n\nRow identifier.\n\n\t\n\nINTEGER\n\n\n\n\nsubdbid\n\n\t\n\nnoop value, always 0.\n\n\t\n\nINTEGER\n\n\n\n\nsubname\n\n\t\n\nName of the subscription.\n\n\t\n\nTEXT\n\n\n\n\nsubowner\n\n\t\n\noid of the owner of the subscription.\n\n\t\n\nINTEGER\n\n\n\n\nsubenabled\n\n\t\n\nWhether the subscription is enabled, always true.\n\n\t\n\nBOOLEAN\n\n\n\n\nsubbinary\n\n\t\n\nNoop value, always true.\n\n\t\n\nBOOLEAN\n\n\n\n\nsubstream\n\n\t\n\nNoop value, always true.\n\n\t\n\nBOOLEAN\n\n\n\n\nsubconninfo\n\n\t\n\nConnection string to the publishing cluster.\n\n\t\n\nTEXT\n\n\n\n\nsubslotname\n\n\t\n\nNoop value, always NULL.\n\n\t\n\nTEXT\n\n\n\n\nsubsynccommit\n\n\t\n\nNoop value, always NULL.\n\n\t\n\nTEXT\n\n\n\n\nsubpublications\n\n\t\n\nArray of subscribed publication names. These publications are defined in the publishing cluster.\n\n\t\n\nARRAY\n\npg_subscription_rel\n\nThe pg_subscription_rel table in the pg_catalog system schema contains the state for each replicated relation in each subscription.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nsrsubid\n\n\t\n\nReference to subscription.\n\n\t\n\nINTEGER\n\n\n\n\nsrrelid\n\n\t\n\nReference to relation.\n\n\t\n\nREGCLASS\n\n\n\n\nsrsubstate\n\n\t\n\nReplication state of the relation. State code: i - initializing; d - restoring; r - monitoring, i.e. waiting for new changes; e - error.\n\n\t\n\nTEXT\n\n\n\n\nsrsubstate_reason\n\n\t\n\nError message if there was a replication error for the relation or NULL.\n\n\t\n\nTEXT\n\n\n\n\nsrsublsn\n\n\t\n\nNoop value, always NULL.\n\n\t\n\nLONG"
  },
  {
    "title": "Runtime configuration — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/runtime-config.html",
    "html": "5.6\nRuntime configuration\n\nThe CrateDB cluster can be configured at runtime using the SET and RESET statement. See the Cluster Settings configuration section for details about the supported settings.\n\nIf SET is used with PERSISTENT the change will survive a cluster restart, if used with TRANSIENT the value will reset to the default value or to the value in the configuration file on a restart.\n\ncr> SET GLOBAL PERSISTENT stats.enabled = false;\nSET OK, 1 row affected (... sec)\n\ncr> select sys.cluster.settings['stats']['enabled'] from sys.cluster;\n+------------------------------+\n| settings['stats']['enabled'] |\n+------------------------------+\n| FALSE                        |\n+------------------------------+\nSELECT 1 row in set (... sec)\n\n\nYou can change multiple values at once:\n\ncr> SET GLOBAL TRANSIENT stats.enabled = true,\n... stats.jobs_log_size = 1024, stats.operations_log_size = 4096;\nSET OK, 1 row affected (... sec)\n\ncr> select settings['stats']['enabled'],\n...   settings['stats']['jobs_log_size'],\n...   settings['stats']['operations_log_size']\n... from sys.cluster;\n+-...------------+-...------------------+-...------------------------+\n| ...['enabled'] | ...['jobs_log_size'] | ...['operations_log_size'] |\n+-...------------+-...------------------+-...------------------------+\n| TRUE           |                 1024 |                       4096 |\n+-...------------+-...------------------+-...------------------------+\nSELECT 1 row in set (... sec)\n\n\nIts also possible to save a complete nested object of settings:\n\ncr> SET GLOBAL TRANSIENT stats = {\n...   jobs_log_size = 2048,\n...   operations_log_size = 8192\n... };\nSET OK, 1 row affected (... sec)\n\ncr> SELECT\n...   settings['stats']['jobs_log_size'] AS jobs_size,\n...   settings['stats']['operations_log_size'] AS op_size\n... FROM sys.cluster;\n+-----------+---------+\n| jobs_size | op_size |\n+-----------+---------+\n|      2048 |    8192 |\n+-----------+---------+\nSELECT 1 row in set (... sec)\n\n\nUsing the RESET statement, a setting will be reset to either on node startup defined configuration file value or to its default value:\n\ncr> RESET GLOBAL stats.enabled, stats.operations_log_size;\nRESET OK, 1 row affected (... sec)\n\ncr> SELECT\n...   settings['stats']['jobs_log_size'] AS jobs_size,\n...   settings['stats']['operations_log_size'] AS op_size\n... FROM sys.cluster;\n+-----------+---------+\n| jobs_size | op_size |\n+-----------+---------+\n|      2048 |   10000 |\n+-----------+---------+\nSELECT 1 row in set (... sec)\n\n\nRESET can also be done on objects:\n\ncr> RESET GLOBAL stats;\nRESET OK, 1 row affected (... sec)\n\ncr> SELECT\n...   settings['stats']['jobs_log_size'] AS jobs_size,\n...   settings['stats']['operations_log_size'] AS op_size\n... FROM sys.cluster;\n+-----------+---------+\n| jobs_size | op_size |\n+-----------+---------+\n|     10000 |   10000 |\n+-----------+---------+\nSELECT 1 row in set (... sec)\n"
  },
  {
    "title": "Compatibility — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/appendices/compatibility.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n3.3\nCompatibility\n\nCrateDB aims to provide an SQL implementation that is familiar to anyone having used other databases providing a standards-compliant SQL language. However, it is worth being aware of some unique characteristics in CrateDB’s SQL dialect.\n\nTable of Contents\n\nImplementation Notes\n\nData Types\n\nCreate Table\n\nAlter Table\n\nSystem Information Tables\n\nBLOB Support\n\nTransactions (BEGIN, COMMIT, and ROLLBACK)\n\nUnsupported Features and Functions\n\nImplementation Notes\nData Types\n\nCrateDB supports a set of primitive data types: integer, long, short, double, float, and byte, with the same ranges as corresponding Java types.\n\nThe following table defines how data types of standard SQL map to CrateDB Data Types.\n\nStandard SQL\n\n\t\n\nCrateDB\n\n\n\n\ninteger\n\n\t\n\ninteger, int\n\n\n\n\nbit[8]\n\n\t\n\nbyte\n\n\n\n\nboolean, bool\n\n\t\n\nboolean, bool\n\n\n\n\nchar [(n)], varchar [(n)]\n\n\t\n\nstring\n\n\n\n\ntimestamp\n\n\t\n\ntimestamp\n\n\n\n\nsmallint\n\n\t\n\nshort\n\n\n\n\nbigint\n\n\t\n\nlong\n\n\n\n\nreal\n\n\t\n\nfloat\n\n\n\n\ndouble precision\n\n\t\n\ndouble\n\nCreate Table\n\nCREATE TABLE supports additional storage and table parameters for sharding, replication and routing of the data, and does not support inheritance.\n\nAlter Table\n\nALTER COLUMN and DROP COLUMN actions are not currently supported (see ALTER TABLE).\n\nSystem Information Tables\n\nThe read-only System Information and Information Schema tables have a slightly different schema than specified in standard SQL. They provide schema information and can be queried to get real-time statistical data about the cluster, its nodes, and their shards.\n\nBLOB Support\n\nStandard SQL defines a binary string type, called BLOB or BINARY LARGE OBJECT. With CrateDB, Binary Data is instead stored in separate BLOB Tables (see Blobs) which can be sharded and replicated.\n\nTransactions (BEGIN, COMMIT, and ROLLBACK)\n\nCrateDB is focused on providing analytical capabilities over supporting traditional transactional use cases, and thus it does not provide transaction control. Every statement commits immediately and is replicated within the cluster.\n\nHowever, every row in CrateDB has a version number that is incremented whenever the record is modified. This version number can be used to implement patterns like Optimistic Concurrency Control, which can be used to solve many of the use cases that would otherwise require traditional transactions.\n\nUnsupported Features and Functions\n\nThese features of standard SQL are not supported:\n\nStored Procedures\n\nTriggers\n\nVALUES list used as constant tables\n\nWITH Statements\n\nSequences\n\nInheritance\n\nConstraints\n\nUnique\n\nForeign key\n\nCheck constraints\n\nExclusion constraints\n\nThese functions are either not supported or only partly supported:\n\nAggregate functions\n\nVarious functions available (see Aggregation)\n\nWindow Functions\n\nVarious functions available (see Window Functions)\n\nENUM support functions\n\nIS DISTINCT FROM\n\nNetwork address functions and operators\n\nMathematical functions\n\nCertain functions supported (see Mathematical Functions)\n\nSet returning functions\n\nTrigger functions\n\nXML functions\n\nThe currently supported and unsupported features in CrateDB are exposed in the Information Schema table (see sql_features for usage).\n\nIf you are missing features, functions or dialect improvements and have a great use case for it, let us know on Github. We’re always improving and extending CrateDB, and we love to hear feedback."
  },
  {
    "title": "Logging — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/config/logging.html",
    "html": "5.6\nLogging\n\nCrateDB supports two kinds of logging:\n\nApplication logging with Log4j\n\nJava Virtual Machine (JVM) garbage collection logging\n\nWe use “application” here to distinguish between CrateDB running as a Java application and the JVM itself, which runs CrateDB.\n\nBecause garbage collection logging is a native feature of the JVM it behaves differently and is configured differently.\n\nTable of contents\n\nApplication logging\n\nLog4j\n\nConfiguration file\n\nLog levels\n\nRun-time configuration\n\nJVM logging\n\nGarbage collection\n\nEnvironment variables\n\nApplication logging\nLog4j\n\nCrateDB uses Log4j.\n\nConfiguration file\n\nYou can configure Log4j with the log4j2.properties file in the CrateDB configuration directory.\n\nThe log4j2.properties file is formatted using YAML and simplifies Log4j configuration by allowing you to use the PropertyConfigurator but without having to tediously repeat the log4j prefix.\n\nHere’s one example:\n\nrootLogger.level = info\nrootLogger.appenderRef.console.ref = console\n\n# log query execution errors for easier debugging\nlogger.action.name = org.crate.action.sql\nlogger.action.level = debug\n\nappender.console.type = Console\nappender.console.name = console\nappender.console.layout.type = PatternLayout\nappender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c{1.}] %marker%m%n\n\n\nAnd here is a snippet of the generated properties ready for use with log4j. You get the point.\n\nSee Also\n\nConsult the PropertyConfigurator documentation or the configuration section of the Log4j documentation for more information.\n\nLog levels\n\nPossible log levels are the same as for Log4j, in order of increasing importance:\n\nTRACE\n\nDEBUG\n\nINFO\n\nWARN\n\nERROR\n\nLog levels must be provided as string literals in the SET statement.\n\nNote\n\nBe careful using the TRACE log level because it’s extremely verbose, can obscure other important log messages and even fill up entire data disks in some cases.\n\nRun-time configuration\n\nIt’s possible to set the log level of loggers at runtime using SET, like so:\n\nSET GLOBAL TRANSIENT \"logger.action\" = 'INFO';\n\n\nIn this example, the log level INFO is applied to the action logger.\n\nIn addition to being able to configure any of the standard loggers, you can configure the root (i.e. default) logger using logger._root.\n\nAs with any setting, you can inspect the current configuration by querying the sys.cluster table.\n\nTip\n\nRun-time logging configuration is particularly useful if you are debugging a problem and you want to increase the log level without restarting nodes.\n\nRun-time logging configuration is applied across the whole cluster, and overrides the start-up configuration defined in each respective log4j2.properties file.\n\nCaution\n\nThe RESET statement is supported but logging configuration is only reset when the whole cluster is restarted.\n\nJVM logging\n\nCrateDB exposes some native JVM logging functionality.\n\nGarbage collection\n\nCrateDB logs JVM garbage collection times using the built-in garbage collection logging of the JVM.\n\nEnvironment variables\n\nThe following environment variables can be used to configure garbage collection logging.\n\nCRATE_DISABLE_GC_LOGGING: boolean integer (default: 0)\n\nWhether to disable garbage collection logging.\n\nSet to 1 to disable.\n\nNote\n\nSince CrateDB 3.0, Garbage collection logging is enabled by default.\n\nCRATE_GC_LOG_DIR: path to logs directory (default: varies)\n\nThe log file directory.\n\nFor a basic installation, the logs directory in the CRATE_HOME directory is the default.\n\nIf you have installed a CrateDB Linux package, the default directory is /var/log/crate instead.\n\nCRATE_GC_LOG_SIZE: file size (default: 64m)\n\nMaximum file size of log files before they are rotated.\n\nCRATE_GC_LOG_FILES: number (default: 16)\n\nThe amount of files kept in rotation.\n\nCaution\n\nWith the default configuration of 16 rotated 64 megabyte log files, garbage collection logs will grow to occupy one gigabyte on disk.\n\nMake sure you have enough available disk space for configuration."
  },
  {
    "title": "General SQL — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/sql/general/index.html",
    "html": "3.3\nGeneral SQL\n\nTable of Contents\n\nConstraints\nValue Expressions\nLexical Structure"
  },
  {
    "title": "Cloud Discovery — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/admin/discovery.html",
    "html": "3.3\nCloud Discovery\n\nTable of Contents\n\nAmazon EC2 Discovery\n\nMicrosoft Azure Discovery\n\nRequirements\n\nBasic Configuration\n\nAuthentication\n\nAmazon EC2 Discovery\n\nCrateDB has native discovery support when running a cluster with Amazon Web Services (AWS). The discovery mechanism uses the Amazon EC2 API to to generate the list of hosts for the unicast host discovery (see Unicast Host Discovery).\n\nThere is a best practice how to configure and run a CrateDB cluster on Amazon EC2.\n\nMicrosoft Azure Discovery\n\nCrateDB has native discovery support when running a cluster on Microsoft Azure infrastructure. The discovery mechanism uses the Azure Resource Management (ARM) API to generate the list of hosts for the unicast host discovery (see Unicast Host Discovery).\n\nThe discovery mechanism is implemented as a plugin and resides within the plugin folder of the CrateDB installation. However, since the Azure Java SDKs does have a lot of dependencies and we want to keep the plugin jar small in size the dependencies are not included in the CrateDB distribution.\n\nRequirements\n\nTo make the plugin work you have to add the following Java libraries to the $CRATE_HOME/plugins/crate-azure-discovery folder:\n\nactivation-1.1.jar\nadal4j-1.0.0.jar\nazure-core-0.9.3.jar\nazure-mgmt-compute-0.9.3.jar\nazure-mgmt-network-0.9.3.jar\nazure-mgmt-resources-0.9.3.jar\nazure-mgmt-storage-0.9.3.jar\nazure-mgmt-utility-0.9.3.jar\nbcprov-jdk15on-1.51.jar\ncommons-io-2.4.jar\ncommons-lang-2.6.jar\ncommons-lang3-3.3.1.jar\ngson-2.2.4.jar\njackson-core-asl-1.9.2.jar\njackson-jaxrs-1.9.2.jar\njackson-mapper-asl-1.9.2.jar\njackson-xc-1.9.2.jar\njavax.inject-1.jar\njaxb-api-2.2.2.jar\njaxb-impl-2.2.3-1.jar\njcip-annotations-1.0.jar\njersey-client-1.13.jar\njersey-core-1.13.jar\njersey-json-1.13.jar\njettison-1.1.jar\njson-smart-1.1.1.jar\nlang-tag-1.4.jar\nmail-1.4.7.jar\nnimbus-jose-jwt-3.1.2.jar\noauth2-oidc-sdk-4.5.jar\nstax-api-1.0-2.jar\n\n\nYou can download the libraries using a simple build.gradle file:\n\napply plugin: \"java\"\n\nrepositories {\n  jcenter()\n}\n\ndependencies {\n    compile('com.microsoft.azure:azure-mgmt-utility:0.9.3') {\n        exclude group: 'stax', module: 'stax-api'\n        exclude group: 'org.slf4j', module: 'slf4j-api'\n        exclude group: 'commons-logging', module: 'commons-logging'\n        exclude group: 'commons-codec', module: 'commons-codec'\n        exclude group: 'com.fasterxml.jackson.core', module: 'jackson-core'\n        exclude group: 'org.apache.httpcomponents', module: 'httpclient'\n    }\n}\n\ntask azureLibs (type: Copy, dependsOn: ['compileJava']) {\n  from configurations.testRuntime\n  into \"libs\"\n}\n\n\nRunning gradle azureLibs will fetch the required jars and put them into the libs/ folder from where you can copy them into the plugin folder.\n\nBasic Configuration\n\nTo enable Azure discovery simply change the discovery.zen.hosts_provider setting to azure:\n\ndiscovery.zen.hosts_provider: azure\n\n\nThe discovery mechanism can discover CrateDB instances within the same vnet or the same subnet of the same resource group. By default it will the vnet.\n\nYou can change the behaviour using the discovery.azure.method setting:\n\ndiscovery.azure.method: subnet\n\n\nThe used resource group also needs to be provided:\n\ncloud.azure.management.resourcegroup.name: production\n\nAuthentication\n\nThe discovery plugin requires authentication as service principle. To do so, you have to create an Active Directory application with a password. We recommened to follow the AD Application Guide .\n\nThe configuration settings for authentication are as follows:\n\ncloud.azure.management:\n  subscription.id: my-id\n  tenant.id: my-tenant\n  app:\n    id: my-app\n    secret: my-secret\n\n\nFor a complete list of settings please refer to Discovery on Microsoft Azure."
  },
  {
    "title": "SQL Statements — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/sql/statements/index.html",
    "html": "3.3\nSQL Statements\n\nTable of Contents\n\nALTER CLUSTER\nALTER TABLE\nALTER USER\nBEGIN\nCOMMIT\nCOPY FROM\nCOPY TO\nCREATE ANALYZER\nCREATE BLOB TABLE\nCREATE FUNCTION\nCREATE INGEST RULE\nCREATE REPOSITORY\nCREATE SNAPSHOT\nCREATE TABLE\nCREATE USER\nCREATE VIEW\nDEALLOCATE\nDELETE\nDENY\nDROP ANALYZER\nDROP FUNCTION\nDROP INGEST RULE\nDROP REPOSITORY\nDROP SNAPSHOT\nDROP TABLE\nDROP USER\nDROP VIEW\nEXPLAIN\nGRANT\nINSERT\nKILL\nOPTIMIZE\nREFRESH\nRESTORE SNAPSHOT\nREVOKE\nSELECT\nSET and RESET\nSET TRANSACTION\nSET LICENSE\nSHOW COLUMNS\nSHOW CREATE TABLE\nSHOW SCHEMAS\nSHOW TABLES\nSHOW (session settings)\nUPDATE"
  },
  {
    "title": "Usage Data Collector — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/admin/udc.html",
    "html": "3.3\nUsage Data Collector\n\nThe CrateDB Usage Data Collector (UDC) is a sub-system that gathers usage data, reporting it to the UDC server at https://udc.crate.io. It is easy to disable, and does not collect any data that is confidential. For more information about what is being sent, see below.\n\nCrateDB uses this information as a form of automatic, effortless feedback from the community. We want to verify that we are doing the right thing by matching download statistics with usage statistics. After each release, we can see if there is a larger retention span of the server software.\n\nThe data collected is clearly stated here. If any future versions of this system collect additional data, we will clearly announce those changes.\n\nCrateDB is concerned about your privacy. We do not disclose any personally identifiable information.\n\nTable of Contents\n\nTechnical Information\n\nAdmin UI Tracking\n\nConfiguration\n\nHow to Disable UDC\n\nBy Configuration\n\nBy System Property\n\nTechnical Information\n\nTo gather good statistics about CrateDB usage, UDC collects this information:\n\nName\n\n\t\n\nDescription\n\n\n\n\nKernel Version\n\n\t\n\nThe build number, and if there are any modifications to the kernel.\n\n\n\n\nCluster Id\n\n\t\n\nA randomized globally unique ID created every time the whole cluster is restarted.\n\n\n\n\nMaster\n\n\t\n\nBoolean whether the current node is master.\n\n\n\n\nPing Count\n\n\t\n\nUDC holds an internal counter per node which is incremented for every ping, and reset on every restart of the a node.\n\n\n\n\nCrateDB Version\n\n\t\n\nThe CrateDB version.\n\n\n\n\nJava Version\n\n\t\n\nThe Java version CrateDB is currently running with.\n\n\n\n\nHardware Address\n\n\t\n\nMAC address to uniquely identify instances behind firewalls.\n\n\n\n\nProcessor count\n\n\t\n\nNumber of available CPUs as reported by Runtime.availableProcessors\n\n\n\n\nEnterprise\n\n\t\n\nIdentifies whether the Enterprise Edition is used.\n\n\n\n\nLicense\n\n\t\n\nLicense information of the CrateDB Enterprise Edition.\n\nAfter startup, UDC waits for 10 minutes before sending the first ping. It does this for two reasons; first, we don’t want the startup to be slower because of UDC, and secondly, we want to keep pings from automatic tests to a minimum. By default, UDC is sending pings every 24 hours. The ping to the UDC servers is done with a HTTP GET.\n\nAdmin UI Tracking\n\nSince Admin UI v0.16.0 we are tracking the user ID along with the cluster ID to know how many active users are currently using CrateDB.\n\nConfiguration\n\nThe Usage Data Collector can be configured by adapting the crate.yml configuration file or adding a system property setting. Refer to Usage Data Collector to see how these settings can be accessed and how they are configured.\n\nHow to Disable UDC\n\nBelow are two ways you can disable UDC. However we hope you support us offering the open source edition, and leave UDC on, so we learn how many people use CrateDB.\n\nNote\n\nIf UDC is disabled on the CrateDB server the Intercom service is deactivated.\n\nBy Configuration\n\nJust add following to your crate.yml configuration file:\n\nudc.enabled:  false\n\nBy System Property\n\nIf you do not want to make any change to the jars or to the configuration, a system property setting like this will also make sure that UDC is never activated:\n\n-Cudc.enabled=false\n"
  },
  {
    "title": "Snapshots — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/admin/snapshots.html",
    "html": "3.3\nSnapshots\n\nTable of Contents\n\nSnapshot\n\nCreating a Repository\n\nCreating a Snapshot\n\nRestore\n\nCleanup\n\nDropping Snapshots\n\nDropping Repositories\n\nRequirements for Using HDFS Repositories\n\nWorking with a Secured HA HDFS\n\nSnapshot\n\nIn CrateDB, backups are called Snapshots. They represent the state of the tables in a CrateDB cluster at the time the Snapshot was created. A Snapshot is always stored in a Repository which has to be created first.\n\nCaution\n\nYou cannot snapshot BLOB tables.\n\nCreating a Repository\n\nRepositories are used to store, manage and restore snapshots.\n\nThey are created using the CREATE REPOSITORY statement:\n\ncr> CREATE REPOSITORY where_my_snapshots_go TYPE fs\n... WITH (location='repo_path', compress=true);\nCREATE OK, 1 row affected (... sec)\n\n\nRepositories are uniquely identified by their name. Every repository has a specific type which determines how snapshots are stored.\n\nCrateDB supports different repository types: fs, hdfs, s3, and url. Support for further types can be added using plugins.\n\nThe creation of a repository configures it inside the CrateDB cluster. In general no data is written, no snapshots inside repositories changed or deleted. This way you can tell the CrateDB cluster about existing repositories which already contain snapshots.\n\nCreating a repository with the same name will result in an error:\n\ncr> CREATE REPOSITORY where_my_snapshots_go TYPE fs\n... WITH (location='another_repo_path', compress=false);\nSQLActionException[RepositoryAlreadyExistsException: Repository 'where_my_snapshots_go' already exists]\n\nCreating a Snapshot\n\nSnapshots are created inside a repository and can contain any number of tables. The CREATE SNAPSHOT statement is used to create a snapshots:\n\ncr> CREATE SNAPSHOT where_my_snapshots_go.snapshot1 ALL\n... WITH (wait_for_completion=true, ignore_unavailable=true);\nCREATE OK, 1 row affected (... sec)\n\n\nA snapshot is referenced by the name of the repository and the snapshot name, separated by a dot. If ALL is used, all user created tables of the cluster (except blob tables) are stored inside the snapshot.\n\nIt’s possible to only save a specific subset of tables in the snapshot by listing them explicitly:\n\ncr> CREATE SNAPSHOT where_my_snapshots_go.snapshot2 TABLE quotes, doc.locations\n... WITH (wait_for_completion=true);\nCREATE OK, 1 row affected (... sec)\n\n\nEven single partition of Partitioned Tables can be selected for backup. This is especially useful if old partitions need to be deleted but it should be possible to restore them if needed:\n\ncr> CREATE SNAPSHOT where_my_snapshots_go.snapshot3 TABLE\n...    locations,\n...    parted_table PARTITION (date='1970-01-01')\n... WITH (wait_for_completion=true);\nCREATE OK, 1 row affected (... sec)\n\n\nSnapshots are incremental. Snapshots of the same cluster created later only store data not already contained in the repository.\n\nAll examples above are used with the argument wait_for_completion set to true. As described in the CREATE REPOSITORY reference documentation, by doing this, the statement will only respond (successfully or not) when the snapshot is fully created. Otherwise the snapshot will be created in the background and the statement will immediately respond as successful. The status of a created snapshot can be retrieved by querying the sys.snapshots system table.\n\nRestore\n\nCaution\n\nIf you are restoring a snapshot into a newer version of CrateDB, be sure to check the Release Notes for upgrade instructions.\n\nOnce a snapshot is created, it can be used to restore its tables to the state when the snapshot was created.\n\nTo get basic information about snapshots the sys.snapshots table can be queried:\n\ncr> SELECT repository, name, state, concrete_indices\n... FROM sys.snapshots\n... ORDER BY repository, name;\n+-----------------------+-----------+---------+--------------------...-+\n| repository            | name      | state   | concrete_indices       |\n+-----------------------+-----------+---------+--------------------...-+\n| where_my_snapshots_go | snapshot1 | SUCCESS | [...]                  |\n| where_my_snapshots_go | snapshot2 | SUCCESS | [...]                  |\n| where_my_snapshots_go | snapshot3 | SUCCESS | [...]                  |\n+-----------------------+-----------+---------+--------------------...-+\nSELECT 3 rows in set (... sec)\n\n\nTo restore a table from a snapshot we have to drop it beforehand:\n\ncr> DROP TABLE quotes;\nDROP OK, 1 row affected (... sec)\n\n\nRestoring a snapshot using the RESTORE SNAPSHOT statement.:\n\ncr> RESTORE SNAPSHOT where_my_snapshots_go.snapshot2 TABLE quotes WITH (wait_for_completion=true);\nRESTORE OK, 1 row affected (... sec)\n\n\nIn this case only the quotes table from snapshot where_my_snapshots_go.snapshot2 is restored. Using ALL instead of listing all tables restores the whole snapshot.\n\nIt’s not possible to restore tables that exist in the current cluster:\n\ncr> RESTORE SNAPSHOT where_my_snapshots_go.snapshot2 TABLE quotes;\nSQLActionException[RelationAlreadyExists: Relation 'doc.quotes' already exists.]\n\n\nSingle partitions can be either imported into an existing partitioned table the partition belongs to.\n\ncr> RESTORE SNAPSHOT where_my_snapshots_go.snapshot3 TABLE\n...    parted_table PARTITION (date='1970-01-01')\n... WITH (wait_for_completion=true);\nRESTORE OK, 1 row affected (... sec)\n\n\nOr if no matching partition table exists, it will be implicitly created during restore.\n\nCaution\n\nThis is only possible with CrateDB version 0.55.5 or greater!\n\nSnapshots of single partitions that have been created with earlier versions of CrateDB may be restored, but lead to orphaned partitions!\n\nWhen using CrateDB prior to 0.55.5 you will have to create the table schema first before restoring.\n\ncr> DROP TABLE parted_table;\nDROP OK, 1 row affected (... sec)\n\ncr> RESTORE SNAPSHOT where_my_snapshots_go.snapshot3 TABLE\n...    parted_table PARTITION (date=0)\n... WITH (wait_for_completion=true);\nRESTORE OK, 1 row affected (... sec)\n\nCleanup\nDropping Snapshots\n\nDropping a snapshot deletes all files inside the repository that are only referenced by this snapshot. Due to its incremental nature this might be very few files (e.g. for intermediate snapshots). Snapshots are dropped using the DROP SNAPSHOT command:\n\ncr> DROP SNAPSHOT where_my_snapshots_go.snapshot3;\nDROP OK, 1 row affected (... sec)\n\nDropping Repositories\n\nIf a repository is not needed anymore, it can be dropped using the DROP REPOSITORY statement:\n\ncr> DROP REPOSITORY \"OldRepository\";\nDROP OK, 1 row affected (... sec)\n\n\nThis statement, like CREATE REPOSITORY, does not manipulate repository contents but only deletes stored configuration for this repository in the cluster state, so it’s not accessible any more.\n\nRequirements for Using HDFS Repositories\n\nCrateDB supports repositories of type hdfs type by default, but required Hadoop java client libraries are not included in any CrateDB distribution and need to be added to CrateDB’s hdfs plugin folder. By default this is $CRATE_HOME/plugins/es-repository-hdfs\n\nBecause some libraries Hadoop depends on are also required (and so deployed) by CrateDB, only the Hadoop libraries listed below must be copied into the $CRATE_HOME/plugins/es-repository-hdfs folder, other libraries will be ignored:\n\n- apacheds-i18n-2.0.0-M15.jar\n- apacheds-kerberos-codec-2.0.0-M15.jar\n- api-asn1-api-1.0.0-M20.jar\n- api-util-1.0.0-M20.jar\n- avro-1.7.4.jar\n- commons-compress-1.4.1.jar\n- commons-configuration-1.6.jar\n- commons-digester-1.8.jar\n- commons-httpclient-3.1.jar\n- commons-io-2.4.jar\n- commons-lang-2.6.jar\n- commons-net-3.1.jar\n- curator-client-2.7.1.jar\n- curator-framework-2.7.1.jar\n- curator-recipes-2.7.1.jar\n- gson-2.2.4.jar\n- hadoop-annotations-2.8.1.jar\n- hadoop-auth-2.8.1.jar\n- hadoop-client-2.8.1.jar\n- hadoop-common-2.8.1.jar\n- hadoop-hdfs-2.8.1.jar\n- hadoop-hdfs-client-2.8.1.jar\n- htrace-core4-4.0.1-incubating.jar\n- jackson-core-asl-1.9.13.jar\n- jackson-mapper-asl-1.9.13.jar\n- jline-0.9.94.jar\n- jsp-api-2.1.jar\n- leveldbjni-all-1.8.jar\n- protobuf-java-2.5.0.jar\n- paranamer-2.3.jar\n- snappy-java-1.0.4.1.jar\n- servlet-api-2.5.jar\n- xercesImpl-2.9.1.jar\n- xmlenc-0.52.jar\n- xml-apis-1.3.04.jar\n- xz-1.0.jar\n- zookeeper-3.4.6.jar\n\n\nNote\n\nOnly Hadoop version 2.x is supported and as of writing this documentation, the latest stable Hadoop (YARN) version is 2.8.1. Required libraries may differ for other versions.\n\nCrate’s packaged es-repository-hdfs plugin depends on a different version of commons-collections, htrace, and xml-apis than Hadoop depends, and the presence of both versions will result in Jar Hell. The es-repository-hdfs plugin’s dependencies should take precedence when encountered, but the above list works for Hadoop v2.8.1.\n\nWorking with a Secured HA HDFS\n\nFor users with Kerberos-secured HA NameNode configurations, configuring the plugin is easy.\n\nFirst, the core-site.xml and hdfs-site.xml files for the HDFS cluster need to be placed in an empty JAR and added to the $CRATE_HOME/plugins/es-repository-hdfs directory. Because Crate plugins are loaded as collections of JARs, plain xml files simply won’t be loaded and the HDFS client won’t be able to find the configuration files. These files should include any relevant keys and values for communicating with the NameNode; this includes any HA config, authentication method, etc.\n\nNote\n\nMake sure the load_defaults parameter to CREATE REPOSITORY is true (it is by default) as this will load the values as described here.\n\nNext, if kerberos is the authentication method, the hdfs plugin will need a keytab to authenticate with. This needs to be placed in a separate config directory for the plugin, $CRATE_HOME/config/repository-hdfs, and must be named krb5.keytab.\n\nLastly, the security.principal parameter passed in the CREATE REPOSITORY statement must be a fully-qualified kerberos identity: a service principal name (SPN) or a user principal name (UPN) will work.\n\nNote\n\nOnly one kerberos identity is supported per Crate cluster.\n\nIf all this has been configured correctly, the HDFS repository plugin should be able to communicate with an optionally-HA, secured HDFS cluster."
  },
  {
    "title": "JMX Monitoring — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/admin/monitoring.html",
    "html": "3.3\nJMX Monitoring\n\nThe JMX monitoring feature exposes query metrics via the JMX API.\n\nNote\n\nJMX monitoring is an enterprise feature.\n\nTable of Contents\n\nSetup\n\nEnable the Enterprise License\n\nEnable Collecting Stats\n\nEnable the JMX API\n\nUsing Docker\n\nJMX Beans\n\nQueryStats MBean\n\nNodeStatus MBean\n\nNodeInfo MBean\n\nConnections MBean\n\nThreadPools MXBean\n\nCircuitBreakers MXBean\n\nExposing JMX via HTTP\n\nSetup\nEnable the Enterprise License\n\nYou can enable the Enterprise License via the CrateDB configuration file.\n\nEnable Collecting Stats\n\nBy default, Collecting Stats is enabled. You can disable collecting stats via the CrateDB configuration file or by running this statement:\n\ncr> SET GLOBAL \"stats.enabled\" = FALSE;\n\nEnable the JMX API\n\nTo monitor CrateDB using the JMX API, you must set the following system properties before you start CrateDB:\n\ncom.sun.management.jmxremote\ncom.sun.management.jmxremote.port=<JMX_PORT>\ncom.sun.management.jmxremote.ssl=false\ncom.sun.management.jmxremote.authenticate=false\n\n\nHere, <JMX_PORT> sets the port number of your JMX server. JMX SSL and authentication are currently not supported.\n\nMore information about the JMX monitoring properties can be found in the JMX documentation.\n\nYou can set the Java system properties with the -D option:\n\nsh$ ./bin/crate -Dcom.sun.management.jmxremote \\\n...             -Dcom.sun.management.jmxremote.port=7979 \\\n...             -Dcom.sun.management.jmxremote.ssl=false \\\n...             -Dcom.sun.management.jmxremote.authenticate=false\n\n\nHowever, the recommended way to set system properties is via the CRATE_JAVA_OPTS environment variable, like so:\n\nsh$ export CRATE_JAVA_OPTS=\"$CRATE_JAVA_OPTS \\\n      -Dcom.sun.management.jmxremote \\\n      -Dcom.sun.management.jmxremote.port=7979 \\\n      -Dcom.sun.management.jmxremote.ssl=false \\\n      -Dcom.sun.management.jmxremote.authenticate=false\"\nsh$ ./bin/crate\n\n\nIf you’re using the CrateDB Debian or RPM packages, you can set this environment variable via the /etc/default/crate configuration file.\n\nUsing Docker\n\nTo enable JMX monitoring when running CrateDB in a Docker container you have to set the following additional Java system properties:\n\n-Djava.rmi.server.hostname=<RMI_PORT>\n-Dcom.sun.management.jmxremote.rmi.port=<RMI_HOSTNAME>\n\n\nHere, <RMI_HOSTNAME> is the IP address or hostname of the Docker host and <RMI_PORT> is the statically assigned port of the RMI server. For convenience, <RMI_PORT> can be set to the same port the JMX server listens on.\n\nThe <RMI_HOSTNAME> and <RMI_PORT> can be used by JMX clients (e.g. JConsole or VisualVM) to connect to the JMX server.\n\nHere’s an example Docker command:\n\nsh> docker run -d -e CRATE_JAVA_OPTS='\\\n      -Dcom.sun.management.jmxremote\n      -Dcom.sun.management.jmxremote.port=7979 \\\n      -Dcom.sun.management.jmxremote.ssl=false \\\n      -Dcom.sun.management.jmxremote.authenticate=false \\\n      -Dcom.sun.management.jmxremote.rmi.port=<RMI_HOSTNAME> \\\n      -Djava.rmi.server.hostname=7979' \\\n      -p 7979:7979 crate \\\n      crate -Cnetwork.host=_site_\n\n\nHere, again, <RMI_HOSTNAME> is the IP address or hostname of the Docker host.\n\nJMX Beans\nQueryStats MBean\n\nThe QueryStats JMX MBean exposes query frequency and average duration in milliseconds for SELECT, UPDATE, DELETE, and INSERT queries. The frequency and average duration are calculated against the interval of time since the QueryStats MBean was queried last. Besides the averages, the QueryStats MBean exposes the sum of durations, in milliseconds, total and failed count of all statements executed since the node was started, grouped by type, for SELECT, UPDATE, DELETE, INSERT, MANAGEMENT, DDL, COPY and UNDEFINED queries.\n\nWarning\n\nThe query frequency and average duration metrics have been deprecated and will be removed in the future. We recommend using the total count and sum of durations metrics instead.\n\nMetrics can be accessed using the JMX MBean object name io.crate.monitoring:type=QueryStats and the following attributes:\n\nFrequency of operations/second in the interval of time since the last time the QueryStats MBean was queried:\n\nSelectQueryFrequency\n\nInsertQueryFrequency\n\nUpdateQueryFrequency\n\nDeleteQueryFrequency\n\nOverallQueryFrequency\n\nAverage duration of operations, in milliseconds, measured against the interval of time since the last time the QueryStats MBean was queried:\n\nSelectQueryAverageDuration\n\nInsertQueryAverageDuration\n\nUpdateQueryAverageDuration\n\nDeleteQueryAverageDuration\n\nOverallQueryAverageDuration\n\nStatements total count since the node was started:\n\nSelectQueryTotalCount\n\nInsertQueryTotalCount\n\nUpdateQueryTotalCount\n\nDeleteQueryTotalCount\n\nManagementQueryTotalCount\n\nDDLQueryTotalCount\n\nCopyQueryTotalCount\n\nUndefinedQueryTotalCount\n\nStatements failed count since the node was started:\n\nSelectQueryFailedCount\n\nInsertQueryFailedCount\n\nUpdateQueryFailedCount\n\nDeleteQueryFailedCount\n\nManagementQueryFailedCount\n\nDDLQueryFailedCount\n\nCopyQueryFailedCount\n\nUndefinedQueryFailedCount\n\nThe sum of the durations, in milliseconds, since the node was started, of all statement executions grouped by type:\n\nSelectQuerySumOfDurations\n\nInsertQuerySumOfDurations\n\nUpdateQuerySumOfDurations\n\nDeleteQuerySumOfDurations\n\nManagementQuerySumOfDurations\n\nDDLQuerySumOfDurations\n\nCopyQuerySumOfDurations\n\nUndefinedQuerySumOfDurations\n\nNodeStatus MBean\n\nThe NodeStatus JMX MBean exposes the status of the current node as boolean values.\n\nNodeStatus can be accessed using the JMX MBean object name io.crate.monitoring:type=NodeStatus and the following attributes:\n\nReady\n\nDefines if the node is able to process SQL statements.\n\nNodeInfo MBean\n\nThe NodeInfo JMX MBean exposes information about the current node;\n\nNodeInfo can be accessed using the JMX MBean object name io.crate.monitoring:type=NodeInfo and the following attributes:\n\nClusterStateVersion\n\nProvides the version of the current applied cluster state\n\nNodeId\n\nProvides the unique identifier of the node in the cluster\n\nNodeName\n\nProvides the human friendly name of the node\n\nConnections MBean\n\nThe Connections MBean exposes information about any open connections to a CrateDB node.\n\nIt can be accessed using the io.crate.monitoring:type=Connections object name and has the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nHttpOpen\n\n\t\n\nThe number of currently established connections via HTTP\n\n\n\n\nHttpTotal\n\n\t\n\nThe number of total connections established via HTTP over the life time of a node\n\n\n\n\nPsqlOpen\n\n\t\n\nThe number of currently established connections via the PostgreSQL protocol\n\n\n\n\nPsqlTotal\n\n\t\n\nThe number of total connections established via the PostgreSQL protocol over the life time of a node\n\n\n\n\nTransportOpen\n\n\t\n\nThe number of currently established connections via the transport protocol\n\nThreadPools MXBean\n\nThe ThreadPools MXBean exposes statistical information about the used thread pools of a CrateDB node.\n\nIt can be accessed using the io.crate.monitoring:type=ThreadPools object name and has following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nGeneric\n\n\t\n\nThread pool statistics of the generic thread pool.\n\n\n\n\nSearch\n\n\t\n\nThread pool statistics of the search thread pool used by read statements on user generated tables.\n\n\n\n\nBulk\n\n\t\n\nThread pool statistics of the bulk thread pool used for writing and deleting data.\n\n\n\n\nManagement\n\n\t\n\nThread pool statistics of the management thread pool used by management tasks like stats collecting, repository information, shard allocations, etc.\n\n\n\n\nIndex\n\n\t\n\nThread pool statistics of the index thread pool used for writing blobs.\n\n\n\n\nFlush\n\n\t\n\nThread pool statistics of the flush thread pool used for fsyncing to disk and merging segments in the storage engine.\n\n\n\n\nRefresh\n\n\t\n\nThread pool statistics of the refresh thread pool used for automatic and on-demand refreshing of tables\n\n\n\n\nSnapshot\n\n\t\n\nThread pool statistics of the snapshot thread pool used for creating and restoring snapshots.\n\n\n\n\nForceMerge\n\n\t\n\nThread pool statistics of the force_merge thread pool used when running an optimize statement.\n\n\n\n\nListener\n\n\t\n\nThread pool statistics of the listener thread pool used on client nodes for asynchronous result listeners.\n\n\n\n\nGet\n\n\t\n\nThread pool statistics of the get thread pool used when querying sys.nodes or sys.shards.\n\n\n\n\nFetchShardStarted\n\n\t\n\nThread pool statistics of the fetch_shard_started thread pool used on shard allocation.\n\n\n\n\nFetchShardStore\n\n\t\n\nThread pool statistics of the fetch_shard_store used on shard replication.\n\nEach of them returns a CompositeData object containing detailed statistics of each thread pool with the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\npoolSize\n\n\t\n\nThe current number of threads in the pool.\n\n\n\n\nlargestPoolSize\n\n\t\n\nThe largest number of threads that have ever simultaneously been in the pool.\n\n\n\n\nqueueSize\n\n\t\n\nThe current number of tasks in the queue.\n\n\n\n\nactive\n\n\t\n\nThe approximate number of threads that are actively executing tasks.\n\n\n\n\ncompleted\n\n\t\n\nThe approximate total number of tasks that have completed execution.\n\n\n\n\nrejected\n\n\t\n\nThe number of rejected executions.\n\nCircuitBreakers MXBean\n\nThe CircuitBreaker MXBean exposes statistical information about all availabe circuit breakers of a CrateDB node.\n\nIt can be accessed using the io.crate.monitoring:type=CircuitBreakers object name and has following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nParent\n\n\t\n\nStatistics of the parent circuit breaker containing summarized counters accross all circuit breakers.\n\n\n\n\nQuery\n\n\t\n\nStatistics of the query circuit breaker used to account memory usage of SQL execution including intermediate states e.g. on aggreation and resulting rows.\n\n\n\n\nJobsLog\n\n\t\n\nStatistics of the jobs_log circuit breaker used to account memory usage of the sys.jobs_log table.\n\n\n\n\nOperationsLog\n\n\t\n\nStatistics of the operations_log circuit breaker used to account memory usage of the sys.operations_log table.\n\n\n\n\nFieldData\n\n\t\n\nStatistics of the field_data circuit breaker used for estimating the amount of memory a field will require to be loaded into memory.\n\n\n\n\nInFlightRequests\n\n\t\n\nStatistics of the in_flight_requests circuit breaker used to account memory usage of all incoming requests on transport or HTTP level.\n\n\n\n\nRequest\n\n\t\n\nStatistics of the request circuit breaker used to account memory usage of per-request data strucutre.\n\nEach of them returns a CompositeData object containing detailed statistics of each circuit breaker with the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nname\n\n\t\n\nThe circuit breaker name this statistic belongs to.\n\n\n\n\nused\n\n\t\n\nThe currently accounted used memory estimations.\n\n\n\n\nlimit\n\n\t\n\nThe configured limit when to trip.\n\n\n\n\noverhead\n\n\t\n\nThe configured overhead used to account estimations.\n\n\n\n\ntrippedCount\n\n\t\n\nThe total number of occured trips.\n\nExposing JMX via HTTP\n\nThe JMX metrics and a readiness endpoint can be exposed via HTTP (e.g. to be used by Prometheus) by using the Crate JMX HTTP Exporter Java agent. See the README in the Crate JMX HTTP Exporter repository for more information."
  },
  {
    "title": "Jobs Management — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/admin/jobs-management.html",
    "html": "3.3\nJobs Management\n\nEach executed sql statement results in a corresponding job. Jobs that are currently executing are logged in the system table sys.jobs (see Jobs, Operations, and Logs).\n\nTo obtain the UUID of a job, stats needs to be enabled (see Collecting Stats). Job logging can be disabled by setting the queue size to zero.\n\nKilling an active job forces CrateDB to stop its execution on the cluster immediately. There are two different SQL commands available for killing jobs.\n\nThe KILL ALL statement stops every single job on each node that is running. It returns the total number of contexts of all jobs that have been killed. A job can have contexts on multiple nodes.\n\ncr> kill all;\nKILL OK, ... rows affected (... sec)\n\n\nKILL job_id kills one single job with the specified job_id. Like KILL ALL it returns the total number of contexts of that job killed on all nodes.\n\ncr> kill '175011ce-9bbc-45f2-a86a-5b7f993a93a6';\nKILL OK, ... rows affected (... sec)\n\n\nSee KILL for detailed syntax information on KILL statements."
  },
  {
    "title": "Secured Communications (SSL/TLS) — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/admin/ssl.html",
    "html": "3.3\nSecured Communications (SSL/TLS)\n\nSecured communication allows you to encrypt traffic between the CrateDB node and a client. This applies to connections using HTTP (i.e. Admin UI, Crash, HTTP Endpoint), the PostgreSQL Wire Protocol (i.e. JDBC, psql), and MQTT Ingestion Source.\n\nConnections are secured using Transport Layer Security (TLS).\n\nNote that once SSL is enabled for HTTP connections, only connections using HTTPS are allowed. Same applies to secure MQTT connections. This is in contrast to the PostgreSQL Wire Protocol, which still allows non-encrypted connections when SSL is enabled. If you want to enforce SSL usage, please consult the Host Based Authentication (HBA).\n\nNote\n\nSecured Communications is an Enterprise Edition feature.\n\nTable of Contents\n\nSSL/TLS Configuration\n\nConfigure the KeyStore\n\nConfigure a Separate Truststore\n\nConnecting to a CrateDB Node Using HTTPS\n\nConnect to a CrateDB Node Using the Admin UI\n\nConnect to a CrateDB Node Using Crash\n\nConnect to a CrateDB Node Using REST\n\nConnecting to a CrateDB Node Using PostgreSQL Wire Protocol With SSL/TLS\n\nConnect to a CrateDB Node Using JDBC\n\nConnect to a CrateDB Node Using psql\n\nConnecting to a CrateDB MQTT Endpoint With SSL/TLS\n\nSetting up a Keystore/Truststore With a Certificate Chain\n\nGenerate Keystore With a Private Key\n\nGenerate a Certificate Signing Request\n\nOptional: Use a Self-Signed Certificate to Act as a Certificate Authority (CA)\n\nGenerate a Self-Signed Certificate\n\nGenerate a Signed Cert\n\nImport the CA Certificate Into the Keystore\n\nImport CA Into Truststore\n\nImport the Signed Certificate\n\nConfiguring CrateDB\n\nSSL/TLS Configuration\n\nTo enable SSL a keystore and a few configuration changes are necessary. These changes need to be made in the crate.yml file on each node that should have secure communications enabled.\n\nSkip to Generate Keystore With a Private Key for a step-by-step instruction on how to create a keystore.\n\nOnce the keystore (and optional truststore) is created, continue with the following steps:\n\nSet ssl.psql.enabled, ssl.http.enabled or ssl.ingestion.mqtt.enabled to true.\n\nConfigure the KeyStore\n\n(Optional) Configure a Separate Truststore\n\nConfigure the KeyStore\n\nSSL/TLS needs a keystore. The keystore holds the node certificate(s) which should be signed by a certificate authority (CA). A third-party CA or your organization’s existing CA can be used.\n\nWhen a client connects to a node using SSL/TLS, the client receives the certificate provided by the node and will determine if the node’s certificate is valid, trusted, and matches the hostname or IP address it is trying to connect to.\n\nCaution\n\nTechnically, it’s possible to disable CA checks for certificates on the client. It is strongly recommended however to use certificates signed by an official CA or by a private CA (company PKI) that is also known to the client. This will help to ensure that establishing trust is as painless as possible.\n\nSee Generate Keystore With a Private Key for information about how to create a keystore.\n\nOnce the keystore is prepared, define the absolute file path to the keystore .jks file on the node using ssl.keystore_filepath setting.\n\nNote\n\nMake sure that the keystore file has the right permissions and is accessible by the system user crate.\n\nAlso, define the password needed to decrypt the keystore by using the ssl.keystore_password setting.\n\nUse ssl.keystore_key_password setting to define the key password used when creating the keystore.\n\nFor a full list of the settings needed to configure SSL/TLS, refer to SSL configuration reference.\n\nConfigure a Separate Truststore\n\nTrusted CA certificates can be stored in a node’s keystore or a separate truststore can be used to store them.\n\nIf you want to use a separate truststore, create a node truststore and import the CA certificate(s) you want to trust. Once the truststore is prepared, define the absolute file path of the truststore .jks file on the node using the ssl.truststore_filepath setting.\n\nNote\n\nMake sure that the truststore file has the right permissions and is accessible by the system user crate.\n\nAlso define the password needed to decrypt the keystore by using the ssl.truststore_password setting.\n\nFor a full list of the settings needed to configure SSL/TLS, refer to SSL configuration reference.\n\nConnecting to a CrateDB Node Using HTTPS\nConnect to a CrateDB Node Using the Admin UI\n\nCrate’s HTTP endpoint remains unchanged. When you have turned on secure communication, it will use HTTPS instead of plain HTTP. Simply point your browser to the same URL you used before but changing the protocol to https:\n\nFor example, http://localhost:4200 becomes https://localhost:4200. If you have not configured the CrateDB node’s Keystore with a signed certificate from a Certificate Authority (CA), then you will get something like the following: NET::ERR_CERT_AUTHORITY_INVALID. You either need to get your certificate signed from one of the CAs included in your browser or import your owned certificates into the browser. A third option is storing an exception for the CrateDB node certification in your browser after verifying that this is indeed a certificate you trust.\n\nConnect to a CrateDB Node Using Crash\n\nYou can connect to a CrateDB node using a secure communication:\n\ncrash --hosts https://localhost:4200\n\n\nTo validate the provided certificates, please see the options --verify-ssl and --key-file.\n\nConnect to a CrateDB Node Using REST\n\nIssue your REST requests to the node using the https:// protocol. You may have to configure your client to validate the received certificate accordingly.\n\nConnecting to a CrateDB Node Using PostgreSQL Wire Protocol With SSL/TLS\nConnect to a CrateDB Node Using JDBC\n\nJDBC needs to validate the CrateDB node’s identity by checking that the node certificate is signed by a trusted authority. If the certificate is signed by a certificate authority (CA) that is known to the Java runtime, there is nothing further to do (as Java comes with copies of the most common CA’s certificates).\n\nIf you have a certificate that is signed by a CA not known to the Java runtime, you need to configure a truststore which contains the node’s certificate and provide the path to the truststore file along with the password when starting your Java application:\n\njava -Djavax.net.ssl.trustStore=mystore -Djavax.net.ssl.trustStorePassword=mypassword com.mycompany.MyApp\n\n\nIn case you face any issues extra debugging information is available by adding -Djavax.net.debug=ssl to your command line.\n\nLast but not least, the connection parameter ssl=true must be added to the connection URL so that the JDBC driver will try and establish an SSL connection.\n\nFor further information, visit jdbc ssl documentation.\n\nConnect to a CrateDB Node Using psql\n\nBy default, psql attempts to use ssl if available on the node. For further information including the different SSL modes please visit the psql documentation.\n\nConnecting to a CrateDB MQTT Endpoint With SSL/TLS\n\nA CrateDB node configured to run as an MQTT endpoint can only accept encrypted connections if SSL ingestion is enabled, namely if the ssl.ingestion.mqtt.enabled setting is configured to true and the ingestion.mqtt.port setting is 8883.\n\nSetting up a Keystore/Truststore With a Certificate Chain\n\nIn case you need to setup a Keystore or a Trustore, here are the commands to get you started. All the commands use a validity of 36500 days (about 100 years). You might want to use less.\n\nGenerate Keystore With a Private Key\n\nThe first step is to create a keystore with a private key using the RSA algorithm. The “first and last name” is the common name (CN) which should overlap with the URL the service it is used with.\n\nCommand:\n\nkeytool -keystore keystore -genkey -keyalg RSA -alias server -validity 36500\n\n\nOutput:\n\nEnter keystore password:\nRe-enter new password:\nWhat is your first and last name?\n  [Unknown]:  ssl.crate.io\nWhat is the name of your organizational unit?\n  [Unknown]:  Cryptography Department\nWhat is the name of your organization?\n  [Unknown]:  Crate.io GmbH\nWhat is the name of your City or Locality?\n  [Unknown]:  Berlin\nWhat is the name of your State or Province?\n  [Unknown]:  Berlin\nWhat is the two-letter country code for this unit?\n  [Unknown]:  DE\nIs CN=ssl.crate.io, OU=Cryptography Department, O=Crate.io GmbH, L=Berlin, ST=Berlin, C=DE correct?\n  [no]:  yes\n\nEnter key password for <server>\n    (RETURN if same as keystore password):\nRe-enter new password:\n\nGenerate a Certificate Signing Request\n\nTo establish trust for this key, we need to sign it. This is done by generating a certificate signing request.\n\nIf you have access to a certificate authority (CA), you can skip the next steps and get the signed certificate from the CA using the signing request which we will generate with the command below. If you don’t have access to a CA, then follow the optional steps after this step to establish your own CA.\n\nCommand:\n\nkeytool -keystore keystore -certreq -alias server -keyalg RSA -file server.csr\n\n\nOutput:\n\nEnter keystore password:\nEnter key password for <server>\n\nOptional: Use a Self-Signed Certificate to Act as a Certificate Authority (CA)\n\nNote\n\nOnly follow these optional steps if you want to create your own Certificate Authority (CA). Otherwise, please request a signed certificate from one of the CAs bundled with Java.\n\nGenerate a Self-Signed Certificate\n\nIf you don’t get your certificate signed from one of the official CAs, you might want to create your own CA with a self-signed certificate. The common name (CN) should overlap with the CN of the server key generated in the first step. For example, ssl.crate.io overlaps with *.crate.io.\n\nNote\n\nIn this step by step guide it is shown how to create a server certificate. If you want to create a client certificate the steps are almost the same with the exception of providing a common name that is equivalent to the crate username as described in client certificate authentication method.\n\nCommand:\n\nopenssl req -x509 -sha256 -nodes -days 36500 -newkey rsa:2048 \\\n    -keyout rootCA.key -out rootCA.crt\n\n\nOutput:\n\nGenerating a 2048 bit RSA private key\n.......................................................................+++\n.............................................................+++\nwriting new private key to 'rootCA.key'\n-----\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n-----\nCountry Name (2 letter code) [AU]:AT\nState or Province Name (full name) [Some-State]:Vorarlberg\nLocality Name (eg, city) []:Dornbirn\nOrganization Name (eg, company) [Internet Widgits Pty Ltd]:Crate.io\nOrganizational Unit Name (eg, section) []:Cryptography Department\nCommon Name (e.g. server FQDN or YOUR name) []:*.crate.io\nEmail Address []:info@crate.io\n\nGenerate a Signed Cert\n\nIn order that the server can prove itself to have a valid and trusted domain it is required that the server certificate contains subjectAltName.\n\nCreate a file called ssl.ext with the following content. In section [alt_names] list valid domain names of the server:\n\nauthorityKeyIdentifier=keyid,issuer\nbasicConstraints=CA:FALSE\nkeyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment\nsubjectAltName = @alt_names\n\n[alt_names]\nDNS.1 = www.example.com\n\n\nNow you can generate a signed cert from our certificate signing request.\n\nCommand:\n\nopenssl x509 -req -in server.csr -CA rootCA.crt -CAkey rootCA.key \\\n    -CAcreateserial -out server.crt -sha256 -days 36500\n\n\nOutput:\n\nSignature ok\nsubject=/C=DE/ST=Berlin/L=Berlin/O=Crate.io GmbH/OU=Cryptography Department/CN=ssl.crate.io\nGetting CA Private Key\n\nImport the CA Certificate Into the Keystore\n\nThe CA needs to be imported to the Keystore for the certificate chain to be available when we import our signed certificate.\n\nCommand:\n\nkeytool -import -keystore keystore -file rootCA.crt -alias theCARoot\n\n\nOutput:\n\nEnter keystore password:\nOwner: EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT\nIssuer: EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT\nSerial number: f13562ec6184401e\nValid from: Mon Jun 12 13:09:17 CEST 2017 until: Wed May 19 13:09:17 CEST 2117\nCertificate fingerprints:\n     MD5:  BB:A1:79:53:FE:71:EC:61:2A:19:81:E8:0E:E8:C9:81\n     SHA1: 96:66:C1:01:49:17:D1:19:FB:DB:83:86:50:3D:3D:AD:DA:F7:C6:A9\n     SHA256: 69:82:C5:24:9A:A1:AE:DF:80:29:7A:26:92:C1:A5:9F:AF:7D:03:56:CC:C3:E9:73:3B:FD:85:66:35:D6:8A:9B\n     Signature algorithm name: SHA256withRSA\n     Version: 3\n\nExtensions:\n\n#1: ObjectId: 2.5.29.35 Criticality=false\nAuthorityKeyIdentifier [\nKeyIdentifier [\n0000: CD 29 4E 07 3D C3 7C D0   16 45 FB 0A CE 8D B4 98  .)N.=....E......\n0010: B7 A8 4C 79                                        ..Ly\n]\n[EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT]\nSerialNumber: [    f13562ec 6184401e]\n]\n\n#2: ObjectId: 2.5.29.19 Criticality=false\nBasicConstraints:[\n  CA:true\n  PathLen:2147483647\n]\n\n#3: ObjectId: 2.5.29.14 Criticality=false\nSubjectKeyIdentifier [\nKeyIdentifier [\n0000: CD 29 4E 07 3D C3 7C D0   16 45 FB 0A CE 8D B4 98  .)N.=....E......\n0010: B7 A8 4C 79                                        ..Ly\n]\n]\n\nTrust this certificate? [no]:  yes\nCertificate was added to keystore\n\nImport CA Into Truststore\n\nIf we are using our own CA, we should also import the certificate to the Truststore, such that it is available for clients which want to verify signatures.\n\nCommand:\n\nkeytool -import -keystore truststore -file rootCA.crt -alias theCARoot\n\n\nOutput:\n\nEnter keystore password:\nRe-enter new password:\nOwner: EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT\nIssuer: EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT\nSerial number: f13562ec6184401e\nValid from: Mon Jun 12 13:09:17 CEST 2017 until: Wed May 19 13:09:17 CEST 2117\nCertificate fingerprints:\n     MD5:  BB:A1:79:53:FE:71:EC:61:2A:19:81:E8:0E:E8:C9:81\n     SHA1: 96:66:C1:01:49:17:D1:19:FB:DB:83:86:50:3D:3D:AD:DA:F7:C6:A9\n     SHA256: 69:82:C5:24:9A:A1:AE:DF:80:29:7A:26:92:C1:A5:9F:AF:7D:03:56:CC:C3:E9:73:3B:FD:85:66:35:D6:8A:9B\n     Signature algorithm name: SHA256withRSA\n     Version: 3\n\nExtensions:\n\n#1: ObjectId: 2.5.29.35 Criticality=false\nAuthorityKeyIdentifier [\nKeyIdentifier [\n0000: CD 29 4E 07 3D C3 7C D0   16 45 FB 0A CE 8D B4 98  .)N.=....E......\n0010: B7 A8 4C 79                                        ..Ly\n]\n[EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT]\nSerialNumber: [    f13562ec 6184401e]\n]\n\n#2: ObjectId: 2.5.29.19 Criticality=false\nBasicConstraints:[\n  CA:true\n  PathLen:2147483647\n]\n\n#3: ObjectId: 2.5.29.14 Criticality=false\nSubjectKeyIdentifier [\nKeyIdentifier [\n0000: CD 29 4E 07 3D C3 7C D0   16 45 FB 0A CE 8D B4 98  .)N.=....E......\n0010: B7 A8 4C 79                                        ..Ly\n]\n]\n\nTrust this certificate? [no]:  yes\nCertificate was added to keystore\n\nImport the Signed Certificate\n\nNow we have a signed certificate, signed by either from a official CA or from our own CA. Let’s import it to the keystore.\n\nCommand:\n\nkeytool -import -keystore keystore -file server.crt -alias server\n\n\nOutput:\n\nEnter keystore password:\nEnter key password for <server>\nCertificate reply was installed in keystore\n\nConfiguring CrateDB\n\nFinally, you want to supply the keystore/truststore configuration in the CrateDB config, see Secured Communications (SSL/TLS)."
  },
  {
    "title": "Optimization — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/admin/optimization.html",
    "html": "3.3\nOptimization\n\nTable of Contents\n\nIntroduction\n\nMultiple Table Optimization\n\nPartition Optimization\n\nSegments Upgrade\n\nIntroduction\n\nIn CrateDB every table (or if partitioned every partition) consists of segments. When inserting/deleting/updating data new segments are created following as an append-only strategy, which gives the advantage of fast writes but on the other hand can result into a big number of segments. As the number of segments increases the read operations become slower since more segments need to be visited. Moreover each segment consumes file handles, memory and CPU. CrateDB solves this problem by merging segments automatically in the background. Small segments are merged into bigger segments, which, in turn, are merged into even bigger segments. Furthermore any deleted rows and documents are not copied to the new bigger segment during this process.\n\nIf required one or more tables or table partitions can be optimized explicitly in order to improve performance. A few parameters can also be configured for the optimization process, like the max number of segments you wish to have when optimization is completed, or if you only wish to merge segments with deleted data, etc. See OPTIMIZE for detailed description of parameters.\n\ncr> OPTIMIZE table locations;\nOPTIMIZE OK, 1 row affected (... sec)\n\n\nNote\n\nSystem tables cannot be optimized.\n\nMultiple Table Optimization\n\nIf needed, multiple tables can be defined comma-separated in a single SQL request. The result message is printed if the request on every given table is completed.\n\ncr> OPTIMIZE TABLE locations, parted_table;\nOPTIMIZE OK, 2 rows affected (... sec)\n\n\nNote\n\nIf one or more tables or partitions do not exist, none of the given tables/partitions are optimized and an error is returned. The error returns only the first non-existent table/partition.\n\nPartition Optimization\n\nAdditionally it is possible to define a specific PARTITION of a partitioned table which should be optimized (see Partitioned Tables).\n\nBy using the PARTITION clause in the optimize statement a separate request for a given partition can be performed. That means that only specific partitions of a partitioned table are optimized. For further details on how to create an optimize request on partitioned tables see the SQL syntax and its synopsis (see OPTIMIZE).\n\ncr> OPTIMIZE TABLE parted_table PARTITION (day='2014-04-08');\nOPTIMIZE OK, 1 row affected (... sec)\n\n\nIn case the PARTITION clause is omitted all partitions will be optimized. If a table has many partitions this should be avoided due to performance reasons.\n\nSegments Upgrade\n\nIn case that some or all of the segments of a table or a table partition are created with an older version of the storage engine, then with the use of OPTIMIZE, these segments can be upgraded to the current version of the storage engine.\n\ncr> OPTIMIZE TABLE locations WITH (upgrade_segments=true);\nOPTIMIZE OK, 1 row affected (... sec)\n"
  },
  {
    "title": "Administration — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/admin/index.html",
    "html": "3.3\nAdministration\n\nThis section of the documentation covers any feature primarily of interest to a database administrator.\n\nTable of Contents\n\nSystem Information\nRuntime Configuration\nUser Management\nPrivileges\nAuthentication\nAuthentication Methods\nHost Based Authentication (HBA)\nSecured Communications (SSL/TLS)\nIngestion Framework\nIngestion Sources\nIngestion Rules\nOptimization\nJobs Management\nJMX Monitoring\nSnapshots\nCloud Discovery\nUsage Data Collector"
  },
  {
    "title": "System Information — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/admin/system-information.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n3.3\nSystem Information\n\nCrateDB provides the sys schema which contains virtual tables. These tables are read-only and can be queried to get statistical real-time information about the cluster, its nodes and their shards:\n\nTable of Contents\n\nCluster\n\nCluster License\n\nlicense\n\nCluster Settings\n\nNodes\n\nid\n\nname\n\nhostname\n\nrest_url\n\nport\n\nload\n\nmem\n\nheap\n\nversion\n\ncluster_state_version\n\nfs\n\nthread_pools\n\nos\n\nCgroup Limitations\n\nUptime Limitations\n\nos_info\n\nnetwork\n\nconnections\n\nprocess\n\nNode Checks\n\nAcknowledge Failed Checks\n\nDescription of Checked Node Settings\n\nRecovery Expected Nodes\n\nRecovery After Nodes\n\nRecovery After Time\n\nRouting Allocation Disk Watermark High\n\nRouting Allocation Disk Watermark Low\n\nJVM Version\n\nShards\n\nTable Schema\n\nExample\n\nJobs, Operations, and Logs\n\nJobs\n\nTable Schema\n\nJobs Metrics\n\nsys.jobs_metrics Table Schema\n\nClassification\n\nOperations\n\nTable Schema\n\nLogs\n\nsys.jobs_log Table Schema\n\nsys.operations_log Table Schema\n\nCluster Checks\n\nCurrent Checks\n\nMinimum Master Nodes\n\nNumber of Partitions\n\nTables need to be recreated\n\nLicense expiry check\n\nHealth\n\nHealth Definition\n\nRepositories\n\nSnapshots\n\nSummits\n\nUsers\n\nAllocations\n\nShard Table Permissions\n\nCluster\n\nBasic information about the CrateDB cluster can be retrieved from the sys.cluster table:\n\nName\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nA unique ID generated by the system.\n\n\t\n\nSTRING\n\n\n\n\nlicense\n\n\t\n\nThe current CrateDB license information.\n\n\t\n\nOBJECT\n\n\n\n\nname\n\n\t\n\nThe cluster name.\n\n\t\n\nSTRING\n\n\n\n\nmaster_node\n\n\t\n\nNode ID of the node which currently operates as master\n\n\t\n\nSTRING\n\n\n\n\nsettings\n\n\t\n\nThe cluster settings.\n\n\t\n\nOBJECT\n\nThe result has at most 1 row:\n\ncr> select name from sys.cluster;\n+--------------+\n| name         |\n+--------------+\n| Testing...   |\n+--------------+\nSELECT 1 row in set (... sec)\n\nCluster License\n\nThe sys.cluster.license expression returns information about the currently registered license.\n\nlicense\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nlicense\n\n\t\n\nThe current CrateDB license information or NULL on CrateDB CE.\n\n\t\n\nOBJECT\n\n\n\n\nlicense['expiry_date']\n\n\t\n\nThe timestamp on which the license expires.\n\n\t\n\nTIMESTAMP\n\n\n\n\nlicense['issued_to']\n\n\t\n\nThe organisation for which the license is issued.\n\n\t\n\nSTRING\n\n\n\n\nlicense['max_nodes']\n\n\t\n\nThe maximum number of nodes the license is valid for.\n\n\t\n\nINTEGER\n\nCluster Settings\n\nThe sys.cluster.settings expression returns information about the currently applied cluster settings.\n\ncr> select settings from sys.cluster;\n+-----------------------------------------------------------------------------------------------------------------------------------------------------...-+\n| settings                                                                                                                                                |\n+-----------------------------------------------------------------------------------------------------------------------------------------------------...-+\n| {\"bulk\": {...}, \"cluster\": {...}, \"discovery\": {...}, \"gateway\": {...}, \"indices\": {...}, \"license\": {...}, \"logger\": [], \"stats\": {...}, \"udc\": {...}} |\n+-----------------------------------------------------------------------------------------------------------------------------------------------------...-+\nSELECT 1 row in set (... sec)\n\ncr> select column_name, data_type from information_schema.columns\n... where column_name like 'settings%'\n... and table_name = 'cluster';\n+-----------------------------------------------------------------------------------+--------------+\n| column_name                                                                       | data_type    |\n+-----------------------------------------------------------------------------------+--------------+\n| settings                                                                          | object       |\n| settings['bulk']                                                                  | object       |\n| settings['bulk']['request_timeout']                                               | string       |\n| settings['cluster']                                                               | object       |\n| settings['cluster']['graceful_stop']                                              | object       |\n| settings['cluster']['graceful_stop']['force']                                     | boolean      |\n| settings['cluster']['graceful_stop']['min_availability']                          | string       |\n| settings['cluster']['graceful_stop']['reallocate']                                | boolean      |\n| settings['cluster']['graceful_stop']['timeout']                                   | string       |\n| settings['cluster']['info']                                                       | object       |\n| settings['cluster']['info']['update']                                             | object       |\n| settings['cluster']['info']['update']['interval']                                 | string       |\n| settings['cluster']['routing']                                                    | object       |\n| settings['cluster']['routing']['allocation']                                      | object       |\n| settings['cluster']['routing']['allocation']['allow_rebalance']                   | string       |\n| settings['cluster']['routing']['allocation']['balance']                           | object       |\n| settings['cluster']['routing']['allocation']['balance']['index']                  | float        |\n| settings['cluster']['routing']['allocation']['balance']['shard']                  | float        |\n| settings['cluster']['routing']['allocation']['balance']['threshold']              | float        |\n| settings['cluster']['routing']['allocation']['cluster_concurrent_rebalance']      | integer      |\n| settings['cluster']['routing']['allocation']['disk']                              | object       |\n| settings['cluster']['routing']['allocation']['disk']['threshold_enabled']         | boolean      |\n| settings['cluster']['routing']['allocation']['disk']['watermark']                 | object       |\n| settings['cluster']['routing']['allocation']['disk']['watermark']['flood_stage']  | string       |\n| settings['cluster']['routing']['allocation']['disk']['watermark']['high']         | string       |\n| settings['cluster']['routing']['allocation']['disk']['watermark']['low']          | string       |\n| settings['cluster']['routing']['allocation']['enable']                            | string       |\n| settings['cluster']['routing']['allocation']['exclude']                           | object       |\n| settings['cluster']['routing']['allocation']['exclude']['_host']                  | string       |\n| settings['cluster']['routing']['allocation']['exclude']['_id']                    | string       |\n| settings['cluster']['routing']['allocation']['exclude']['_ip']                    | string       |\n| settings['cluster']['routing']['allocation']['exclude']['_name']                  | string       |\n| settings['cluster']['routing']['allocation']['include']                           | object       |\n| settings['cluster']['routing']['allocation']['include']['_host']                  | string       |\n| settings['cluster']['routing']['allocation']['include']['_id']                    | string       |\n| settings['cluster']['routing']['allocation']['include']['_ip']                    | string       |\n| settings['cluster']['routing']['allocation']['include']['_name']                  | string       |\n| settings['cluster']['routing']['allocation']['node_concurrent_recoveries']        | integer      |\n| settings['cluster']['routing']['allocation']['node_initial_primaries_recoveries'] | integer      |\n| settings['cluster']['routing']['allocation']['require']                           | object       |\n| settings['cluster']['routing']['allocation']['require']['_host']                  | string       |\n| settings['cluster']['routing']['allocation']['require']['_id']                    | string       |\n| settings['cluster']['routing']['allocation']['require']['_ip']                    | string       |\n| settings['cluster']['routing']['allocation']['require']['_name']                  | string       |\n| settings['cluster']['routing']['rebalance']                                       | object       |\n| settings['cluster']['routing']['rebalance']['enable']                             | string       |\n| settings['discovery']                                                             | object       |\n| settings['discovery']['zen']                                                      | object       |\n| settings['discovery']['zen']['minimum_master_nodes']                              | integer      |\n| settings['discovery']['zen']['ping_timeout']                                      | string       |\n| settings['discovery']['zen']['publish_timeout']                                   | string       |\n| settings['gateway']                                                               | object       |\n| settings['gateway']['expected_nodes']                                             | integer      |\n| settings['gateway']['recover_after_nodes']                                        | integer      |\n| settings['gateway']['recover_after_time']                                         | string       |\n| settings['indices']                                                               | object       |\n| settings['indices']['breaker']                                                    | object       |\n| settings['indices']['breaker']['fielddata']                                       | object       |\n| settings['indices']['breaker']['fielddata']['limit']                              | string       |\n| settings['indices']['breaker']['fielddata']['overhead']                           | double       |\n| settings['indices']['breaker']['query']                                           | object       |\n| settings['indices']['breaker']['query']['limit']                                  | string       |\n| settings['indices']['breaker']['query']['overhead']                               | double       |\n| settings['indices']['breaker']['request']                                         | object       |\n| settings['indices']['breaker']['request']['limit']                                | string       |\n| settings['indices']['breaker']['request']['overhead']                             | double       |\n| settings['indices']['recovery']                                                   | object       |\n| settings['indices']['recovery']['internal_action_long_timeout']                   | string       |\n| settings['indices']['recovery']['internal_action_timeout']                        | string       |\n| settings['indices']['recovery']['max_bytes_per_sec']                              | string       |\n| settings['indices']['recovery']['recovery_activity_timeout']                      | string       |\n| settings['indices']['recovery']['retry_delay_network']                            | string       |\n| settings['indices']['recovery']['retry_delay_state_sync']                         | string       |\n| settings['license']                                                               | object       |\n| settings['license']['enterprise']                                                 | boolean      |\n| settings['license']['ident']                                                      | string       |\n| settings['logger']                                                                | object_array |\n| settings['logger']['level']                                                       | string       |\n| settings['logger']['name']                                                        | string       |\n| settings['stats']                                                                 | object       |\n| settings['stats']['breaker']                                                      | object       |\n| settings['stats']['breaker']['log']                                               | object       |\n| settings['stats']['breaker']['log']['jobs']                                       | object       |\n| settings['stats']['breaker']['log']['jobs']['limit']                              | string       |\n| settings['stats']['breaker']['log']['jobs']['overhead']                           | double       |\n| settings['stats']['breaker']['log']['operations']                                 | object       |\n| settings['stats']['breaker']['log']['operations']['limit']                        | string       |\n| settings['stats']['breaker']['log']['operations']['overhead']                     | double       |\n| settings['stats']['enabled']                                                      | boolean      |\n| settings['stats']['jobs_log_expiration']                                          | string       |\n| settings['stats']['jobs_log_filter']                                              | string       |\n| settings['stats']['jobs_log_persistent_filter']                                   | string       |\n| settings['stats']['jobs_log_size']                                                | integer      |\n| settings['stats']['operations_log_expiration']                                    | string       |\n| settings['stats']['operations_log_size']                                          | integer      |\n| settings['stats']['service']                                                      | object       |\n| settings['stats']['service']['interval']                                          | string       |\n| settings['udc']                                                                   | object       |\n| settings['udc']['enabled']                                                        | boolean      |\n| settings['udc']['initial_delay']                                                  | string       |\n| settings['udc']['interval']                                                       | string       |\n| settings['udc']['url']                                                            | string       |\n+-----------------------------------------------------------------------------------+--------------+\nSELECT ... rows in set (... sec)\n\n\nFor further details, see the Cluster Settings configuration section.\n\nNodes\n\nTo get information about the nodes query for sys.nodes.\n\nThis table can be queried for one, multiple or all nodes within a cluster.\n\nThe table schema is as follows:\n\nid\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nA unique ID within the cluster generated by the system.\n\n\t\n\nSTRING\n\nname\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nname\n\n\t\n\nThe node name within a cluster. The system will choose a random name. You can specify the node name via your own custom configuration.\n\n\t\n\nSTRING\n\nhostname\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nhostname\n\n\t\n\nThe specified host name of the machine the node is running on.\n\n\t\n\nSTRING\n\nrest_url\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nrest_url\n\n\t\n\nFull http(s) address where the REST API of the node is exposed, including schema, hostname (or IP) and port.\n\n\t\n\nSTRING\n\nport\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nport\n\n\t\n\nThe specified ports for both HTTP and binary transport interfaces. You can specify the ports via your own custom configuration.\n\n\t\n\nOBJECT\n\n\n\n\nport['http']\n\n\t\n\nCrateDB’s HTTP port.\n\n\t\n\nINTEGER\n\n\n\n\nport['transport']\n\n\t\n\nCrateDB’s binary transport port.\n\n\t\n\nINTEGER\n\n\n\n\nport['psql']\n\n\t\n\nThe PostgreSQL wire protocol port.\n\n\t\n\nINTEGER\n\nload\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nload\n\n\t\n\nSystem load statistics\n\n\t\n\nOBJECT\n\n\n\n\nload['1']\n\n\t\n\nAverage load over the last 1 minute.\n\n\t\n\nDOUBLE\n\n\n\n\nload['5']\n\n\t\n\nAverage load over the last 5 minutes.\n\n\t\n\nDOUBLE\n\n\n\n\nload['15']\n\n\t\n\nAverage load over the last 15 minutes.\n\n\t\n\nDOUBLE\n\n\n\n\nload['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the load probe.\n\n\t\n\nLONG\n\nmem\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nmem\n\n\t\n\nMemory utilization statistics of the host.\n\n\t\n\nOBJECT\n\n\n\n\nmem['used']\n\n\t\n\nCurrently used memory in bytes.\n\n\t\n\nLONG\n\n\n\n\nmem['used_percent']\n\n\t\n\nCurrently used memory in percent of total.\n\n\t\n\nSHORT\n\n\n\n\nmem['free']\n\n\t\n\nCurrently available memory in bytes.\n\n\t\n\nLONG\n\n\n\n\nmem['free_percent']\n\n\t\n\nCurrently available memory in percent of total.\n\n\t\n\nSHORT\n\n\n\n\nmem['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the memory probe.\n\n\t\n\nLONG\n\nheap\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nheap\n\n\t\n\nHeap memory utilization statistics.\n\n\t\n\nOBJECT\n\n\n\n\nheap['used']\n\n\t\n\nCurrently used heap memory in bytes.\n\n\t\n\nLONG\n\n\n\n\nheap['max']\n\n\t\n\nMaximum available heap memory. You can specify the max heap memory CrateDB should use in the configuration.\n\n\t\n\nLONG\n\n\n\n\nheap['free']\n\n\t\n\nCurrently available heap memory in bytes.\n\n\t\n\nLONG\n\n\n\n\nheap['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the heap probe.\n\n\t\n\nLONG\n\nversion\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nversion\n\n\t\n\nCrateDB version information.\n\n\t\n\nOBJECT\n\n\n\n\nversion['number']\n\n\t\n\nVersion string in format \"major.minor.hotfix\"\n\n\t\n\nSTRING\n\n\n\n\nversion['build_hash']\n\n\t\n\nSHA hash of the Github commit which this build was built from.\n\n\t\n\nSTRING\n\n\n\n\nversion['build_snapshot']\n\n\t\n\nIndicates whether this build is a snapshot build.\n\n\t\n\nBOOLEAN\n\ncluster_state_version\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\ncluster_state_version\n\n\t\n\nThe current version of the cluster state. The cluster state is an immutable structure and that is recreated when a change is published.\n\n\t\n\nLONG\n\nfs\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nfs\n\n\t\n\nUtilization statistics about the file system.\n\n\t\n\nOBJECT\n\n\n\n\nfs['total']\n\n\t\n\nAggregated usage statistic of all disks on the host.\n\n\t\n\nOBJECT\n\n\n\n\nfs['total']['size']\n\n\t\n\nTotal size of all disks in bytes.\n\n\t\n\nLONG\n\n\n\n\nfs['total']['used']\n\n\t\n\nTotal used space of all disks in bytes.\n\n\t\n\nLONG\n\n\n\n\nfs['total']['available']\n\n\t\n\nTotal available space of all disks in bytes.\n\n\t\n\nLONG\n\n\n\n\nfs['total']['reads']\n\n\t\n\nTotal number of reads on all disks.\n\n\t\n\nLONG\n\n\n\n\nfs['total']['bytes_read']\n\n\t\n\nTotal size of reads on all disks in bytes.\n\n\t\n\nLONG\n\n\n\n\nfs['total']['writes']\n\n\t\n\nTotal number of writes on all disks.\n\n\t\n\nLONG\n\n\n\n\nfs['total']['bytes_written']\n\n\t\n\nTotal size of writes on all disks in bytes.\n\n\t\n\nLONG\n\n\n\n\nfs['disks']\n\n\t\n\nUsage statistics of individual disks on the host.\n\n\t\n\nARRAY\n\n\n\n\nfs['disks']['dev']\n\n\t\n\nDevice name\n\n\t\n\nSTRING\n\n\n\n\nfs['disks']['size']\n\n\t\n\nTotal size of the disk in bytes.\n\n\t\n\nLONG\n\n\n\n\nfs['disks']['used']\n\n\t\n\nUsed space of the disk in bytes.\n\n\t\n\nLONG\n\n\n\n\nfs['disks']['available']\n\n\t\n\nAvailable space of the disk in bytes.\n\n\t\n\nLONG\n\n\n\n\nfs['disks']['reads']\n\n\t\n\nNumber of reads on the disk.\n\nDEPRECATED: always returns -1\n\n\t\n\nLONG\n\n\n\n\nfs['disks']['bytes_read']\n\n\t\n\nTotal size of reads on the disk in bytes.\n\nDEPRECATED: always returns -1\n\n\t\n\nLONG\n\n\n\n\nfs['disks']['writes']\n\n\t\n\nNumber of writes on the disk.\n\nDEPRECATED: always returns -1\n\n\t\n\nLONG\n\n\n\n\nfs['disks']['bytes_written']\n\n\t\n\nTotal size of writes on the disk in bytes.\n\nDEPRECATED: always returns -1\n\n\t\n\nLONG\n\n\n\n\nfs['data']\n\n\t\n\nInformation about data paths used by the node.\n\n\t\n\nARRAY\n\n\n\n\nfs['data']['dev']\n\n\t\n\nDevice name\n\n\t\n\nSTRING\n\n\n\n\nfs['data']['path']\n\n\t\n\nFile path where the data of the node resides.\n\n\t\n\nSTRING\n\nthread_pools\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nthread_pools\n\n\t\n\nUsage statistics of Java thread pools.\n\n\t\n\nARRAY\n\n\n\n\nthread_pools['name']\n\n\t\n\nName of the pool.\n\n\t\n\nSTRING\n\n\n\n\nthread_pools['active']\n\n\t\n\nNumber of currently running thread in the thread pool.\n\n\t\n\nINTEGER\n\n\n\n\nthread_pools['rejected']\n\n\t\n\nTotal number of rejected threads in the thread pool.\n\n\t\n\nLONG\n\n\n\n\nthread_pools['largest']\n\n\t\n\nLargest number of threads that have ever simultaniously been in the pool.\n\n\t\n\nINTEGER\n\n\n\n\nthread_pools['completed']\n\n\t\n\nTotal number of completed thread in teh thread pool.\n\n\t\n\nLONG\n\n\n\n\nthread_pools['threads']\n\n\t\n\nSize of the thread pool.\n\n\t\n\nINTEGER\n\n\n\n\nthread_pools['queue']\n\n\t\n\nNumber of thread currently in the queue.\n\n\t\n\nINTEGER\n\nos\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nos\n\n\t\n\nOperating system stats\n\n\t\n\nOBJECT\n\n\n\n\nos['uptime']\n\n\t\n\nSystem uptime in milliseconds\n\nRequires allowing system calls on Windows and macOS. See notes in Uptime Limitations.\n\n\t\n\nLONG\n\n\n\n\nos['timestamp']\n\n\t\n\nUNIX timestamp in millisecond resolution\n\n\t\n\nLONG\n\n\n\n\nos['cpu']\n\n\t\n\nInformation about CPU utilization\n\n\t\n\nOBJECT\n\n\n\n\nos['cpu']['used']\n\n\t\n\nSystem CPU usage as percentage\n\n\t\n\nSHORT\n\n\n\n\nos['cpu']['system']\n\n\t\n\nCPU time used by the system\n\nDEPRECATED: always returns -1\n\n\t\n\nSHORT\n\n\n\n\nos['cpu']['user']\n\n\t\n\nCPU time used by applications\n\nDEPRECATED: always returns -1\n\n\t\n\nSHORT\n\n\n\n\nos['cpu']['idle']\n\n\t\n\nIdle CPU time\n\nDEPRECATED: always returns -1\n\n\t\n\nSHORT\n\n\n\n\nos['cpu']['stolen']\n\n\t\n\nThe amount of CPU ‘stolen’ from this virtual machine by the hypervisor for other tasks.\n\nDEPRECATED: always returns -1\n\n\t\n\nSHORT\n\n\n\n\nos['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the OS probe.\n\n\t\n\nLONG\n\n\n\n\nos['cgroup']\n\n\t\n\nInformation about Cgroups (Linux only)\n\n\t\n\nOBJECT\n\n\n\n\nos['cgroup']['cpuacct']\n\n\t\n\nInformation about CPU accounting\n\n\t\n\nOBJECT\n\n\n\n\nos['cgroup']['cpuacct']['control_group']\n\n\t\n\nThe path to the cpu accounting cgroup\n\n\t\n\nSTRING\n\n\n\n\nos['cgroup']['cpuacct']['usage_nanos']\n\n\t\n\nThe total CPU time (in nanoseconds) consumed by all tasks in this cgroup.\n\n\t\n\nLONG\n\n\n\n\nos['cgroup']['cpu']\n\n\t\n\nInformation about the CPU subsystem\n\n\t\n\nOBJECT\n\n\n\n\nos['cgroup']['cpu']['control_group']\n\n\t\n\nThe path to the cpu cgroup\n\n\t\n\nSTRING\n\n\n\n\nos['cgroup']['cpu']['cfs_period_micros']\n\n\t\n\nThe period of time (in microseconds) the cgroup access to the CPU gets reallocated.\n\n\t\n\nLONG\n\n\n\n\nos['cgroup']['cpu']['cfs_quota_micros']\n\n\t\n\nThe total amount of time (in microseconds) for which all tasks in the cgroup can run during one period (cfs_period_micros).\n\n\t\n\nLONG\n\n\n\n\nos['cgroup']['cpu']['num_elapsed_periods']\n\n\t\n\nThe nr. of period intervals (cfs_period_micros) that have elapsed.\n\n\t\n\nLONG\n\n\n\n\nos['cgroup']['cpu']['num_times_throttled']\n\n\t\n\nThe nr. of times tasks in the cgroup have been throttled.\n\n\t\n\nLONG\n\n\n\n\nos['cgroup']['cpu']['time_throttled_nanos']\n\n\t\n\nThe total time (in nanoseconds) for which tasks in the cgroup have been throttled.\n\n\t\n\nLONG\n\n\n\n\nos['cgroup']['mem']\n\n\t\n\nInformation about memory resources used by tasks in a cgroup.\n\n\t\n\nOBJECT\n\n\n\n\nos['cgroup']['mem']['control_group']\n\n\t\n\nThe path to the memory cgroup\n\n\t\n\nSTRING\n\n\n\n\nos['cgroup']['mem']['usage_bytes']\n\n\t\n\nThe total current memory usage by processes in the cgroup.\n\n\t\n\nSTRING\n\n\n\n\nos['cgroup']['mem']['limit_bytes']\n\n\t\n\nThe max. amount of user memory in the cgroup.\n\n\t\n\nSTRING\n\nThe cpu information values are cached for 1s. They might differ from the actual values at query time. Use the probe timestamp to get the time of collection. When analyzing the cpu usage over time, always use os['probe_timestamp'] to calculate the time difference between 2 probes.\n\nCgroup Limitations\n\nNote\n\nCgroup metrics only work if the stats are available from /sys/fs/cgroup/cpu and /sys/fs/cgroup/cpuacct.\n\nUptime Limitations\n\nNote\n\nos[‘uptime’] required a system call when running CrateDB on Windows or macOS, however, system calls are not permitted by default. If you require this metric you need to allow system calls by setting bootstrap.seccomp to false. This setting must be set in the crate.yml or via command line argument and cannot be changed at runtime.\n\nos_info\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nos_info\n\n\t\n\nOperating system information\n\n\t\n\nOBJECT\n\n\n\n\nos_info['available_processors']\n\n\t\n\nNumber of processors that are available in the JVM. This is usually equal to the number of cores of the CPU.\n\n\t\n\nINTEGER\n\n\n\n\nos_info['name']\n\n\t\n\nName of the operating system (ex: Linux, Windows, macOS)\n\n\t\n\nSTRING\n\n\n\n\nos_info['arch']\n\n\t\n\nName of the JVM architecture (ex: amd64, x86)\n\n\t\n\nSTRING\n\n\n\n\nos_info['version']\n\n\t\n\nVersion of the operating system\n\n\t\n\nSTRING\n\n\n\n\nos_info['jvm']\n\n\t\n\nInformation about the JVM (Java Virtual Machine)\n\n\t\n\nOBJECT\n\n\n\n\nos_info['jvm']['version']\n\n\t\n\nThe JVM version\n\n\t\n\nSTRING\n\n\n\n\nos_info['jvm']['vm_name']\n\n\t\n\nThe name of the JVM (eg. OpenJDK, Java Hotspot(TM) )\n\n\t\n\nSTRING\n\n\n\n\nos_info['jvm']['vm_vendor']\n\n\t\n\nThe vendor name of the JVM\n\n\t\n\nSTRING\n\n\n\n\nos_info['jvm']['vm_version']\n\n\t\n\nThe version of the JVM\n\n\t\n\nSTRING\n\nnetwork\n\nNetwork statistics are deprecated in CrateDB 2.3 and may completely be removed in subsequent versions. All LONG columns always return 0.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nnetwork\n\n\t\n\nStatistics about network activity on the host.\n\n\t\n\nOBJECT\n\n\n\n\nnetwork['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the network probe.\n\n\t\n\nLONG\n\n\n\n\nnetwork['tcp']\n\n\t\n\nTCP network activity on the host.\n\n\t\n\nOBJECT\n\n\n\n\nnetwork['tcp']['connections']\n\n\t\n\nInformation about TCP network connections.\n\n\t\n\nOBJECT\n\n\n\n\nnetwork['tpc']['connections']['initiated']\n\n\t\n\nTotal number of initiated TCP connections.\n\n\t\n\nLONG\n\n\n\n\nnetwork['tpc']['connections']['accepted']\n\n\t\n\nTotal number of accepted TCP connections.\n\n\t\n\nLONG\n\n\n\n\nnetwork['tpc']['connections']['curr_established']\n\n\t\n\nTotal number of currently established TCP connections.\n\n\t\n\nLONG\n\n\n\n\nnetwork['tcp']['connections']['dropped']\n\n\t\n\nTotal number of dropped TCP connections.\n\n\t\n\nLONG\n\n\n\n\nnetwork['tcp']['connections']['embryonic_dropped']\n\n\t\n\nTotal number of TCP connections that have been dropped before they were accepted.\n\n\t\n\nLONG\n\n\n\n\nnetwork['tcp']['packets']\n\n\t\n\nInformation about TCP packets.\n\n\t\n\nOBJECT\n\n\n\n\nnetwork['tpc']['packets']['sent']\n\n\t\n\nTotal number of TCP packets sent.\n\n\t\n\nLONG\n\n\n\n\nnetwork['tcp']['packets']['received']\n\n\t\n\nTotal number of TCP packets received.\n\n\t\n\nLONG\n\n\n\n\nnetwork['tpc']['packets']['retransmitted']\n\n\t\n\nTotal number of TCP packets retransmitted due to an error.\n\n\t\n\nLONG\n\n\n\n\nnetwork['tcp']['packets']['errors_received']\n\n\t\n\nTotal number of TCP packets that contained checksum errors, had a bad offset, were dropped because of a lack of memory or were too short.\n\n\t\n\nLONG\n\n\n\n\nnetwork['tcp']]['packets']['rst_sent']\n\n\t\n\nTotal number of RST packets sent due to left unread data in queue when socket is closed. See tools.ietf.org.\n\n\t\n\nLONG\n\nconnections\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nhttp\n\n\t\n\nNumber of connections established via HTTP\n\n\t\n\nOBJECT\n\n\n\n\nhttp['open']\n\n\t\n\nThe currently open connections established via HTTP\n\n\t\n\nLONG\n\n\n\n\nhttp['total']\n\n\t\n\nThe total number of connections that have been established via HTTP over the life time of a CrateDB node\n\n\t\n\nLONG\n\n\n\n\npsql\n\n\t\n\nNumber of connections established via Postgres protocol\n\n\t\n\nOBJECT\n\n\n\n\npsql['open']\n\n\t\n\nThe currently open connections established via Postgres protocol\n\n\t\n\nLONG\n\n\n\n\npsql['total']\n\n\t\n\nThe total number of connections that have been established via Postgres protocol over the life time of a CrateDB node\n\n\t\n\nLONG\n\n\n\n\ntransport\n\n\t\n\nNumber of connections established via Transport protocol\n\n\t\n\nOBJECT\n\n\n\n\ntransport['open']\n\n\t\n\nThe currently open connections established via Transport protocol\n\n\t\n\nLONG\n\nprocess\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nprocess\n\n\t\n\nStatistics about the CrateDB process.\n\n\t\n\nOBJECT\n\n\n\n\nprocess['open_file_descriptors']\n\n\t\n\nNumber of currently open file descriptors used by the CrateDB process.\n\n\t\n\nLONG\n\n\n\n\nprocess['max_open_file_descriptors']\n\n\t\n\nThe maximum number of open file descriptors CrateDB can use.\n\n\t\n\nLONG\n\n\n\n\nprocess['probe_timestamp']\n\n\t\n\nThe system UNIX timestamp at the moment of the probe collection.\n\n\t\n\nLONG\n\n\n\n\nprocess['cpu']\n\n\t\n\nInformation about the CPU usage of the CrateDB process.\n\n\t\n\nOBJECT\n\n\n\n\nprocess['cpu']['percent']\n\n\t\n\nThe CPU usage of the CrateDB JVM process given in percent.\n\n\t\n\nSHORT\n\n\n\n\nprocess['cpu']['user']\n\n\t\n\nThe process CPU user time in milliseconds.\n\nDEPRECATED: always returns -1\n\n\t\n\nLONG\n\n\n\n\nprocess['cpu']['system']\n\n\t\n\nThe process CPU kernel time in milliseconds.\n\nDEPRECATED: always returns -1\n\n\t\n\nLONG\n\nThe cpu information values are cached for 1s. They might differ from the actual values at query time. Use the probe timestamp to get the time of the collect. When analyzing the cpu usage over time, always use process['probe_timestamp'] to calculate the time difference between 2 probes.\n\nNote\n\nIf one of the queried nodes is not responding within three seconds it returns null every column except id and name. This behaviour could be used to detect hanging nodes.\n\nNode Checks\n\nThe table sys.node_checks exposes a list of internal node checks and results of their validation.\n\nThe table schema is the following:\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe unique check ID.\n\n\t\n\nINTEGER\n\n\n\n\nnode_id\n\n\t\n\nThe unique node ID.\n\n\t\n\nSTRING\n\n\n\n\nseverity\n\n\t\n\nThe level of severity. The higher the value of the field the higher severity.\n\n\t\n\nINTEGER\n\n\n\n\ndescription\n\n\t\n\nThe description message for the setting check.\n\n\t\n\nSTRING\n\n\n\n\npassed\n\n\t\n\nThe flag determines whether the check for the setting has passed.\n\n\t\n\nBOOLEAN\n\n\n\n\nacknowledged\n\n\t\n\nThe flag determines whether the check for this setting has been acknowledged by the user in order to ignored the value of passed column. This column can be updated.\n\n\t\n\nBOOLEAN\n\nExample query:\n\ncr> select id, node_id, description from sys.node_checks order by id, node_id;\n+----+---------...-+--------------------------------------------------------------...-+\n| id | node_id     | description                                                      |\n+----+---------...-+--------------------------------------------------------------...-+\n|  1 | ...         | The value of the cluster setting 'gateway.expected_nodes' mus... |\n|  2 | ...         | The value of the cluster setting 'gateway.recover_after_nodes... |\n|  3 | ...         | If any of the \"expected nodes\" recovery settings are set, the... |\n|  5 | ...         | The high disk watermark is exceeded on the node. The cluster ... |\n|  6 | ...         | The low disk watermark is exceeded on the node. The cluster w... |\n|  7 | ...         | The flood stage disk watermark is exceeded on the node. Table... |\n|  8 | ...         | The JVM version with which CrateDB is running should be >= 11... |\n+----+---------...-+--------------------------------------------------------------...-+\nSELECT 7 rows in set (... sec)\n\nAcknowledge Failed Checks\n\nIt is possible to acknowledge every check by updating the acknowledged column. By doing this, specially CrateDB’s built-in Admin-UI won’t complain anymore about failing checks.\n\nImagine we’ve added a new node to our cluster, but as the gateway.expected_nodes column can only be set via config-file or command-line argument, the check for this setting will not pass on the already running nodes until the config-file or command-line argument on these nodes is updated and the nodes are restarted (which is not what we want on a healthy well running cluster).\n\nIn order to make the Admin-UI accept a failing check (so the checks label goes green again), we must acknowledge this check by updating it’s acknowledged flag:\n\ncr> update sys.node_checks set acknowledged = true where id = 1;\nUPDATE OK, 1 row affected (... sec)\n\n\nCaution\n\nUpdates on this column are transient, so changed values are lost after the affected node is restarted.\n\nDescription of Checked Node Settings\nRecovery Expected Nodes\n\nThe check for the gateway.expected_nodes setting checks that the number of nodes that should be waited for the immediate cluster state recovery, must be equal to the maximum number of data and master nodes in the cluster.\n\nRecovery After Nodes\n\nThe check for the gateway.recover_after_nodes verifies that the number of started nodes before the cluster starts must be greater than the half of the expected number of nodes and equal/less than number of nodes in the cluster.\n\n(E / 2) < R <= E\n\n\nwhere R is the number of recovery nodes, E is the number of expected nodes.\n\nRecovery After Time\n\nIf gateway.recover_after_nodes is set, then gateway.recover_after_time must not be set to 0s, otherwise the gateway.recover_after_nodes setting wouldn’t have any effect.\n\nRouting Allocation Disk Watermark High\n\nThe check for the cluster.routing.allocation.disk.watermark.high setting verifies that the high watermark is not exceeded on the current node. The usage of each disk for configured CrateDB data paths is verified against the threshold setting. If one or more verification fails the check is marked as not passed.\n\nRouting Allocation Disk Watermark Low\n\nThe check for the cluster.routing.allocation.disk.watermark.low which controls the low watermark for the node disk usage. The check verifies that the low watermark is not exceeded on the current node. The verification is done against each disk for configured CrateDB data paths. The check is not passed if the verification for one or more disk fails.\n\nJVM Version\n\nThe check for the JVM version checks if CrateDB is running under Java 11 or later. If not the check fails as we’re dropping support for earlier versions in future release. This is a low severity check that doesn’t require immediate action. But to be able to upgrade to future version the JVM should be upgraded eventually.\n\nShards\n\nThe table sys.shards contains real-time statistics for all shards of all (non-system) tables.\n\nTable Schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\t\t\n\n\n_node\n\n\t\n\nInformation about the node the shard is located at.\n\nContains the same information as the sys.nodes table.\n\n\t\n\nOBJECT\n\n\n\n\nblob_path\n\n\t\n\nPath to the directory which contains the blob files of the shard, or null if the shard is not a blob shard.\n\n\t\n\nSTRING\n\n\n\n\nid\n\n\t\n\nThe shard ID.\n\nThis shard ID is managed by the managed by the system ranging from 0 and up to the specified number of shards of a table (by default the number of shards is 5).\n\n\t\n\nINTEGER\n\n\n\n\nmin_lucene_version\n\n\t\n\nShows the oldest lucene segment version used in this shard.\n\n\t\n\nSTRING\n\n\n\n\nnum_docs\n\n\t\n\nThe total amount of docs within a shard.\n\n\t\n\nLONG\n\n\n\n\norphan_partition\n\n\t\n\nTrue if the partition has NO table associated with. In rare situations the table is missing.\n\nFalse on non-partitioned tables.\n\n\t\n\nBOOLEAN\n\n\n\n\npartition_ident\n\n\t\n\nThe partition ident of a partitioned table.\n\nEmpty string on non-partitioned tables.\n\n\t\n\nSTRING\n\n\n\n\npath\n\n\t\n\nPath to the shard directory on the filesystem.\n\nThis directory contains state and index files.\n\n\t\n\nSTRING\n\n\n\n\nprimary\n\n\t\n\nDescribes if the shard is the primary shard.\n\n\t\n\nBOOLEAN\n\n\n\n\nrecovery\n\n\t\n\nRepresents recovery statistic of the particular shard.\n\nRecovery is the process of moving a table shard to a different node or loading it from disk, e.g. during node startup (local gateway recovery), replication, shard rebalancing or snapshot recovery.\n\n\t\n\nOBJECT\n\n\n\n\nrecovery['files']\n\n\t\n\nShards recovery statistic in files.\n\n\t\n\nOBJECT\n\n\n\n\nrecovery['files']['percent']\n\n\t\n\nPercentage of files already recovered.\n\n\t\n\nFLOAT\n\n\n\n\nrecovery['files']['recovered']\n\n\t\n\nNumber of actual files recovered in the shard. Includes both existing and reused files.\n\n\t\n\nINTEGER\n\n\n\n\nrecovery['files']['reused']\n\n\t\n\nTotal number of files reused from a local copy while recovering the shard.\n\n\t\n\nINTEGER\n\n\n\n\nrecovery['files']['used']\n\n\t\n\nTotal number of files in the shard.\n\n\t\n\nINTEGER\n\n\n\n\nrecovery['size']\n\n\t\n\nShards recovery statistic in bytes.\n\n\t\n\nOBJECT\n\n\n\n\nrecovery['size']['percent']\n\n\t\n\nPercentage of bytes already recovered.\n\n\t\n\nFLOAT\n\n\n\n\nrecovery['size']['recovered']\n\n\t\n\nNumber of actual bytes recovered in the shard. Includes both existing and reused bytes.\n\n\t\n\nLONG\n\n\n\n\nrecovery['size']['reused']\n\n\t\n\nNumber of bytes reused from a local copy while recovering the shard.\n\n\t\n\nLONG\n\n\n\n\nrecovery['size']['used']\n\n\t\n\nTotal number of bytes in the shard.\n\n\t\n\nLONG\n\n\n\n\nrecovery['stage']\n\n\t\n\nRecovery stage:\n\ninit: Recovery has not started\n\nindex: Reading the Lucene index meta-data and copying bytes from source to destination\n\nstart: Starting the engine, opening the index for use\n\ntranslog: Replaying transaction log\n\nfinalize: Cleanup\n\ndone: Complete\n\n\t\n\nSTRING\n\n\n\n\nrecovery['total_time']\n\n\t\n\nReturns elapsed time from the start of the shard recovery.\n\n\t\n\nLONG\n\n\n\n\nrecovery['type']\n\n\t\n\nRecovery type:\n\ngateway\n\nsnapshot\n\nreplica\n\nrelocating\n\n\t\n\nSTRING\n\n\n\n\nrelocating_node\n\n\t\n\nThe node ID which the shard is getting relocated to at the time.\n\n\t\n\nSTRING\n\n\n\n\nrouting_state\n\n\t\n\nThe current state of a shard as defined by the routing.\n\nPossible states of the shard routing are:\n\nUNASSIGNED,\n\nINITIALIZING\n\nSTARTED\n\nRELOCATING\n\n\t\n\nSTRING\n\n\n\n\nschema_name\n\n\t\n\nThe schema name.\n\nThis will be “blob” for shards of blob tables and “doc” for shards of common tables without a defined schema.\n\n\t\n\nSTRING\n\n\n\n\nsize\n\n\t\n\nCurrent size in bytes.\n\nThis value is cached for max. 10 seconds to reduce file system access.\n\n\t\n\nLONG\n\n\n\n\nstate\n\n\t\n\nThe current state of the shard.\n\nPossible states are:\n\nCREATED\n\nRECOVERING\n\nPOST_RECOVERY\n\nSTARTED\n\nRELOCATED\n\nCLOSED\n\nINITIALIZING\n\nUNASSIGNED\n\n\t\n\nSTRING\n\n\n\n\ntable_name\n\n\t\n\nThe table name.\n\n\t\n\nSTRING\n\nNote\n\nThe sys.shards table is subject to Shard Table Permissions.\n\nExample\n\nFor example, you can query shards like this:\n\ncr> select schema_name as schema,\n...   table_name as t,\n...   id,\n...   partition_ident as p_i,\n...   num_docs as docs,\n...   primary,\n...   relocating_node as r_n,\n...   routing_state as r_state,\n...   state,\n...   orphan_partition as o_p\n... from sys.shards where table_name = 'locations' and id = 1;\n+--------+-----------+----+-----+------+---------+------+---------+---------+-------+\n| schema | t         | id | p_i | docs | primary | r_n  | r_state |  state  | o_p   |\n+--------+-----------+----+-----+------+---------+------+---------+---------+-------+\n| doc    | locations |  1 |     |    8 | TRUE    | NULL | STARTED | STARTED | FALSE |\n+--------+-----------+----+-----+------+---------+------+---------+---------+-------+\nSELECT 1 row in set (... sec)\n\nJobs, Operations, and Logs\n\nTo let you inspect the activities currently taking place in a cluster, CrateDB provides system tables that let you track current cluster jobs and operations. See Jobs Table and Operations Table.\n\nJobs and operations that finished executing are additionally recorded in memory. There are two retention policies available to control how many records should be kept.\n\nOne option is to configure the maximum number of records which should be kept. Once the configured table size is reached, the older log records are deleted as newer records are added. This is configurable using stats.jobs_log_size and stats.operations_log_size.\n\nAnother option is to configure an expiration time for the records. In this case, the records in the logs tables are periodically cleared if they are older than the expiry time. This behaviour is configurable using stats.jobs_log_expiration and stats.operations_log_expiration.\n\nIn addition to these retention policies, there is a memory limit in place preventing these tables from taking up too much memory. The amount of memory that can be used to store the jobs can be configured using stats.breaker.log.jobs.limit and stats.breaker.log.operations.limit. If the memory limit is reached, an error message will be logged and the log table will be cleared completely.\n\nIt is also possible to define a filter which must match for jobs to be recorded after they finished executing. This can be useful to only record slow queries or queries that failed due to an error. This filter can be configured using the stats.jobs_log_filer setting.\n\nFurthermore, there is a second filter setting which also results in a log entry in the regular CrateDB log file for all finished jobs that match this filter. This can be configured using stats.jobs_log_persistent_filter. This could be used to create a persistent slow query log.\n\nJobs\n\nThe sys.jobs table is a constantly updated view of all jobs that are currently being executed in the cluster.\n\nTable Schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe job UUID.\n\nThis job ID is generated by the sytem.\n\n\t\n\nSTRING\n\n\n\n\nnode\n\n\t\n\nInformation about the node that created the job.\n\n\t\n\nOBJECT\n\n\n\n\nnode['id']\n\n\t\n\nThe id of the node.\n\n\t\n\nSTRING\n\n\n\n\nnode['name']\n\n\t\n\nThe name of the node.\n\n\t\n\nSTRING\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the job started.\n\n\t\n\nTIMESTAMP\n\n\n\n\nstmt\n\n\t\n\nShows the data query or manipulation statement represented by this job.\n\n\t\n\nSTRING\n\n\n\n\nusername\n\n\t\n\nThe user who is executing the statement.\n\n\t\n\nSTRING\n\nThe field username corresponds to the SESSION_USER that is performing the query:\n\ncr> select stmt, username, started from sys.jobs where stmt like 'sel% from %jobs%';\n+---------------------------------------------------------------------------------+----------+-...-----+\n| stmt                                                                            | username | started |\n+---------------------------------------------------------------------------------+----------+-...-----+\n| select stmt, username, started from sys.jobs where stmt like 'sel% from %jobs%' | crate    | ...     |\n+---------------------------------------------------------------------------------+----------+-...-----+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nIf the enterprise edition is disabled or the user management module is not available, the username is represented as crate.\n\nEvery request that queries data or manipulates data is considered a “job” if it is a valid query. Requests that are not valid queries (for example, a request that tries to query a non-existent table) will not show up as jobs.\n\nJobs Metrics\n\nThe sys.jobs_metrics table provides an overview of the query latency in the cluster. Jobs metrics are not persisted across node restarts.\n\nThe metrics are aggregated for each node and each unique classification of the statements.\n\nNote\n\nIn order to reduce the memory requirements for these metrics, the times are statistically sampled and therefore may have slight inaccuracies. In addition, durations are only tracked up to 10 minutes. Statements taking longer than that are capped to 10 minutes.\n\nsys.jobs_metrics Table Schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nnode\n\n\t\n\nAn object containing the id and name of the node on which the metrics have been sampled.\n\n\t\n\nOBJECT\n\n\n\n\nclassification\n\n\t\n\nAn object containing the statement classification.\n\n\t\n\nOBJECT\n\n\n\n\nclassification['type']\n\n\t\n\nThe general type of the statement. Types are: INSERT, SELECT, UPDATE, DELETE, COPY, DDL, and MANAGEMENT.\n\n\t\n\nSTRING\n\n\n\n\nclassification['labels']\n\n\t\n\nLabels are only available for certain statement types that can be classified more accurately than just by their type.\n\n\t\n\nSTRING_ARRAY\n\n\n\n\ntotal_count\n\n\t\n\nTotal number of queries executed\n\n\t\n\nLONG\n\n\n\n\nfailed_count\n\n\t\n\nTotal number of queries that failed to complete successfully.\n\n\t\n\nLONG\n\n\n\n\nsum_of_durations\n\n\t\n\nSum of durations in ms of all executed queries per statement type.\n\n\t\n\nLONG\n\n\n\n\nstdev\n\n\t\n\nThe standard deviation of the query latencies\n\n\t\n\nDOUBLE\n\n\n\n\nmean\n\n\t\n\nThe mean query latency in ms\n\n\t\n\nDOUBLE\n\n\n\n\nmax\n\n\t\n\nThe maximum query latency in ms\n\n\t\n\nLONG\n\n\n\n\nmin\n\n\t\n\nThe minimum query latency in ms\n\n\t\n\nLONG\n\n\n\n\npercentiles\n\n\t\n\nAn object containing different percentiles\n\n\t\n\nOBJECT\n\nClassification\n\nCertain statement types (such as SELECT statements) have additional labels in their classification. These labels are the names of the logical plan operators that are involved in the query.\n\nFor example, the following UNION statement:\n\nSELECT name FROM t1 where id = 1\nUNION ALL\nSELECT name FROM t2 where id < 2\n\n\nwould result in the following labels:\n\nUnion` for the UNION ALL\n\nGet for the left SELECT\n\nCollect for the right SELECT\n\nNote\n\nLabels may be subject to change as they only represent internal properties of the statement!\n\nOperations\n\nThe sys.operations table is a constantly updated view of all operations that are currently being executed in the cluster:\n\ncr> select node['name'], job_id, name, used_bytes from sys.operations\n... order by name limit 1;\n+--------------+--------...-+-----...-+------------+\n| node['name'] | job_id     | name    | used_bytes |\n+--------------+--------...-+-----...-+------------+\n| crate        | ...        | ...     | ...        |\n+--------------+--------...-+-----...-+------------+\nSELECT 1 row in set (... sec)\n\n\nAn operation is a node-specific sub-component of a job (for when a job involves multi-node processing). Jobs that do not require multi-node processing will not produce any operations.\n\nTable Schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe operation UUID.\n\nThis operation ID is generated by the sytem.\n\n\t\n\nSTRING\n\n\n\n\njob_id\n\n\t\n\nThe job id this operation belongs to.\n\n\t\n\nSTRING\n\n\n\n\nname\n\n\t\n\nThe name of the operation.\n\n\t\n\nSTRING\n\n\n\n\nnode\n\n\t\n\nInformation about the node that created the operation.\n\n\t\n\nOBJECT\n\n\n\n\nnode['id']\n\n\t\n\nThe id of the node.\n\n\t\n\nSTRING\n\n\n\n\nnode['name']\n\n\t\n\nThe name of the node.\n\n\t\n\nSTRING\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the operation started.\n\n\t\n\nTIMESTAMP\n\n\n\n\nused_bytes\n\n\t\n\nCurrently loaded amount of data by the operation.\n\n\t\n\nLONG\n\nNote\n\nIn some cases, operations are generated for internal CrateDB work that does not directly correspond to a user request. These entries do not have corresponding entries in sys.jobs.\n\nLogs\n\nThe sys.jobs and sys.operations tables have corresponding log tables: sys.jobs_log and sys.operations_log.\n\nsys.jobs_log Table Schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe job ID.\n\n\t\n\nSTRING\n\n\n\n\nended\n\n\t\n\nThe point in time when the job finished.\n\n\t\n\nTIMESTAMP\n\n\n\n\nerror\n\n\t\n\nIf the job encountered an error, this will hold the error message.\n\n\t\n\nSTRING\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the job started.\n\n\t\n\nTIMESTAMP\n\n\n\n\nstmt\n\n\t\n\nShows the data query or manipulation statement executed by the job.\n\n\t\n\nSTRING\n\n\n\n\nusername\n\n\t\n\nThe user who executed the statement.\n\n\t\n\nSTRING\n\n\n\n\nclassification\n\n\t\n\nAn object containing the statement classification.\n\n\t\n\nOBJECT\n\n\n\n\nclassification['type']\n\n\t\n\nThe general type of the statement. Types are: INSERT, SELECT, UPDATE, DELETE, COPY, DDL, and MANAGEMENT.\n\n\t\n\nSTRING\n\n\n\n\nclassification['labels']\n\n\t\n\nLabels are only available for certain statement types that can be classified more accurately than just by their type.\n\n\t\n\nSTRING_ARRAY\n\nNote\n\nYou can control which jobs are recorded using the stats.jobs_log_filter\n\nsys.operations_log Table Schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe operation ID.\n\n\t\n\nSTRING\n\n\n\n\njob_id\n\n\t\n\nThe job id.\n\n\t\n\nSTRING\n\n\n\n\nended\n\n\t\n\nThe point in time when the operation finished.\n\n\t\n\nTIMESTAMP\n\n\n\n\nerror\n\n\t\n\nIf the operation encountered an error, this will hold the error message.\n\n\t\n\nSTRING\n\n\n\n\nname\n\n\t\n\nThe name of the operation.\n\n\t\n\nSTRING\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the operation started.\n\n\t\n\nTIMESTAMP\n\n\n\n\nused_bytes\n\n\t\n\nThe amount of data loaded by the operation.\n\n\t\n\nLONG\n\nAfter a job or operation finishes, the corresponding entry will be moved into the corresponding log table:\n\ncr> select id, stmt, username, started, ended, error\n... from sys.jobs_log order by ended desc limit 2;\n+-...+----------------------------------------------...-+----------+-...-----+-...---+-------+\n| id | stmt                                             | username | started | ended | error |\n+-...+----------------------------------------------...-+----------+-...-----+-...---+-------+\n| ...| select node['name'], ...                         | crate    | ...     | ...   |  NULL |\n| ...| select stmt, username, started from sys.jobs ... | crate    | ...     | ...   |  NULL |\n+-...+----------------------------------------------...-+----------+-...-----+-...---+-------+\nSELECT 2 rows in set (... sec)\n\n\nInvalid queries are also logged in the sys.jobs_log table, i.e. queries that never make it to the sys.jobs table because they could not be executed.\n\nThe log tables are bound by a fixed size (stats.jobs_log_size) or by an expiration time (stats.jobs_log_expiration)\n\nSee Collecting Stats for information on how to configure logs.\n\nCaution\n\nIf you deactivate statistics tracking, the logs tables will be truncated.\n\nCluster Checks\n\nThe table sys.checks exposes a list of internal cluster checks and results of their validation.\n\nThe sys.checks table looks like this:\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe unique check id.\n\n\t\n\nINTEGER\n\n\n\n\nseverity\n\n\t\n\nThe level of severity. The higher the value of the field the higher severity.\n\n\t\n\nINTEGER\n\n\n\n\ndescription\n\n\t\n\nThe description message for the setting check.\n\n\t\n\nSTRING\n\n\n\n\npassed\n\n\t\n\nThe flag determines whether the check for the setting has passed.\n\n\t\n\nBOOLEAN\n\nHere’s an example query:\n\ncr> select id, description from sys.checks order by id;\n+----+--------------------------------------------------------------...-+\n| id | description                                                      |\n+----+--------------------------------------------------------------...-+\n|  1 | The setting 'discovery.zen.minimum_master_nodes' must not be ... |\n|  2 | The total number of partitions of one or more partitioned tab... |\n|  3 | The following tables need to be recreated for compatibility w... |\n|  6 | Your CrateDB license is valid. Enjoy CrateDB!                    |\n+----+--------------------------------------------------------------...-+\nSELECT 4 rows in set (... sec)\n\n\nCluster checks are also indicated in the CrateDB admin console. When all cluster checks (and all Node Checks) pass, the Checks icon will be green. Here’s what it looks like when some checks are failing at the CRITICAL severity level:\n\nCurrent Checks\nMinimum Master Nodes\n\nThe check for the discovery.zen.minimum_master_nodes setting verifies that the minimum number of nodes is equal/greater than the half of maximum number of nodes in the cluster.\n\n(N / 2) + 1 <= M\n\n\nwhere N is the number of nodes in the cluster, and M is the value of the setting discovery.zen.minimum_master_nodes.\n\nYou can change the value (via SET and RESET) permanently by issuing the following SQL statement:\n\nSET GLOBAL PERSISTENT discovery.zen.minimum_master_nodes = M;\n\nNumber of Partitions\n\nThis check warns if any partitioned table has more than 1000 partitions to detect the usage of a high cardinality field for partitioning.\n\nTables need to be recreated\n\nWarning\n\nDo not attempt to upgrade your cluster to a newer major version if this cluster check is failing. Follow the instructions below to get this cluster check passing.\n\nThis check warns you if there are tables that need to be recreated for compatibility with future major versions of CrateDB.\n\nIf you try to upgrade to the next major version of CrateDB with tables that have not been recreated, CrateDB will refuse to start.\n\nTo recreate a table, you have to create new tables, copy over the data and rename or remove the old table.\n\n1) Use SHOW CREATE TABLE to get the schema required to create an empty copy of the table to recreate:\n\nSHOW CREATE TABLE your_table;\n\n\n2) Create a new temporary table, using the schema returned from SHOW CREATE TABLE:\n\nCREATE TABLE tmp_your_table (...);\n\n\nPrevent inserts to the original table:\n\nALTER TABLE your_table SET (\"blocks.read_only\" = true);\n\n\nCopy the data:\n\nINSERT INTO tmp_your_table (...) (SELECT ... FROM your_table);\n\n\nSwap the tables:\n\nALTER CLUSTER SWAP TABLE tmp_your_table TO your_table;\n\n\nConfirm the new your_table contains all data and has the new version:\n\nSELECT count(*) FROM your_table;\nSELECT version FROM information_schema.tables where table_name = 'your_table';\n\n\nDrop the now obsolete old table:\n\nALTER TABLE tmp_your_table SET (\"blocks.read_only\" = false);\nDROP TABLE tmp_your_table;\n\n\nWhen all tables have been recreated, this cluster check will pass.\n\nNote\n\nSnapshots of your tables created prior to them being upgraded will not work with future versions of CrateDB. For this reason, you should create a new snapshot for each of your tables. (See Snapshots.)\n\nLicense expiry check\n\nThis check warns you when your license is close to expiration. It will yield a MEDIUM alert when your license is valid for less than 15 days and a HIGH alert when your license is valid for less than a day. It’s highly recommended you request a new license when this check triggers in order to avoid the situation where operations are rejected due to an invalid license.\n\nHealth\n\nThe sys.health table lists the health of each table and table partition. The health is computed by checking the states of the shard of each table/partition.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\ntable_name\n\n\t\n\nThe table name.\n\n\t\n\nSTRING\n\n\n\n\ntable_schema\n\n\t\n\nThe schema of the table.\n\n\t\n\nSTRING\n\n\n\n\npartition_ident\n\n\t\n\nThe ident of the partition. NULL for non-partitioned tables.\n\n\t\n\nSTRING\n\n\n\n\nhealth\n\n\t\n\nThe health label. Can be RED, YELLOW or GREEN.\n\n\t\n\nSTRING\n\n\n\n\nseverity\n\n\t\n\nThe health as a short value. Useful when ordering on health.\n\n\t\n\nSHORT\n\n\n\n\nmissing_shards\n\n\t\n\nThe number of not assigned or started shards.\n\n\t\n\nINTEGER\n\n\n\n\nunderreplicated_shards\n\n\t\n\nThe number of shards which are not fully replicated.\n\n\t\n\nINTEGER\n\nBoth missing_shards and underreplicated_shards might return -1 if the cluster is in an unhealthy state that prevents the exact number from being calculated. This could be the case when the cluster can’t elect a master, because there are not enough eligible nodes available.\n\ncr> select * from sys.health order by severity desc, table_name;\n+--------+----------------+-----------------+----------+------------+--------------+------------------------+\n| health | missing_shards | partition_ident | severity | table_name | table_schema | underreplicated_shards |\n+--------+----------------+-----------------+----------+------------+--------------+------------------------+\n| GREEN  |              0 |                 |        1 | locations  | doc          |                      0 |\n| GREEN  |              0 |                 |        1 | quotes     | doc          |                      0 |\n+--------+----------------+-----------------+----------+------------+--------------+------------------------+\nSELECT 2 rows in set (... sec)\n\n\nThe health with the highest severity will always define the health of the query scope.\n\nExample of getting a cluster health (health of all tables):\n\ncr> select health from sys.health order by severity desc limit 1;\n+--------+\n| health |\n+--------+\n| GREEN  |\n+--------+\nSELECT 1 row in set (... sec)\n\nHealth Definition\n\nHealth\n\n\t\n\nDescription\n\n\n\n\nRED\n\n\t\n\nAt least one primary shard is missing (primary shard not started or unassigned).\n\n\n\n\nYELLOW\n\n\t\n\nAt least one shard is underreplicated (replica shard not started or unassigned).\n\n\n\n\nGREEN\n\n\t\n\nAll primary and replica shards have been started.\n\nNote\n\nThe sys.health table is subject to Shard Table Permissions as it will expose a summary of table shard states.\n\nRepositories\n\nThe table sys.repositories lists all configured repositories that can be used to create, manage and restore snapshots (see Snapshots).\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nname\n\n\t\n\nThe repository name\n\n\t\n\nSTRING\n\n\n\n\ntype\n\n\t\n\nThe type of the repository determining how and where the repository stores its snapshots.\n\n\t\n\nSTRING\n\n\n\n\nsettings\n\n\t\n\nThe configuration settings the repository has been created with. The specific settings depend on the repository type, see CREATE REPOSITORY.\n\n\t\n\nOBJECT\n\ncr> SELECT name, type, settings FROM sys.repositories\n... ORDER BY name;\n+---------+------+---------------------------------------------------...--+\n| name    | type | settings                                               |\n+---------+------+---------------------------------------------------...--+\n| my_repo | fs   | {\"compress\": \"true\", \"location\": \"repo_location\", ...} |\n+---------+------+---------------------------------------------------...--+\nSELECT 1 row in set (... sec)\n\nSnapshots\n\nThe table sys.snapshots lists all existing snapshots in all configured repositories (see Snapshots).\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nname\n\n\t\n\nThe name of the snapshot\n\n\t\n\nSTRING\n\n\n\n\nrepository\n\n\t\n\nThe name of the repository that contains this snapshot.\n\n\t\n\nSTRING\n\n\n\n\nconcrete_indices\n\n\t\n\nContains the names of all tables and partitions that are contained in this snapshot how they are represented as ES index names.\n\n\t\n\nARRAY\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the creation of the snapshot started. Changes made after that are not stored in this snapshot.\n\n\t\n\nTIMESTAMP\n\n\n\n\nfinished\n\n\t\n\nThe point in time when the snapshot creation finished.\n\n\t\n\nTIMESTAMP\n\n\n\n\nstate\n\n\t\n\nThe current state of the snapshot. One of: IN_PROGRESS, SUCCESS, PARTIAL, or FAILED.\n\n\t\n\nSTRING\n\n\n\n\nversion\n\n\t\n\nAn internal version this snapshot was created with.\n\n\t\n\nSTRING\n\nSnapshot/Restore operates on a per-shard basis. Hence, the state column indicates whether all (SUCCESS), some (PARTIAL), or no shards(FAILED) have been backed up. PARTIAL snapshots are the result of some primaries becoming unavailable while taking the snapshot when there are no replicas at hand (cluster state is RED). If there are replicas of the (now unavailable) primaries (cluster state is YELLOW) the snapshot succeeds and all shards are included (state SUCCESS). Building on a PARTIAL snapshot will include all primaries again.\n\nWarning\n\nIn case of a PARTIAL state another snapshot should be created in order to guarantee a full backup! Only SUCCESS includes all shards.\n\nThe concrete_indices column contains the names of all Elasticsearch indices that were stored in the snapshot. A normal CrateDB table maps to one Elasticsearch index, a partitioned table maps to one Elasticsearch index per partition. The mapping follows the following pattern:\n\nCrateDB table / partition name\n\n\t\n\nconcrete_indices entry\n\n\n\n\ndoc.my_table\n\n\t\n\nmy_table\n\n\n\n\nmy_schema.my_table\n\n\t\n\nmy_schema.my_table\n\n\n\n\ndoc.parted_table (value=null)\n\n\t\n\n.partitioned.my_table.0400\n\n\n\n\nmy_schema.parted_table (value=null)\n\n\t\n\nmy_schema..partitioned.my_table.0400\n\ncr> SELECT \"repository\", name, state, concrete_indices\n... FROM sys.snapshots order by \"repository\", name;\n+------------+-------------+---------+-----------------...-+\n| repository | name        | state   | concrete_indices    |\n+------------+-------------+---------+-----------------...-+\n| my_repo    | my_snapshot | SUCCESS | [...]               |\n+------------+-------------+---------+-----------------...-+\nSELECT 1 row in set (... sec)\n\nSummits\n\nThe sys.summits table contains the information about the mountains in the Alps higher than 2000m. The mountain names from the table are also used to generate random nodes names.\n\nUsers\n\nThe sys.users table contains all existing database users in the cluster. The table is only available in the CrateDB Enterprise Edition.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nname\n\n\t\n\nThe name of the database user.\n\n\t\n\nSTRING\n\n\n\n\nsuperuser\n\n\t\n\nBOOLEAN flag to indicate whether the user is a superuser.\n\n\t\n\nBOOLEAN\n\nAllocations\n\nThe sys.allocations table contains information about shards and their allocation state. The table contains:\n\nshards that are unassigned and why they are unassigned\n\nshards that are assigned but cannot be moved or rebalanced and why they remain on their current node\n\nIt can help to identify problems if shard allocations behave different than expected, e.g. when a shard stays unassigned or a shard does not move off a node.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\ntable_schema\n\n\t\n\nSchema name of the table of the shard.\n\n\t\n\nSTRING\n\n\n\n\ntable_name\n\n\t\n\nTable name of the shard.\n\n\t\n\nSTRING\n\n\n\n\npartition_ident\n\n\t\n\nIdentifier of the partition of the shard. NULL if the table is not partitioned.\n\n\t\n\nSTRING\n\n\n\n\nshard_id\n\n\t\n\nID of the effected shard.\n\n\t\n\nINTEGER\n\n\n\n\nnode_id\n\n\t\n\nID of the node on which the shard resides. NULL if the shard is unassigned.\n\n\t\n\nSTRING\n\n\n\n\nprimary\n\n\t\n\nWhether the shard is a primary shard.\n\n\t\n\nBOOLEAN\n\n\n\n\ncurrent_state\n\n\t\n\nCurrent state of the shard. Possible states are: UNASSIGNED, INITIALIZING, STARTED, RELOCATING\n\n\t\n\nSTRING\n\n\n\n\nexplanation\n\n\t\n\nExplanation why the shard cannot be allocated, moved or rebalanced.\n\n\t\n\nSTRING\n\n\n\n\ndecisions\n\n\t\n\nA list of decisions that describe in detail why the shard in the current state.\n\n\t\n\nARRAY\n\n\n\n\ndecisions['node_id']\n\n\t\n\nID of the node of the decision.\n\n\t\n\nSTRING\n\n\n\n\ndecisions['node_name']\n\n\t\n\nName of the node of the decision.\n\n\t\n\nSTRING\n\n\n\n\ndecisions['explanations']\n\n\t\n\nDetailed list of human readable explanations why the node decided whether to allocate or rebalance the shard. Returns NULL if there is no need to rebalance the shard.\n\n\t\n\nARRAY\n\nNote\n\nThe sys.allocations table is subject to Shard Table Permissions.\n\nShard Table Permissions\n\nAccessing tables that return shards (sys.shards, sys.allocations) is subjected to the same privileges constraints as the other tables. Namely, in order to query them, the connected user needs to have the DQL privilege on that particular table, either directly or inherited from the SCHEMA or CLUSTER (for more information on privileges inheritance see Hierarchical Inheritance of Privileges).\n\nHowever, being able to query shard returning system tables will not allow the user to retrieve all the rows in the table, as they may contain information related to tables, which the connected user does not have any privileges for. The only rows that will be returned will be the ones the user is allowed to access.\n\nFor example, if the user john has any privilege on the doc.books table but no privilege at all on doc.locations, when john issues a SELECT * FROM sys.shards statement, the shards information related to the doc.locations table will not be returned."
  },
  {
    "title": "Privileges — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/admin/privileges.html",
    "html": "3.3\nPrivileges\n\nThe superuser is allowed to execute any statement without any privilege checks.\n\nThe superuser uses GRANT, DENY and REVOKE statements to control access to the resource. The privileges are either applied on the whole cluster or on instances of objects such as schema, or table.\n\nCurrently, only DQL, DML and DDL privileges can be granted.\n\nAny statements which are not allowed with those privileges, such as GRANT, DENY and REVOKE, can only be issued by a superuser.\n\nNote\n\nPrivileges are an enterprise feature.\n\nWhen the CrateDB Enterprise Edition is disabled, there will be no user privilege checks, and every statement will be executed without the validation of privileges.\n\nTable of Contents\n\nPrivilege Types\n\nDQL\n\nDML\n\nDDL\n\nHierarchical Inheritance of Privileges\n\nBehavior of GRANT, DENY and REVOKE\n\nGRANT\n\nDENY\n\nREVOKE\n\nList Privileges\n\nPrivilege Types\nDQL\n\nGranting Data Query Language (DQL) privilege to a user, indicates that this user is allowed to execute SELECT, SHOW, REFRESH, COPY TO, and SET SESSION statements, as well as using the available user defined functions, on the object for which the privilege applies.\n\nDML\n\nGranting Data Manipulation Language (DML) privilege to a user, indicates that this user is allowed to execute INSERT, COPY FROM, UPDATE and DELETE statements, on the object for which the privilege applies.\n\nDDL\n\nGranting Data Definition Language (DDL) privilege to a user, indicates that this user is allowed to execute CREATE TABLE, DROP TABLE, CREATE VIEW, DROP VIEW,``CREATE FUNCTION``, DROP FUNCTION, CREATE REPOSITORY, DROP REPOSITORY, CREATE SNAPSHOT, DROP SNAPSHOT, RESTORE SNAPSHOT, and ALTER statements, on the object for which the privilege applies.\n\nHierarchical Inheritance of Privileges\n\nPrivileges can be managed on three different levels, namely: CLUSTER, SCHEMA, and TABLE/VIEW.\n\nWhen a privilege is assigned on a certain level, the privilege will propagate down the hierarchy. Privileges defined on a lower level will always override those from a higher level:\n\n  cluster\n    ||\n  schema\n   /  \\\ntable view\n\n\nThis statement will grant DQL privilege to user riley on all the tables and functions of the doc schema:\n\ncr> GRANT DQL ON SCHEMA doc TO riley;\nGRANT OK, 1 row affected (... sec)\n\n\nThis statement will deny DQL privilege to user riley on the doc schema table doc.accounting. However, user riley, will still have DQL privilege on all the other tables of the doc schema:\n\ncr> DENY DQL ON TABLE doc.accounting TO riley;\nDENY OK, 1 row affected (... sec)\n\n\nNote\n\nIn CrateDB, schemas are just namespaces that are created and dropped implicitly. Therefore, when GRANT, DENY or `REVOKE are invoked on a schema level, CrateDB takes the schema name provided without further validation.\n\nPrivileges can be managed on all schemas and tables of the cluster, except the information_schema.\n\nViews are on the same hierarchy with tables, i.e. a privilege on a view is gained through a GRANT on either the view itself, the schema the view belongs to, or a cluster-wide privilege. Privileges on relations which are referenced in the view do not grant any privileges on the view itself. On the contrary, even if the user does not have any privileges on a view’s referenced relations but on the view itself, the user can still access the relations through the view. For example:\n\ncr> CREATE VIEW first_customer as SELECT * from doc.accounting ORDER BY id LIMIT 1\nCREATE OK, 1 row affected (... sec)\n\n\nPreviously we had issued a DENY for user riley on doc.accounting but we can still access it through the view because we have access to it through the doc schema:\n\ncr> SELECT id from first_customer;\n+----+\n| id |\n+----+\n|  1 |\n+----+\nSELECT 1 row in set (... sec)\n\n\nFor more information regarding views, please see the views page.\n\nBehavior of GRANT, DENY and REVOKE\n\nCaution\n\nStale permissions might be introduced if DDL statements were invoked while the Enterprise Edition is temporarily disabled. To allow clients to remove such stale permissions even, if the table does not exist anymore, the REVOKE statement does not perform any validation checks on the table ident.\n\nGRANT\n\nTo grant a privilege to an existing user on the whole cluster, we use the GRANT SQL statement, for example:\n\ncr> GRANT DML TO wolfgang;\nGRANT OK, 1 row affected (... sec)\n\n\nDQL privilege can be granted on the sys schema to user wolfgang, like this:\n\ncr> GRANT DQL ON SCHEMA sys TO wolfgang;\nGRANT OK, 1 row affected (... sec)\n\n\nThe following statement will grant all privileges on table doc.books to user wolfgang:\n\ncr> GRANT ALL PRIVILEGES ON TABLE doc.books TO wolfgang;\nGRANT OK, 3 rows affected (... sec)\n\n\nUsing “ALL PRIVILEGES” is a shortcut to grant all the currently grantable privileges to a user, namely DQL, DML and DDL.\n\nNote\n\nIf no schema is specified in the table ident, the table will be looked up in the current schema.\n\nIf a user with the username specified in the SQL statement does not exist the statement returns an error:\n\ncr> GRANT DQL TO layla;\nSQLActionException[UserUnknownException: User 'layla' does not exist]\n\n\nTo grant ALL PRIVILEGES to user will on the cluster, we can use the following syntax:\n\ncr> GRANT ALL PRIVILEGES TO will;\nGRANT OK, 3 rows affected (... sec)\n\n\nUsing ALL PRIVILEGES is a shortcut to grant all the currently grantable privileges to a user, namely DQL, DML and DDL.\n\nPrivileges can be granted to multiple users in the same statement, like so:\n\ncr> GRANT DDL ON TABLE doc.books TO wolfgang, will;\nGRANT OK, 1 row affected (... sec)\n\nDENY\n\nTo deny a privilege to an existing user on the whole cluster, use the DENY SQL statement, for example:\n\ncr> DENY DDL TO will;\nDENY OK, 1 row affected (... sec)\n\n\nDQL privilege can be denied on the sys schema to user wolfgang, like this:\n\ncr> DENY DQL ON SCHEMA sys TO wolfgang;\nDENY OK, 1 row affected (... sec)\n\n\nThe following statement will deny DQL privilege on table doc.books to user wolfgang:\n\ncr> DENY DQL ON TABLE doc.books TO wolfgang;\nDENY OK, 1 row affected (... sec)\n\n\nDENY ALL or DENY ALL PRIVILEGES will deny all privileges to a user, on the cluster it can be used like this:\n\ncr> DENY ALL TO will;\nDENY OK, 2 rows affected (... sec)\n\nREVOKE\n\nTo revoke a privilege that was previously granted or denied to a user use the REVOKE SQL statement, for example the DQL privilege that was previously denied to user wolfgang on the sys schema, can be revoked like this:\n\ncr> REVOKE DQL ON SCHEMA sys FROM wolfgang;\nREVOKE OK, 1 row affected (... sec)\n\n\nThe privileges that were granted and denied to user wolfgang on doc.books can be revoked like this:\n\ncr> REVOKE ALL ON TABLE doc.books FROM wolfgang;\nREVOKE OK, 3 rows affected (... sec)\n\n\nThe privileges that were granted to user will on the cluster can be revoked like this:\n\ncr> REVOKE ALL FROM will;\nREVOKE OK, 3 rows affected (... sec)\n\n\nNote\n\nThe REVOKE statement can remove only privileges that have been granted or denied through the GRANT or DENY statements. If the privilege on a specific object was not explicitly granted, the REVOKE statement has no effect. The effect of the REVOKE statement will be reflected in the row count.\n\nList Privileges\n\nCrateDB exposes privileges sys.privileges system table.\n\nBy querying the sys.privileges table you can get all information regarding the existing privileges. E.g.:\n\ncr> SELECT * FROM sys.privileges order by grantee, class, ident;\n+---------+----------+---------+----------------+-------+------+\n| class   | grantee  | grantor | ident          | state | type |\n+---------+----------+---------+----------------+-------+------+\n| SCHEMA  | riley    | crate   | doc            | GRANT | DQL  |\n| TABLE   | riley    | crate   | doc.accounting | DENY  | DQL  |\n| TABLE   | will     | crate   | doc.books      | GRANT | DDL  |\n| CLUSTER | wolfgang | crate   | NULL           | GRANT | DML  |\n+---------+----------+---------+----------------+-------+------+\nSELECT 4 rows in set (... sec)\n\n\nThe column grantor shows the user who granted or denied the privilege, the column grantee shows the user for whom the privilege was granted or denied. The column class identifies on which type of context the privilege applies. ident stands for the ident of the object that the privilege is set on and finally type stands for the type of privileges that was granted or denied."
  },
  {
    "title": "Runtime Configuration — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/admin/runtime-config.html",
    "html": "3.3\nRuntime Configuration\n\nThe CrateDB cluster can be configured at runtime using the SET and RESET statement. See the Cluster Settings configuration section for details about the supported settings.\n\nIf SET is used with PERSISTENT the change will survive a cluster restart, if used with TRANSIENT the value will be restored to default or config file value on a restart:\n\ncr> SET GLOBAL PERSISTENT stats.enabled = false;\nSET OK, 1 row affected (... sec)\n\ncr> select sys.cluster.settings['stats']['enabled'] from sys.cluster;\n+------------------------------+\n| settings['stats']['enabled'] |\n+------------------------------+\n| FALSE                        |\n+------------------------------+\nSELECT 1 row in set (... sec)\n\n\nYou can change multiple values at once:\n\ncr> SET GLOBAL TRANSIENT stats.enabled = true,\n... stats.jobs_log_size = 1024, stats.operations_log_size = 4096;\nSET OK, 1 row affected (... sec)\n\ncr> select settings['stats']['enabled'],\n...   settings['stats']['jobs_log_size'],\n...   settings['stats']['operations_log_size']\n... from sys.cluster;\n+-...------------+-...------------------+-...------------------------+\n| ...['enabled'] | ...['jobs_log_size'] | ...['operations_log_size'] |\n+-...------------+-...------------------+-...------------------------+\n| TRUE           |                 1024 |                       4096 |\n+-...------------+-...------------------+-...------------------------+\nSELECT 1 row in set (... sec)\n\n\nIts also possible to save a complete nested object of settings:\n\ncr> SET GLOBAL TRANSIENT stats = {\n...   jobs_log_size = 2048,\n...   operations_log_size = 8192\n... };\nSET OK, 1 row affected (... sec)\n\ncr> SELECT\n...   settings['stats']['jobs_log_size'] AS jobs_size,\n...   settings['stats']['operations_log_size'] AS op_size\n... FROM sys.cluster;\n+-----------+---------+\n| jobs_size | op_size |\n+-----------+---------+\n|      2048 |    8192 |\n+-----------+---------+\nSELECT 1 row in set (... sec)\n\n\nUsing the RESET statement, a setting will be reset to either on node startup defined configuration file value or to its default value:\n\ncr> RESET GLOBAL stats.enabled, stats.operations_log_size;\nRESET OK, 1 row affected (... sec)\n\ncr> SELECT\n...   settings['stats']['jobs_log_size'] AS jobs_size,\n...   settings['stats']['operations_log_size'] AS op_size\n... FROM sys.cluster;\n+-----------+---------+\n| jobs_size | op_size |\n+-----------+---------+\n|      2048 |   10000 |\n+-----------+---------+\nSELECT 1 row in set (... sec)\n\n\nRESET can also be done on objects:\n\ncr> RESET GLOBAL stats;\nRESET OK, 1 row affected (... sec)\n\ncr> SELECT\n...   settings['stats']['jobs_log_size'] AS jobs_size,\n...   settings['stats']['operations_log_size'] AS op_size\n... FROM sys.cluster;\n+-----------+---------+\n| jobs_size | op_size |\n+-----------+---------+\n|     10000 |   10000 |\n+-----------+---------+\nSELECT 1 row in set (... sec)\n"
  },
  {
    "title": "Authentication — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/admin/auth/index.html",
    "html": "3.3\nAuthentication\n\nTable of Contents\n\nAuthentication Methods\nTrust Method\nPassword Authentication Method\nClient Certificate Authentication Method\nHost Based Authentication (HBA)\nAuthentication Against CrateDB\nAuthenticating as a Superuser\nAuthenticating to Admin UI"
  },
  {
    "title": "User Management — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/admin/user-management.html",
    "html": "3.3\nUser Management\n\nNote\n\nUser management is an enterprise feature.\n\nTable of Contents\n\nIntroduction\n\nCREATE USER\n\nALTER USER\n\nDROP USER\n\nList Users\n\nIntroduction\n\nA CrateDB cluster contains a set of database users. A database user is a principle at cluster level.\n\nUser account information is stored in the cluster metatdata of CrateDB and supports the following statements to create, alter and drop users:\n\nCREATE USER\n\nALTER USER\n\nDROP USER\n\nThese statements are database management statements that can only be invoked by superusers that already exist in the CrateDB cluster. Also the users table sys.users is only readable by superusers.\n\nWhen CrateDB is started, the cluster contains one predefined superuser. This user is called crate. It is not possible to create any other superusers\n\nUsers cannot be backed up or restored.\n\nCREATE USER\n\nTo create a new user for the CrateDB database cluster use the CREATE USER SQL statement:\n\ncr> CREATE USER user_a;\nCREATE OK, 1 row affected (... sec)\n\n\nThe newly created user does not have any special Privileges. It can be used to connect to the database cluster using available authentication methods. You can specify the user’s password in the WITH clause of the CREATE statement. This is required if you want to use the Password Authentication Method:\n\ncr> CREATE USER user_b WITH (password = 'a_secret_password');\nCREATE OK, 1 row affected (... sec)\n\n\nThe username parameter of the statement follows the principles of an identifier which means that it must be double-quoted if it contains special characters (e.g. whitespace) or if the case needs to be maintained:\n\ncr> CREATE USER \"Custom User\";\nCREATE OK, 1 row affected (... sec)\n\n\nIf a user with the username specified in the SQL statement already exists the statement returns an error:\n\ncr> CREATE USER \"Custom User\";\nSQLActionException[UserAlreadyExistsException: User 'Custom User' already exists]\n\nALTER USER\n\nTo alter the password for an existing user from the CrateDB database cluster use the ALTER USER SQL statement:\n\ncr> ALTER USER user_a SET (password = 'pass');\nALTER OK, 1 row affected (... sec)\n\n\nThe password can be reset (cleared) if specified as NULL:\n\ncr> ALTER USER user_a SET (password = NULL);\nALTER OK, 1 row affected (... sec)\n\n\nNote\n\nThe built-in superuser crate has no password and it is not possible to set a new password for this user.\n\nDROP USER\n\nTo remove an existing user from the CrateDB database cluster use the DROP USER SQL statement:\n\ncr> DROP USER user_c;\nDROP OK, 1 row affected (... sec)\n\n\nIf a user with the username specified in the SQL statement does not exist the statement returns an error:\n\ncr> DROP USER user_c;\nSQLActionException[UserUnknownException: User 'user_c' does not exist]\n\n\nNote\n\nIt is not possible to drop the built-in superuser crate.\n\nList Users\n\nCrateDB exposes database users via the read-only sys.users system table. The sys.users table shows all users in the cluster which can be used for authentication. The initial superuser crate which is available for all CrateDB clusters is also part of that list.\n\nTo list all existing users query that table:\n\ncr> SELECT * FROM sys.users order by name;\n+-------------+----------+-----------+\n| name        | password | superuser |\n+-------------+----------+-----------+\n| Custom User |     NULL | FALSE     |\n| crate       |     NULL | TRUE      |\n| user_a      |     NULL | FALSE     |\n| user_b      | ******** | FALSE     |\n+-------------+----------+-----------+\nSELECT 4 rows in set (... sec)\n\n\nThe column name shows the unique name of the user, the column superuser shows whether the user has superuser privileges or not.\n\nNote\n\nCrateDB also supports retrieving the current connected user using the system information functions: CURRENT_USER, USER and SESSION_USER.\n\nWarning\n\nWhen the Elasticsearch HTTP REST API is enabled, it is possible to read the users data via the Elasticsearch API. Therefore access to the users table is not restricted."
  },
  {
    "title": "Environment Variables — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/config/environment.html",
    "html": "3.3\nEnvironment Variables\n\nCrateDB can be configured with some environment variables.\n\nThere are many different ways to set environment variables, depending on how CrateDB is being deployed.\n\nHere is a trivial example:\n\nsh$ export CRATE_HOME=/tmp/crate\nsh$ ./bin/crate\n\n\nHere, we set CRATE_HOME to /tmp/crate, export it so that sub-processes of the shell have access, and then start CrateDB.\n\nCrateDB supports two kinds of environment variables:\n\nApplication variables\n\nJava Virtual Machine (JVM) variables\n\nWe use “application” here to distinguish between CrateDB running as a Java application and the JVM itself, which runs CrateDB.\n\nTable of Contents\n\nApplication Variables\n\nJVM Variables\n\nGeneral\n\nGarbage Collection\n\nCollector\n\nLogging\n\nApplication Variables\nCRATE_HOME: directory path\n\nThe home directory of the CrateDB installation.\n\nThis directory is used as the root for the configuration directory, data directory, log directory, and so on.\n\nIf you have installed CrateDB from a package, this variable should be set for you.\n\nIf you are installing manually, in most cases, this should be set to the directory from which you would normally execute bin/crate, i.e. the root directory of the expanded tarball.\n\nJVM Variables\nGeneral\nCRATE_JAVA_OPTS: Java options\n\nThe Java options to use when running CrateDB.\n\nFor example, you could change the stack size like this:\n\nCRATE_JAVA_OPTS=-Xss500k\n\n\nSee Also\n\nFor more information about Java options, consult the documentation for Microsoft Windows or Unix-like operating systems.\n\nCRATE_HEAP_SIZE: size\n\nThe Java heap size, i.e. the amount of memory that can be used.\n\nYou can set the heap size to four gigabytes like this:\n\nCRATE_HEAP_SIZE=4g\n\n\nUse g for gigabytes or m for megabytes.\n\nSee Also\n\nAppropriate memory configuration is important for optimal performance.\n\nCRATE_HEAP_DUMP_PATH: file or directory path (default: varies)\n\nThe directory to be used for heap dumps in the case of a crash.\n\nIf a directory path is configured, new heap dumps will be written to that directory every time CrateDB crashes.\n\nIf a file path is configured (i.e. the last node of the path is non-existent or exists and is a file) CrateDB will overwrite that file with a heap dump every time it crashes.\n\nDefault values are as follows:\n\nFor basic installations, the process working directory\n\nIf you have installed a CrateDB Linux package, /var/lib/crate\n\nWhen running CrateDB on Docker, /data/data\n\nWarning\n\nMake sure there is enough disk space available for heap dumps.\n\nGarbage Collection\nCollector\n\nCrateDB uses the Concurrent Mark Sweep garbage collector by default. When running CrateBD with a JVM >= 10, using the G1 garbage collector may result in better latency.\n\nExample of using CRATE_JAVA_OPTS to enable the G1:\n\nexport CRATE_JAVA_OPTS=\"-XX:-UseConcMarkSweepGC -XX:-UseCMSInitiatingOccupancyOnly -XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=75\"\n\nLogging\n\nCrateDB logs JVM garbage collection times using the built-in garbage collection logging of the JVM.\n\nSee Also\n\nThe logging configuration documentation has the complete list of garbage collection logging environment variables."
  },
  {
    "title": "Release Notes — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/appendices/release-notes/index.html",
    "html": "3.3\nRelease Notes\n\nInformation about individual CrateDB releases, typically including upgrade information and changelog.\n\nNote\n\nThe latest stable, testing, and nightly version is always available from the CrateDB download page.\n\nOlder version of CrateDB are available in the release archives.\n\nVersions\nUnreleased Changes\n3.x\n3.3.x\nVersion 3.3.6\nVersion 3.3.5\nVersion 3.3.4\nVersion 3.3.3\nVersion 3.3.2\nVersion 3.3.1\nVersion 3.3.0\n3.2.x\nVersion 3.2.6\nVersion 3.2.5\nVersion 3.2.4\nVersion 3.2.3\nVersion 3.2.2\nVersion 3.2.1\nVersion 3.2.0\n3.1.x\nVersion 3.1.6\nVersion 3.1.5\nVersion 3.1.4\nVersion 3.1.3\nVersion 3.1.2\nVersion 3.1.1\nVersion 3.1.0\n3.0.x\nVersion 3.0.7\nVersion 3.0.6\nVersion 3.0.5\nVersion 3.0.4\nVersion 3.0.3\nVersion 3.0.2\nVersion 3.0.1\nVersion 3.0.0\n2.x\n2.3.x\nVersion 2.3.11\nVersion 2.3.10\nVersion 2.3.9\nVersion 2.3.8\nVersion 2.3.7\nVersion 2.3.6\nVersion 2.3.5\nVersion 2.3.4\nVersion 2.3.3\nVersion 2.3.2\nVersion 2.3.1\nVersion 2.3.0\n2.2.x\nVersion 2.2.7\nVersion 2.2.6\nVersion 2.2.5\nVersion 2.2.4\nVersion 2.2.3\nVersion 2.2.2\nVersion 2.2.1\nVersion 2.2.0\n2.1.x\nVersion 2.1.10\nVersion 2.1.9\nVersion 2.1.8\nVersion 2.1.7\nVersion 2.1.6\nVersion 2.1.5\nVersion 2.1.4\nVersion 2.1.3\nVersion 2.1.2\nVersion 2.1.1\nVersion 2.1.0\n2.0.x\nVersion 2.0.7\nVersion 2.0.6\nVersion 2.0.5\nVersion 2.0.4\nVersion 2.0.3\nVersion 2.0.2\nVersion 2.0.1\nVersion 2.0.0\n1.x\n1.2.x\nVersion 1.2.0\n1.1.x\nVersion 1.1.6\nVersion 1.1.5\nVersion 1.1.4\nVersion 1.1.3\nVersion 1.1.2\nVersion 1.1.1\nVersion 1.1.0\n1.0.x\nVersion 1.0.6\nVersion 1.0.5\nVersion 1.0.4\nVersion 1.0.3\nVersion 1.0.2\nVersion 1.0.1\nVersion 1.0.0\nOlder Versions\n\nFor older versions, see the 0.57.8 CHANGES.txt file."
  },
  {
    "title": "Information Schema — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/general/information-schema.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n3.3\nInformation Schema\n\ninformation_schema is a special schema that contains virtual tables which are read-only and can be queried to get information about the state of the cluster.\n\nTable of Contents\n\nAccess\n\nVirtual Tables\n\ntables\n\nsettings\n\nviews\n\ncolumns\n\ntable_constraints\n\nkey_column_usage\n\ntable_partitions\n\nroutines\n\nschemata\n\nsql_features\n\ningestion_rules\n\nAccess\n\nWhen the user management is enabled, accessing the information_schema is open to all users and it does not require any privileges.\n\nHowever, being able to query information_schema tables will not allow the user to retrieve all the rows in the table, as it can contain information related to tables over which the connected user does not have any privileges. The only rows that will be returned will be the ones the user is allowed to access.\n\nFor example, if the user john has any privilege on the doc.books table but no privilege at all on doc.locations, when john issues a SELECT * FROM information_schema.tables statement, the tables information related to the doc.locations table will not be returned.\n\nVirtual Tables\ntables\n\nThe information_schema.tables virtual table can be queried to get a list of all available tables and views and their settings, such as number of shards or number of replicas.\n\ncr> SELECT table_schema, table_name, table_type, number_of_shards, number_of_replicas\n... FROM information_schema.tables\n... ORDER BY table_schema ASC, table_name ASC;\n+--------------------+-------------------------+------------+------------------+--------------------+\n| table_schema       | table_name              | table_type | number_of_shards | number_of_replicas |\n+--------------------+-------------------------+------------+------------------+--------------------+\n| doc                | galaxies                | VIEW       |             NULL | NULL               |\n| doc                | locations               | BASE TABLE |                2 | 0                  |\n| doc                | partitioned_table       | BASE TABLE |                4 | 0-1                |\n| doc                | quotes                  | BASE TABLE |                2 | 0                  |\n| information_schema | columns                 | BASE TABLE |             NULL | NULL               |\n| information_schema | ingestion_rules         | BASE TABLE |             NULL | NULL               |\n| information_schema | key_column_usage        | BASE TABLE |             NULL | NULL               |\n| information_schema | referential_constraints | BASE TABLE |             NULL | NULL               |\n| information_schema | routines                | BASE TABLE |             NULL | NULL               |\n| information_schema | schemata                | BASE TABLE |             NULL | NULL               |\n| information_schema | sql_features            | BASE TABLE |             NULL | NULL               |\n| information_schema | table_constraints       | BASE TABLE |             NULL | NULL               |\n| information_schema | table_partitions        | BASE TABLE |             NULL | NULL               |\n| information_schema | tables                  | BASE TABLE |             NULL | NULL               |\n| information_schema | views                   | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_attrdef              | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_attribute            | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_class                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_constraint           | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_database             | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_description          | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_index                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_namespace            | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_type                 | BASE TABLE |             NULL | NULL               |\n| sys                | allocations             | BASE TABLE |             NULL | NULL               |\n| sys                | checks                  | BASE TABLE |             NULL | NULL               |\n| sys                | cluster                 | BASE TABLE |             NULL | NULL               |\n| sys                | health                  | BASE TABLE |             NULL | NULL               |\n| sys                | jobs                    | BASE TABLE |             NULL | NULL               |\n| sys                | jobs_log                | BASE TABLE |             NULL | NULL               |\n| sys                | jobs_metrics            | BASE TABLE |             NULL | NULL               |\n| sys                | node_checks             | BASE TABLE |             NULL | NULL               |\n| sys                | nodes                   | BASE TABLE |             NULL | NULL               |\n| sys                | operations              | BASE TABLE |             NULL | NULL               |\n| sys                | operations_log          | BASE TABLE |             NULL | NULL               |\n| sys                | privileges              | BASE TABLE |             NULL | NULL               |\n| sys                | repositories            | BASE TABLE |             NULL | NULL               |\n| sys                | shards                  | BASE TABLE |             NULL | NULL               |\n| sys                | snapshots               | BASE TABLE |             NULL | NULL               |\n| sys                | summits                 | BASE TABLE |             NULL | NULL               |\n| sys                | users                   | BASE TABLE |             NULL | NULL               |\n+--------------------+-------------------------+------------+------------------+--------------------+\nSELECT 41 rows in set (... sec)\n\n\nThe table also contains additional information such as specified routing (Sharding) and partitioned by (Partitioned Tables) columns:\n\ncr> SELECT table_name, clustered_by, partitioned_by\n... FROM information_schema.tables\n... WHERE table_schema = 'doc'\n... ORDER BY table_schema ASC, table_name ASC;\n+-------------------+--------------+----------------+\n| table_name        | clustered_by | partitioned_by |\n+-------------------+--------------+----------------+\n| galaxies          | NULL         | NULL           |\n| locations         | id           | NULL           |\n| partitioned_table | _id          | [\"date\"]       |\n| quotes            | id           | NULL           |\n+-------------------+--------------+----------------+\nSELECT 4 rows in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nDescription\n\n\t\n\nData Type\n\n\n\n\nblobs_path\n\n\t\n\nThe data path of the blob table\n\n\t\n\nString\n\n\n\n\nclosed\n\n\t\n\nThe state of the table\n\n\t\n\nBoolean\n\n\n\n\nclustered_by\n\n\t\n\nThe routing column used to cluster the table\n\n\t\n\nString\n\n\n\n\ncolumn_policy\n\n\t\n\nDefines whether the table uses a STRICT or a DYNAMIC Column Policy\n\n\t\n\nString\n\n\n\n\nnumber_of_replicas\n\n\t\n\nThe number of replicas the table currently has\n\n\t\n\nInteger\n\n\n\n\nnumber_of_shards\n\n\t\n\nThe number of shards the table is currently distributed across\n\n\t\n\nInteger\n\n\n\n\npartitioned_by\n\n\t\n\nThe column used to partition the table\n\n\t\n\nString\n\n\n\n\nreference_generation\n\n\t\n\nSpecifies how values in the self-referencing column are generated\n\n\t\n\nString\n\n\n\n\nrouting_hash_function\n\n\t\n\nThe name of the hash function used for internal routing\n\n\t\n\nString\n\n\n\n\nself_referencing_column_name\n\n\t\n\nThe name of the column that uniquely identifies each row (always _id)\n\n\t\n\nString\n\n\n\n\nsettings\n\n\t\n\nWITH\n\n\t\n\nObject\n\n\n\n\ntable_catalog\n\n\t\n\nRefers to the table_schema\n\n\t\n\nString\n\n\n\n\ntable_name\n\n\t\n\nThe name of the table\n\n\t\n\nString\n\n\n\n\ntable_schema\n\n\t\n\nThe name of the schema the table belongs to\n\n\t\n\nString\n\n\n\n\ntable_type\n\n\t\n\nThe type of the table (BASE TABLE for tables, VIEW for views)\n\n\t\n\nString\n\n\n\n\nversion\n\n\t\n\nA collection of version numbers relevent to the table\n\n\t\n\nObject\n\nsettings\n\nTable settings specify configuration parameters for tables. Some settings can be set during Cluster runtime and others are only applied on cluster restart.\n\nThis list of table settings in WITH shows detailed information of each parameter.\n\nTable parameters can be applied with CREATE TABLE on creation of a table. With ALTER TABLE they can be set on already existing tables.\n\nThe following statement creates a new table and sets the refresh interval of shards to 500 ms and sets the shard allocation for primary shards only:\n\ncr> create table parameterized_table (id int, content string)\n... with (\"refresh_interval\"=500, \"routing.allocation.enable\"='primaries');\nCREATE OK, 1 row affected (... sec)\n\n\nThe settings can be verified by querying information_schema.tables:\n\ncr> select settings['routing']['allocation']['enable'] as alloc_enable,\n...   settings['refresh_interval'] as refresh_interval\n... from information_schema.tables\n... where table_name='parameterized_table';\n+--------------+------------------+\n| alloc_enable | refresh_interval |\n+--------------+------------------+\n| primaries    |              500 |\n+--------------+------------------+\nSELECT 1 row in set (... sec)\n\n\nOn existing tables this needs to be done with ALTER TABLE statement:\n\ncr> alter table parameterized_table\n... set (\"routing.allocation.enable\"='none');\nALTER OK, -1 rows affected (... sec)\n\nviews\n\nThe table information_schema.views contains the name, definition and options of all available views.\n\ncr> SELECT table_schema, table_name, view_definition\n... FROM information_schema.views\n... ORDER BY table_schema ASC, table_name ASC;\n+--------------+------------+-------------------------+\n| table_schema | table_name | view_definition         |\n+--------------+------------+-------------------------+\n| doc          | galaxies   | SELECT                  |\n|              |            |   \"id\"                  |\n|              |            | , \"name\"                |\n|              |            | , \"description\"         |\n|              |            | FROM \"locations\"        |\n|              |            | WHERE \"kind\" = 'Galaxy' |\n+--------------+------------+-------------------------+\nSELECT 1 row in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nDescription\n\n\t\n\nData Type\n\n\n\n\ntable_catalog\n\n\t\n\nThe catalog of the table of the view (refers to table_schema)\n\n\t\n\nString\n\n\n\n\ntable_schema\n\n\t\n\nThe schema of the table of the view\n\n\t\n\nString\n\n\n\n\ntable_name\n\n\t\n\nThe name of the table of the view\n\n\t\n\nString\n\n\n\n\nview_definition\n\n\t\n\nThe SELECT statement that defines the view\n\n\t\n\nString\n\n\n\n\ncheck_option\n\n\t\n\nNot applicable for CrateDB, always return NONE\n\n\t\n\nString\n\n\n\n\nis_updatable\n\n\t\n\nWhether the view is updatable. Not applicable for CrateDB, always returns FALSE\n\n\t\n\nBoolean\n\n\n\n\nowner\n\n\t\n\nThe user that created the view\n\n\t\n\nString\n\nNote\n\nIf you drop the table of a view, the view will still exist and show up in the information_schema.tables and information_schema.views tables.\n\ncolumns\n\nThis table can be queried to get a list of all available columns of all tables and views and their definition like data type and ordinal position inside the table:\n\ncr> select table_name, column_name, ordinal_position as pos, data_type\n... from information_schema.columns\n... where table_schema = 'doc' and table_name not like 'my_table%'\n... order by table_name asc, column_name asc;\n+-------------------+--------------------------------+------+--------------+\n| table_name        | column_name                    |  pos | data_type    |\n+-------------------+--------------------------------+------+--------------+\n| locations         | date                           |    1 | timestamp    |\n| locations         | description                    |    2 | string       |\n| locations         | id                             |    3 | string       |\n| locations         | information                    |    4 | object_array |\n| locations         | information['evolution_level'] | NULL | short        |\n| locations         | information['population']      | NULL | long         |\n| locations         | kind                           |    5 | string       |\n| locations         | name                           |    6 | string       |\n| locations         | position                       |    7 | integer      |\n| locations         | race                           |    8 | object       |\n| locations         | race['description']            | NULL | string       |\n| locations         | race['interests']              | NULL | string_array |\n| locations         | race['name']                   | NULL | string       |\n| partitioned_table | date                           |    1 | timestamp    |\n| partitioned_table | id                             |    2 | long         |\n| partitioned_table | title                          |    3 | string       |\n| quotes            | id                             |    1 | integer      |\n| quotes            | quote                          |    2 | string       |\n+-------------------+--------------------------------+------+--------------+\nSELECT 18 rows in set (... sec)\n\n\nYou can even query this tables’ own columns (attention: this might lead to infinite recursion of your mind, beware!):\n\ncr> select column_name, data_type, ordinal_position\n... from information_schema.columns\n... where table_schema = 'information_schema'\n... and table_name = 'columns' order by ordinal_position asc;\n+---------------------------+-----------+------------------+\n| column_name               | data_type | ordinal_position |\n+---------------------------+-----------+------------------+\n| character_maximum_length  | integer   |                1 |\n| character_octet_length    | integer   |                2 |\n| character_set_catalog     | string    |                3 |\n| character_set_name        | string    |                4 |\n| character_set_schema      | string    |                5 |\n| check_action              | integer   |                6 |\n| check_references          | string    |                7 |\n| collation_catalog         | string    |                8 |\n| collation_name            | string    |                9 |\n| collation_schema          | string    |               10 |\n| column_default            | string    |               11 |\n| column_name               | string    |               12 |\n| data_type                 | string    |               13 |\n| datetime_precision        | integer   |               14 |\n| domain_catalog            | string    |               15 |\n| domain_name               | string    |               16 |\n| domain_schema             | string    |               17 |\n| generation_expression     | string    |               18 |\n| interval_precision        | integer   |               19 |\n| interval_type             | string    |               20 |\n| is_generated              | boolean   |               21 |\n| is_nullable               | boolean   |               22 |\n| numeric_precision         | integer   |               23 |\n| numeric_precision_radix   | integer   |               24 |\n| numeric_scale             | integer   |               25 |\n| ordinal_position          | short     |               26 |\n| table_catalog             | string    |               27 |\n| table_name                | string    |               28 |\n| table_schema              | string    |               29 |\n| user_defined_type_catalog | string    |               30 |\n| user_defined_type_name    | string    |               31 |\n| user_defined_type_schema  | string    |               32 |\n+---------------------------+-----------+------------------+\nSELECT 32 rows in set (... sec)\n\n\nNote\n\nColumns are always sorted alphabetically in ascending order regardless of the order they were defined on table creation. Thus the ordinal_position reflects the alphabetical position.\n\nSchema\n\nName\n\n\t\n\nDescription\n\n\t\n\nData Type\n\n\n\n\ntable_catalog\n\n\t\n\nRefers to the table_schema\n\n\t\n\nString\n\n\n\n\ntable_schema\n\n\t\n\nSchema name containing the table\n\n\t\n\nString\n\n\n\n\ntable_name\n\n\t\n\nTable Name\n\n\t\n\nString\n\n\n\n\ncolumn_name\n\n\t\n\nColumn Name For fields in object columns this is not an identifier but a path and therefore must not be double quoted when programmatically obtained.\n\n\t\n\nString\n\n\n\n\nordinal_position\n\n\t\n\nThe position of the column within the table\n\n\t\n\nInteger\n\n\n\n\nis_nullable\n\n\t\n\nWhether the column is nullable\n\n\t\n\nBoolean\n\n\n\n\ndata_type\n\n\t\n\nThe data type of the column\n\nFor further information see Data Types\n\n\t\n\nString\n\n\n\n\ncolumn_default\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nString\n\n\n\n\ncharacter_maximum_length\n\n\t\n\nNot implemented (always returns NULL)\n\nPlease refer to string type\n\n\t\n\nInteger\n\n\n\n\ncharacter_octet_length\n\n\t\n\nNot implemented (always returns NULL)\n\nPlease refer to string type\n\n\t\n\nInteger\n\n\n\n\nnumeric_precision\n\n\t\n\nIndicates the number of significant digits for a numeric data_type. For all other data types this column is NULL.\n\n\t\n\nInteger\n\n\n\n\nnumeric_precision_radix\n\n\t\n\nIndicates in which base the value in the column numeric_precision for a numeric data_type is exposed. This can either be 2 (binary) or 10 (decimal). For all other data types this column is NULL.\n\n\t\n\nInteger\n\n\n\n\nnumeric_scale\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nInteger\n\n\n\n\ndatetime_precision\n\n\t\n\nContains the fractional seconds precision for a timestamp data_type. For all other data types this column is null.\n\n\t\n\nInteger\n\n\n\n\ninterval_type\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nString\n\n\n\n\ninterval_precision\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nInteger\n\n\n\n\ncharacter_set_catalog\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nString\n\n\n\n\ncharacter_set_schema\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nString\n\n\n\n\ncharacter_set_name\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nString\n\n\n\n\ncollation_catalog\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nString\n\n\n\n\ncollation_schema\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nString\n\n\n\n\ncollation_name\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nString\n\n\n\n\ndomain_catalog\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nString\n\n\n\n\ndomain_schema\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nString\n\n\n\n\ndomain_name\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nString\n\n\n\n\nuser_defined_type_catalog\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nString\n\n\n\n\nuser_defined_type_schema\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nString\n\n\n\n\nuser_defined_type_name\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nString\n\n\n\n\ncheck_references\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nString\n\n\n\n\ncheck_action\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nInteger\n\n\n\n\ngeneration_expression\n\n\t\n\nThe expression used to generate ad column. If the column is not generated NULL is returned.\n\n\t\n\nString\n\n\n\n\nis_generated\n\n\t\n\nReturns true or false wether the column is generated or not\n\n\t\n\nBoolean\n\ntable_constraints\n\nThis table can be queried to get a list of all defined table constraints, their type, name and which table they are defined in.\n\nNote\n\nCurrently only PRIMARY_KEY constraints are supported.\n\ncr> select table_schema, table_name, constraint_name, constraint_type as type\n... from information_schema.table_constraints\n... where table_name = 'tables'\n...   or table_name = 'quotes'\n...   or table_name = 'documents'\n...   or table_name = 'tbl'\n... order by table_schema desc, table_name asc limit 10;\n+--------------------+------------+-...------------------+-------------+\n| table_schema       | table_name | constraint_name      | type        |\n+--------------------+------------+-...------------------+-------------+\n| information_schema | tables     | tables_pk            | PRIMARY KEY |\n| doc                | quotes     | quotes_pk            | PRIMARY KEY |\n| doc                | tbl        | doc_tbl_col_not_null | CHECK       |\n+--------------------+------------+-...------------------+-------------+\nSELECT 3 rows in set (... sec)\n\nkey_column_usage\n\nThis table may be queried to retrieve primary key information from all user tables:\n\ncr> select constraint_name, table_name, column_name, ordinal_position\n... from information_schema.key_column_usage\n... where table_name = 'students'\n+-----------------+------------+-------------+------------------+\n| constraint_name | table_name | column_name | ordinal_position |\n+-----------------+------------+-------------+------------------+\n| students_pk     | students   | id          |                1 |\n| students_pk     | students   | department  |                2 |\n+-----------------+------------+-------------+------------------+\nSELECT 2 rows in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nDescription\n\n\t\n\nData Type\n\n\n\n\nconstraint_catalog\n\n\t\n\nRefers to table_catalog\n\n\t\n\nString\n\n\n\n\nconstraint_schema\n\n\t\n\nRefers to table_schema\n\n\t\n\nString\n\n\n\n\nconstraint_name\n\n\t\n\nName of the constraint\n\n\t\n\nString\n\n\n\n\ntable_catalog\n\n\t\n\nRefers to table_schema\n\n\t\n\nString\n\n\n\n\ntable_schema\n\n\t\n\nName of the schema that contains the table that contains the constraint\n\n\t\n\nString\n\n\n\n\ntable_name\n\n\t\n\nName of the table that contains the constraint\n\n\t\n\nString\n\n\n\n\ncolumn_name\n\n\t\n\nName of the column that contains the constraint\n\n\t\n\nString\n\n\n\n\nordinal_position\n\n\t\n\nPosition of the column within the contraint (starts with 1)\n\n\t\n\nInteger\n\ntable_partitions\n\nThis table can be queried to get information about all partitioned tables, Each partition of a table is represented as one row. The row contains the information table name, schema name, partition ident, and the values of the partition. values is a key-value object with the ‘partitioned by column’ as key(s) and the corresponding value as value(s).\n\nFor further information see Partitioned Tables.\n\ncr> insert into a_partitioned_table (id, content) values (1, 'content_a');\nINSERT OK, 1 row affected (... sec)\n\ncr> alter table a_partitioned_table set (number_of_shards=5);\nALTER OK, -1 rows affected (... sec)\n\ncr> insert into a_partitioned_table (id, content) values (2, 'content_b');\nINSERT OK, 1 row affected (... sec)\n\n\nThe following example shows a table where the column ‘content’ of table ‘a_partitioned_table’ has been used to partition the table. The table has two partitions. The partitions are introduced when data is inserted where ‘content’ is ‘content_a’, and ‘content_b’.:\n\ncr> select table_name, schema_name as schema, partition_ident, \"values\"\n... from information_schema.table_partitions\n... order by table_name, partition_ident;\n+---------------------+--------+--------------------+--------------------------+\n| table_name          | schema | partition_ident    | values                   |\n+---------------------+--------+--------------------+--------------------------+\n| a_partitioned_table | doc    | 04566rreehimst2vc4 | {\"content\": \"content_a\"} |\n| a_partitioned_table | doc    | 04566rreehimst2vc8 | {\"content\": \"content_b\"} |\n+---------------------+--------+--------------------+--------------------------+\nSELECT 2 rows in set (... sec)\n\n\nThe second partition has been created after the number of shards for future partitions have been changed on the partitioned table, so they show 5 instead of 4:\n\ncr> select table_name, partition_ident,\n... number_of_shards, number_of_replicas\n... from information_schema.table_partitions\n... order by table_name, partition_ident;\n+---------------------+--------------------+------------------+--------------------+\n| table_name          | partition_ident    | number_of_shards | number_of_replicas |\n+---------------------+--------------------+------------------+--------------------+\n| a_partitioned_table | 04566rreehimst2vc4 |                4 | 0-1                |\n| a_partitioned_table | 04566rreehimst2vc8 |                5 | 0-1                |\n+---------------------+--------------------+------------------+--------------------+\nSELECT 2 rows in set (... sec)\n\nroutines\n\nThe routines table contains tokenizers, token-filters, char-filters, custom analyzers created by CREATE ANALYZER statements (see Create a Custom Analyzer), and functions created by CREATE FUNCTION statements:\n\ncr> select routine_name, routine_type\n... from information_schema.routines\n... group by routine_name, routine_type\n... order by routine_name asc limit 5;\n+----------------------+--------------+\n| routine_name         | routine_type |\n+----------------------+--------------+\n| PathHierarchy        | TOKENIZER    |\n| apostrophe           | TOKEN_FILTER |\n| arabic               | ANALYZER     |\n| arabic_normalization | TOKEN_FILTER |\n| arabic_stem          | TOKEN_FILTER |\n+----------------------+--------------+\nSELECT 5 rows in set (... sec)\n\n\nFor example you can use this table to list existing tokenizers like this:\n\ncr> select routine_name\n... from information_schema.routines\n... where routine_type='TOKENIZER'\n... order by routine_name asc limit 10;\n+---------------+\n| routine_name  |\n+---------------+\n| PathHierarchy |\n| char_group    |\n| classic       |\n| edgeNGram     |\n| edge_ngram    |\n| keyword       |\n| letter        |\n| lowercase     |\n| nGram         |\n| ngram         |\n+---------------+\nSELECT 10 rows in set (... sec)\n\n\nOr get an overview of how many routines and routine types are available:\n\ncr> select count(*), routine_type\n... from information_schema.routines\n... group by routine_type\n... order by routine_type;\n+----------+--------------+\n| count(*) | routine_type |\n+----------+--------------+\n|       45 | ANALYZER     |\n|        3 | CHAR_FILTER  |\n|       18 | TOKENIZER    |\n|       65 | TOKEN_FILTER |\n+----------+--------------+\nSELECT 4 rows in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nData Type\n\n\n\n\nroutine_name\n\n\t\n\nString\n\n\n\n\nroutine_type\n\n\t\n\nString\n\n\n\n\nroutine_body\n\n\t\n\nString\n\n\n\n\nroutine_schema\n\n\t\n\nString\n\n\n\n\ndata_type\n\n\t\n\nString\n\n\n\n\nis_deterministic\n\n\t\n\nBoolean\n\n\n\n\nroutine_definition\n\n\t\n\nString\n\n\n\n\nspecific_name\n\n\t\n\nString\n\nroutine_name\n\nName of the routine (might be duplicated in case of overloading)\n\nroutine_type\n\nType of the routine. Can be FUNCTION, ANALYZER, CHAR_FILTER, TOKEN_FILTER or TOKEN_FILTER.\n\nroutine_schema\n\nThe schema where the routine was defined. If it doesn’t apply, then NULL.\n\nroutine_body\n\nThe language used for the routine implementation. If it doesn’t apply, then NULL.\n\ndata_type\n\nThe return type of the function. If it doesn’t apply, then NULL.\n\nis_deterministic\n\nIf the routine is deterministic then True, else False (NULL if it doesn’t apply).\n\nroutine_definition\n\nThe function definition (NULL if it doesn’t apply).\n\nspecific_name\n\nUsed to uniquely identify the function in a schema, even if the function is overloaded. Currently the specific name contains the types of the function arguments. As the format might change in the future, it should be only used to compare it to other instances of specific_name.\n\nschemata\n\nThe schemata table lists all existing schemas. These schemas are always available: blob, doc, information_schema and sys:\n\ncr> select schema_name from information_schema.schemata order by schema_name;\n+--------------------+\n| schema_name        |\n+--------------------+\n| blob               |\n| doc                |\n| information_schema |\n| pg_catalog         |\n| sys                |\n+--------------------+\nSELECT 5 rows in set (... sec)\n\nsql_features\n\nThe sql_features table outlines supported and unsupported SQL features of CrateDB based to the current SQL standard (see SQL Standard Compliance):\n\ncr> select feature_name, is_supported, sub_feature_id, sub_feature_name\n... from information_schema.sql_features\n... where feature_id='F501';\n+--------------------------------+--------------+----------------+--------------------+\n| feature_name                   | is_supported | sub_feature_id | sub_feature_name   |\n+--------------------------------+--------------+----------------+--------------------+\n| Features and conformance views | FALSE        |                |                    |\n| Features and conformance views | TRUE         | 1              | SQL_FEATURES view  |\n| Features and conformance views | FALSE        | 2              | SQL_SIZING view    |\n| Features and conformance views | FALSE        | 3              | SQL_LANGUAGES view |\n+--------------------------------+--------------+----------------+--------------------+\nSELECT 4 rows in set (... sec)\n\n\nName\n\n\t\n\nData Type\n\n\t\n\nNullable\n\n\n\n\nfeature_id\n\n\t\n\nString\n\n\t\n\nNO\n\n\n\n\nfeature_name\n\n\t\n\nString\n\n\t\n\nNO\n\n\n\n\nsub_feature_id\n\n\t\n\nString\n\n\t\n\nNO\n\n\n\n\nsub_feature_name\n\n\t\n\nString\n\n\t\n\nNO\n\n\n\n\nis_supported\n\n\t\n\nString\n\n\t\n\nNO\n\n\n\n\nis_verified_by\n\n\t\n\nString\n\n\t\n\nYES\n\n\n\n\ncomments\n\n\t\n\nString\n\n\t\n\nYES\n\nfeature_id\n\nIdentifier of the feature\n\nfeature_name\n\nDescriptive name of the feature by the Standard\n\nsub_feature_id\n\nIdentifier of the subfeature; If it has zero-length, this is a feature\n\nsub_feature_name\n\nDescriptive name of the subfeature by the Standard; If it has zero-length, this is a feature\n\nis_supported\n\nYES if the feature is fully supported by the current version of CrateDB, NO if not\n\nis_verified_by\n\nIdentifies the conformance test used to verify the claim;\n\nAlways NULL since the CrateDB development group does not perform formal testing of feature conformance\n\ncomments\n\nEither NULL or shows a comment about the supported status of the feature\n\ningestion_rules\n\nThe ingestion_rules table contains rules created by CREATE INGEST RULE statements.\n\nSchema\n\nName\n\n\t\n\nData Type\n\n\n\n\nrule_name\n\n\t\n\nString\n\n\n\n\nsource_ident\n\n\t\n\nString\n\n\n\n\ntarget_table\n\n\t\n\nString\n\n\n\n\ncondition\n\n\t\n\nString\n\nrule_name\n\nThe rule name\n\nsource_ident\n\nThe ingestion source identifier\n\ntarget_table\n\nThe target table identifier\n\ncondition\n\nA boolean expression used to filter the source data"
  },
  {
    "title": "Built-in Functions and Operators — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/general/builtins/index.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n3.3\nBuilt-in Functions and Operators\n\nThis chapter provides an overview of built-in functions and operators.\n\nOperators are reserved keywords that are primarily used in a WHERE Clause to perform arithmetic operations or comparisons.\n\nTable of Contents\n\nScalar Functions\nString Functions\nDate and Time Functions\nGeo Functions\nMathematical Functions\nRegular Expression Functions\nArray Functions\nConditional Functions and Expressions\nSystem Information Functions\nSpecial Functions\nAggregation\nIntroduction\ncount\nmin\nmax\nsum\navg and mean\ngeometric_mean\nvariance\nstddev\npercentile\narbitrary\nhyperloglog_distinct\nLimitations\nArithmetic Operators\nSupported operators\nResult types\nTable Functions\nempty_row( )\nunnest( array [ array , ] )\ngenerate_series(start, stop, [step])\nComparison Operators\nArray Comparisons\nIN (value [, ...])\nANY/SOME (array expression)\nSubquery Expressions\nIN (subquery)\nANY/SOME (subquery)\nWindow Functions\nIntroduction\nWindow Definition\nGeneral-Purpose Window Functions\nAggregate Window Functions"
  },
  {
    "title": "Optimistic Concurrency Control — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/general/occ.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n3.3\nOptimistic Concurrency Control\n\nTable of Contents\n\nIntroduction\n\nOptimistic Update\n\nOptimistic Delete\n\nKnown Limitations\n\nIntroduction\n\nEven though CrateDB does not support transactions, Optimistic Concurrency Control can be achieved by using the internal system column _version.\n\nEvery new row has an initial version of 1. This value is increased by 1 on every update.\n\nIt’s possible to fetch the _version by selecting it:\n\ncr> select name, id, \"_version\" from locations\n... where kind = 'Star System' order by name asc;\n+----------------+----+----------+\n| name           | id | _version |\n+----------------+----+----------+\n| Aldebaran      | 4  | 3        |\n| Algol          | 5  | 3        |\n| Alpha Centauri | 6  | 3        |\n| Altair         | 7  | 1        |\n+----------------+----+----------+\nSELECT 4 rows in set (... sec)\n\n\nThese _version values can now be used on updates and deletes.\n\nNote\n\nOptimistic concurrency control only works using the = operator, checking for the exact _version your update/delete is based on.\n\nOptimistic Update\n\nQuerying for the correct _version ensures that no concurrent update has taken place:\n\ncr> update locations set description = 'Updated description'\n... where id=5 and \"_version\" = 3;\nUPDATE OK, 1 row affected (... sec)\n\n\nUpdating a row with a wrong or outdated version number will not execute the update and results in 0 affected rows:\n\ncr> update locations set description = 'Updated description'\n... where id=5 and \"_version\" = 2;\nUPDATE OK, 0 rows affected (... sec)\n\nOptimistic Delete\n\nOf course the same can be done when deleting a row:\n\ncr> delete from locations where id = '6' and \"_version\" = 3;\nDELETE OK, 1 row affected (... sec)\n\nKnown Limitations\n\nThe _version column can only be used when specifying the whole primary key in a query. For example, the query below is not possible with our used testing data because name is not declared as a primary key and results in an error:\n\ncr> delete from locations where name = 'Aldebaran' and \"_version\" = 3;\nSQLActionException... \"_version\" column can only be used in the WHERE ...\n\n\nNote\n\nBoth, DELETE and UPDATE, commands will return a row count of 0 if the given required version does not match the actual version of the relevant row."
  },
  {
    "title": "User-Defined Functions — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/general/user-defined-functions.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n3.3\nUser-Defined Functions\n\nTable of Contents\n\nCREATE OR REPLACE\n\nSupported Types\n\nOverloading\n\nDeterminism\n\nDROP FUNCTION\n\nSupported Languages\n\nJavaScript\n\nSupported Types\n\nWorking with NUMBERS\n\nWorking With Array Methods\n\nCREATE OR REPLACE\n\nCrateDB supports user-defined functions. See CREATE FUNCTION for a full syntax description.\n\nThese functions can be created like so:\n\ncr> CREATE FUNCTION my_subtract_function(integer, integer)\n...  RETURNS integer\n...  LANGUAGE JAVASCRIPT\n...  AS 'function my_subtract_function(a, b) { return a - b; }';\nCREATE OK, 1 row affected  (... sec)\n\ncr> SELECT doc.my_subtract_function(3, 1);\n+--------------------------------+\n| doc.my_subtract_function(3, 1) |\n+--------------------------------+\n|                              2 |\n+--------------------------------+\nSELECT 1 row in set (... sec)\n\n\nOR REPLACE can be used to replace an existing function:\n\ncr> CREATE OR REPLACE FUNCTION log10(long)\n...  RETURNS double\n...  LANGUAGE JAVASCRIPT\n...  AS 'function log10(a) { return Math.log(a)/Math.log(10); }';\nCREATE OK, 1 row affected  (... sec)\n\ncr> SELECT doc.log10(10);\n+---------------+\n| doc.log10(10) |\n+---------------+\n|           1.0 |\n+---------------+\nSELECT 1 row in set (... sec)\n\n\nArguments can be named in the function definition.\n\nFor example, if you wanted two geo_point arguments named start_point and end_point, you would do it like this:\n\ncr> CREATE OR REPLACE FUNCTION calculate_distance(start_point geo_point, end_point geo_point)\n...  RETURNS float\n...  LANGUAGE JAVASCRIPT\n...  AS 'function calculate_distance(start_point, end_point){\n...        return Math.sqrt( Math.pow(end_point[0] - start_point[0], 2), Math.pow(end_point[1] - start_point[1], 2));\n...      }';\nCREATE OK, 1 row affected  (... sec)\n\n\nNote\n\nArgument names are used for query documentation purposes only. You cannot reference arguments by name in the function body.\n\nOptionally, you can specify a schema for the function. If you omit the schema, the current session schema is used.\n\nYou can explicitly assign a schema like this:\n\ncr> CREATE OR REPLACE FUNCTION my_schema.log10(long)\n...  RETURNS double\n...  LANGUAGE JAVASCRIPT\n...  AS 'function log10(a) { return Math.log(a)/Math.log(10); }';\nCREATE OK, 1 row affected  (... sec)\n\n\nWarning\n\nSnapshots can’t be used to backup functions, because snapshots contain table data only.\n\nSupported Types\n\nThe argument types, and the return type of the function can be any of the CrateDB supported Data Types. Data types of values passed into a function must strictly correspond to its argument data types.\n\nNote\n\nThe value returned by the function will be casted to the return type provided in the definition if required. An exception will be thrown if the cast is not successful.\n\nOverloading\n\nWithin a specific schema, you can overload functions by defining two functions with the same name that have a different set of arguments:\n\ncr> CREATE FUNCTION my_schema.my_multiply(integer, integer)\n...  RETURNS integer\n...  LANGUAGE JAVASCRIPT\n...  AS 'function my_multiply(a, b) { return a * b; }';\nCREATE OK, 1 row affected  (... sec)\n\n\nThis would overload our my_multiply function with different argument types:\n\ncr> CREATE FUNCTION my_schema.my_multiply(long, long)\n...  RETURNS long\n...  LANGUAGE JAVASCRIPT\n...  AS 'function my_multiply(a, b) { return a * b; }';\nCREATE OK, 1 row affected  (... sec)\n\n\nThis would overload our my_multiply function with more arguments:\n\ncr> CREATE FUNCTION my_schema.my_multiply(long, long, long)\n...  RETURNS long\n...  LANGUAGE JAVASCRIPT\n...  AS 'function my_multiply(a, b, c) { return a * b * c; }';\nCREATE OK, 1 row affected  (... sec)\n\n\nCaution\n\nIt is considered bad practice to create functions that have the same name as the CrateDB built-in functions!\n\nNote\n\nIf you call a function without a schema name, CrateDB will look it up in the built-in functions first and only then in the user-defined functions available in the search_path.\n\nTherefore a built-in function with the same name as a user-defined function will hide the latter, even if it contains a different set of arguments! However, such functions can still be called if the schema name is explicitly provided.\n\nDeterminism\n\nCaution\n\nUser-defined functions need to be deterministic, meaning that they must always return the same result value when called with the same argument values, because CrateDB might cache the returned values and reuse the value if the function is called multiple times with the same arguments.\n\nDROP FUNCTION\n\nFunctions can be dropped like this:\n\ncr> DROP FUNCTION doc.log10(long);\nDROP OK, 1 row affected  (... sec)\n\n\nAdding IF EXISTS prevents from raising an error if the function doesn’t exist:\n\ncr> DROP FUNCTION IF EXISTS doc.log10(integer);\nDROP OK, 1 row affected  (... sec)\n\n\nOptionally, argument names can be specified within the drop statement:\n\ncr> DROP FUNCTION IF EXISTS doc.calculate_distance(start_point geo_point, end_point geo_point);\nDROP OK, 1 row affected  (... sec)\n\n\nOptionally, you can provide a schema:\n\ncr> DROP FUNCTION my_schema.log10(long);\nDROP OK, 1 row affected  (... sec)\n\nSupported Languages\n\nCrateDB currently only supports the UDF language javascript.\n\nJavaScript\n\nThe UDF language javascript supports the ECMAScript 5.1 standard.\n\nNote\n\nThe JavaScript language is an enterprise feature.\n\nCrateDB uses the Java built-in JavaScript engine Nashorn to interpret and execute functions written in JavaScript. The engine is initialized using the --no-java option which basically restricts all access to Java APIs from within the JavaScript context. CrateDB’s engine also does not allow non-standard syntax extensions (--no-syntax-extensions).\n\nThis, however, does not mean that JavaScript is securely sandboxed.\n\nAlso, even though Nashorn runs ECMA-complient JavaScript, objects that are normally accessible with a web browser (e.g. window, console and so on) are are not available.\n\nCaution\n\nThe JavaScript language is an experimental feature and is disabled by default. You can enable the Javascript Language via the configuration file.\n\nSupported Types\n\nJavaScript functions can handle all CrateDB data types. However, for some return types the function output must correspond to the certain format.\n\nIf a function requires geo_point as a return type, then the JavaScript function must return a double array of size 2, WKT string or GeoJson object.\n\nHere is an example of a JavaScript function returning a double array:\n\ncr> CREATE FUNCTION rotate_point(point geo_point, angle float)\n...  RETURNS geo_point\n...  LANGUAGE JAVASCRIPT\n...  AS 'function rotate_point(point, angle) {\n...        var cos = Math.cos(angle);\n...        var sin = Math.sin(angle);\n...        var x = cos * point[0] - sin * point[1];\n...        var y = sin * point[0] + cos * point[1];\n...        return [x, y];\n...      }';\nCREATE OK, 1 row affected  (... sec)\n\n\nBelow is an example of a JavaScript function returning a WKT string, which will be cast to geo_point:\n\ncr> CREATE FUNCTION symmetric_point(point geo_point)\n...  RETURNS geo_point\n...  LANGUAGE JAVASCRIPT\n...  AS 'function symmetric_point (point, angle) {\n...        var x = - point[0],\n...            y = - point[1];\n...        return \"POINT (\\\" + x + \\\", \\\" + y +\\\")\";\n...      }';\nCREATE OK, 1 row affected  (... sec)\n\n\nSimilarly, if the function specifies the geo_shape return data type, then the JavaScript function should return a GeoJson object or``WKT`` string:\n\ncr> CREATE FUNCTION line(start_point array(double), end_point array(double))\n...  RETURNS object\n...  LANGUAGE JAVASCRIPT\n...  AS 'function line(start_point, end_point) {\n...        return { \"type\": \"LineString\", \"coordinates\" : [start_point, end_point] };\n...      }';\nCREATE OK, 1 row affected  (... sec)\n\n\nNote\n\nIf the return value of the JavaScript function is undefined, it is converted to NULL.\n\nWorking with NUMBERS\n\nThe JavaScript engine Nashorn interprets numbers as java.lang.Double, java.lang.Long, or java.lang.Integer, depending on the computation performed. In most cases, this is not an issue, since the return type of the JavaScript function will be cast to the return type specified in the CREATE FUNCTION statement, although cast might result in a loss of precision.\n\nHowever, when you try to cast DOUBLE to TIMESTAMP, it will be interpreted as UTC seconds and will result in a wrong value:\n\ncr> CREATE FUNCTION utc(long, long, long)\n...  RETURNS TIMESTAMP\n...  LANGUAGE JAVASCRIPT\n...  AS 'function utc(year, month, day) {\n...        return Date.UTC(year, month, day, 0, 0, 0);\n...      }';\nCREATE OK, 1 row affected  (... sec)\n\ncr> SELECT date_format(utc(2016,04,6)) as epoque;\n+------------------------------+\n| epoque                       |\n+------------------------------+\n| 48314-07-22T00:00:00.000000Z |\n+------------------------------+\nSELECT 1 row in set (... sec)\n\n\nTo avoid this behavior, the numeric value should be divided by 1000 before it is returned:\n\ncr> CREATE FUNCTION utc(long, long, long)\n...  RETURNS TIMESTAMP\n...  LANGUAGE JAVASCRIPT\n...  AS 'function utc(year, month, day) {\n...        return Date.UTC(year, month, day, 0, 0, 0)/1000;\n...      }';\nCREATE OK, 1 row affected  (... sec)\n\ncr> SELECT date_format(utc(2016,04,6)) as epoque;\n+-----------------------------+\n| epoque                      |\n+-----------------------------+\n| 2016-05-06T00:00:00.000000Z |\n+-----------------------------+\nSELECT 1 row in set (... sec)\n\nWorking With Array Methods\n\nThe JavaScript Array object has a number of prototype methods you can use, such as join, map, sort, slice, reduce, and so on.\n\nNormally, you can call these methods directly from an Array object, like so:\n\nfunction array_join(a, b) {\n    return a.join(b);\n}\n\n\nHowever, when writing JavaScript for use with CrateDB, you must explicitly use the prototype method:\n\nfunction array_join(a, b) {\n    return Array.prototype.join.call(a, b);\n}\n\n\nYou must do it like this because arguments are not passed as Array objects, and so do not have the associated prototype methods available. Arguments are instead passed as array-like objects."
  },
  {
    "title": "Blobs — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/general/blobs.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n3.3\nBlobs\n\nCrateDB includes support to store binary large objects. By utilizing CrateDB’s cluster features the files can be replicated and sharded just like regular data.\n\nTable of Contents\n\nCreating a Table for Blobs\n\nCustom Location for Storing Blob Data\n\nGlobal by Config\n\nPer Blob Table Setting\n\nList\n\nAltering a Blob Table\n\nDeleting a Blob Table\n\nUsing Blob Tables\n\nUploading\n\nDownload\n\nDelete\n\nCreating a Table for Blobs\n\nBefore adding blobs a blob table must be created. Blob tables can be sharded. This makes it possible to distribute binaries over multiple nodes. Lets use the CrateDB shell crash to issue the SQL statement:\n\nsh$ crash -c \"create blob table myblobs clustered into 3 shards with (number_of_replicas=0)\"\nCREATE OK, 1 row affected (... sec)\n\n\nNow CrateDB is configured to allow blobs to be management under the /_blobs/myblobs endpoint.\n\nCustom Location for Storing Blob Data\n\nIt is possible to define a custom directory path for storing blob data which can be completely different than the normal data path. Best use case for this is storing normal data on a fast SSD and blob data on a large cheap spinning disk.\n\nThe custom blob data path can be set either globally by config or while creating a blob table. The path can be either absolute or relative and must be creatable/writable by the user CrateDB is running as. A relative path value is relative to CRATE_HOME.\n\nBlob data will be stored under this path with the following layout:\n\n/<blobs.path>/nodes/<NODE_NO>/indices/<INDEX_UUID>/<SHARD_ID>/blobs\n\nGlobal by Config\n\nJust uncomment or add following entry at the CrateDB config in order to define a custom path globally for all blob tables:\n\nblobs.path: /path/to/blob/data\n\n\nAlso see Configuration.\n\nPer Blob Table Setting\n\nIt is also possible to define a custom blob data path per table instead of global by config. Also per table setting take precedence over the config setting.\n\nSee CREATE BLOB TABLE for details.\n\nCreating a blob table with a custom blob data path:\n\nsh$ crash -c \"create blob table myblobs clustered into 3 shards with (blobs_path='/tmp/crate_blob_data')\" # doctest: +SKIP\nCREATE OK, 1 row affected (... sec)\n\nList\n\nTo list all blobs inside a blob table a SELECT statement can be used:\n\nsh$ crash -c \"select digest, last_modified from blob.myblobs\"\n+------------------------------------------+---------------+\n| digest                                   | last_modified |\n+------------------------------------------+---------------+\n| 4a756ca07e9487f482465a99e8286abc86ba4dc7 | ...           |\n+------------------------------------------+---------------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nTo query blob tables it is necessary to always specify the schema name blob.\n\nAltering a Blob Table\n\nThe number of replicas a blob table has can be changed using the ALTER BLOB TABLE clause:\n\nsh$ crash -c \"alter blob table myblobs set (number_of_replicas=0)\"\nALTER OK, 1 row affected (... sec)\n\nDeleting a Blob Table\n\nBlob tables can be deleted similar to normal tables:\n\nsh$ crash -c \"drop blob table myblobs\"\nDROP OK, 1 row affected (... sec)\n\nUsing Blob Tables\n\nThe usage of Blob Tables is only supported using the HTTP/HTTPS protocol. This section describes how binaries can be stored, fetched and deleted.\n\nNote\n\nFor the reason of internal optimization any successful request could lead to a 307 Temporary Redirect response.\n\nUploading\n\nTo upload a blob the sha1 hash of the blob has to be known upfront since this has to be used as the ID of the new blob. For this example we use a fancy Python one-liner to compute the shasum:\n\nsh$ python3 -c 'import hashlib;print(hashlib.sha1(\"contents\".encode(\"utf-8\")).hexdigest())'\n4a756ca07e9487f482465a99e8286abc86ba4dc7\n\n\nThe blob can now be uploaded by issuing a PUT request:\n\nsh$ curl -isSX PUT '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7' -d 'contents'\nHTTP/1.1 201 Created\ncontent-length: 0\n\n\nIf a blob already exists with the given hash a 409 Conflict is returned:\n\nsh$ curl -isSX PUT '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7' -d 'contents'\nHTTP/1.1 409 Conflict\ncontent-length: 0\n\nDownload\n\nTo download a blob simply use a GET request:\n\nsh$ curl -sS '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7'\ncontents\n\n\nIf the blob doesn’t exist a 404 Not Found error is returned:\n\nsh$ curl -isS '127.0.0.1:4200/_blobs/myblobs/e5fa44f2b31c1fb553b6021e7360d07d5d91ff5e'\nHTTP/1.1 404 Not Found\ncontent-length: 0\n\n\nTo determine if a blob exists without downloading it, a HEAD request can be used:\n\nsh$ curl -sS -I '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7'\nHTTP/1.1 200 OK\ncontent-length: 8\naccept-ranges: bytes\nexpires: Thu, 31 Dec 2037 23:59:59 GMT\ncache-control: max-age=315360000\n\n\nNote\n\nThe cache headers for blobs are static and basically allows clients to cache the response forever since the blob is immutable.\n\nDelete\n\nTo delete a blob simply use a DELETE request:\n\nsh$ curl -isS -XDELETE '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7'\nHTTP/1.1 204 No Content\n\n\nIf the blob doesn’t exist a 404 Not Found error is returned:\n\nsh$ curl -isS -XDELETE '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7'\nHTTP/1.1 404 Not Found\ncontent-length: 0\n"
  },
  {
    "title": "Querying Crate — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/general/dql/index.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n3.3\nQuerying Crate\n\nThis section provides an overview on how to query documents using SQL. See Data Definition for information about Table creation and other Data Definition statements.\n\nTable of Contents\n\nSelecting Data\nIntroduction\nFROM Clause\nJoins\nDISTINCT Clause\nWHERE Clause\nRegular Expressions\nLIKE\nNOT\nIN\nIS NULL\nIS NOT NULL\nANY (array)\nLimits\nInner Objects and Nested Objects\nObject Arrays\nData Aggregation\nWindow Functions\nGROUP BY\nJoins\nCross Joins\nInner Joins\nLeft Outer Joins\nRight Outer Joins\nFull Outer Joins\nJoin Conditions\nAvailable Join Algorithms\nLimitations\nUnion All\nRefresh\nIntroduction\nMultiple Table Refresh\nPartition Refresh\nFulltext Search\nMATCH Predicate\nUsage\nSearching On Multiple Columns\nNegative Search\nFilter By _score\nGeo Search\nIntroduction\nMatch Predicate\nExact Queries"
  },
  {
    "title": "Data Definition — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/general/ddl/index.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n3.3\nData Definition\n\nTable of Contents\n\nCreating Tables\nIntroduction\nSchemas\nNaming Restrictions\nData Types\nClassification\nboolean\nstring\nNumeric Types\nip\ntimestamp\ngeo_point\ngeo_shape\nobject\narray\nType Conversion\nType aliases\nSystem Columns\nGenerated Columns\nConstraints\nPrimary Key\nNot Null\nStorage\nColumn Store\nPartitioned Tables\nIntroduction\nCreation\nInformation Schema\nInsert\nUpdate\nDelete\nQuerying\nAlter\nLimitations\nConsistency Notes Related to Concurrent DML Statement\nSharding\nIntroduction\nNumber of Shards\nRouting\nReplication\nConfiguration\nShard Allocation Filtering\nSettings\nSpecial Attributes\nColumn Policy\nstrict\ndynamic\nFulltext Indices\nIndex Definition\nDisable indexing\nPlain index (Default)\nCreate a Custom Analyzer\nExtending a Bultin Analyzer\nFulltext Analyzers\nOverview\nBuilt-in Analyzers\nBuilt-in Tokenizers\nBuilt-in Token Filters\nBuiltin Char Filter\nShow Create Table\nViews\nIntroduction: Creating Views\nQuerying Views\nDropping Views\nAltering Tables\nUpdating Parameters\nAdding Columns\nClosing and Opening Tables\nRenaming Tables\nReroute Shards"
  },
  {
    "title": "Data Manipulation — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/general/dml.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n3.3\nData Manipulation\n\nTable of Contents\n\nInserting Data\n\nInserting Data By Query\n\nUpserts (ON CONFLICT DO UPDATE SET)\n\nUpdating Data\n\nDeleting Data\n\nImport and Export\n\nImporting Data\n\nImport From a File URI\n\nImport With Detailed Error Reporting\n\nExporting Data\n\nInserting Data\n\nInserting data to CrateDB is done by using the SQL INSERT statement.\n\nNote\n\nThe column list is always ordered alphabetically by column name. If the insert columns are omitted, the values in the VALUES clauses must correspond to the table columns in that order.\n\nInserting a row:\n\ncr> insert into locations (id, date, description, kind, name, position)\n... values (\n...   '14',\n...   '2013-09-12T21:43:59.000Z',\n...   'Blagulon Kappa is the planet to which the police are native.',\n...   'Planet',\n...   'Blagulon Kappa',\n...   7\n... );\nINSERT OK, 1 row affected (... sec)\n\n\nWhen inserting a single row, if an error occurs an error is returned as a response.\n\nInserting multiple rows at once (aka. bulk insert) can be done by defining multiple values for the INSERT statement:\n\ncr> insert into locations (id, date, description, kind, name, position) values\n... (\n...   '16',\n...   '2013-09-14T21:43:59.000Z',\n...   'Blagulon Kappa II is the planet to which the police are native.',\n...   'Planet',\n...   'Blagulon Kappa II',\n...   19\n... ),\n... (\n...   '17',\n...   '2013-09-13T16:43:59.000Z',\n...   'Brontitall is a planet with a warm, rich atmosphere and no mountains.',\n...   'Planet',\n...   'Brontitall',\n...   10\n... );\nINSERT OK, 2 rows affected (... sec)\n\n\nWhen inserting multiple rows, if an error occurs for some of these rows there is no error returned but instead the number of rows affected would be decreased by the number of rows that failed to be inserted.\n\nWhen inserting into tables containing Generated Columns, their value can be safely omitted. It is generated upon insert:\n\ncr> insert into debit_card (owner, num_part1, num_part2) values\n... ('Zaphod Beeblebrox', 1234, 5678);\nINSERT OK, 1 row affected (... sec)\n\n\nIf a value is given, it is validated against the generation clause of the column and the currently inserted row:\n\ncr> insert into debit_card (owner, num_part1, num_part2, check_sum) values\n... ('Arthur Dent', 9876, 5432, 642935);\nSQLActionException[SQLParseException: Given value 642935 for generated column does not match defined generated expression value 642936]\n\nInserting Data By Query\n\nIt is possible to insert data using a query instead of values. Column data types of source and target table can differ as long as the values are castable. This gives the opportunity to restructure the tables data, renaming a field, changing a field’s data type or convert a normal table into a partitioned one.\n\nExample of changing a field’s data type, in this case, changing the position data type from integer to short:\n\ncr> create table locations2 (\n...     id string primary key,\n...     name string,\n...     date timestamp,\n...     kind string,\n...     position short,\n...     description string\n... ) clustered by (id) into 2 shards with (number_of_replicas = 0);\nCREATE OK, 1 row affected (... sec)\n\ncr> insert into locations2 (id, name, date, kind, postition, description)\n... (\n...     select id, name, date, kind, position, description\n...     from locations\n...     where position < 10\n... );\nINSERT OK, 14 rows affected (... sec)\n\n\nExample of creating a new partitioned table out of the locations table with data partitioned by year:\n\ncr> create table locations_parted (\n...     id string primary key,\n...     name string,\n...     year string primary key,\n...     date timestamp,\n...     kind string,\n...     position integer\n... ) clustered by (id) into 2 shards\n... partitioned by (year) with (number_of_replicas = 0);\nCREATE OK, 1 row affected (... sec)\n\ncr> insert into locations_parted (id, name, year, date, kind, postition)\n... (\n...     select\n...         id,\n...         name,\n...         date_format('%Y', date),\n...         date,\n...         kind,\n...         position\n...     from locations\n... );\nINSERT OK, 16 rows affected (... sec)\n\n\nResulting partitions of the last insert by query:\n\ncr> select table_name, partition_ident, values, number_of_shards, number_of_replicas\n... from information_schema.table_partitions\n... where table_name = 'locations_parted'\n... order by partition_ident;\n+------------------+-----------------+------------------+------------------+--------------------+\n| table_name       | partition_ident | values           | number_of_shards | number_of_replicas |\n+------------------+-----------------+------------------+------------------+--------------------+\n| locations_parted | 042j2e9n74      | {\"year\": \"1979\"} |                2 |                  0 |\n| locations_parted | 042j4c1h6c      | {\"year\": \"2013\"} |                2 |                  0 |\n+------------------+-----------------+------------------+------------------+--------------------+\nSELECT 2 rows in set (... sec)\n\n\nNote\n\nlimit, offset and order by are not supported inside the query statement.\n\nUpserts (ON CONFLICT DO UPDATE SET)\n\nThe ON CONFLICT DO UPDATE SET clause is used to update the existing row if inserting is not possible because of a duplicate-key conflict if a document with the same PRIMARY KEY already exists. This is type of opperation is commonly referred to as an upsert, being a combination of “update” and “insert”.\n\ncr> select\n...     name,\n...     visits,\n...     extract(year from last_visit) as last_visit\n... from uservisits order by name;\n+----------+--------+------------+\n| name     | visits | last_visit |\n+----------+--------+------------+\n| Ford     |      1 | 2013       |\n| Trillian |      3 | 2013       |\n+----------+--------+------------+\nSELECT 2 rows in set (... sec)\n\ncr> insert into uservisits (id, name, visits, last_visit) values\n... (\n...     0,\n...     'Ford',\n...     1,\n...     '2015-09-12'\n... ) on conflict (id) do update set\n...     visits = visits + 1,\n...     last_visit = '2015-01-12';\nINSERT OK, 1 row affected (... sec)\n\ncr> select\n...     name,\n...     visits,\n...     extract(year from last_visit) as last_visit\n... from uservisits where id = 0;\n+------+--------+------------+\n| name | visits | last_visit |\n+------+--------+------------+\n| Ford |      2 | 2015       |\n+------+--------+------------+\nSELECT 1 row in set (... sec)\n\n\nIt’s possible to refer to values which would be inserted if no duplicate-key conflict occured, by using the special excluded table. This table is especially useful in multiple-row inserts, to refer to the current rows values:\n\ncr> insert into uservisits (id, name, visits, last_visit) values\n... (\n...     0,\n...     'Ford',\n...     2,\n...     '2016-01-13'\n... ),\n... (\n...     1,\n...     'Trillian',\n...     5,\n...     '2016-01-15'\n... ) on conflict (id) do update set\n...     visits = visits + excluded.visits,\n...     last_visit = excluded.last_visit;\nINSERT OK, 2 rows affected (... sec)\n\ncr> select\n...     name,\n...     visits,\n...     extract(year from last_visit) as last_visit\n... from uservisits order by name;\n+----------+--------+------------+\n| name     | visits | last_visit |\n+----------+--------+------------+\n| Ford     |      4 | 2016       |\n| Trillian |      8 | 2016       |\n+----------+--------+------------+\nSELECT 2 rows in set (... sec)\n\n\nThis can also be done when using a query instead of values:\n\ncr> create table uservisits2 (\n...   id integer primary key,\n...   name string,\n...   visits integer,\n...   last_visit timestamp\n... ) clustered by (id) into 2 shards with (number_of_replicas = 0);\nCREATE OK, 1 row affected (... sec)\n\ncr> insert into uservisits2 (id, name, visits, last_visit)\n... (\n...     select id, name, visits, last_visit\n...     from uservisits\n... );\nINSERT OK, 2 rows affected (... sec)\n\ncr> insert into uservisits2 (id, name, visits, last_visit)\n... (\n...     select id, name, visits, last_visit\n...     from uservisits\n... ) on conflict (id) do update set\n...     visits = visits + excluded.visits,\n...     last_visit = excluded.last_visit;\nINSERT OK, 2 rows affected (... sec)\n\ncr> select\n...     name,\n...     visits,\n...     extract(year from last_visit) as last_visit\n... from uservisits order by name;\n+----------+--------+------------+\n| name     | visits | last_visit |\n+----------+--------+------------+\n| Ford     |      4 | 2016       |\n| Trillian |      8 | 2016       |\n+----------+--------+------------+\nSELECT 2 rows in set (... sec)\n\nUpdating Data\n\nIn order to update documents in CrateDB the SQL UPDATE statement can be used:\n\ncr> update locations set description = 'Updated description'\n... where name = 'Bartledan';\nUPDATE OK, 1 row affected (... sec)\n\n\nUpdating nested objects is also supported:\n\ncr> update locations set race['name'] = 'Human' where name = 'Bartledan';\nUPDATE OK, 1 row affected (... sec)\n\n\nIt’s also possible to reference a column within the expression, for example to increment a number like this:\n\ncr> update locations set position = position + 1 where position < 3;\nUPDATE OK, 6 rows affected (... sec)\n\n\nNote\n\nIf the same documents are updated concurrently an VersionConflictException might occur. CrateDB contains a retry logic that tries to resolve the conflict automatically.\n\nDeleting Data\n\nDeleting rows in CrateDB is done using the SQL DELETE statement:\n\ncr> delete from locations where position > 3;\nDELETE OK, ... rows affected (... sec)\n\nImport and Export\nImporting Data\n\nUsing the COPY FROM SQL statement, data can be imported into CrateDB.\n\nThe supported data formats are JSON and CSV. The format is inferred from the file extension, if possible. Alternatively the format can also be provided as an option (see WITH). If the format is not provided and cannot be inferred from the file extension, it will be processed as JSON.\n\nJSON files must contain a single JSON object per line.\n\nExample JSON data:\n\n{\"id\": 1, \"quote\": \"Don't panic\"}\n{\"id\": 2, \"quote\": \"Ford, you're turning into a penguin. Stop it.\"}\n\n\nCSV files must contain a header with comma-separated values, which will be added as columns.\n\nExample CSV data:\n\nid,quote\n1,\"Don't panic\"\n2,\"Ford, you're turning into a penguin. Stop it.\"\n\n\nNote\n\nThe COPY FROM statement will not convert or validate your data. Please make sure that it fits your schema.\n\nValues for generated columns will be computed if the data does not contain them, otherwise they will be imported but not validated, so please make sure that they are correct.\n\nFurthermore, column names in your data are considered case sensitive (as if they were quoted in a SQL statement).\n\nFor further information, including how to import data to Partitioned Tables, take a look at the COPY FROM reference.\n\nImport From a File URI\n\nAn example import from a file URI:\n\ncr> COPY quotes FROM 'file:///tmp/import_data/quotes.json';\nCOPY OK, 3 rows affected (... sec)\n\n\nNote\n\nIf you are using Microsoft Windows, you must include the drive letter in the file URI. Consult the Windows documentation for more information.\n\nIf all files inside a directory should be imported a * wildcard has to be used:\n\ncr> COPY quotes FROM '/tmp/import_data/*' WITH (bulk_size = 4);\nCOPY OK, 3 rows affected (... sec)\n\n\nThis wildcard can also be used to only match certain files:\n\ncr> COPY quotes FROM '/tmp/import_data/qu*.json';\nCOPY OK, 3 rows affected (... sec)\n\nImport With Detailed Error Reporting\n\nIf the RETURN_SUMMARY clause is specified, a result set containing information about failures and successfully imported records is returned.\n\ncr> COPY locations FROM '/tmp/import_data/locations_with_failure/locations*.json' RETURN SUMMARY;\n +--...--+----------...--------+---------------+-------------+-------------------...--------------------------------------+\n | node  | uri                 | success_count | error_count | errors                                                     |\n +--...--+----------...--------+---------------+-------------+-------------------...--------------------------------------+\n | {...} | .../locations1.json |             6 |           0 | {}                                                         |\n | {...} | .../locations2.json |             5 |           2 | {\"failed to parse ...{\"count\": 2, \"line_numbers\": [1, 2]}} |\n +--...--+----------...--------+---------------+-------------+-------------------...--------------------------------------+\n COPY 2 rows in set (... sec)\n\n\nIf an error happens while processing the URI in general, the error_count and success_count columns will contains NULL values to indicate that no records were processed.\n\ncr> COPY locations FROM '/tmp/import_data/not-existing.json' RETURN SUMMARY;\n +--...--+-----------...---------+---------------+-------------+------------------------...------------------------+\n | node  | uri                   | success_count | error_count | errors                                            |\n +--...--+-----------...---------+---------------+-------------+------------------------...------------------------+\n | {...} | .../not-existing.json |          NULL |        NULL | {\"...not-existing.json (...)\": {\"count\": 1, ...}} |\n +--...--+-----------...---------+---------------+-------------+------------------------...------------------------+\nCOPY 1 row in set (... sec)\n\n\nSee COPY FROM for more information.\n\nExporting Data\n\nData can be exported using the COPY TO statement. Data is exported in a distributed way, meaning each node will export its own data.\n\nReplicated data is not exported. So every row of an exported table is stored only once.\n\nThis example shows how to export a given table into files named after the table and shard ID with gzip compression:\n\ncr> refresh table quotes;\nREFRESH OK...\n\ncr> copy quotes to DIRECTORY '/tmp/' with (compression='gzip');\nCOPY OK, 3 rows affected ...\n\n\nInstead of exporting a whole table, rows can be filtered by an optional WHERE clause condition. This is useful if only a subset of the data needs to be exported:\n\ncr> copy quotes where match(quote_ft, 'time') to DIRECTORY '/tmp/' with (compression='gzip');\nCOPY OK, 2 rows affected ...\n\n\nFor further details see COPY TO."
  },
  {
    "title": "Logging — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/config/logging.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n3.3\nLogging\n\nCrateDB supports two kinds of logging:\n\nApplication logging with Log4j\n\nJava Virtual Machine (JVM) garbage collection logging\n\nWe use “application” here to distinguish between CrateDB running as a Java application and the JVM itself, which runs CrateDB.\n\nBecause garbage collection logging is a native feature of the JVM it behaves differently and is configured differently.\n\nTable of Contents\n\nApplication Logging\n\nLog4j\n\nConfiguration File\n\nLog Levels\n\nRun-Time Configuration\n\nJVM Logging\n\nGarbage Collection\n\nEnvironment Variables\n\nApplication Logging\nLog4j\n\nCrateDB uses Log4j.\n\nConfiguration File\n\nYou can configure Log4j with the log4j2.properties file in the CrateDB configuration directory.\n\nThe log4j2.properties file is formated using YAML and simplifies Log4j configuration by allowing you to use the PropertyConfigurator but without having to tediously repeat the log4j prefix.\n\nHere’s one example:\n\nrootLogger.level = info\nrootLogger.appenderRef.console.ref = console\n\n# log query execution errors for easier debugging\nlogger.action.name = org.crate.action.sql\nlogger.action.level = debug\n\nappender.console.type = Console\nappender.console.name = console\nappender.console.layout.type = PatternLayout\nappender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c{1.}] %marker%m%n\n\n\nAnd here is a snippet of the generated properties ready for use with log4j. You get the point.\n\nSee Also\n\nConsult the PropertyConfigurator documentation or the configuration section of the Log4j documentation for more information.\n\nLog Levels\n\nPossible log levels are the same as for Log4j, in order of increasing importance:\n\nTRACE\n\nDEBUG\n\nINFO\n\nWARN\n\nERROR\n\nLog levels must be provided as string literals in the SET statement.\n\nNote\n\nBe careful using the TRACE log level because it’s extremely verbose, can obscure other important log messages and even fill up entire data disks in some cases.\n\nRun-Time Configuration\n\nIt’s possible to set the log level of loggers at runtime using SET, like so:\n\nSET GLOBAL TRANSIENT \"logger.action\" = 'INFO';\n\n\nIn this example, the log level INFO is applied to the action logger.\n\nIn addition to being able to configure any of the standard loggers, you can configure the root (i.e. default) logger using logger._root.\n\nAs with any setting, you can inspect the current configuration by querying the sys.cluster table.\n\nTip\n\nRun-time logging configuration is particularly useful if you are debugging a problem and you want to increase the log level without restarting nodes.\n\nRun-time logging configuration is applied across the whole cluster, and overrides the start-up configuration defined in each respective log4j2.properties file.\n\nCaution\n\nThe RESET statement is supported but logging configuration is only reset when the whole cluster is restarted.\n\nJVM Logging\n\nCrateDB exposes some native JVM logging functionality.\n\nGarbage Collection\n\nCrateDB logs JVM garbage collection times using the built-in garbage collection logging of the JVM.\n\nEnvironment Variables\n\nThe following environment variables can be used to configure garbage collection logging.\n\nCRATE_DISABLE_GC_LOGGING: boolean integer (default: 0)\n\nWhether to disable garbage collection logging.\n\nSet to 1 to disable.\n\nNote\n\nSince since CrateDB 3.0, Garbage collection logging is enabled by default.\n\nCRATE_GC_LOG_DIR: path to logs directory (default: varies)\n\nThe log file directory.\n\nFor basic installations, the logs directory in the CRATE_HOME directory is default.\n\nIf you have installed a CrateDB Linux package, the default directory is /var/log/crate instead.\n\nCRATE_GC_LOG_SIZE: file size (default: 64m)\n\nMaximum file size of log files before they are rotated.\n\nCRATE_GC_LOG_FILES: number (default: 16)\n\nThe amount of files kept in rotation.\n\nCaution\n\nWith the default configuration of 16 rotated 64 megabyte log files, garbage collection logs will grow to occupy one gigabyte on disk.\n\nMake sure you have enough available disk space for configuration."
  },
  {
    "title": "Session Settings — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/config/session.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n3.3\nSession Settings\n\nThis section documents settings that only apply to the currently connected client session. Currently, there is only one of these settings.\n\nSee Also\n\nSettings can be changed during a session with SET and RESET.\n\nsearch_path: schema names (default: doc)\n\nThe list of schemas that will be used to look for a relation that is referenced without a schema.\n\nCrateDB will try to resolve an unqualified relation name against the configured search path by iterating over the configured schemas in the order they were declared in. The first matching relation in the search path is used, or an error is reported if there is no match.\n\nIn order to configure the search path we use\n\nSET search_path TO myschema, doc;\n\n\nThe current search path value could be retrieved using SHOW (session settings):\n\nSHOW search_path;\n\n\nThis setting mirrors the PostgreSQL search_path setting.\n\nSome clients, which generally connect to CrateDB using the PostgreSQL wire protocol, require access to various tables in the pg_catalog schema, usually to extract information about built-in data types or functions. CrateDB implements the system pg_catalog schema and it implicitly includes it in the search_path before the configured schemas, unless it is explicitly declared in the search_path on any position."
  },
  {
    "title": "Cluster Wide Settings — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/config/cluster.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n3.3\nCluster Wide Settings\n\nAll current applied cluster settings can be read by querying the sys.cluster.settings column. Most cluster settings can be changed at runtime. This is documented at each setting.\n\nTable of Contents\n\nNon-Runtime Cluster Wide Settings\n\nCollecting Stats\n\nUsage Data Collector\n\nGraceful Stop\n\nBulk Operations\n\nDiscovery\n\nUnicast Host Discovery\n\nDiscovery via DNS\n\nDiscovery on Amazon EC2\n\nDiscovery on Microsoft Azure\n\nRouting Allocation\n\nAwareness\n\nBalanced Shards\n\nCluster-Wide Allocation Filtering\n\nDisk-based Shard Allocation\n\nRecovery\n\nQuery Circuit Breaker\n\nField Data Circuit Breaker\n\nRequest Circuit Breaker\n\nAccounting Circuit Breaker\n\nStats Circuit Breakers\n\nThread Pools\n\nSettings for fixed thread pools\n\nMetadata\n\nMetadata Gateway\n\nCredentials for S3 Repositories\n\nNon-Runtime Cluster Wide Settings\n\nCluster wide settings which cannot be changed at runtime need to be specified in the configuration of each node in the cluster.\n\nCaution\n\nCluster settings specified via node configurations are required to be exactly the same on every node in the cluster for proper operation of the cluster.\n\nCollecting Stats\nstats.enabled\nDefault: true\nRuntime: yes\n\nA boolean indicating whether or not to collect statistical information about the cluster.\n\nCaution\n\nThe collection of statistical information incurs a slight performance penalty, as details about every job and operation across the cluster will cause data to be inserted into the corresponding system tables.\n\nstats.jobs_log_size\nDefault: 10000\nRuntime: yes\n\nThe maximum number of job records kept to be kept in the sys.jobs_log table on each node.\n\nA job record corresponds to a single SQL statement to be executed on the cluster. These records are used for performance analytics. A larger job log produces more comprehensive stats, but uses more RAM.\n\nOlder job records are deleted as newer records are added, once the limit is reached.\n\nSetting this value to 0 disables collecting job information.\n\nstats.jobs_log_expiration\nDefault: 0s (disabled)\nRuntime: yes\n\nThe job record expiry time in seconds.\n\nJob records in the sys.jobs_log table are periodically cleared if they are older than the expiry time. This setting overrides stats.jobs_log_size.\n\nIf the value is set to 0, time based log entry eviction is disabled.\n\nNote\n\nIf both the stats.operations_log_size and stats.operations_log_expiration settings are disabled, jobs will not be recorded.\n\nstats.jobs_log_filter\nDefault: true (Include everything)\nRuntime: yes\n\nAn expression to determine if a job should be recorded into sys.jobs_log. The expression must evaluate to a boolean. If it evaluates to true the statement will show up sys.jobs_log until it’s evicted due to one of the other rules. (expiration or size limit reached).\n\nThe expression may reference all columns contained in sys.jobs_log. A common use case is to include only jobs that took a certain amount of time to execute:\n\ncr> SET GLOBAL \"stats.jobs_log_filter\" = 'ended - started > 100';\n\nstats.jobs_log_persistent_filter\nDefault: false (Include nothing)\nRuntime: yes\n\nAn expression to determine if a job should also be recorded to the regular CrateDB log. Entries that match this filter will be logged under the StatementLog logger with the INFO level.\n\nThis is similar to stats.jobs_log_filter except that these entries are persisted to the log file. This should be used with caution and shouldn’t be set to an expression that matches many queries as the logging operation will block on IO and can therefore affect performance.\n\nA common use case is to use this for slow query logging.\n\nstats.operations_log_size\nDefault: 10000\nRuntime: yes\n\nThe maximum number of operations records to be kept in the sys.operations_log table on each node.\n\nA job consists of one or more individual operations. Operations records are used for performance analytics. A larger operations log produces more comprehensive stats, but uses more RAM.\n\nOlder operations records are deleted as newer records are added, once the limit is reached.\n\nSetting this value to 0 disables collecting operations information.\n\nstats.operations_log_expiration\nDefault: 0s (disabled)\nRuntime: yes\n\nEntries of sys.operations_log are cleared by a periodically job when they are older than the specified expire time. This setting overrides stats.operations_log_size. If the value is set to 0 the time based log entry eviction is disabled.\n\nNote\n\nIf both setttings stats.operations_log_size and stats.operations_log_expiration are disabled, no job information will be collected.\n\nstats.service.interval\nDefault: 1h\nRuntime: yes\n\nDefines the refresh interval to refresh tables statistics used to produce optimal query execution plans.\n\nThis field expects a time value either as a long or double or alternatively as a string literal with a time suffix (ms, s, m, h, d, w).\n\nIf the value provided is 0 then the refresh is disabled.\n\nCaution\n\nUsing a very small value can cause a high load on the cluster.\n\nUsage Data Collector\n\nThe settings of the Usage Data Collector are read-only and cannot be set during runtime. Please refer to Usage Data Collector to get further information about its usage.\n\nudc.enabled\nDefault: true\nRuntime: no\n\ntrue: Enables the Usage Data Collector.\n\nfalse: Disables the Usage Data Collector.\n\nudc.initial_delay\nDefault: 10m\nRuntime: no\n\nThe delay for first ping after start-up.\n\nThis field expects a time value either as a long or double or alternatively as a string literal with a time suffix (ms, s, m, h, d, w).\n\nudc.interval\nDefault: 24h\nRuntime: no\n\nThe interval a UDC ping is sent.\n\nThis field expects a time value either as a long or double or alternatively as a string literal with a time suffix (ms, s, m, h, d, w).\n\nudc.url\nDefault: https://udc.crate.io\nRuntime: no\n\nThe URL the ping is sent to.\n\nGraceful Stop\n\nBy default, when the CrateDB process stops it simply shuts down, possibly making some shards unavailable which leads to a red cluster state and lets some queries fail that required the now unavailable shards. In order to safely shutdown a CrateDB node, the graceful stop procedure can be used.\n\nThe following cluster settings can be used to change the shutdown behaviour of nodes of the cluster:\n\ncluster.graceful_stop.min_availability\nDefault: primaries\nRuntime: yes\nAllowed Values: none | primaries | full\n\nnone: No minimum data availability is required. The node may shut down even if records are missing after shutdown.\n\nprimaries: At least all primary shards need to be available after the node has shut down. Replicas may be missing.\n\nfull: All records and all replicas need to be available after the node has shut down. Data availability is full.\n\nNote\n\nThis option is ignored if there is only 1 node in a cluster!\n\ncluster.graceful_stop.reallocate\nDefault: true\nRuntime: yes\n\ntrue: The graceful stop command allows shards to be reallocated before shutting down the node in order to ensure minimum data availability set with min_availability.\n\nfalse: This has no effect, meaning that the behaviour is the same as setting this to true. For this reason, this setting is deprecated and will be removed in a future release.\n\nWarning\n\nMake sure you have enough nodes and enough disk space for the reallocation.\n\ncluster.graceful_stop.timeout\nDefault: 2h\nRuntime: yes\n\nDefines the maximum waiting time in milliseconds for the reallocation process to finish. The force setting will define the behaviour when the shutdown process runs into this timeout.\n\nThe timeout expects a time value either as a long or double or alternatively as a string literal with a time suffix (ms, s, m, h, d, w).\n\ncluster.graceful_stop.force\nDefault: false\nRuntime: yes\n\nDefines whether graceful stop should force stopping of the node if it runs into the timeout which is specified with the cluster.graceful_stop.timeout setting.\n\nBulk Operations\n\nSQL DML Statements involving a huge amount of rows like COPY FROM, INSERT or UPDATE can take an enormous amount of time and resources. The following settings change the behaviour of those queries.\n\nbulk.request_timeout\nDefault: 1m\nRuntime: yes\n\nDefines the timeout of internal shard-based requests involved in the execution of SQL DML Statements over a huge amount of rows.\n\nDiscovery\ndiscovery.zen.minimum_master_nodes\nDefault: 1\nRuntime: yes\n\nSet to ensure a node sees N other master eligible nodes to be considered operational within the cluster. It’s recommended to set it to a higher value than 1 when running more than 2 nodes in the cluster.\n\ndiscovery.zen.ping_interval\nDefault: 1s\nRuntime: yes\n\nHow often to ping other nodes.\n\nNodes must remain responsive to pings or they will be marked as failed and removed from the cluster.\n\ndiscovery.zen.ping_timeout\nDefault: 3s\nRuntime: yes\n\nThe time to wait for ping responses from other nodes when discovering. Set this option to a higher value on a slow or congested network to minimize discovery failures.\n\ndiscovery.zen.ping_retries\nDefault: 3\nRuntime: yes\n\nHow many ping failures (network timeouts) indicate that a node has failed.\n\ndiscovery.zen.publish_timeout\nDefault: 30s\nRuntime: yes\n\nTime a node is waiting for responses from other nodes to a published cluster state.\n\nNote\n\nMulticast used to be an option for node discovery, but was deprecated in CrateDB 1.0.3 and removed in CrateDB 1.1.\n\nUnicast Host Discovery\n\nCrateDB has built-in support for several different mechanisms of node discovery. The simplest mechanism is to specify a list of hosts in the configuration file.\n\ndiscovery.zen.ping.unicast.hosts\nDefault: not set\nRuntime: no\n\nCurrently there are three other discovery types: via DNS, via EC2 API and via Microsoft Azure mechanisms.\n\nWhen a node starts up with one of these discovery types enabled, it performs a lookup using the settings for the specified mechanism listed below. The hosts and ports retrieved from the mechanism will be used to generate a list of unicast hosts for node discovery.\n\nThe same lookup is also performed by all nodes in a cluster whenever the master is re-elected (see Cluster Meta Data).\n\ndiscovery.zen.hosts_provider\nDefault: not set\nRuntime: no\nAllowed Values: srv, ec2, azure\n\nSee also: Discovery.\n\nDiscovery via DNS\n\nCrate has built-in support for discovery via DNS. To enable DNS discovery the discovery.zen.hosts_provider setting needs to be set to srv.\n\nThe order of the unicast hosts is defined by the priority, weight and name of each host defined in the SRV record. For example:\n\n_crate._srv.example.com. 3600 IN SRV 2 20 4300 crate1.example.com.\n_crate._srv.example.com. 3600 IN SRV 1 10 4300 crate2.example.com.\n_crate._srv.example.com. 3600 IN SRV 2 10 4300 crate3.example.com.\n\n\nwould result in a list of discovery nodes ordered like:\n\ncrate2.example.com:4300, crate3.example.com:4300, crate1.example.com:4300\n\ndiscovery.srv.query\nRuntime: no\n\nThe DNS query that is used to look up SRV records, usually in the format _service._protocol.fqdn If not set, the service discovery will not be able to look up any SRV records.\n\ndiscovery.srv.resolver\nRuntime: no\n\nThe hostname or IP of the DNS server used to resolve DNS records. If this is not set, or the specified hostname/IP is not resolvable, the default (system) resolver is used.\n\nOptionally a custom port can be specified using the format hostname:port.\n\nDiscovery on Amazon EC2\n\nCrateDB has built-in support for discovery via the EC2 API. To enable EC2 discovery the discovery.zen.hosts_provider settings needs to be set to ec2.\n\ndiscovery.ec2.access_key\nRuntime: no\n\nThe access key ID to identify the API calls.\n\ndiscovery.ec2.secret_key\nRuntime: no\n\nThe secret key to identify the API calls.\n\nFollowing settings control the discovery:\n\ndiscovery.ec2.groups\nRuntime: no\n\nA list of security groups; either by ID or name. Only instances with the given group will be used for unicast host discovery.\n\ndiscovery.ec2.any_group\nRuntime: no\nDefault: true\n\nDefines whether all (false) or just any (true) security group must be present for the instance to be used for discovery.\n\ndiscovery.ec2.host_type\nRuntime: no\nDefault: private_ip\nAllowed Values: private_ip, public_ip, private_dns, public_dns\n\nDefines via which host type to communicate with other instances.\n\ndiscovery.ec2.availability_zones\nRuntime: no\n\nA list of availability zones. Only instances within the given availability zone will be used for unicast host discovery.\n\ndiscovery.ec2.tag.<name>\nRuntime: no\n\nEC2 instances for discovery can also be filtered by tags using the discovery.ec2.tag. prefix plus the tag name.\n\nE.g. to filter instances that have the environment tags with the value dev your setting will look like: discovery.ec2.tag.environment: dev.\n\ndiscovery.ec2.endpoint\nRuntime: no\n\nIf you have your own compatible implementation of the EC2 API service you can set the endpoint that should be used.\n\nDiscovery on Microsoft Azure\n\nCrateDB has built-in support for discovery via the Azure Virtual Machine API. To enable Azure discovery set the discovery.zen.hosts_provider setting to azure.\n\ncloud.azure.management.resourcegroup.name\nRuntime: no\n\nThe name of the resource group the CrateDB cluster is running on.\n\nAll nodes need to be started within the same resource group.\n\ncloud.azure.management.subscription.id\nRuntime: no\n\nThe subscription ID of your Azure account.\n\nYou can find the ID on the Azure Portal.\n\ncloud.azure.management.tenant.id\nRuntime: no\n\nThe tenant ID of the Active Directory application.\n\ncloud.azure.management.app.id\nRuntime: no\n\nThe application ID of the Active Directory application.\n\ncloud.azure.management.app.secret\nRuntime: no\n\nThe password of the Active Directory application.\n\ndiscovery.azure.method\nRuntime: no\nDefault: vnet\nAllowed Values: vnet | subnet\n\nDefines the scope of the discovery. vnet will discover all VMs within the same virtual network (default), subnet will discover all VMs within the same subnet of the CrateDB instance.\n\nRouting Allocation\ncluster.routing.allocation.enable\nDefault: all\nRuntime: yes\nAllowed Values: all | none | primaries | new_primaries\n\nall allows all shard allocations, the cluster can allocate all kinds of shards.\n\nnone allows no shard allocations at all. No shard will be moved or created.\n\nprimaries only primaries can be moved or created. This includes existing primary shards.\n\nnew_primaries allows allocations for new primary shards only. This means that for example a newly added node will not allocate any replicas. However it is still possible to allocate new primary shards for new indices. Whenever you want to perform a zero downtime upgrade of your cluster you need to set this value before gracefully stopping the first node and reset it to all after starting the last updated node.\n\nNote\n\nThis allocation setting has no effect on recovery of primary shards! Even when cluster.routing.allocation.enable is set to none, nodes will recover their unassigned local primary shards immediatelly after restart.\n\ncluster.routing.rebalance.enable\nDefault: all\nRuntime: yes\nAllowed Values: all | none | primaries | replicas\n\nEnables/Disables rebalancing for different types of shards.\n\nall allows shard rebalancing for all types of shards.\n\nnone disables shard rebalancing for any types.\n\nprimaries allows shard rebalancing only for primary shards.\n\nreplicas allows shard rebalancing only for replica shards.\n\ncluster.routing.allocation.allow_rebalance\nDefault: indices_all_active\nRuntime: yes\nAllowed Values: always | indices_primary_active | indices_all_active\n\nAllow to control when rebalancing will happen based on the total state of all the indices shards in the cluster. Defaulting to indices_all_active to reduce chatter during initial recovery.\n\ncluster.routing.allocation.cluster_concurrent_rebalance\nDefault: 2\nRuntime: yes\n\nDefine how many concurrent rebalancing tasks are allowed cluster wide.\n\ncluster.routing.allocation.node_initial_primaries_recoveries\nDefault: 4\nRuntime: yes\n\nDefine the number of initial recoveries of primaries that are allowed per node. Since most times local gateway is used, those should be fast and we can handle more of those per node without creating load.\n\ncluster.routing.allocation.node_concurrent_recoveries\nDefault: 2\nRuntime: yes\n\nHow many concurrent recoveries are allowed to happen on a node.\n\nAwareness\n\nCluster allocation awareness allows to configure shard and replicas allocation across generic attributes associated with nodes.\n\ncluster.routing.allocation.awareness.attributes\nRuntime: no\n\nDefine node attributes which will be used to do awareness based on the allocation of a shard and its replicas. For example, let’s say we have defined an attribute rack_id and we start 2 nodes with node.attr.rack_id set to rack_one, and deploy a single table with 5 shards and 1 replica. The table will be fully deployed on the current nodes (5 shards and 1 replica each, total of 10 shards).\n\nNow, if we start two more nodes, with node.attr.rack_id set to rack_two, shards will relocate to even the number of shards across the nodes, but a shard and its replica will not be allocated in the same rack_id value.\n\nThe awareness attributes can hold several values\n\ncluster.routing.allocation.awareness.force.*.values\nRuntime: no\n\nAttributes on which shard allocation will be forced. * is a placeholder for the awareness attribute, which can be defined using the cluster.routing.allocation.awareness.attributes setting. Let’s say we configured an awareness attribute zone and the values zone1, zone2 here, start 2 nodes with node.attr.zone set to zone1 and create a table with 5 shards and 1 replica. The table will be created, but only 5 shards will be allocated (with no replicas). Only when we start more shards with node.attr.zone set to zone2 the replicas will be allocated.\n\nBalanced Shards\n\nAll these values are relative to one another. The first three are used to compose a three separate weighting functions into one. The cluster is balanced when no allowed action can bring the weights of each node closer together by more then the fourth setting. Actions might not be allowed, for instance, due to forced awareness or allocation filtering.\n\ncluster.routing.allocation.balance.shard\nDefault: 0.45f\nRuntime: yes\n\nDefines the weight factor for shards allocated on a node (float). Raising this raises the tendency to equalize the number of shards across all nodes in the cluster.\n\ncluster.routing.allocation.balance.index\nDefault: 0.55f\nRuntime: yes\n\nDefines a factor to the number of shards per index allocated on a specific node (float). Increasing this value raises the tendency to equalize the number of shards per index across all nodes in the cluster.\n\ncluster.routing.allocation.balance.threshold\nDefault: 1.0f\nRuntime: yes\n\nMinimal optimization value of operations that should be performed (non negative float). Increasing this value will cause the cluster to be less aggressive about optimising the shard balance.\n\nCluster-Wide Allocation Filtering\n\nAllow to control the allocation of all shards based on include/exclude filters.\n\nE.g. this could be used to allocate all the new shards on the nodes with specific IP addresses or custom attributes.\n\ncluster.routing.allocation.include.*\nRuntime: no\n\nPlace new shards only on nodes where one of the specified values matches the attribute. e.g.: cluster.routing.allocation.include.zone: “zone1,zone2”\n\ncluster.routing.allocation.exclude.*\nRuntime: no\n\nPlace new shards only on nodes where none of the specified values matches the attribute. e.g.: cluster.routing.allocation.exclude.zone: “zone1”\n\ncluster.routing.allocation.require.*\nRuntime: no\n\nUsed to specify a number of rules, which all MUST match for a node in order to allocate a shard on it. This is in contrast to include which will include a node if ANY rule matches.\n\nDisk-based Shard Allocation\ncluster.routing.allocation.disk.threshold_enabled\nDefault: true\nRuntime: yes\n\nPrevent shard allocation on nodes depending of the disk usage.\n\ncluster.routing.allocation.disk.watermark.low\nDefault: 85%\nRuntime: yes\n\nDefines the lower disk threshold limit for shard allocations. New shards will not be allocated on nodes with disk usage greater than this value. It can also be set to an absolute bytes value (like e.g. 500mb) to prevent the cluster from allocating new shards on node with less free disk space than this value.\n\ncluster.routing.allocation.disk.watermark.high\nDefault: 90%\nRuntime: yes\n\nDefines the higher disk threshold limit for shard allocations. The cluster will attempt to relocate existing shards to another node if the disk usage on a node rises above this value. It can also be set to an absolute bytes value (like e.g. 500mb) to relocate shards from nodes with less free disk space than this value.\n\ncluster.routing.allocation.disk.watermark.flood_stage\nDefault: 95%\nRuntime: yes\n\nDefines the threshold on which CrateDB enforces a read-only block on every index that has at least one shard allocated on a node with at least one disk exceeding the flood stage. Note, that the read-only blocks are not automatically removed from the indices if the disk space is freed and the threshold is undershot. To remove the block, execute ALTER TABLE ... SET (\"blocks.read_only_allow_delete\" = FALSE) for affected tables (see blocks.read_only_allow_delete).\n\ncluster.routing.allocation.disk.watermark settings may be defined as percentages or bytes values. However, it is not possible to mix the value types.\n\nBy default, the cluster will retrieve information about the disk usage of the nodes every 30 seconds. This can also be changed by setting the cluster.info.update.interval setting.\n\nNote\n\nThe watermark settings are also used for the Routing Allocation Disk Watermark Low and Routing Allocation Disk Watermark High node check. Setting cluster.routing.allocation.disk.threshold_enabled to false will disable the allocation decider, but the node checks will still be active and warn users about running low on disk space.\n\nRecovery\nindices.recovery.max_bytes_per_sec\nDefault: 40mb\nRuntime: yes\n\nSpecifies the maximum number of bytes that can be transferred during shard recovery per seconds. Limiting can be disabled by setting it to 0. This setting allows to control the network usage of the recovery process. Higher values may result in higher network utilization, but also faster recovery process.\n\nindices.recovery.retry_delay_state_sync\nDefault: 500ms\nRuntime: yes\n\nDefines the time to wait after an issue caused by cluster state syncing before retrying to recover.\n\nindices.recovery.retry_delay_network\nDefault: 5s\nRuntime: yes\n\nDefines the time to wait after an issue caused by the network before retrying to recover.\n\nindices.recovery.internal_action_timeout\nDefault: 15m\nRuntime: yes\n\nDefines the timeout for internal requests made as part of the recovery.\n\nindices.recovery.internal_action_long_timeout\nDefault: 30m\nRuntime: yes\n\nDefines the timeout for internal requests made as part of the recovery that are expected to take a long time. Defaults to twice internal_action_timeout.\n\nindices.recovery.recovery_activity_timeout\nDefault: 30m\nRuntime: yes\n\nRecoveries that don’t show any activity for more then this interval will fail. Defaults to internal_action_long_timeout.\n\nQuery Circuit Breaker\n\nThe Query circuit breaker will keep track of the used memory during the execution of a query. If a query consumes too much memory or if the cluster is already near its memory limit it will terminate the query to ensure the cluster keeps working.\n\nindices.breaker.query.limit\nDefault: 60%\nRuntime: yes\n\nSpecifies the limit for the query breaker. Provided values can either be absolute values (interpreted as a number of bytes), byte sizes (eg. 1mb) or percentage of the heap size (eg. 12%). A value of -1 disables breaking the circuit while still accounting memory usage.\n\nindices.breaker.query.overhead\nDefault: 1.09\nRuntime: no\n\nA constant that all data estimations are multiplied with to determine a final estimation.\n\nField Data Circuit Breaker\n\nThe field data circuit breaker allows estimation of needed heap memory required for loading field data into memory. If a certain limit is reached an exception is raised.\n\nindices.breaker.fielddata.limit\nDefault: 60%\nRuntime: yes\n\nSpecifies the JVM heap limit for the fielddata breaker.\n\nindices.breaker.fielddata.overhead\nDefault: 1.03\nRuntime: yes\n\nA constant that all field data estimations are multiplied with to determine a final estimation.\n\nRequest Circuit Breaker\n\nThe request circuit breaker allows an estimation of required heap memory per request. If a single request exceeds the specified amount of memory, an exception is raised.\n\nindices.breaker.request.limit\nDefault: 60%\nRuntime: yes\n\nSpecifies the JVM heap limit for the request circuit breaker.\n\nindices.breaker.request.overhead\nDefault: 1.0\nRuntime: yes\n\nA constant that all request estimations are multiplied with to determine a final estimation.\n\nAccounting Circuit Breaker\n\nTracks things that are held in memory independent of queries. For example the memory used by Lucene for segments.\n\nindices.breaker.accounting.limit\nDefault: 100%\nRuntime: yes\n\nSpecifies the JVM heap limit for the accounting circuit breaker\n\nindices.breaker.accounting.overhead\nDefault: 1.0\nRuntime: yes\n\nA constant that all accounting estimations are multiplied with to determine a final estimation.\n\nStats Circuit Breakers\n\nSettings that control the behaviour of the stats circuit breaker. There are two breakers in place, one for the jobs log and one for the operations log. For each of them, the breaker limit can be set.\n\nstats.breaker.log.jobs.limit\nDefault: 5%\nRuntime: yes\n\nThe maximum memory that can be used from CRATE_HEAP_SIZE for the sys.jobs_log table on each node.\n\nWhen this memory limit is reached, the job log circuit breaker logs an error message and clears the sys.jobs_log table completely.\n\nstats.breaker.log.operations.limit\nDefault: 5%\nRuntime: yes\n\nThe maximum memory that can be used from CRATE_HEAP_SIZE for the sys.operations_log table on each node.\n\nWhen this memory limit is reached, the operations log circuit breaker logs an error message and clears the sys.operations_log table completely.\n\nThread Pools\n\nEvery node holds several thread pools to improve how threads are managed within a node. There are several pools, but the important ones include:\n\nindex: For index/delete operations, defaults to fixed\n\nsearch: For count/search operations, defaults to fixed\n\nget: For queries that are optimized to do a direct lookup by primary key, defaults to fixed\n\nbulk: For bulk operations, defaults to fixed\n\nrefresh: For refresh operations, defaults to cache\n\nthread_pool.<name>.type\nRuntime: no\nAllowed Values: fixed | scaling\n\nfixed holds a fixed size of threads to handle the requests. It also has a queue for pending requests if no threads are available.\n\nscaling ensures that a thread pool holds a dynamic number of threads that are proportional to the workload.\n\nSettings for fixed thread pools\n\nIf the type of a thread pool is set to fixed there are a few optional settings.\n\nthread_pool.<name>.size\nRuntime: no\n\nNumber of threads. The default size of the different thread pools depend on the number of available CPU cores.\n\nthread_pool.<name>.queue_size\nDefault index: 200\nDefault search: 1000\nDefault get: 1000\nDefault bulk: 50\nRuntime: no\n\nSize of the queue for pending requests. A value of -1 sets it to unbounded.\n\nMetadata\ncluster.info.update.interval\nDefault: 30s\nRuntime: yes\n\nDefines how often the cluster collect metadata information (e.g. disk usages etc.) if no concrete event is triggered.\n\nMetadata Gateway\n\nThe gateway persists cluster meta data on disk every time the meta data changes. This data is stored persistently across full cluster restarts and recovered after nodes are started again.\n\ngateway.expected_nodes\nDefault: -1\nRuntime: no\n\nThe setting gateway.expected_nodes defines the number of nodes that should be waited for until the cluster state is recovered immediately. The value of the setting should be equal to the number of nodes in the cluster, because you only want the cluster state to be recovered after all nodes are started.\n\ngateway.recover_after_time\nDefault: 0ms\nRuntime: no\n\nThe gateway.recover_after_time setting defines the time to wait before starting starting the recovery once the number of nodes defined in gateway.recover_after_nodes are started. The setting is relevant if gateway.recover_after_nodes is less than gateway.expected_nodes.\n\ngateway.recover_after_nodes\nDefault: -1\nRuntime: no\n\nThe gateway.recover_after_nodes setting defines the number of nodes that need to be started before the cluster state recovery will start. Ideally the value of the setting should be equal to the number of nodes in the cluster, because you only want the cluster state to be recovered once all nodes are started. However, the value must be bigger than the half of the expected number of nodes in the cluster.\n\nCredentials for S3 Repositories\n\nCrateDB has built-in support for configuring S3 buckets as repositories for snapshots. If no credentials are provided as parameters to the SQL statement the following default credentials will be used:\n\ns3.client.default.access_key\nRuntime: no\n\nThe access key ID to identify the API calls.\n\ns3.client.default.secret_key\nRuntime: no\n\nThe secret key to identify the API calls.\n\nTip\n\nConfiguring the settings above in the crate.yml file, is an easy way to prevent credentials from being exposed.\n\nIf a repository is created with the credentials passed as parameters to the SQL statement, then those credentials will be visible as plain text to anyone querying the sys.repositories table."
  },
  {
    "title": "Node Specific Settings — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/config/node.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n3.3\nNode Specific Settings\n\nTable of Contents\n\nBasics\n\nNode Types\n\nRead-only node\n\nHosts\n\nPorts\n\nPaths\n\nPlugins\n\nCPU\n\nMemory\n\nGarbage Collection\n\nAuthentication\n\nTrust Authentication\n\nHost Based Authentication\n\nHBA Entries\n\nSecured Communications (SSL/TLS)\n\nElasticsearch HTTP REST API\n\nCross-Origin Resource Sharing (CORS)\n\nBlobs\n\nRepositories\n\nQueries\n\nJavascript Language\n\nCustom Attributes\n\nIngestion Framework\n\nMQTT\n\nEnterprise License\n\nBasics\ncluster.name\nDefault: crate\nRuntime: no\n\nThe name of the CrateDB cluster the node should join to.\n\nnode.name\nRuntime: no\n\nThe name of the node. If no name is configured a random one will be generated.\n\nNote\n\nNode names must be unique in a CrateDB cluster.\n\nnode.max_local_storage_nodes\nDefault: 1\nRuntime: no\n\nDefines how many nodes are allowed to be started on the same machine using the same configured data path defined via path.data.\n\nNode Types\n\nCrateDB supports different kinds of nodes.\n\nThe following settings can be used to differentiate nodes upon startup:\n\nnode.master\nDefault: true\nRuntime: no\n\nWhether or not this node is able to get elected as master node in the cluster.\n\nnode.data\nDefault: true\nRuntime: no\n\nWhether or not this node will store data.\n\nUsing different combinations of these two settings, you can create four different types of node. Each type of node is differentiate by what types of load it will handle.\n\nThe four types of node possible are:\n\n\t\n\nMaster\n\n\t\n\nNo Master\n\n\n\n\nData\n\n\t\n\nCan handle all loads.\n\n\t\n\nHandles request handling and query execution loads.\n\n\n\n\nNo Data\n\n\t\n\nCan handle cluster management loads.\n\n\t\n\nHandles request handling loads.\n\nNodes marked as node.master will only handle cluster management loads if they are elected as the cluster master. All other loads are shared equally.\n\nRead-only node\nnode.sql.read_only\nDefault: false\nRuntime: no\n\nIf set to true, the node will only allow SQL statements which are resulting in read operations.\n\nHosts\nnetwork.host\nDefault: _local_\nRuntime: no\n\nThe IP address CrateDB will bind itself to. This setting sets both the network.bind_host and network.publish_host values.\n\nnetwork.bind_host\nDefault: _local_\nRuntime: no\n\nThis setting determines to which address CrateDB should bind itself to.\n\nnetwork.publish_host\nDefault: _local_\nRuntime: no\n\nThis setting is used by a CrateDB node to publish its own address to the rest of the cluster.\n\nTip\n\nApart from IPv4 and IPv6 addresses there are some special values that can be used for all above settings:\n\n_local_\n\n\t\n\nAny loopback addresses on the system, for example 127.0.0.1.\n\n\n\n\n_site_\n\n\t\n\nAny site-local addresses on the system, for example 192.168.0.1.\n\n\n\n\n_global_\n\n\t\n\nAny globally-scoped addresses on the system, for example 8.8.8.8.\n\n\n\n\n_[INTERFACE]_\n\n\t\n\nAddresses of a network interface, for example _en0_.\n\nPorts\nhttp.port\nRuntime: no\n\nThis defines the TCP port range to which the CrateDB HTTP service will be bound to. It defaults to 4200-4300. Always the first free port in this range is used. If this is set to an integer value it is considered as an explicit single port.\n\nThe HTTP protocol is used for the REST endpoint which is used by all clients except the Java client.\n\nhttp.publish_port\nRuntime: no\n\nThe port HTTP clients should use to communicate with the node. It is necessary to define this setting if the bound HTTP port (http.port) of the node is not directly reachable from outside, e.g. running it behind a firewall or inside a Docker container.\n\ntransport.tcp.port\nRuntime: no\n\nThis defines the TCP port range to which the CrateDB transport service will be bound to. It defaults to 4300-4400. Always the first free port in this range is used. If this is set to an integer value it is considered as an explicit single port.\n\nThe transport protocol is used for internal node-to-node communication.\n\ntransport.publish_port\nRuntime: no\n\nThe port that the node publishes to the cluster for its own discovery. It is necessary to define this setting when the bound tranport port (transport.tcp.port) of the node is not directly reachable from outside, e.g. running it behind a firewall or inside a Docker container.\n\npsql.port\nRuntime: no\n\nThis defines the TCP port range to which the CrateDB Postgres service will be bound to. It defaults to 5432-5532. Always the first free port in this range is used. If this is set to an integer value it is considered as an explicit single port.\n\nPaths\npath.conf\nRuntime: no\n\nFilesystem path to the directory containing the configuration files crate.yml and log4j2.properties.\n\npath.data\nRuntime: no\n\nFilesystem path to the directory where this CrateDB node stores its data (table data and cluster metadata).\n\nMultiple paths can be set by using a comma separated list and each of these paths will hold full shards (instead of striping data across them). In case CrateDB finds striped shards at the provided locations (from CrateDB <0.55.0), these shards will be migrated automatically on startup.\n\npath.logs\nRuntime: no\n\nFilesystem path to a directory where log files should be stored.\n\nCan be used as a variable inside log4j2.properties.\n\nFor example:\n\nappender:\n  file:\n    file: ${path.logs}/${cluster.name}.log\n\npath.repo\nRuntime: no\n\nA list of filesystem or UNC paths where repositories of type fs may be stored.\n\nWithout this setting a CrateDB user could write snapshot files to any directory that is writable by the CrateDB process. To safeguard against this security issue, the possible paths have to be whitelisted here.\n\nSee also location setting of repository type fs.\n\nPlugins\nplugin.mandatory\nRuntime: no\n\nA list of plugins that are required for a node to startup.\n\nIf any plugin listed here is missing, the CrateDB node will fail to start.\n\nCPU\nprocessors\nRuntime: no\n\nThe number of available processes is automatically guessed, and so most of the time you will not need to configure this explicitly.\n\nHowever, in some situations, such as when CrateDB is being run on top of Docker, the number of processors may be guessed incorrectly. If this happens, you can manually configure the number of processors using this setting.\n\nYou can also use this setting to manually constrain the number of CPUs made available to CrateDB. You might want to do this if you’re running CrateDB in a multitenant setup (i.e. more than one CrateDB node running on the same hardware).\n\nMemory\nbootstrap.memory_lock\nRuntime: no\nDefault: false\n\nCrateDB performs poorly when the JVM starts swapping: you should ensure that it never swaps. If set to true, CrateDB will use the mlockall system call on startup to ensure that the memory pages of the CrateDB process are locked into RAM.\n\nGarbage Collection\n\nCrateDB logs if JVM garbage collection on different memory pools takes too long. The following settings can be used to adjust these timeouts:\n\nmonitor.jvm.gc.collector.young.warn\nDefault: 1000ms\nRuntime: no\n\nCrateDB will log a warning message if it takes more than the configured timespan to collect the Eden Space (heap).\n\nmonitor.jvm.gc.collector.young.info\nDefault: 700ms\nRuntime: no\n\nCrateDB will log an info message if it takes more than the configured timespan to collect the Eden Space (heap).\n\nmonitor.jvm.gc.collector.young.debug\nDefault: 400ms\nRuntime: no\n\nCrateDB will log a debug message if it takes more than the configured timespan to collect the Eden Space (heap).\n\nmonitor.jvm.gc.collector.old.warn\nDefault: 10000ms\nRuntime: no\n\nCrateDB will log a warning message if it takes more than the configured timespan to collect the Old Gen / Tenured Gen (heap).\n\nmonitor.jvm.gc.collector.old.info\nDefault: 5000ms\nRuntime: no\n\nCrateDB will log an info message if it takes more than the configured timespan to collect the Old Gen / Tenured Gen (heap).\n\nmonitor.jvm.gc.collector.old.debug\nDefault: 2000ms\nRuntime: no\n\nCrateDB will log a debug message if it takes more than the configured timespan to collect the Old Gen / Tenured Gen (heap).\n\nAuthentication\n\nNote\n\nAuthentication is an enterprise feature.\n\nTrust Authentication\nauth.trust.http_default_user\nRuntime: no\nDefault: crate\n\nThe default user that should be used for authentication when clients connect to CrateDB via HTTP protocol and they do not specify a user via the Authorization request header.\n\nHost Based Authentication\n\nAuthentication settings (auth.host_based.*) are node settings, which means that their values apply only to the node where they are applied and different nodes may have different authentication settings.\n\nauth.host_based.enabled\nRuntime: no\nDefault: false\n\nSetting to enable or disable Host Based Authentication (HBA). It is disabled by default.\n\nHBA Entries\n\nThe auth.host_based.config. setting is a group setting that can have zero, one or multiple groups that are defined by their group key (${order}) and their fields (user, address, method, protocol, ssl).\n\n${order}:\nAn identifier that is used as a natural order key when looking up the host\nbased configuration entries. For example, an order key of a will be\nlooked up before an order key of b. This key guarantees that the entry\nlookup order will remain independent from the insertion order of the\nentries.\n\nThe Host Based Authentication (HBA) setting is a list of predicates that users can specify to restrict or allow access to CrateDB.\n\nThe meaning of the fields of the are as follows:\n\nauth.host_based.config.${order}.user\nRuntime: no\nSpecifies an existing CrateDB username, only crate user (superuser) is\navailable. If no user is specified in the entry, then all existing users\ncan have access.\nauth.host_based.config.${order}.address\nRuntime: no\nThe client machine addresses that the client matches, and which are allowed\nto authenticate. This field may contain an IPv4 address, an IPv6 address or\nan IPv4 CIDR mask. For example: 127.0.0.1 or 127.0.0.1/32. It also\nmay contain the special _local_ notation which will match both IPv4 and\nIPv6 connections from localhost. If no address is specified in the entry,\nthen access to CrateDB is open for all hosts.\nauth.host_based.config.${order}.method\nRuntime: no\nThe authentication method to use when a connection matches this entry.\nValid values are trust, cert, and password. If no method is\nspecified, the trust method is used by default.\nSee Trust Method, Client Certificate Authentication Method and Password Authentication Method for more\ninformation about these methods.\nauth.host_based.config.${order}.protocol\nRuntime: no\nSpecifies the protocol for which the authentication entry should be used.\nIf no protocol is specified, then this entry will be valid for all\nprotocols that rely on host based authentication see Trust Method).\nauth.host_based.config.${order}.ssl\nRuntime: no\nDefault: optional\nSpecifies whether the client must use SSL/TLS to connect to the cluster.\nIf set to on then the client must be connected through SSL/TLS\notherwise is not authenticated. If set to off then the client must\nnot be connected via SSL/TLS otherwise is not authenticated. Finally\noptional, which is the value when the option is completely skipped,\nmeans that the client can be authenticated regardless of SSL/TLS is used\nor not.\n\nNote\n\nauth.host_based.config.${order}.ssl is available only for pg protocol.\n\nExample of config groups:\n\nauth.host_based.config:\n  entry_a:\n    user: crate\n    address: 127.16.0.0/16\n  entry_b:\n    method: trust\n  entry_3:\n    user: crate\n    address: 172.16.0.0/16\n    method: trust\n    protocol: pg\n    ssl: on\n\nSecured Communications (SSL/TLS)\n\nSecured communications via SSL allows you to encrypt traffic between CrateDB nodes and clients connecting to them. Connections are secured using Transport Layer Security (TLS).\n\nNote\n\nSSL is an enterprise feature.\n\nssl.http.enabled\nRuntime: no\nDefault: false\n\nSet this to true to enable secure communication between the CrateDB node and the client through SSL via the HTTPS protocol.\n\nssl.psql.enabled\nRuntime: no\nDefault: false\n\nSet this to true to enable secure communication between the CrateDB node and the client through SSL via the PostgreSQL wire protocol.\n\nssl.ingestion.mqtt.enabled\nRuntime: no\nDefault: false\n\nSet this to true to enable secure communication between the CrateDB node and the client through SSL via the MQTT protocol.\n\nssl.keystore_filepath\nRuntime: no\n\nThe full path to the node keystore file.\n\nssl.keystore_password\nRuntime: no\n\nThe password used to decrypt the keystore file defined with ssl.keystore_filepath.\n\nssl.keystore_key_password\nRuntime: no\n\nThe password entered at the end of the keytool -genkey command.\n\nNote\n\nOptionally trusted CA certificates can be stored separately from the node’s keystore into a truststore for CA certificates.\n\nssl.truststore_filepath\nRuntime: no\n\nThe full path to the node truststore file. If not defined, then only a keystore will be used.\n\nssl.truststore_password\nRuntime: no\n\nThe password used to decrypt the truststore file defined with ssl.truststore_filepath.\n\nElasticsearch HTTP REST API\nes.api.enabled\nDefault: false\nRuntime: no\n\nEnable or disable elasticsearch HTTP REST API.\n\nWarning\n\nThis setting is deprecated and will be removed in the future.\n\nManipulating your data via elasticsearch API and not via SQL might result in inconsistent data. You have been warned!\n\nCross-Origin Resource Sharing (CORS)\n\nMany browsers support the same-origin policy which requires web applications to explicitly allow requests across origins. The cross-origin resource sharing settings in CrateDB allow for configuring these.\n\nhttp.cors.enabled\nDefault: false\nRuntime: no\n\nEnable or disable cross-origin resource sharing.\n\nhttp.cors.allow-origin\nDefault: <empty>\nRuntime: no\n\nDefine allowed origins of a request. * allows any origin (which can be a substantial security risk) and by prepending a / the string will be treated as a regular expression. For example /https?:\\/\\/crate.io/ will allow requests from http://crate.io and https://crate.io. This setting disallows any origin by default.\n\nhttp.cors.max-age\nDefault: 1728000 (20 days)\nRuntime: no\n\nMax cache age of a preflight request in seconds.\n\nhttp.cors.allow-methods\nDefault: OPTIONS, HEAD, GET, POST, PUT, DELETE\nRuntime: no\n\nAllowed HTTP methods.\n\nhttp.cors.allow-headers\nDefault: X-Requested-With, Content-Type, Content-Length\nRuntime: no\n\nAllowed HTTP headers.\n\nhttp.cors.allow-credentials\nDefault: false\nRuntime: no\n\nAdd the Access-Control-Allow-Credentials header to responses.\n\nBlobs\nblobs.path\nRuntime: no\n\nPath to a filesystem directory where to store blob data allocated for this node.\n\nBy default blobs will be stored under the same path as normal data. A relative path value is interpreted as relative to CRATE_HOME.\n\nRepositories\n\nRepositories are used to backup a CrateDB cluster.\n\nrepositories.url.allowed_urls\nRuntime: no\n\nThis setting only applies to repositories of type url.\n\nWith this setting a list of urls can be specified which are allowed to be used if a repository of type url is created.\n\nWildcards are supported in the host, path, query and fragment parts.\n\nThis setting is a security measure to prevent access to arbitrary resources.\n\nIn addition, the supported protocols can be restricted using the repositories.url.supported_protocols setting.\n\nrepositories.url.supported_protocols\nDefault: http, https, ftp, file and jar\nRuntime: no\n\nA list of protocols that are supported by repositories of type url.\n\nThe jar protocol is used to access the contents of jar files. For more info, see the java JarURLConnection documentation.\n\nSee also the path.repo Setting.\n\nQueries\nindices.query.bool.max_clause_count\nDefault: 8192\nRuntime: no\n\nThis setting defines the maximum number of elements an array can have so that the != ANY(), LIKE ANY() and the NOT LIKE ANY() operators can be applied on it.\n\nNote\n\nIncreasing this value to a large number (e.g. 10M) and applying those ANY operators on arrays of that length can lead to heavy memory, consumption which could cause nodes to crash with OutOfMemory exceptions.\n\nJavascript Language\nlang.js.enabled\nDefault: false\nRuntime: no\n\nSetting to enable the Javascript language. As The Javascript language is an experimental feature and is not securely sandboxed its disabled by default.\n\nNote\n\nThis is an enterprise feature.\n\nCustom Attributes\n\nThe node.attr namespace is a bag of custom attributes.\n\nYou can create any attribute you want under this namespace, like node.attr.key: value. These attributes use the node.attr namespace to distinguish them from core node attribute like node.name.\n\nCustom attributes are not validated by CrateDB, unlike core node attributes.\n\nCustom attributes can, however, be used to control shard allocation.\n\nIngestion Framework\nMQTT\n\nSettings for the MQTT Ingestion Source.\n\nNote\n\nThe MQTT ingestion source an enterprise feature.\n\ningestion.mqtt.enabled\nDefault: false\nRuntime: no\n\nEnables the MQTT ingestion source on this node.\n\ningestion.mqtt.port\nDefault: 1883\nRuntime: no\n\nTCP port on which the endpoint is exposed.\n\nCan either be a number, or a string defining a port range. The first free port of this range is used.\n\ningestion.mqtt.timeout\nDefault: 10s\nRuntime: no\n\nThe default keep-alive timeout for establised connections.\n\nThis timeout is used if the client does not specify a keepAlive option when sending the CONNECT message.\n\nEnterprise License\nlicense.enterprise\nDefault: true\nRuntime: no\n\nThis is a deprecated setting that allows you to disable the Enterprise Edition features of CrateDB, enabling you to use all community edition features of CrateDB without restrictions or license. The setting will be removed in CrateDB 4.0 and instead you’ll have to Acquire a license or switch to the Community Edition distribution."
  },
  {
    "title": "CrateDB Editions — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/editions/index.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n3.3\nCrateDB Editions\nEnterprise Features\nFeature List\nTrial\nCommunity Edition"
  },
  {
    "title": "CrateDB How-To Guides — CrateDB: How-Tos",
    "url": "https://cratedb.com/docs/crate/howtos/en/latest/",
    "html": "latest\nCrateDB How-To Guides\n\nCrateDB is a distributed SQL database that makes it simple to store and analyze massive amounts of data in real-time.\n\nNote\n\nThis resource assumes you know the basics. If not, check out the Tutorials section for beginner material.\n\nSee Also\n\nThis is an open source documentation project. We host the source code and issue tracker on GitHub.\n\nTable of contents\n\nGetting started\nFirst use\nCreate user\nGenerate time series data\nGenerate time series data from the command line\nGenerate time series data using Python\nGenerate time series data using Node.js\nGenerate time series data using Go\nNormalize time series data intervals\nAdministration\nGeneral upgrade guidelines\nRolling upgrade\nFull restart upgrade\nBootstrap checks\nClustering\nCrateDB multi-node setup\nCrateDB multi-zone setup\nScaling CrateDB on Kubernetes\nLogical replication setup between CrateDB clusters\nPerformance\nMemory configuration\nSharding guide\nInsert performance\nInsert methods\nBulk inserts\nParallel inserts\nConfiguration tuning for inserts\nTesting inserts performance\nSelect performance\nGoing into production\nBest practices\nMigrating from MySQL\nMigrating from MongoDB\nTroubleshooting with sys-tables\nTroubleshooting with the crate-node command\nAbout Sharding and Partitioning\nReference Architectures\nCrateDB on Azure IoT\nDistributed Machine Learning At The Edge\nIntegrations\nData Ingestion using Kafka and Kafka Connect\nData Stream Pipelines with CrateDB and StreamSets Data Collector\nData Enrichment using IoT Hubs, Azure Functions and CrateDB\nCrateDB with R\nReports with CrateDB and Power BI Desktop\nReal Time Reports with CrateDB and Power BI\nDistributed Deep-Learning with CrateDB and TensorFlow\nTroubleshooting\nTroubleshooting CrateDB on Docker with jcmd"
  },
  {
    "title": "Running CrateDB — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/run.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n3.3\nRunning CrateDB\n\nThis document covers the basics of running CrateDB from the command line.\n\nSee Also\n\nFor help installing CrateDB for the first time, check out Getting Started With CrateDB.\n\nIf you’re deploying CrateDB, check out the CrateDB Guide.\n\nTable of Contents\n\nIntroduction\n\nCommand Line Options\n\nSignal Handling\n\nIntroduction\n\nCrateDB ships with a crate command in the bin directory.\n\nThe simplest way to start a CrateDB instance is to invoke crate without parameters. This will start the process in the foreground.\n\nsh$ ./bin/crate\n\n\nYou can also start CrateDB in the background using the -d option. When starting CrateDB in the background it is helpful to write the process ID into a pid file so you can find out the process id easlily:\n\nsh$ ./bin/crate -d -p ./crate.pid\n\n\nTo stop the process that is running in the background send the TERM or INT signal to it.\n\nsh$ kill -TERM `cat ./crate.pid`\n\n\nThe crate executable supports the following command line options:\n\nCommand Line Options\n\nOption\n\n\t\n\nDescription\n\n\n\n\n-d\n\n\t\n\nStart the daemon in the background\n\n\n\n\n-h\n\n\t\n\nPrint usage information\n\n\n\n\n-p <pidfile>\n\n\t\n\nLog the pid to a file\n\n\n\n\n-v\n\n\t\n\nPrint version information\n\n\n\n\n-C\n\n\t\n\nSet a CrateDB configuration value (overrides configuration file)\n\n\n\n\n-D\n\n\t\n\nSet a Java system property value\n\n\n\n\n-X\n\n\t\n\nSet a nonstandard java option\n\nExample:\n\nsh$ ./bin/crate -d -p ./crate.pid\n\nSignal Handling\n\nThe CrateDB process can handle the following signals.\n\nSignal\n\n\t\n\nDescription\n\n\n\n\nTERM\n\n\t\n\nStops a running CrateDB process\n\nkill -TERM `cat /path/to/pidfile.pid`\n\n\n\n\nINT\n\n\t\n\nStops a running CrateDB process\n\nSame behaviour as TERM.\n\n\n\n\nUSR2\n\n\t\n\nStops a running CrateDB process gracefully. See Rolling Upgrade for more information\n\nkill -USR2 `cat /path/to/pidfile.pid`\n\nUSR2 is not supported on Windows."
  },
  {
    "title": "CrateDB Reference — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/index.html",
    "html": "5.6\nCrateDB Reference\n\nCrateDB is a distributed SQL database that makes it simple to store and analyze massive amounts of machine data in real-time.\n\nNote\n\nThis resource assumes you know the basics. If not, check out the Tutorials section for beginner material.\n\nSee Also\n\nCrateDB is an open source project and is hosted on GitHub.\n\nTable of contents\n\nConcepts\nJoins\nClustering\nStorage and consistency\nResiliency\nCLI tools\ncrate\ncrate-node\nConfiguration\nNode-specific settings\nCluster-wide settings\nSession settings\nLogging\nEnvironment variables\nGeneral use\nData definition\nData manipulation\nQuerying\nBuilt-in functions and operators\nUser-defined functions\nBlobs\nOptimistic Concurrency Control\nInformation schema\nAdministration\nSystem information\nRuntime configuration\nUsers and roles management\nPrivileges\nAuthentication\nSecured communications (SSL/TLS)\nOptimization\nJobs management\nJMX monitoring\nSnapshots\nLogical replication\nCloud discovery\nUsage Data Collector\nSQL syntax\nGeneral SQL\nSQL Statements\nClient interfaces\nHTTP endpoint\nPostgreSQL wire protocol\nAppendices\nRelease Notes\nSQL compatibility\nSQL standard compliance\nResiliency Issues\nGlossary"
  },
  {
    "title": "CrateDB Reference — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/index.html",
    "html": "5.6\nCrateDB Reference\n\nCrateDB is a distributed SQL database that makes it simple to store and analyze massive amounts of machine data in real-time.\n\nNote\n\nThis resource assumes you know the basics. If not, check out the Tutorials section for beginner material.\n\nSee Also\n\nCrateDB is an open source project and is hosted on GitHub.\n\nTable of contents\n\nConcepts\nJoins\nClustering\nStorage and consistency\nResiliency\nCLI tools\ncrate\ncrate-node\nConfiguration\nNode-specific settings\nCluster-wide settings\nSession settings\nLogging\nEnvironment variables\nGeneral use\nData definition\nData manipulation\nQuerying\nBuilt-in functions and operators\nUser-defined functions\nBlobs\nOptimistic Concurrency Control\nInformation schema\nAdministration\nSystem information\nRuntime configuration\nUsers and roles management\nPrivileges\nAuthentication\nSecured communications (SSL/TLS)\nOptimization\nJobs management\nJMX monitoring\nSnapshots\nLogical replication\nCloud discovery\nUsage Data Collector\nSQL syntax\nGeneral SQL\nSQL Statements\nClient interfaces\nHTTP endpoint\nPostgreSQL wire protocol\nAppendices\nRelease Notes\nSQL compatibility\nSQL standard compliance\nResiliency Issues\nGlossary"
  },
  {
    "title": "Installation — CrateDB: Tutorials",
    "url": "https://cratedb.com/docs/crate/tutorials/en/latest/",
    "html": "latest\nInstallation\nIntroduction\n\nThis part of the documentation covers the installation of CrateDB on Linux, macOS and Windows systems. The first step to using any software package is getting it properly installed. Please read this section carefully.\n\nCrateDB Cloud\n\nThe easiest way to get started with CrateDB is to use a 30 day free CrateDB Cloud cluster, no credit card required. Visit the sign up page to start your CrateDB cluster today.\n\nCrateDB locally\n\nIf you want to try out CrateDB locally on Linux or macOS but would prefer to avoid the hassle of manual installation or extracting release archives, you can get a fresh CrateDB node up and running in your current working directory with a single command:\n\nsh$ bash -c \"$(curl -L https://try.crate.io/)\"\n\n\nNote\n\nThis is a quick way to try out CrateDB. It is not the recommended method to install CrateDB in a durable way. The following sections will outline that method.\n\nInstalling CrateDB\n\nThis section of the documentation shows you how to deploy CrateDB in different environments.\n\nTable of contents\n\nGetting started\nSelf-hosted CrateDB\nDebian GNU/Linux\nRed Hat Linux\nUbuntu\nWindows\nCrateDB and containers\nRun CrateDB on Docker\nRun CrateDB on Kubernetes\nCrateDB and Kubernetes\nRun CrateDB with Kubernetes Operator\nCrateDB and cloud hosting\nRun CrateDB on Amazon Web Services (AWS)\nRunning CrateDB on Amazon EC2\nRunning CrateDB via Terraform\nUsing Amazon S3 as a snapshot repository\nRun CrateDB on Microsoft Azure\nRunning CrateDB on Azure VMs\nRunning CrateDB via Terraform"
  },
  {
    "title": "Clustering — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/concepts/clustering.html",
    "html": "5.6\nClustering\n\nThe aim of this document is to describe, on a high level, how the distributed SQL database CrateDB uses a shared nothing architecture to form high- availability, resilient database clusters with minimal effort of configuration.\n\nIt will lay out the core concepts of the shared nothing architecture at the heart of CrateDB. The main difference to a primary-secondary architecture is that every node in the CrateDB cluster can perform every operation - hence all nodes are equal in terms of functionality (see Components of a CrateDB Node) and are configured the same.\n\nTable of contents\n\nComponents of a CrateDB Node\n\nSQL Handler\n\nJob Execution Service\n\nCluster State Service\n\nData storage\n\nMulti-node setup: Clusters\n\nCluster state management\n\nSettings, metadata, and routing\n\nMaster Node Election\n\nDiscovery\n\nNetworking\n\nCluster behavior\n\nApplication use case\n\nComponents of a CrateDB Node\n\nTo understand how a CrateDB cluster works it makes sense to first take a look at the components of an individual node of the cluster.\n\nFigure 1\n\nMultiple interconnected instances of CrateDB form a single database cluster. The components of each node are equal.\n\nFigure 1 shows that in CrateDB each node of a cluster contains the same components that (a) interface with each other, (b) with the same component from a different node and/or (c) with the outside world. These four major components are: SQL Handler, Job Execution Service, Cluster State Service, and Data Storage.\n\nSQL Handler\n\nThe SQL Handler part of a node is responsible for three aspects:\n\nhandling incoming client requests,\n\nparsing and analyzing the SQL statement from the request and\n\ncreating an execution plan based on the analyzed statement (abstract syntax tree)\n\nThe SQL Handler is the only of the four components that interfaces with the “outside world”. CrateDB supports three protocols to handle client requests:\n\nHTTP\n\na Binary Transport Protocol\n\nthe PostgreSQL Wire Protocol\n\nA typical request contains a SQL statement and its corresponding arguments.\n\nJob Execution Service\n\nThe Job Execution Service is responsible for the execution of a plan (“job”). The phases of the job and the resulting operations are already defined in the execution plan. A job usually consists of multiple operations that are distributed via the Transport Protocol to the involved nodes, be it the local node and/or one or multiple remote nodes. Jobs maintain IDs of their individual operations. This allows CrateDB to “track” (or for example “kill”) distributed queries.\n\nCluster State Service\n\nThe three main functions of the Cluster State Service are:\n\ncluster state management,\n\nelection of the master node and\n\nnode discovery, thus being the main component for cluster building (as described in section Multi-node setup: Clusters).\n\nIt communicates using the Binary Transport Protocol.\n\nData storage\n\nThe data storage component handles operations to store and retrieve data from disk based on the execution plan.\n\nIn CrateDB, the data stored in the tables is sharded, meaning that tables are divided and (usually) stored across multiple nodes. Each shard is a separate Lucene index that is stored physically on the filesystem. Reads and writes are operating on a shard level.\n\nMulti-node setup: Clusters\n\nA CrateDB cluster is a set of two or more CrateDB instances (referred to as nodes) running on different hosts which form a single, distributed database.\n\nFor inter-node communication, CrateDB uses a software specific transport protocol that utilizes byte-serialized Plain Old Java Objects (POJOs) and operates on a separate port. That so-called “transport port” must be open and reachable from all nodes in the cluster.\n\nCluster state management\n\nThe cluster state is versioned and all nodes in a cluster keep a copy of the latest cluster state. However, only a single node in the cluster – the master node – is allowed to change the state at runtime.\n\nSettings, metadata, and routing\n\nThe cluster state contains all necessary meta information to maintain the cluster and coordinate operations:\n\nGlobal cluster settings\n\nDiscovered nodes and their status\n\nSchemas of tables\n\nThe status and location of primary and replica shards\n\nWhen the master node updates the cluster state it will publish the new state to all nodes in the cluster and wait for all nodes to respond before processing the next update.\n\nMaster Node Election\n\nIn a CrateDB cluster there can only be one master node at any single time. The cluster only becomes available to serve requests once a master has been elected, and a new election takes place if the current master node becomes unavailable.\n\nBy default, all nodes are master-eligible, but a node setting is available to indicate, if desired, that a node must not take on the role of master.\n\nTo elect a master among the eligible nodes, a majority (floor(half)+1), also known as quorum, is required among a subset of all master-eligible nodes, this subset of nodes is known as the voting configuration. The voting configuration is a list which is persisted as part of the cluster state. It is maintained automatically in a way that makes so that split-brain scenarios are never possible.\n\nEvery time a node joins the cluster, or leaves the cluster, even if it is for a few seconds, CrateDB re-evaluates the voting configuration. If the new number of master-eligible nodes in the cluster is odd, CrateDB will put them all in the voting configuration. If the number is even, CrateDB will exclude one of the master-eligible nodes from the voting configuration.\n\nThe voting configuration is not shrunk below 3 nodes, meaning that if there were 3 nodes in the voting configuration and one of them becomes unavailable, they all stay in the voting configuration and a quorum of 2 nodes is still required. A master node rescinds its role if it cannot contact a quorum of nodes from the latest voting configuration.\n\nWarning\n\nIf you do infrastructure maintenance, please note that as nodes are shutdown or rebooted, they will temporarily leave the voting configuration, and for the cluster to elect a master a quorum is required among the nodes that were last in the voting configuration.\n\nFor instance, if you have a 5-nodes cluster, with all nodes master-eligible, and node 1 is currently the master, and you shutdown node 5, then node 4, then node 3, the cluster will stay available as the voting configuration will have adapted to only have nodes 1, 2, and 3 on it.\n\nIf you then shutdown one more node the cluster will become unavailable as a quorum of 2 nodes is now required and not available. To bring the cluster back online at this point you will require two nodes among 1, 2, and 3. Bringing back nodes 3, 4, and 5, will not be sufficient.\n\nNote\n\nSpecial settings and considerations applied prior to CrateDB version 4.0.0.\n\nDiscovery\n\nThe process of finding, adding and removing nodes is done in the discovery module.\n\nFigure 2\n\nPhases of the node discovery process. n1 and n2 already form a cluster where n1 is the elected master node, n3 joins the cluster. The cluster state update happens in parallel!\n\nNode discovery happens in multiple steps:\n\nCrateDB requires a list of potential host addresses for other CrateDB nodes when it is starting up. That list can either be provided by a static configuration or can be dynamically generated, for example by fetching DNS SRV records, querying the Amazon EC2 API, and so on.\n\nAll potential host addresses are pinged. Nodes which receive the request respond to it with information about the cluster it belongs to, the current master node, and its own node name.\n\nNow that the node knows the master node, it sends a join request. The Primary verifies the incoming request and adds the new node to the cluster state that now contains the complete list of all nodes in the cluster.\n\nThe cluster state is then published across the cluster. This guarantees the common knowledge of the node addition.\n\nCaution\n\nIf a node is started without any initial_master_nodes or a discovery_type set to single-node (e.g., the default configuration), it will never join a cluster even if the configuration is subsequently changed.\n\nIt is possible to force the node to forget its current cluster state by using the crate-node CLI tool. However, be aware that this may result in data loss.\n\nNetworking\n\nIn a CrateDB cluster all nodes have a direct link to all other nodes; this is known as full mesh topology. Due to simplicity reasons every node maintains a one-way connections to every other node in the network. The network topology of a 5 node cluster looks like this:\n\nFigure 3\n\nNetwork topology of a 5 node CrateDB cluster. Each line represents a one-way connection.\n\nThe advantages of a fully connected network are that it provides a high degree of reliability and the paths between nodes are the shortest possible. However, there are limitations in the size of such networked applications because the number of connections (c) grows quadratically with the number of nodes (n):\n\nc = n * (n - 1)\n\nCluster behavior\n\nThe fact that each CrateDB node in a cluster is equal allows applications and users to connect to any node and get the same response for the same operations. As already described in section Components of a CrateDB Node, the SQL handler is responsible for handling incoming client SQL requests, either using the HTTP transport protocol, or the PostgreSQL wire protocol.\n\nThe “handler node” that accepts the client request also returns the response to the client. It does neither redirect nor delegate the request to a different nodes. The handler node parses the incoming request into a syntax tree, analyzes it and creates an execution plan locally. Then the operations of the plan are executed in a distributed manner. The upstream of the final phase of the execution is always the handler which then returns the response to the client.\n\nApplication use case\n\nIn a conventional setup of an application using a primary-secondary database the deployed stack looks similar to this:\n\nFigure 4\n\nConventional deployment of an application-database stack.\n\nHowever, this given setup does not scale because all application servers use the same, single entry point to the database for writes (the application can still read from secondaries) and if that entry point is unavailable the complete stack is broken.\n\nChoosing a shared nothing architecture allows DevOps to deploy their applications in an “elastic” manner without SPoF. The idea is to extend the shared nothing architecture from the database to the application which in most cases is stateless already.\n\nFigure 5\n\nElastic deployment making use of the shared nothing architecture.\n\nIf you deploy an instance of CrateDB together with every application server you will be able to dynamically scale up and down your database backend depending on your needs. The application only needs to communicate to its “bound” CrateDB instance on localhost. The load balancer tracks the health of the hosts and if either the application or the database on a single host fails the complete host will taken out of the load balancing."
  },
  {
    "title": "Glossary — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/appendices/glossary.html",
    "html": "4.8\nGlossary\n\nThis glossary defines key terms used in the CrateDB reference manual.\n\nTable of contents\n\nTerms\n\nB\n\nC\n\nE\n\nF\n\nM\n\nN\n\nO\n\nP\n\nR\n\nS\n\nU\n\nV\n\nTerms\nB\nBinary operator\n\nSee operation.\n\nC\nCLUSTERED BY column\n\nSee routing column.\n\nE\nEvaluation\n\nSee expression.\n\nExpression\n\nAny valid SQL that produces a value (e.g., column references, comparison operators, and functions) through a process known as evaluation.\n\nContrary to a statement.\n\nSee Also\n\nSQL: Value expressions\n\nBuilt-ins: Subquery expressions\n\nData definition: Generation expressions\n\nScalar functions: Conditional functions and expressions\n\nAggregation: Aggregation expressions\n\nF\nFunction\n\nA token (e.g., replace) that takes zero or more arguments (e.g., three strings), performs a specific task, and may return one or more values (e.g., a modified string). Functions that return more than one value are called multi-valued functions.\n\nFunctions may be called in an SQL statement, like so:\n\ncr> SELECT replace('Hello world!', 'world', 'friend') as result;\n+---------------+\n| result        |\n+---------------+\n| Hello friend! |\n+---------------+\nSELECT 1 row in set (... sec)\n\n\nSee Also\n\nScalar functions\n\nAggregate functions\n\nTable functions\n\nWindow functions\n\nUser-defined functions\n\nM\nMetadata gateway\n\nPersists cluster metadata on disk every time the metadata changes. This data is stored persistently across full cluster restarts and recovered after nodes are started again.\n\nSee Also\n\nCluster configuration: Metadata gateway\n\nMulti-valued function\n\nA function that returns two or more values.\n\nSee Also\n\nTable functions\n\nWindow functions\n\nN\nNonscalar\n\nA data type that can have more than one value (e.g., arrays and objects).\n\nContrary to a scalar.\n\nSee Also\n\nGeographic types\n\nContainer types\n\nO\nOperand\n\nSee operator.\n\nOperation\n\nSee operator.\n\nOperator\n\nA reserved keyword (e.g., IN) or sequence of symbols (e.g., >=) that can be used in an SQL statement to manipulate one or more expressions and return a result (e.g., true or false). This process is known as an operation and the expressions can be called operands or arguments.\n\nAn operator that takes one operand is known as a unary operator and an operator that takes two is known as a binary operator.\n\nSee Also\n\nArithmetic operators\n\nComparison operators\n\nArray comparisons\n\nP\nPartition column\n\nA column used to partition a table. Specified by the PARTITIONED BY clause.\n\nAlso known as a PARTITIONED BY column or partitioned column.\n\nA table may be partitioned by one or more columns:\n\nIf a table is partitioned by one column, a new partition is created for every unique value in that partition column\n\nIf a table is partitioned by multiple columns, a new partition is created for every unique combination of row values in those partition columns\n\nSee Also\n\nData definition: Partitioned tables\n\nGenerated columns: Partitioning\n\nCREATE TABLE: PARTITIONED BY clause\n\nALTER TABLE: PARTITION clause\n\nREFRESH: PARTITION clause\n\nOPTIMIZE: PARTITION clause\n\nCOPY TO: PARTITION clause\n\nCOPY FROM: PARTITION clause\n\nCREATE SNAPSHOT: PARTITION clause\n\nRESTORE SNAPSHOT: PARTITION clause\n\nPARTITIONED BY column\n\nSee partition column.\n\nPartitioned column\n\nSee partition column.\n\nR\nRegular expression\n\nAn expression used to search for patterns in a string.\n\nSee Also\n\nWikipedia: Regular expression\n\nData definition: Fulltext analyzers\n\nQuerying: Regular expressions\n\nScalar functions: Regular expressions\n\nTable functions: regexp_matches\n\nRouting column\n\nValues in this column are used to compute a hash which is then used to route the corresponding row to a specific shard.\n\nAlso known as the CLUSTERED BY column.\n\nAll rows that have the same routing column row value are stored in the same shard.\n\nNote\n\nThe routing of rows to a specific shard is not the same as the routing of shards to a specific node (also known as shard allocation).\n\nSee Also\n\nStorage and consistency: Addressing documents\n\nSharding: Routing\n\nCREATE TABLE: CLUSTERED clause\n\nS\nScalar\n\nA data type with a single value (e.g., numbers and strings).\n\nContrary to a nonscalar.\n\nSee Also\n\nPrimitive types\n\nShard allocation\n\nThe process by which CrateDB allocates shards to a specific nodes.\n\nNote\n\nShard allocation is sometimes referred to as shard routing, which is not to be confused with row routing.\n\nSee Also\n\nShard allocation filtering\n\nCluster configuration: Routing allocation\n\nSharding: Number of shards\n\nAltering tables: Changing the number of shards\n\nAltering tables: Reroute shards\n\nShard recovery\n\nThe process by which CrateDB synchronizes a replica shard from a primary shard.\n\nShard recovery can happen during node startup, after node failure, when replicating a primary shard, when moving a shard to another node (i.e., when rebalancing the cluster), or during snapshot restoration.\n\nA shard that is being recovered cannot be queried until the recovery process is complete.\n\nSee Also\n\nCluster settings: Recovery\n\nSystem information: Checked node settings\n\nShard routing\n\nSee shard allocation.\n\nStatement\n\nAny valid SQL that serves as a database instruction (e.g., CREATE TABLE, INSERT, and SELECT) instead of producing a value.\n\nContrary to an expression.\n\nSee Also\n\nData definition\n\nData manipulation\n\nQuerying\n\nSQL Statements\n\nSubquery\n\nA SELECT statement used as a relation in the FROM clause of a parent SELECT statement.\n\nAlso known as a subselect.\n\nSubselect\n\nSee subquery.\n\nU\nUnary operator\n\nSee operation.\n\nUncorrelated subquery\n\nA scalar subquery that does not reference any relations (e.g., tables) in the parent SELECT statement.\n\nSee Also\n\nBuilt-ins: Subquery expressions\n\nV\nValue expression\n\nSee expression."
  },
  {
    "title": "Joins — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/concepts/joins.html",
    "html": "5.6\nJoins\n\nJoins are essential operations in relational databases. They create a link between rows based on common values and allow the meaningful combination of these rows. CrateDB supports joins and due to its distributed nature allows you to work with large amounts of data.\n\nIn this document we will present the following topics. First, an overview of the existing types of joins and algorithms provided. Then a description of how CrateDB implements them along with the necessary optimizations, which allows us to work with huge datasets.\n\nTable of contents\n\nJoin types\n\nCross join\n\nInner join\n\nEqui Join\n\nOuter join\n\nJoin algorithms\n\nNested loop join\n\nPrimitive nested loop\n\nDistributed nested loop\n\nHash join\n\nBasic algorithm\n\nBlock hash join\n\nSwitch tables optimization\n\nDistributed block hash join\n\nJoin optimizations\n\nQuery then fetch\n\nPush-down query optimization\n\nCross join elimination\n\nJoin types\n\nA join is a relational operation that merges two data sets based on certain properties. Join Types (Inspired by this article) shows which elements appear in which join.\n\nJoin Types\n\nFrom left to right, top to bottom: left join, right join, inner join, outer join, and cross join of a set L and R.\n\nCross join\n\nA cross join returns the Cartesian product of two or more relations. The result of the Cartesian product on the relation L and R consists of all possible permutations of each tuple of the relation L with every tuple of the relation R.\n\nInner join\n\nAn inner join is a join of two or more relations that returns only tuples that satisfy the join condition.\n\nEqui Join\n\nAn equi join is a subset of an inner join and a comparison-based join, that uses equality comparisons in the join condition. The equi join of the relation L and R combines tuple l of relation L with a tuple r of the relation R if the join attributes of both tuples are identical.\n\nOuter join\n\nAn outer join returns a relation consisting of tuples that satisfy the join condition and dangling tuples from both or one of the relations, respectively to the outer join type.\n\nAn outer join can be one of the following types:\n\nLeft outer join returns tuples of the relation L matching tuples of the relation R and dangling tuples of the relation R padded with null values.\n\nRight outer join returns tuples of the relation R matching tuples of the relation L and dangling tuples from the relation L padded with null values.\n\nFull outer join returns matching tuples of both relations and dangling tuples produced by left and right outer joins.\n\nJoin algorithms\n\nCrateDB supports (a) CROSS JOIN, (b) INNER JOIN, (c) EQUI JOIN, (d) LEFT JOIN, (e) RIGHT JOIN and (f) FULL JOIN. All of these join types are executed using the nested loop join algorithm except for the Equi Joins which are executed using the hash join algorithm. Special optimizations, according to the specific use cases, are applied to improve execution performance.\n\nNested loop join\n\nThe nested loop join is the simplest join algorithm. One of the relations is nominated as the inner relation and the other as the outer relation. Each tuple of the outer relation is compared with each tuple of the inner relation and if the join condition is satisfied, the tuples of the relation L and R are concatenated and added into the returned virtual relation:\n\nfor each tuple l ∈ L do\n    for each tuple r ∈ R do\n        if l.a Θ r.b\n            put tuple(l, r) in Q\n\n\nListing 1. Nested loop join algorithm.\n\nPrimitive nested loop\n\nFor joins on some relations, the nested loop operation can be executed directly on the handler node. Specifically for queries involving a CROSS JOIN or joins on system tables /information_schema each shard sends the data to the handler node. Afterwards, this node runs the nested loop, applies limits, etc. and ultimately returns the results. Similarly, joins can be nested, so instead of collecting data from shards the rows can be the result of a previous join or table function.\n\nDistributed nested loop\n\nRelations are usually distributed to different nodes which require the nested loop to acquire the data before being able to join. After finding the locations of the required shards (which is done in the planning stage), the smaller data set (based on the row count) is broadcast amongst all the nodes holding the shards they are joined with.\n\nAfter that, each of the receiving nodes can start running a nested loop on the subset it has just received. Finally, these intermediate results are pushed to the original (handler) node to merge and return the results to the requesting client (see Nodes that are holding the smaller shards broadcast the data to the processing nodes which then return the results to the requesting node.).\n\nNodes that are holding the smaller shards broadcast the data to the processing nodes which then return the results to the requesting node.\n\nQueries can be optimized if they contain (a) ORDER BY, (b) LIMIT, or (c) if INNER/EQUI JOIN. In any of these cases, the nested loop can be terminated earlier:\n\nOrdering allows determining whether there are records left\n\nLimit states the maximum number of rows that are returned\n\nConsequently, the number of rows is significantly reduced allowing the operation to complete much faster.\n\nHash join\n\nThe Hash Join algorithm is used to execute certain types of joins in a more efficient way than Nested Loop.\n\nBasic algorithm\n\nThe operation takes place in one node (the handler node to which the client is connected). The rows of the left relation of the join are read and a hashing algorithm is applied on the fields of the relation which participate in the join condition. The hashing algorithm generates a hash value which is used to store every row of the left relation in the proper position in a hash table.\n\nThen the rows of the right relation are read one-by-one and the same hashing algorithm is applied on the fields that participate in the join condition. The generated hash value is used to make a lookup in the hash table. If no entry is found, the row is skipped and the processing continues with the next row from the right relation. If an entry is found, the join condition is validated (handling hash collisions) and on successful validation the combined tuple of left and right relation is returned.\n\nBasic hash join algorithm\n\nBlock hash join\n\nThe Hash Join algorithm requires a hash table containing all the rows of the left relation to be stored in memory. Therefore, depending on the size of the relation (number of rows) and the size of each row, the size of this hash table might exceed the available memory of the node executing the hash join. To resolve this limitation the rows of the left relation are loaded into the hash table in blocks.\n\nOn every iteration the maximum available size of the hash table is calculated, based on the number of rows and size of each row of the table but also taking into account the available memory for query execution on the node. Once this block-size is calculated the rows of the left relation are processed and inserted into the hash table until the block-size is reached.\n\nThe operation then starts reading the rows of the right relation, process them one-by-one and performs the lookup and the join condition validation. Once all rows from the right relation are processed the hash table is re-initialized based on a new calculation of the block size and a new iteration starts until all rows of the left relation are processed.\n\nWith this algorithm the memory limitation is handled in expense of having to iterate over the rows of the right table multiple times, and it is the default algorithm used for Hash Join execution by CrateDB.\n\nSwitch tables optimization\n\nSince the right table can be processed multiple times (number of rows from left / block-size) the right table should be the smaller (in number of rows) of the two relations participating in the join. Therefore, if originally the right relation is larger than the left the query planner performs a switch to take advantage of this detail and execute the hash join with better performance.\n\nDistributed block hash join\n\nSince CrateDB is a distributed database and a standard deployment consists of at least three nodes and in most case of much more, the Hash Join algorithm execution can be further optimized (performance-wise) by executing it in a distributed manner across the CrateDB cluster.\n\nThe idea is to have the hash join operation executing in multiple nodes of the cluster in parallel and then merge the intermediate results before returning them to the client.\n\nA hashing algorithm is applied on every row of both the left and right relations. On the integer value generated by this hash, a modulo, by the number of nodes in the cluster, is applied and the resulting number defines the node to which this row should be sent. As a result each node of the cluster receives a subset of the whole data set which is ensured (by the hashing and modulo) to contain all candidate matching rows.\n\nEach node in turn performs a block hash join on this subset and sends its result tuples to the handler node (where the client issued the query). Finally, the handler node receives those intermediate results, merges them and applies any pending ORDER BY, LIMIT and OFFSET and sends the final result to the client.\n\nThis algorithm is used by CrateDB for most cases of hash join execution except for joins on complex subqueries that contain LIMIT and/or OFFSET.\n\nDistributed hash join algorithm\n\nJoin optimizations\nQuery then fetch\n\nJoin operations on large relation can be extremely slow especially if the join is executed with a Nested Loop. - which means that the runtime complexity grows quadratically (O(n*m)). Specifically for cross joins this results in large amounts of data sent over the network and loaded into memory at the handler node. CrateDB reduces the volume of data transferred by employing “Query Then Fetch”: First, filtering and ordering are applied (if possible where the data is located) to obtain the required document IDs. Next, as soon as the final data set is ready, CrateDB fetches the selected fields and returns the data to the client.\n\nPush-down query optimization\n\nComplex queries such as Listing 2 require the planner to decide when to filter, sort, and merge in order to efficiently execute the plan. In this case, the query would be split internally into subqueries before running the join. As shown in Figure 5, first filtering (and ordering) is applied to relations L and R on their shards, then the result is directly broadcast to the nodes running the join. Not only will this behavior reduce the number of rows to work with, it also distributes the workload among the nodes so that the (expensive) join operation can run faster.\n\nSELECT L.a, R.x\nFROM L, R\nWHERE L.id = R.id\n  AND L.b > 100\n  AND R.y < 10\nORDER BY L.a\n\n\nListing 2. An INNER JOIN on ids (effectively an EQUI JOIN) which can be optimized.\n\nFigure 5\n\nComplex queries are broken down into subqueries that are run on their shards before joining.\n\nCross join elimination\n\nThe optimizer will try to eliminate cross joins in the query plan by changing the join-order. Cross join elimination replaces a CROSS JOIN with an INNER JOIN if query conditions used in the WHERE clause or other join conditions allow for it. An example:\n\nSELECT *\nFROM t1 CROSS JOIN t2\nINNER JOIN t3\nON t3.z = t1.x AND t3.z = t2.y\n\n\nThe cross join elimination will change the order of the query from t1, t2, t3 to t2, t1, t3 so that each join has a join condition and the CROSS JOIN can be replaced by an INNER JOIN. When reordering, it will try to preserve the original join order as much as possible. If a CROSS JOIN cannot be eliminated, the original join order will be maintained. This optimizer rule can be disabled with the optimizer eliminate cross join session setting:\n\nSET optimizer_eliminate_cross_join = false\n\n\nNote that this setting is experimental, and may change in the future."
  },
  {
    "title": "Resiliency Issues — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/appendices/resiliency.html",
    "html": "4.8\nResiliency Issues\n\nCrateDB uses Elasticsearch for data distribution and replication. Most of the resiliency issues exist in the Elasticsearch layer and can be tested by Jepsen.\n\nTable of contents\n\nKnown issues\n\nRetry of updates causes double execution\n\nFixed issues\n\nRepeated cluster partitions can cause lost cluster updates\n\nVersion number representing ambiguous row versions\n\nReplicas can fall out of sync when a primary shard fails\n\nLoss of rows due to network partition\n\nDirty reads caused by bad primary handover\n\nChanges are overwritten by old data in danger of lost data\n\nMake table creation resilient to closing and full cluster crashes\n\nUnaware master accepts cluster updates\n\nKnown issues\nRetry of updates causes double execution\nStatus\tWork ongoing (More info)\nSeverity\tModerate\nLikelihood\tVery rare\nCause\tNetwork issues, unresponsive nodes\nWorkloads\tNon-Idempotent writes\n\nScenario\n\nA node with a primary shard receives an update, writes it to disk, but goes offline before having sent a confirmation back to the executing node. When the node comes back online, it receives an update retry and executes the update again.\n\nConsequence\n\nIncorrect data for non-idempotent writes.\n\nFor example:\n\nAn double insert on a table without an explicit primary key would be executed twice and would result in duplicate data.\n\nA double update would incorrectly increment the row version number twice.\n\nFixed issues\nRepeated cluster partitions can cause lost cluster updates\nStatus\tFixed in CrateDB v4.0 (#32006, #32171)\nSeverity\tModerate\nLikelihood\tVery rare\nCause\tNetwork issues, unresponsive nodes\nWorkloads\tAll\n\nScenario\n\nA cluster is partitioned and a new master is elected on the side that has quorum. The cluster is repaired and simultaneously a change is made to the cluster state. The cluster is partitioned again before the new master node has a chance to publish the new cluster state and the partition the master lands on does not have quorum.\n\nConsequence\n\nThe node steps down as master and the uncommunicated state changes are lost.\n\nCluster state is very important and contains information like shard location, schemas, and so on. Lost cluster state updates can cause data loss, reset settings, and problems with table structures.\n\nPartially fixed\n\nThis problem is mostly fixed by #20384 (CrateDB v2.0.x), which uses committed cluster state updates during master election process. This does not fully solve this rare problem but considerably reduces the chance of occurrence. The reason is that if the second partition happens concurrently with a cluster state update and blocks the cluster state commit message from reaching a majority of nodes, it may be that the in flight update is lost. If the now-isolated master can still acknowledge the cluster state update to the client this will result to the loss of an acknowledged change.\n\nVersion number representing ambiguous row versions\nStatus\tFixed in CrateDB v4.0 (#19269, #10708)\nSeverity\tSignificant\nLikelihood\tVery rare\nCause\tNetwork issues, unresponsive nodes\nWorkloads\tVersioned reads with replicated tables while writing.\n\nScenario\n\nA client is writing to a primary shard. The node holding the primary shard is partitioned from the cluster. It usually takes between 30 and 60 seconds (depending on ping configuration) before the master node notices the partition. During this time, the same row is updated on both the primary shard (partitioned) and a replica shard (not partitioned).\n\nConsequence\n\nThere are two different versions of the same row using the same version number. When the primary shard rejoins the cluster and its data is replicated, the update that was made on the replicated shard is lost but the new version number matches the lost update. This will break Optimistic Concurrency Control.\n\nReplicas can fall out of sync when a primary shard fails\nStatus\tFixed in CrateDB v4.0 (#10708)\nSeverity\tModest\nLikelihood\tRare\nCause\tPrimary fails and in-flight writes are only written to a subset of its replicas\nWorkloads\tWrites on replicated table\n\nScenario\n\nWhen a primary shard fails, a replica shard will be promoted to be the primary shard. If there is more than one replica shard, it is possible for the remaining replicas to be out of sync with the new primary shard. This is caused by operations that were in-flight when the primary shard failed and may not have been processed on all replica shards. Currently, the discrepancies are not repaired on primary promotion but instead would be repaired if replica shards are relocated (e.g., from hot to cold nodes); this does mean that the length of time which replicas can be out of sync with the primary shard is unbounded.\n\nConsequence\n\nStale data may be read from replicas.\n\nLoss of rows due to network partition\nStatus\tFixed in Crate v2.0.x (#7572, #14252)\nSeverity\tSignificant\nLikelihood\tVery rare\nCause\tSingle node isolation\nWorkloads\tWrites on replicated table\n\nScenario\n\nA node with a primary shard is partitioned from the cluster. The node continues to accept writes until it notices the network partition. In the meantime, another shard has been elected as the primary. Eventually, the partitioned node rejoins the cluster.\n\nConsequence\n\nData that was written to the original primary shard on the partitioned node is lost as data from the newly elected primary shard replaces it when it rejoins the cluster.\n\nThe risk window depends on your ping configuration. The default configuration of a 30 second ping timeout with three retries corresponds to a 90 second risk window. However, it is very rare for a node to lose connectivity within the cluster but maintain connectivity with clients.\n\nDirty reads caused by bad primary handover\nStatus\tFixed in CrateDB v2.0.x (#15900, #12573)\nSeverity\tModerate\nLikelihood\tRare\nCause\tRace Condition\nWorkloads\tReads\n\nScenario\n\nDuring a primary handover, there is a small risk window when a shard can find out it has been elected as the new primary before the old primary shard notices that it is no longer the primary.\n\nA primary handover can happen in the following scenarios:\n\nA shard is relocated and then elected as the new primary, as two separate but sequential actions. Relocating a shard means creating a new shard and then deleting the old shard.\n\nAn existing replica shard gets promoted to primary because the primary shard was partitioned from the cluster.\n\nConsequence\n\nWrites that occur on the new primary during the risk window will not be replicated to the old shard (which still believes it is the primary) so any subsequent reads on the old shard may return incorrect data.\n\nChanges are overwritten by old data in danger of lost data\nStatus\tFixed in CrateDB v2.0.x (#14671)\nSeverity\tSignificant\nLikelihood\tVery rare\nCause\tNetwork problems\nWorkloads\tWrites\n\nScenario\n\nA node with a primary that contains new data is partitioned from the cluster.\n\nConsequence\n\nCrateDB prefers old data over no data, and so promotes an a shard with stale data as a new primary. The data on the original primary shard is lost. Even if the node with the original primary shard rejoins the cluster, CrateDB has no way of distinguishing correct and incorrect data, so that data replaced with data from the new primary shard.\n\nMake table creation resilient to closing and full cluster crashes\nStatus\tThe issue has been fixed with the following issues. Table recovery: #9126 Reopening tables: #14739 Allocation IDs: #15281\nSeverity\tModest\nLikelihood\tVery Rare\nCause\tEither the cluster fails while recovering a table or the table is closed during shard creation.\nWorkloads\tTable creation\n\nScenario\n\nRecovering a table requires a quorum of shard copies to be available to allocate a primary. This means that a primary cannot be assigned if the cluster dies before enough shards have been allocated. The same happens if a table is closed before enough shard copies were started, making it impossible to reopen the table. Allocation IDs solve this issue by tracking allocated shard copies in the cluster. This makes it possible to safely recover a table in the presence of a single shard copy. Allocation IDs can also distinguish the situation where a table has been created but none of the shards have been started. If such an table was inadvertently closed before at least one shard could be started, a fresh shard will be allocated upon reopening the table.\n\nConsequence\n\nThe primary shard of the table cannot be assigned or a closed table cannot be re-opened.\n\nUnaware master accepts cluster updates\nStatus\tFixed in CrateDB v2.0.x (#13062)\nSeverity\tModerate\nLikelihood\tVery rare\nCause\tNetwork problems\nWorkloads\tDDL statements\n\nScenario\n\nIf a master has lost quorum (i.e. the number of nodes it is in communication with has fallen below the configured minimum) it should step down as master and stop answering requests to perform cluster updates. There is a small risk window between losing quorum and noticing that quorum has been lost, depending on your ping configuration.\n\nConsequence\n\nIf a cluster update request is made to the node between losing quorum and noticing the loss of quorum, that request will be confirmed. However, those updates will be lost because the node will not be able to perform a successful cluster update.\n\nCluster state is very important and contains information like shard location, schemas, and so on. Lost cluster state updates can cause data loss, reset settings, and problems with table structures."
  },
  {
    "title": "SQL compatibility — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/appendices/compatibility.html",
    "html": "4.8\nSQL compatibility\n\nCrateDB provides a standards-based SQL implementation similar to many other SQL databases. In particular, CrateDB aims for compatibility with PostgreSQL. However, CrateDB’s SQL dialect does have some unique characteristics, documented on this page.\n\nSee Also\n\nSQL: Syntax reference\n\nTable of contents\n\nImplementation notes\n\nData types\n\nCreate table\n\nAlter table\n\nSystem information tables\n\nBLOB support\n\nTransactions (BEGIN, START, COMMIT, and ROLLBACK)\n\nUnsupported features and functions\n\nImplementation notes\nData types\n\nCrateDB supports a set of primitive data types. The following table defines how data types of standard SQL map to CrateDB Data types.\n\nStandard SQL\n\n\t\n\nCrateDB\n\n\n\n\ninteger\n\n\t\n\ninteger, int, int4\n\n\n\n\nbit[8]\n\n\t\n\nbyte, char\n\n\n\n\nboolean, bool\n\n\t\n\nboolean\n\n\n\n\nchar [(n)], varchar [(n)]\n\n\t\n\nstring, text, varchar, character varying\n\n\n\n\ntimestamp with time zone\n\n\t\n\ntimestamp with time zone, timestamptz\n\n\n\n\ntimestamp\n\n\t\n\ntimestamp without time zone\n\n\n\n\nsmallint\n\n\t\n\nshort, int2, smallint\n\n\n\n\nbigint\n\n\t\n\nlong, bigint, int8\n\n\n\n\nreal\n\n\t\n\nfloat, real\n\n\n\n\ndouble precision\n\n\t\n\ndouble, double precision\n\nCreate table\n\nCREATE TABLE supports additional storage and table parameters for sharding, replication and routing of data, and does not support inheritance.\n\nAlter table\n\nALTER COLUMN and DROP COLUMN actions are not currently supported (see ALTER TABLE).\n\nSystem information tables\n\nThe read-only System information and Information schema tables have a slightly different schema than specified in standard SQL. They provide schema information and can be queried to get real-time statistical data about the cluster, its nodes, and their shards.\n\nBLOB support\n\nStandard SQL defines a binary string type, called BLOB or BINARY LARGE OBJECT. With CrateDB, Binary Data is instead stored in separate BLOB Tables (see Blobs) which can be sharded and replicated.\n\nTransactions (BEGIN, START, COMMIT, and ROLLBACK)\n\nCrateDB is focused on providing analytical capabilities over supporting traditional transactional use cases, and thus it does not provide transaction control. Every statement commits immediately and is replicated within the cluster.\n\nHowever, every row in CrateDB has a version number that is incremented whenever the record is modified. This version number can be used to implement patterns like Optimistic Concurrency Control, which can be used to solve many of the use cases that would otherwise require traditional transactions.\n\nUnsupported features and functions\n\nThese features of standard SQL are not supported:\n\nStored procedures\n\nTriggers\n\nWITH Queries (Common Table Expressions)\n\nSequences\n\nInheritance\n\nConstraints\n\nUnique\n\nForeign key\n\nExclusion constraints\n\nThese functions of standard SQL are either not supported or only partly supported:\n\nAggregate functions\n\nVarious functions available (see Aggregation)\n\nWindow functions\n\nVarious functions available (see Window functions)\n\nENUM support functions\n\nIS DISTINCT FROM\n\nNetwork address functions and operators\n\nMathematical functions\n\nCertain functions supported (see Mathematical functions)\n\nSet returning functions\n\nTrigger functions\n\nXML functions\n\nNote\n\nThe currently supported and unsupported features in CrateDB are exposed in the Information schema table (see sql_features for usage).\n\nCrateDB also supports the PostgreSQL wire protocol.\n\nIf you have use cases for any missing features, functions, or dialect improvements, let us know on GitHub! We are always improving and extending CrateDB and would love to hear your feedback."
  },
  {
    "title": "SQL standard compliance — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/appendices/compliance.html",
    "html": "4.8\nSQL standard compliance\n\nThis page documents the standard SQL (ISO/IEC 9075) features that CrateDB supports, along with implementation notes and any associated caveats.\n\nCaution\n\nThis list is approximate and features that are listed as supported might be nonconforming in their implementation. However, the main reference documentation always contains the most accurate information about the features CrateDB supports and how to use them.\n\nSee Also\n\nSQL compatibility\n\nID\n\n\t\n\nPackage\n\n\t\n\n#\n\n\t\n\nDescription\n\n\t\n\nComments\n\n\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n1\n\n\t\n\nINTEGER and SMALLINT data types\n\n\t\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n2\n\n\t\n\nREAL, DOUBLE PRECISION, and FLOAT data types\n\n\t\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n3\n\n\t\n\nDECIMAL and NUMERIC data types\n\n\t\n\nNot supported in DDL\n\n\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n4\n\n\t\n\nArithmetic operators\n\n\t\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n5\n\n\t\n\nNumeric comparison\n\n\t\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n6\n\n\t\n\nImplicit casting among the numeric data types\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n2\n\n\t\n\nCHARACTER VARYING data type\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n3\n\n\t\n\nCharacter literals\n\n\t\n\nOnly simple ‘ quoting\n\n\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n4\n\n\t\n\nCHARACTER_LENGTH function\n\n\t\n\nchar_length only\n\n\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n5\n\n\t\n\nOCTET_LENGTH function\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n7\n\n\t\n\nCharacter concatenation\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n8\n\n\t\n\nUPPER and LOWER functions\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n9\n\n\t\n\nTRIM function\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n10\n\n\t\n\nImplicit casting among the character string types\n\n\t\n\njust one type\n\n\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n12\n\n\t\n\nCharacter comparison\n\n\t\n\n\nE031\n\n\t\n\nIdentifiers\n\n\t\t\t\n\n\nE031\n\n\t\n\nIdentifiers\n\n\t\n\n1\n\n\t\n\nDelimited identifiers\n\n\t\n\n\nE031\n\n\t\n\nIdentifiers\n\n\t\n\n2\n\n\t\n\nLower case identifiers\n\n\t\n\n\nE031\n\n\t\n\nIdentifiers\n\n\t\n\n3\n\n\t\n\nTrailing underscore\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n1\n\n\t\n\nSELECT DISTINCT\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n2\n\n\t\n\nGROUP BY clause\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n4\n\n\t\n\nGROUP BY can contain columns not in <select list>\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n5\n\n\t\n\nSelect list items can be renamed\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n6\n\n\t\n\nHAVING clause\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n7\n\n\t\n\nQualified * in select list\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n8\n\n\t\n\nCorrelation names in the FROM clause\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n9\n\n\t\n\nRename columns in the FROM clause\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n1\n\n\t\n\nComparison predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n2\n\n\t\n\nBETWEEN predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n3\n\n\t\n\nIN predicate with list of values\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n4\n\n\t\n\nLIKE predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n6\n\n\t\n\nNULL predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n9\n\n\t\n\nSubqueries in comparison predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n11\n\n\t\n\nSubqueries in IN predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n12\n\n\t\n\nSubqueries in quantified comparison predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n14\n\n\t\n\nSearch condition\n\n\t\n\n\nE071\n\n\t\n\nBasic query expressions\n\n\t\n\n1\n\n\t\n\nUNION DISTINCT table operator\n\n\t\n\n\nE071\n\n\t\n\nBasic query expressions\n\n\t\n\n2\n\n\t\n\nUNION ALL table operator\n\n\t\n\n\nE081\n\n\t\n\nBasic Privileges\n\n\t\t\t\n\n\nE081\n\n\t\n\nBasic Privileges\n\n\t\n\n1\n\n\t\n\nSELECT privilege\n\n\t\n\n\nE081\n\n\t\n\nBasic Privileges\n\n\t\n\n2\n\n\t\n\nDELETE privilege\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n1\n\n\t\n\nAVG\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n2\n\n\t\n\nCOUNT\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n3\n\n\t\n\nMAX\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n4\n\n\t\n\nMIN\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n5\n\n\t\n\nSUM\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n7\n\n\t\n\nDISTINCT quantifier\n\n\t\n\n\nE101\n\n\t\n\nBasic data manipulation\n\n\t\t\t\n\n\nE101\n\n\t\n\nBasic data manipulation\n\n\t\n\n1\n\n\t\n\nINSERT statement\n\n\t\n\n\nE101\n\n\t\n\nBasic data manipulation\n\n\t\n\n3\n\n\t\n\nSearched UPDATE statement\n\n\t\n\n\nE101\n\n\t\n\nBasic data manipulation\n\n\t\n\n4\n\n\t\n\nSearched DELETE statement\n\n\t\n\n\nE131\n\n\t\n\nNull value support (nulls in lieu of values)\n\n\t\t\t\n\n\nE141\n\n\t\n\nBasic integrity constraints\n\n\t\n\n1\n\n\t\n\nNOT NULL constraints\n\n\t\n\n\nE141\n\n\t\n\nBasic integrity constraints\n\n\t\n\n3\n\n\t\n\nPRIMARY KEY constraints\n\n\t\n\n\nE141\n\n\t\n\nBasic integrity constraints\n\n\t\n\n6\n\n\t\n\nCHECK constraints\n\n\t\n\n\nE141\n\n\t\n\nBasic integrity constraints\n\n\t\n\n7\n\n\t\n\nColumn defaults\n\n\t\n\n\nE141\n\n\t\n\nBasic integrity constraints\n\n\t\n\n8\n\n\t\n\nNOT NULL inferred on PRIMARY KEY\n\n\t\n\n\nE151\n\n\t\n\nTransaction support\n\n\t\n\n1\n\n\t\n\nCOMMIT statement\n\n\t\n\n\nE152\n\n\t\n\nBasic SET TRANSACTION statement\n\n\t\t\t\n\n\nE152\n\n\t\n\nBasic SET TRANSACTION statement\n\n\t\n\n1\n\n\t\n\nSET TRANSACTION statement: ISOLATION LEVEL SERIALIZABLE clause\n\n\t\n\nIs ignored\n\n\n\n\nE152\n\n\t\n\nBasic SET TRANSACTION statement\n\n\t\n\n2\n\n\t\n\nSET TRANSACTION statement: READ ONLY and READ WRITE clauses\n\n\t\n\nIs ignored\n\n\n\n\nE161\n\n\t\n\nSQL comments using leading double minus\n\n\t\t\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n1\n\n\t\n\nCOLUMNS view\n\n\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n2\n\n\t\n\nTABLES view\n\n\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n3\n\n\t\n\nVIEWS view\n\n\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n4\n\n\t\n\nTABLE_CONSTRAINTS view\n\n\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n5\n\n\t\n\nREFERENTIAL_CONSTRAINTS view\n\n\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n6\n\n\t\n\nCHECK_CONSTRAINTS view\n\n\t\n\n\nF031\n\n\t\n\nBasic schema manipulation\n\n\t\n\n1\n\n\t\n\nCREATE TABLE statement to create persistent base tables\n\n\t\n\n\nF031\n\n\t\n\nBasic schema manipulation\n\n\t\n\n2\n\n\t\n\nCREATE VIEW statement\n\n\t\n\n\nF031\n\n\t\n\nBasic schema manipulation\n\n\t\n\n3\n\n\t\n\nGRANT statement\n\n\t\n\n\nF031\n\n\t\n\nBasic schema manipulation\n\n\t\n\n4\n\n\t\n\nALTER TABLE statement: ADD COLUMN clause\n\n\t\n\n\nF034\n\n\t\n\nExtended REVOKE statement\n\n\t\t\t\n\n\nF034\n\n\t\n\nExtended REVOKE statement\n\n\t\n\n1\n\n\t\n\nREVOKE statement performed by other than the owner of a schema object\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n1\n\n\t\n\nInner join (but not necessarily the INNER keyword)\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n2\n\n\t\n\nINNER keyword\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n3\n\n\t\n\nLEFT OUTER JOIN\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n4\n\n\t\n\nRIGHT OUTER JOIN\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n5\n\n\t\n\nOuter joins can be nested\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n7\n\n\t\n\nThe inner table in a left or right outer join can also be used in an inner join\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n8\n\n\t\n\nAll comparison operators are supported (rather than just =)\n\n\t\n\n\nF051\n\n\t\n\nBasic date and time\n\n\t\n\n1\n\n\t\n\nDATE data type (including support of DATE literal)\n\n\t\n\n\nF051\n\n\t\n\nBasic date and time\n\n\t\n\n3\n\n\t\n\nTIMESTAMP data type (including support of TIMESTAMP literal) with fractional seconds precision of at least 0 and 6\n\n\t\n\n\nF051\n\n\t\n\nBasic date and time\n\n\t\n\n4\n\n\t\n\nComparison predicate on DATE, TIME, and TIMESTAMP data types\n\n\t\n\n\nF051\n\n\t\n\nBasic date and time\n\n\t\n\n5\n\n\t\n\nExplicit CAST between datetime types and character string types\n\n\t\n\n\nF051\n\n\t\n\nBasic date and time\n\n\t\n\n6\n\n\t\n\nCURRENT_DATE\n\n\t\n\n\nF052\n\n\t\n\nIntervals and datetime arithmetic\n\n\t\t\t\n\n\nF111\n\n\t\n\nIsolation levels other than SERIALIZABLE\n\n\t\t\t\n\n\nF111\n\n\t\n\nIsolation levels other than SERIALIZABLE\n\n\t\n\n1\n\n\t\n\nREAD UNCOMMITTED isolation level\n\n\t\n\nIs ignored\n\n\n\n\nF111\n\n\t\n\nIsolation levels other than SERIALIZABLE\n\n\t\n\n2\n\n\t\n\nREAD COMMITTED isolation level\n\n\t\n\nIs ignored\n\n\n\n\nF111\n\n\t\n\nIsolation levels other than SERIALIZABLE\n\n\t\n\n3\n\n\t\n\nREPEATABLE READ isolation level\n\n\t\n\nIs ignored\n\n\n\n\nF131\n\n\t\n\nGrouped operations\n\n\t\n\n1\n\n\t\n\nWHERE, GROUP BY, and HAVING clauses supported in queries with grouped views\n\n\t\n\n\nF131\n\n\t\n\nGrouped operations\n\n\t\n\n3\n\n\t\n\nSet functions supported in queries with grouped views\n\n\t\n\n\nF171\n\n\t\n\nMultiple schemas per user\n\n\t\t\t\n\n\nF201\n\n\t\n\nCAST function\n\n\t\t\t\n\n\nF221\n\n\t\n\nExplicit defaults\n\n\t\t\t\n\n\nF222\n\n\t\n\nINSERT statement: DEFAULT VALUES clause\n\n\t\t\t\n\n\nF261\n\n\t\n\nCASE expression\n\n\t\t\t\n\n\nF261\n\n\t\n\nCASE expression\n\n\t\n\n1\n\n\t\n\nSimple CASE\n\n\t\n\n\nF261\n\n\t\n\nCASE expression\n\n\t\n\n2\n\n\t\n\nSearched CASE\n\n\t\n\n\nF261\n\n\t\n\nCASE expression\n\n\t\n\n3\n\n\t\n\nNULLIF\n\n\t\n\n\nF261\n\n\t\n\nCASE expression\n\n\t\n\n4\n\n\t\n\nCOALESCE\n\n\t\n\n\nF262\n\n\t\n\nExtended CASE expression\n\n\t\t\t\n\n\nF311\n\n\t\n\nSchema definition statement\n\n\t\n\n2\n\n\t\n\nCREATE TABLE for persistent base tables\n\n\t\n\n\nF391\n\n\t\n\nLong identifiers\n\n\t\t\t\n\n\nF401\n\n\t\n\nExtended joined table\n\n\t\n\n2\n\n\t\n\nFULL OUTER JOIN\n\n\t\n\n\nF401\n\n\t\n\nExtended joined table\n\n\t\n\n4\n\n\t\n\nCROSS JOIN\n\n\t\n\n\nF471\n\n\t\n\nScalar subquery values\n\n\t\t\t\n\n\nF481\n\n\t\n\nExpanded NULL predicate\n\n\t\t\t\n\n\nF501\n\n\t\n\nFeatures and conformance views\n\n\t\n\n1\n\n\t\n\nSQL_FEATURES view\n\n\t\n\n\nF571\n\n\t\n\nTruth value tests\n\n\t\t\t\n\n\nF763\n\n\t\n\nCURRENT_SCHEMA\n\n\t\t\t\n\n\nF850\n\n\t\n\nTop-level <order by clause> in <query expression>\n\n\t\t\t\n\n\nF851\n\n\t\n\n<order by clause> in subqueries\n\n\t\t\t\n\n\nF855\n\n\t\n\nNested <order by clause> in <query expression>\n\n\t\t\t\n\n\nS091\n\n\t\n\nBasic array support\n\n\t\t\t\n\nspecial syntax\n\n\n\n\nS091\n\n\t\n\nBasic array support\n\n\t\n\n1\n\n\t\n\nArrays of built-in data types\n\n\t\n\nspecial syntax\n\n\n\n\nS098\n\n\t\n\nARRAY_AGG\n\n\t\t\t\n\n\nT031\n\n\t\n\nBOOLEAN data type\n\n\t\t\t\n\n\nT051\n\n\t\n\nRow types\n\n\t\t\t\n\nLimited to built-in table functions\n\n\n\n\nT071\n\n\t\n\nBIGINT data type\n\n\t\t\t\n\n\nT175\n\n\t\n\nGenerated columns\n\n\t\t\t\n\n\nT321\n\n\t\n\nBasic SQL-invoked routines\n\n\t\n\n1\n\n\t\n\nUser-defined functions with no overloading\n\n\t\n\n\nT321\n\n\t\n\nBasic SQL-invoked routines\n\n\t\n\n3\n\n\t\n\nFunction invocation\n\n\t\n\n\nT321\n\n\t\n\nBasic SQL-invoked routines\n\n\t\n\n6\n\n\t\n\nROUTINES view\n\n\t\n\n\nT441\n\n\t\n\nABS and MOD functions\n\n\t\t\t\n\n\nT461\n\n\t\n\nSymmetric BETWEEN predicate\n\n\t\t\t\n\n\nT471\n\n\t\n\nResult sets return value\n\n\t\t\t\n\n\nT615\n\n\t\n\nLEAD and LAG functions\n\n\t\t\t\n\n\nT617\n\n\t\n\nFIRST_VALUE and LAST_VALUE function\n\n\t\t\t\n\n\nT618\n\n\t\n\nNTH_VALUE function\n\n\t\t\t\n\n\nT631\n\n\t\n\nIN predicate with one list element\n\n\t\t\t"
  },
  {
    "title": "Release Notes — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/appendices/release-notes/index.html",
    "html": "4.8\nRelease Notes\n\nInformation about individual CrateDB releases, typically including upgrade information and changelog.\n\nNote\n\nThe latest stable, testing, and nightly version is always available from the CrateDB download page.\n\nOlder version of CrateDB are available in the release archives.\n\nVersions\nUnreleased Changes\n4.x\n4.8.x\nVersion 4.8.4\nVersion 4.8.3\nVersion 4.8.2\nVersion 4.8.1\nVersion 4.8.0\n4.7.x\nVersion 4.7.3\nVersion 4.7.2\nVersion 4.7.1\nVersion 4.7.0\n4.6.x\nVersion 4.6.8\nVersion 4.6.7\nVersion 4.6.6\nVersion 4.6.5\nVersion 4.6.4\nVersion 4.6.3\nVersion 4.6.2\nVersion 4.6.1\nVersion 4.6.0\n4.5.x\nVersion 4.5.5\nVersion 4.5.4\nVersion 4.5.3\nVersion 4.5.2\nVersion 4.5.1\nVersion 4.5.0\n4.4.x\nVersion 4.4.3\nVersion 4.4.2\nVersion 4.4.1\nVersion 4.4.0\n4.3.x\nVersion 4.3.4\nVersion 4.3.3\nVersion 4.3.2\nVersion 4.3.1\nVersion 4.3.0\n4.2.x\nVersion 4.2.7\nVersion 4.2.6\nVersion 4.2.5\nVersion 4.2.4\nVersion 4.2.3\nVersion 4.2.2\nVersion 4.2.1\nVersion 4.2.0\n4.1.x\nVersion 4.1.8\nVersion 4.1.7\nVersion 4.1.6\nVersion 4.1.5\nVersion 4.1.4\nVersion 4.1.3\nVersion 4.1.2\nVersion 4.1.1\nVersion 4.1.0\n4.0.x\nVersion 4.0.12\nVersion 4.0.11\nVersion 4.0.10\nVersion 4.0.9\nVersion 4.0.8\nVersion 4.0.7\nVersion 4.0.6\nVersion 4.0.5\nVersion 4.0.4\nVersion 4.0.3\nVersion 4.0.2\nVersion 4.0.1\nVersion 4.0.0\n3.x\n3.3.x\nVersion 3.3.6\nVersion 3.3.5\nVersion 3.3.4\nVersion 3.3.3\nVersion 3.3.2\nVersion 3.3.1\nVersion 3.3.0\n3.2.x\nVersion 3.2.8\nVersion 3.2.7\nVersion 3.2.6\nVersion 3.2.5\nVersion 3.2.4\nVersion 3.2.3\nVersion 3.2.2\nVersion 3.2.1\nVersion 3.2.0\n3.1.x\nVersion 3.1.6\nVersion 3.1.5\nVersion 3.1.4\nVersion 3.1.3\nVersion 3.1.2\nVersion 3.1.1\nVersion 3.1.0\n3.0.x\nVersion 3.0.7\nVersion 3.0.6\nVersion 3.0.5\nVersion 3.0.4\nVersion 3.0.3\nVersion 3.0.2\nVersion 3.0.1\nVersion 3.0.0\n2.x\n2.3.x\nVersion 2.3.11\nVersion 2.3.10\nVersion 2.3.9\nVersion 2.3.8\nVersion 2.3.7\nVersion 2.3.6\nVersion 2.3.5\nVersion 2.3.4\nVersion 2.3.3\nVersion 2.3.2\nVersion 2.3.1\nVersion 2.3.0\n2.2.x\nVersion 2.2.7\nVersion 2.2.6\nVersion 2.2.5\nVersion 2.2.4\nVersion 2.2.3\nVersion 2.2.2\nVersion 2.2.1\nVersion 2.2.0\n2.1.x\nVersion 2.1.10\nVersion 2.1.9\nVersion 2.1.8\nVersion 2.1.7\nVersion 2.1.6\nVersion 2.1.5\nVersion 2.1.4\nVersion 2.1.3\nVersion 2.1.2\nVersion 2.1.1\nVersion 2.1.0\n2.0.x\nVersion 2.0.7\nVersion 2.0.6\nVersion 2.0.5\nVersion 2.0.4\nVersion 2.0.3\nVersion 2.0.2\nVersion 2.0.1\nVersion 2.0.0\n1.x\n1.2.x\nVersion 1.2.0\n1.1.x\nVersion 1.1.6\nVersion 1.1.5\nVersion 1.1.4\nVersion 1.1.3\nVersion 1.1.2\nVersion 1.1.1\nVersion 1.1.0\n1.0.x\nVersion 1.0.6\nVersion 1.0.5\nVersion 1.0.4\nVersion 1.0.3\nVersion 1.0.2\nVersion 1.0.1\nVersion 1.0.0\nOlder Versions\n\nFor older versions, see the 0.57.8 CHANGES.txt file."
  },
  {
    "title": "HTTP endpoint — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/interfaces/http.html",
    "html": "4.8\nHTTP endpoint\n\nCrateDB provides a HTTP Endpoint that can be used to submit SQL queries. The endpoint is accessible under <servername:port>/_sql.\n\nSQL statements are sent to the _sql endpoint in json format, whereby the statement is sent as value associated to the key stmt.\n\nSee Also\n\nData manipulation\n\nA simple SELECT statement can be submitted like this:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' \\\n... -d '{\"stmt\":\"select name, position from locations order by id limit 2\"}'\n{\n  \"cols\": [\n    \"name\",\n    \"position\"\n  ],\n  \"rows\": [\n    [\n      \"North West Ripple\",\n      1\n    ],\n    [\n      \"Outer Eastern Rim\",\n      2\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nNote\n\nWe’re using a simple command line invocation of curl here so you can see how to run this by hand in the terminal. For the rest of the examples in this document, we use here documents (i.e. EOF) for multi line readability.\n\nTable of contents\n\nParameter substitution\n\nDefault schema\n\nColumn types\n\nBulk operations\n\nError handling\n\nBulk errors\n\nParameter substitution\n\nIn addition to the stmt key the request body may also contain an args key which can be used for SQL parameter substitution.\n\nThe SQL statement has to be changed to use placeholders where the values should be inserted. Placeholders can either be numbered (in the form of $1, $2, etc.) or unnumbered using a question mark ?.\n\nThe placeholders will then be substituted with values from an array that is expected under the args key:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\":\n...     \"select date,position from locations\n...     where date <= \\$1 and position < \\$2 order by position\",\n...   \"args\": [\"1979-10-12\", 3]\n... }\n... EOF\n{\n  \"cols\": [\n    \"date\",\n    \"position\"\n  ],\n  \"rows\": [\n    [\n      308534400000,\n      1\n    ],\n    [\n      308534400000,\n      2\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nNote\n\nIn this example the placeholders start with an backslash due to shell escaping.\n\nWarning\n\nParameter substitution must not be used within subscript notation.\n\nFor example, column[?] is not allowed.\n\nThe same query using question marks as placeholders looks like this:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\":\n...     \"select date,position from locations\n...     where date <= ? and position < ? order by position\",\n...   \"args\": [\"1979-10-12\", 3]\n... }\n... EOF\n{\n  \"cols\": [\n    \"date\",\n    \"position\"\n  ],\n  \"rows\": [\n    [\n      308534400000,\n      1\n    ],\n    [\n      308534400000,\n      2\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nNote\n\nWith some queries the row count is not ascertainable. In this cases rowcount is -1.\n\nDefault schema\n\nIt is possible to set a default schema while querying the CrateDB cluster via _sql end point. In such case the HTTP request should contain the Default-Schema header with the specified schema name:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' \\\n... -H 'Default-Schema: doc' -d@- <<- EOF\n... {\n...   \"stmt\":\"select name, position from locations order by id limit 2\"\n... }\n... EOF\n{\n  \"cols\": [\n    \"name\",\n    \"position\"\n  ],\n  \"rows\": [\n    [\n      \"North West Ripple\",\n      1\n    ],\n    [\n      \"Outer Eastern Rim\",\n      2\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nIf the schema name is not specified in the header, the default doc schema will be used instead.\n\nColumn types\n\nCrateDB can respond a list col_types with the data type ID of every responded column. This way one can know what exact data type a column is holding.\n\nIn order to get the list of column data types, a types query parameter must be passed to the request:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql?types' -d@- <<- EOF\n... {\n...   \"stmt\":\n...     \"select date, position from locations\n...      where date <= \\$1 and position < \\$2 order by position\",\n...   \"args\": [\"1979-10-12\", 3]\n... }\n... EOF\n{\n  \"cols\": [\n    \"date\",\n    \"position\"\n  ],\n  \"col_types\": [\n    11,\n    9\n  ],\n  \"rows\": [\n    [\n      308534400000,\n      1\n    ],\n    [\n      308534400000,\n      2\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nThe Array collection data type is displayed as a list where the first value is the collection type and the second is the inner type. The inner type could also be a collection.\n\nExample of JSON representation of a column list of (String, Integer[]):\n\n\"column_types\": [ 4, [ 100, 9 ] ]\n\n\nIDs of all currently available data types:\n\nID\n\n\t\n\nData type\n\n\n\n\n0\n\n\t\n\nNULL\n\n\n\n\n1\n\n\t\n\nNot supported\n\n\n\n\n2\n\n\t\n\nCHAR\n\n\n\n\n3\n\n\t\n\nBOOLEAN\n\n\n\n\n4\n\n\t\n\nTEXT\n\n\n\n\n5\n\n\t\n\nIP\n\n\n\n\n6\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\n7\n\n\t\n\nREAL\n\n\n\n\n8\n\n\t\n\nSMALLINT\n\n\n\n\n9\n\n\t\n\nINTEGER\n\n\n\n\n10\n\n\t\n\nBIGINT\n\n\n\n\n11\n\n\t\n\nTIMESTAMP\n\n\n\n\n12\n\n\t\n\nOBJECT\n\n\n\n\n13\n\n\t\n\nGEO_POINT\n\n\n\n\n14\n\n\t\n\nGEO_SHAPE\n\n\n\n\n15\n\n\t\n\nUnchecked object\n\n\n\n\n19\n\n\t\n\nREGPROC\n\n\n\n\n20\n\n\t\n\nTIME\n\n\n\n\n21\n\n\t\n\nOIDVECTOR\n\n\n\n\n23\n\n\t\n\nREGCLASS\n\n\n\n\n24\n\n\t\n\nDATE\n\n\n\n\n100\n\n\t\n\nARRAY\n\nBulk operations\n\nThe REST endpoint allows to issue bulk operations which are executed as single calls on the back-end site. It can be compared to prepared statement.\n\nA bulk operation can be expressed simply as an SQL statement.\n\nSupported bulk SQL statements are:\n\nInsert\n\nUpdate\n\nDelete\n\nInstead of the args (Parameter substitution) key, use the key bulk_args. This allows to specify a list of lists, containing all the records which shall be processed. The inner lists need to match the specified columns.\n\nThe bulk response contains a results array, with a row count for each bulk operation. Those results are in the same order as the issued operations of the bulk operation.\n\nThe following example describes how to issue an insert bulk operation and insert three records at once:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\": \"INSERT INTO locations (id, name, kind, description)\n...           VALUES (?, ?, ?, ?)\",\n...   \"bulk_args\": [\n...     [1337, \"Earth\", \"Planet\", \"An awesome place to spend some time on.\"],\n...     [1338, \"Sun\", \"Star\", \"An extraordinarily hot place.\"],\n...     [1339, \"Titan\", \"Moon\", \"Titan, where it rains fossil fuels.\"]\n...   ]\n... }\n... EOF\n{\n  \"cols\": [],\n  \"duration\": ...,\n  \"results\": [\n    {\n      \"rowcount\": 1\n    },\n    {\n      \"rowcount\": 1\n    },\n    {\n      \"rowcount\": 1\n    }\n  ]\n}\n\nError handling\n\nQueries that are invalid or cannot be satisfied will result in an error response. The response will contain an error code, an error message and in some cases additional arguments that are specific to the error code.\n\nClient libraries should use the error code to translate the error into an appropriate exception:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\":\"select name, position from foo.locations\"\n... }\n... EOF\n{\n  \"error\": {\n    \"message\": \"SchemaUnknownException[Schema 'foo' unknown]\",\n    \"code\": 4045\n  }\n}\n\n\nTo get more insight into what exactly went wrong an additional error_trace GET parameter can be specified to return the stack trace:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql?error_trace=true' -d@- <<- EOF\n... {\n...   \"stmt\":\"select name, position from foo.locations\"\n... }\n... EOF\n{\n  \"error\": {\n    \"message\": \"SchemaUnknownException[Schema 'foo' unknown]\",\n    \"code\": 4045\n  },\n  \"error_trace\": \"...\"\n}\n\n\nNote\n\nThis parameter is intended for CrateDB developers or for users requesting support for CrateDB. Client libraries shouldn’t make use of this option and not include the stack trace.\n\nCurrently the defined error codes are:\n\nCode\n\n\t\n\nError\n\n\n\n\n4000\n\n\t\n\nThe statement contains an invalid syntax or unsupported SQL statement\n\n\n\n\n4001\n\n\t\n\nThe statement contains an invalid analyzer definition.\n\n\n\n\n4002\n\n\t\n\nThe name of the relation is invalid.\n\n\n\n\n4003\n\n\t\n\nField type validation failed\n\n\n\n\n4004\n\n\t\n\nPossible feature not supported (yet)\n\n\n\n\n4005\n\n\t\n\nAlter table using a table alias is not supported.\n\n\n\n\n4006\n\n\t\n\nThe used column alias is ambiguous.\n\n\n\n\n4007\n\n\t\n\nThe operation is not supported on this relation, as it is not accessible.\n\n\n\n\n4008\n\n\t\n\nThe name of the column is invalid.\n\n\n\n\n4009\n\n\t\n\nCrateDB License is expired. (Deprecated.)\n\n\n\n\n4010\n\n\t\n\nUser is not authorized to perform the SQL statement.\n\n\n\n\n4011\n\n\t\n\nMissing privilege for user.\n\n\n\n\n4031\n\n\t\n\nOnly read operations are allowed on this node.\n\n\n\n\n4041\n\n\t\n\nUnknown relation.\n\n\n\n\n4042\n\n\t\n\nUnknown analyzer.\n\n\n\n\n4043\n\n\t\n\nUnknown column.\n\n\n\n\n4044\n\n\t\n\nUnknown type.\n\n\n\n\n4045\n\n\t\n\nUnknown schema.\n\n\n\n\n4046\n\n\t\n\nUnknown Partition.\n\n\n\n\n4047\n\n\t\n\nUnknown Repository.\n\n\n\n\n4048\n\n\t\n\nUnknown Snapshot.\n\n\n\n\n4049\n\n\t\n\nUnknown user-defined function.\n\n\n\n\n40410\n\n\t\n\nUnknown user.\n\n\n\n\n4091\n\n\t\n\nA document with the same primary key exists already.\n\n\n\n\n4092\n\n\t\n\nA VersionConflict. Might be thrown if an attempt was made to update the same document concurrently.\n\n\n\n\n4093\n\n\t\n\nA relation with the same name exists already.\n\n\n\n\n4094\n\n\t\n\nThe used table alias contains tables with different schema.\n\n\n\n\n4095\n\n\t\n\nA repository with the same name exists already.\n\n\n\n\n4096\n\n\t\n\nA snapshot with the same name already exists in the repository.\n\n\n\n\n4097\n\n\t\n\nA partition for the same values already exists in this table.\n\n\n\n\n4098\n\n\t\n\nA user-defined function with the same signature already exists.\n\n\n\n\n4099\n\n\t\n\nA user with the same name already exists.\n\n\n\n\n5000\n\n\t\n\nUnhandled server error.\n\n\n\n\n5001\n\n\t\n\nThe execution of one or more tasks failed.\n\n\n\n\n5002\n\n\t\n\nOne or more shards are not available.\n\n\n\n\n5003\n\n\t\n\nThe query failed on one or more shards\n\n\n\n\n5004\n\n\t\n\nCreating a snapshot failed\n\n\n\n\n5030\n\n\t\n\nThe query was killed by a kill statement\n\nBulk errors\n\nIf a bulk operation fails, the resulting row count will be -2 and the resulting object may contain an error_message depending on the resulting error:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\": \"INSERT into locations (name, id) values (?,?)\",\n...   \"bulk_args\": [\n...     [\"Mars\", 1341],\n...     [\"Sun\", 1341]\n...   ]\n... }\n... EOF\n{\n  \"cols\": [],\n  \"duration\": ...,\n  \"results\": [\n    {\n      \"rowcount\": 1\n    },\n    {\n      \"rowcount\": -2\n    }\n  ]\n}\n\n\nNote\n\nEvery bulk operation will be executed, independent if one of the operation fails."
  },
  {
    "title": "SQL Statements — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/sql/statements/index.html",
    "html": "4.8\nSQL Statements\n\nTable of contents\n\nALTER CLUSTER\nALTER PUBLICATION\nALTER TABLE\nALTER USER\nANALYZE\nBEGIN\nSTART TRANSACTION\nCOMMIT\nCOPY FROM\nCOPY TO\nCREATE ANALYZER\nCREATE BLOB TABLE\nCREATE FUNCTION\nCREATE PUBLICATION\nCREATE REPOSITORY\nCREATE SNAPSHOT\nCREATE SUBSCRIPTION\nCREATE TABLE\nCREATE TABLE AS\nCREATE USER\nCREATE VIEW\nDEALLOCATE\nDELETE\nDENY\nDISCARD\nDROP ANALYZER\nDROP FUNCTION\nDROP PUBLICATION\nDROP REPOSITORY\nDROP SNAPSHOT\nDROP SUBSCRIPTION\nDROP TABLE\nDROP USER\nDROP VIEW\nEND\nEXPLAIN\nGRANT\nINSERT\nKILL\nOPTIMIZE\nREFRESH\nRESTORE SNAPSHOT\nREVOKE\nSELECT\nSET and RESET\nSET LICENSE\nSET AND RESET SESSION AUTHORIZATION\nSET TRANSACTION\nSHOW COLUMNS\nSHOW CREATE TABLE\nSHOW SCHEMAS\nSHOW TABLES\nSHOW (session settings)\nUPDATE\nVALUES"
  },
  {
    "title": "General SQL — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/sql/general/index.html",
    "html": "4.8\nGeneral SQL\n\nTable of contents\n\nConstraints\nValue expressions\nLexical structure"
  },
  {
    "title": "Usage Data Collector — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/admin/udc.html",
    "html": "4.8\nUsage Data Collector\n\nThe CrateDB Usage Data Collector (UDC) is a sub-system that gathers usage data, reporting it to the UDC server at https://udc.crate.io. It is easy to disable, and does not collect any data that is confidential. For more information about what is being sent, see below.\n\nCrateDB uses this information as a form of automatic, effortless feedback from the community. We want to verify that we are doing the right thing by matching download statistics with usage statistics. After each release, we can see if there is a larger retention span of the server software.\n\nThe data collected is clearly stated here. If any future versions of this system collect additional data, we will clearly announce those changes.\n\nCrateDB is concerned about your privacy. We do not disclose any personally identifiable information.\n\nTable of contents\n\nTechnical information\n\nAdmin UI tracking\n\nConfiguration\n\nHow to disable UDC\n\nBy configuration\n\nBy system property\n\nTechnical information\n\nTo gather good statistics about CrateDB usage, UDC collects this information:\n\nName\n\n\t\n\nDescription\n\n\n\n\nKernel Version\n\n\t\n\nThe build number, and if there are any modifications to the kernel.\n\n\n\n\nCluster Id\n\n\t\n\nA randomized globally unique ID created every time the whole cluster is restarted.\n\n\n\n\nMaster\n\n\t\n\nBoolean whether the current node is master.\n\n\n\n\nPing Count\n\n\t\n\nUDC holds an internal counter per node which is incremented for every ping, and reset on every restart of the a node.\n\n\n\n\nCrateDB Version\n\n\t\n\nThe CrateDB version.\n\n\n\n\nJava Version\n\n\t\n\nThe Java version CrateDB is currently running with.\n\n\n\n\nHardware Address\n\n\t\n\nMAC address to uniquely identify instances behind firewalls.\n\n\n\n\nProcessor count\n\n\t\n\nNumber of available CPUs as reported by Runtime.availableProcessors\n\n\n\n\nEnterprise\n\n\t\n\nIdentifies whether the Enterprise Edition is used. 1\n\nAfter startup, UDC waits for 10 minutes before sending the first ping. It does this for two reasons; first, we don’t want the startup to be slower because of UDC, and secondly, we want to keep pings from automatic tests to a minimum. By default, UDC is sending pings every 24 hours. The ping to the UDC servers is done with a HTTP GET.\n\nAdmin UI tracking\n\nSince Admin UI v0.16.0 we are tracking the user ID along with the cluster ID to know how many active users are currently using CrateDB.\n\nConfiguration\n\nThe Usage Data Collector can be configured by adapting the crate.yml configuration file or adding a system property setting. Refer to Usage data collector to see how these settings can be accessed and how they are configured.\n\nHow to disable UDC\n\nBelow are two ways you can disable UDC. However we hope you support us offering the open source edition, and leave UDC on, so we learn how many people use CrateDB.\n\nBy configuration\n\nJust add following to your crate.yml configuration file:\n\nudc.enabled:  false\n\nBy system property\n\nIf you do not want to make any change to the jars or to the configuration, a system property setting like this will also make sure that UDC is never activated:\n\n-Cudc.enabled=false\n\n1\n\nThe “CrateDB Enterprise Edition” has been dissolved starting with CrateDB 4.5.0, see also Farewell to the CrateDB Enterprise License."
  },
  {
    "title": "PostgreSQL wire protocol — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/interfaces/postgres.html",
    "html": "4.8\nPostgreSQL wire protocol\n\nCrateDB supports the PostgreSQL wire protocol v3.\n\nIf a node is started with PostgreSQL wire protocol support enabled it will bind to port 5432 by default. To use a custom port, set the corresponding Ports in the Configuration.\n\nHowever, even though connecting PostgreSQL tools and client libraries is supported, the actual SQL statements have to be supported by CrateDB’s SQL dialect. A notable difference is that CrateDB doesn’t support transactions, which is why clients should generally enable autocommit.\n\nNote\n\nIn order to use setFetchSize in JDBC it is possible to set auto commit to false.\n\nThe client will utilize the fetchSize on SELECT statements and only load up to fetchSize rows into memory.\n\nSee the PostgreSQL JDBC Query docs for more information.\n\nWrite operations will still behave as if auto commit was enabled and commit or rollback calls are ignored.\n\nTable of contents\n\nServer compatibility\n\nStart-up\n\nSSL Support\n\nAuthentication\n\nParameterStatus\n\nDatabase selection\n\nQuery modes\n\nSimple query\n\nExtended query\n\nCopy operations\n\nFunction call\n\nCanceling requests\n\npg_catalog\n\npg_type\n\nOID types\n\nShow transaction isolation\n\nBEGIN, START, and COMMIT statements\n\nClient compatibility\n\nJDBC\n\nLimitations\n\nConnection failover and load balancing\n\nImplementation differences\n\nCopy operations\n\nExpressions\n\nData types\n\nDates and times\n\nObjects\n\nArrays\n\nDeclaration of arrays\n\nAccessing arrays\n\nType casts\n\nText search functions and operators\n\nServer compatibility\n\nCrateDB emulates PostgreSQL server version 10.5.\n\nStart-up\nSSL Support\n\nSSL can be configured using Secured communications (SSL/TLS).\n\nAuthentication\n\nAuthentication methods can be configured using Host-Based Authentication (HBA).\n\nParameterStatus\n\nAfter the authentication succeeded, the server has the possibility to send multiple ParameterStatus messages to the client. These are used to communicate information like server_version (emulates PostgreSQL 9.5) or server_encoding.\n\nCrateDB also sends a message containing the crate_version parameter. This contains the current CrateDB version number.\n\nThis information is useful for clients to detect that they’re connecting to CrateDB instead of a PostgreSQL instance.\n\nDatabase selection\n\nSince CrateDB uses schemas instead of databases, the database parameter sets the default schema name for future queries. If no schema is specified, the schema doc will be used as default. Additionally, the only supported charset is UTF8.\n\nQuery modes\nSimple query\n\nThe PostgreSQL simple query protocol mode is fully implemented.\n\nExtended query\n\nThe PostgreSQL extended query protocol mode is implemented with the following limitations:\n\nThe ParameterDescription message works for the most common use cases except for DDL statements.\n\nTo optimize the execution of bulk operations the execution of statements is delayed until the Sync message is received\n\nCopy operations\n\nCrateDB does not support the COPY sub-protocol, see also Copy operations.\n\nFunction call\n\nThe function call sub-protocol is not supported since it’s a legacy feature.\n\nCanceling requests\n\nPostgreSQL cancelling requests is fully implemented.\n\npg_catalog\n\nFor improved compatibility, the pg_catalog schema is implemented containing following tables:\n\npg_am\n\npg_attrdef\n\npg_attribute\n\npg_class\n\npg_constraint\n\npg_database\n\npg_description\n\npg_enum\n\npg_index\n\npg_indexes\n\npg_locks\n\npg_namespace\n\npg_proc\n\npg_publication\n\npg_publication_tables\n\npg_subscription\n\npg_subscription_rel\n\npg_range\n\npg_roles\n\npg_settings\n\npg_tablespace\n\npg_type\n\npg_type\n\nSome clients require the pg_catalog.pg_type in order to be able to stream arrays or other non-primitive types.\n\nFor compatibility reasons, there is a trimmed down pg_type table available in CrateDB:\n\ncr> SELECT oid, typname, typarray, typelem, typlen, typtype, typcategory\n... FROM pg_catalog.pg_type\n... ORDER BY oid;\n+------+------------------------------+----------+---------+--------+---------+-------------+\n|  oid | typname                      | typarray | typelem | typlen | typtype | typcategory |\n+------+------------------------------+----------+---------+--------+---------+-------------+\n|   16 | bool                         |     1000 |       0 |      1 | b       | N           |\n|   18 | char                         |     1002 |       0 |      1 | b       | S           |\n|   19 | name                         |       -1 |       0 |     64 | b       | S           |\n|   20 | int8                         |     1016 |       0 |      8 | b       | N           |\n|   21 | int2                         |     1005 |       0 |      2 | b       | N           |\n|   23 | int4                         |     1007 |       0 |      4 | b       | N           |\n|   24 | regproc                      |     1008 |       0 |      4 | b       | N           |\n|   25 | text                         |     1009 |       0 |     -1 | b       | S           |\n|   26 | oid                          |     1028 |       0 |      4 | b       | N           |\n|   30 | oidvector                    |     1013 |      26 |     -1 | b       | A           |\n|  114 | json                         |      199 |       0 |     -1 | b       | U           |\n|  199 | _json                        |        0 |     114 |     -1 | b       | A           |\n|  600 | point                        |     1017 |       0 |     16 | b       | G           |\n|  700 | float4                       |     1021 |       0 |      4 | b       | N           |\n|  701 | float8                       |     1022 |       0 |      8 | b       | N           |\n| 1000 | _bool                        |        0 |      16 |     -1 | b       | A           |\n| 1002 | _char                        |        0 |      18 |     -1 | b       | A           |\n| 1005 | _int2                        |        0 |      21 |     -1 | b       | A           |\n| 1007 | _int4                        |        0 |      23 |     -1 | b       | A           |\n| 1008 | _regproc                     |        0 |      24 |     -1 | b       | A           |\n| 1009 | _text                        |        0 |      25 |     -1 | b       | A           |\n| 1015 | _varchar                     |        0 |    1043 |     -1 | b       | A           |\n| 1016 | _int8                        |        0 |      20 |     -1 | b       | A           |\n| 1017 | _point                       |        0 |     600 |     -1 | b       | A           |\n| 1021 | _float4                      |        0 |     700 |     -1 | b       | A           |\n| 1022 | _float8                      |        0 |     701 |     -1 | b       | A           |\n| 1043 | varchar                      |     1015 |       0 |     -1 | b       | S           |\n| 1082 | date                         |     1182 |       0 |      8 | b       | D           |\n| 1114 | timestamp without time zone  |     1115 |       0 |      8 | b       | D           |\n| 1115 | _timestamp without time zone |        0 |    1114 |     -1 | b       | A           |\n| 1182 | _date                        |        0 |    1082 |     -1 | b       | A           |\n| 1184 | timestamptz                  |     1185 |       0 |      8 | b       | D           |\n| 1185 | _timestamptz                 |        0 |    1184 |     -1 | b       | A           |\n| 1186 | interval                     |     1187 |       0 |     16 | b       | T           |\n| 1187 | _interval                    |        0 |    1186 |     -1 | b       | A           |\n| 1231 | _numeric                     |        0 |    1700 |     -1 | b       | A           |\n| 1266 | timetz                       |     1270 |       0 |     12 | b       | D           |\n| 1270 | _timetz                      |        0 |    1266 |     -1 | b       | A           |\n| 1560 | bit                          |     1561 |       0 |     -1 | b       | V           |\n| 1561 | _bit                         |        0 |    1560 |     -1 | b       | A           |\n| 1700 | numeric                      |     1231 |       0 |     -1 | b       | N           |\n| 2205 | regclass                     |     2210 |       0 |      4 | b       | N           |\n| 2210 | _regclass                    |        0 |    2205 |     -1 | b       | A           |\n| 2249 | record                       |     2287 |       0 |     -1 | p       | P           |\n| 2276 | any                          |        0 |       0 |      4 | p       | P           |\n| 2277 | anyarray                     |        0 |    2276 |     -1 | p       | P           |\n| 2287 | _record                      |        0 |    2249 |     -1 | p       | A           |\n+------+------------------------------+----------+---------+--------+---------+-------------+\nSELECT 47 rows in set (... sec)\n\n\nNote\n\nThis is just a snapshot of the table.\n\nCheck table information_schema.columns to get information for all supported columns.\n\nOID types\n\nObject Identifiers (OIDs) are used internally by PostgreSQL as primary keys for various system tables.\n\nCrateDB supports the oid type and the following aliases:\n\nName\n\n\t\n\nReference\n\n\t\n\nDescription\n\n\t\n\nExample\n\n\n\n\nregproc\n\n\t\n\npg_proc\n\n\t\n\nA function name\n\n\t\n\nsum\n\n\n\n\nregclass\n\n\t\n\npg_class\n\n\t\n\nA relation name\n\n\t\n\npg_type\n\nCrateDB also supports the oidvector type.\n\nNote\n\nCasting a string or an integer to the regproc type does not result in a function lookup (as it does with PostgreSQL).\n\nInstead:\n\nCasting a string to the regproc type results in an object of the regproc type with a name equal to the string value and an oid equal to an integer hash of the string.\n\nCasting an integer to the regproc type results in an object of the regproc type with a name equal to the string representation of the integer and an oid equal to the integer value.\n\nConsult the CrateDB data types reference for more information about each OID type (including additional type casting behaviour).\n\nShow transaction isolation\n\nFor compatibility with JDBC the SHOW TRANSACTION ISOLATION LEVEL statement is implemented:\n\ncr> show transaction isolation level;\n+-----------------------+\n| transaction_isolation |\n+-----------------------+\n| read uncommitted      |\n+-----------------------+\nSHOW 1 row in set (... sec)\n\nBEGIN, START, and COMMIT statements\n\nFor compatibility with clients that use the PostgresSQL wire protocol (e.g., the Golang lib/pq and pgx drivers), CrateDB will accept the BEGIN, COMMIT, and START TRANSACTION statements. For example:\n\ncr> BEGIN TRANSACTION ISOLATION LEVEL READ UNCOMMITTED,\n...                   READ ONLY,\n...                   NOT DEFERRABLE;\nBEGIN OK, 0 rows affected  (... sec)\n\ncr> COMMIT\nCOMMIT OK, 0 rows affected  (... sec)\n\n\nCrateDB will silently ignore the COMMIT, BEGIN, and START TRANSACTION statements and all respective parameters.\n\nClient compatibility\nJDBC\n\npgjdbc JDBC drivers version 9.4.1209 and above are compatible.\n\nLimitations\n\nReflection methods like conn.getMetaData().getTables(...) won’t work since the required tables are unavailable in CrateDB.\n\nAs a workaround it’s possible to use SHOW TABLES or query the information_schema tables manually using SELECT statements.\n\nOBJECT and GEO_SHAPE columns can be streamed as JSON but require pgjdbc version 9.4.1210 or newer.\n\nMultidimensional arrays will be streamed as JSON encoded string to avoid a protocol limitation where all sub-arrays are required to have the same length.\n\nThe behavior of PreparedStatement.executeBatch in error cases depends on in which stage an error occurs: A BatchUpdateException is thrown if no processing has been done yet, whereas single operations failing after the processing started are indicated by an EXECUTE_FAILED (-3) return value.\n\nTransaction limitations as described above.\n\nHaving escape processing enabled could prevent the usage of Object Literals in case an object key’s starting character clashes with a JDBC escape keyword (see also JDBC escape syntax). Disabling escape processing will remedy this appropriately for pgjdbc version >= 9.4.1212.\n\nConnection failover and load balancing\n\nConnection failover and load balancing is supported as described here: PostgreSQL JDBC connection failover.\n\nNote\n\nIt is not recommended to use the targetServerType parameter since CrateDB has no concept of master-replica nodes.\n\nImplementation differences\n\nThe PostgreSQL Wire Protocol makes it easy to use many PostgreSQL compatible tools and libraries directly with CrateDB. However, many of these tools assume that they are talking to PostgreSQL specifically, and thus rely on SQL extensions and idioms that are unique to PostgreSQL. Because of this, some tools or libraries may not work with other SQL databases such as CrateDB.\n\nCrateDB’s SQL query engine enables real-time search & aggregations for online analytic processing (OLAP) and business intelligence (BI) with the benefit of the ability to scale horizontally. The use-cases of CrateDB are different than those of PostgreSQL, as CrateDB’s specialized storage schema and query execution engine addresses different needs (see Clustering).\n\nThe features listed below cover the main differences in implementation and dialect between CrateDB and PostgreSQL. A detailed comparison between CrateDB’s SQL dialect and standard SQL is outlined in SQL compatibility.\n\nCopy operations\n\nCrateDB does not support the distinct sub-protocol that is used to serve COPY operations and provides another implementation for transferring bulk data using the COPY FROM and COPY TO statements.\n\nExpressions\n\nUnlike PostgreSQL, expressions are not evaluated if the query results in 0 rows, either because the table is empty or by not matching the WHERE clause.\n\nData types\nDates and times\n\nAt the moment, CrateDB does not support TIME without a time zone.\n\nAdditionally, CrateDB does not support the INTERVAL input units MILLENNIUM, CENTURY, DECADE, MILLISECOND, or MICROSECOND.\n\nObjects\n\nThe definition of structured values by using JSON types, composite types or HSTORE are not supported. CrateDB alternatively allows the definition of nested documents (of type OBJECT) that store fields containing any CrateDB supported data type, including nested object types.\n\nArrays\nDeclaration of arrays\n\nWhile multidimensional arrays in PostgreSQL must have matching extends for each dimension, CrateDB allows different length nested arrays as this example shows:\n\ncr> select [[1,2,3],[1,2]] from sys.cluster;\n+---------------------+\n| [[1, 2, 3], [1, 2]] |\n+---------------------+\n| [[1, 2, 3], [1, 2]] |\n+---------------------+\nSELECT 1 row in set (... sec)\n\nAccessing arrays\n\nFetching arbitrary rectangular slices of an array using lower-bound:upper-bound expression in the array subscript is not supported.\n\nSee Also\n\nPostgreSQL Arrays\n\nType casts\n\nCrateDB accepts the Type casting syntax for conversion of one data type to another.\n\nSee Also\n\nPostgreSQL value expressions\n\nCrateDB value expressions\n\nText search functions and operators\n\nThe functions and operators provided by PostgreSQL for full-text search (see PostgreSQL fulltext Search) are not compatible with those provided by CrateDB.\n\nIf you are missing features, functions or dialect improvements and have a great use case for it, let us know on GitHub. We’re always improving and extending CrateDB and we love to hear feedback."
  },
  {
    "title": "Cloud discovery — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/admin/discovery.html",
    "html": "4.8\nCloud discovery\n\nTable of contents\n\nAmazon EC2 discovery\n\nMicrosoft Azure discovery\n\nRequirements\n\nBasic configuration\n\nAuthentication\n\nAmazon EC2 discovery\n\nCrateDB has native discovery support when running a cluster with Amazon Web Services (AWS). The discovery mechanism uses the Amazon EC2 API to generate the list of hosts for the unicast host discovery (see Unicast host discovery).\n\nThere is a best practice how to configure and run a CrateDB cluster on Amazon EC2.\n\nMicrosoft Azure discovery\n\nCrateDB has native discovery support when running a cluster on Microsoft Azure infrastructure. The discovery mechanism uses the Azure Resource Management (ARM) API to generate the list of hosts for the unicast host discovery (see Unicast host discovery).\n\nThe discovery mechanism is implemented as a plugin and resides within the plugin folder of the CrateDB installation. However, since the Azure Java SDKs does have a lot of dependencies and we want to keep the plugin jar small in size the dependencies are not included in the CrateDB distribution.\n\nRequirements\n\nTo make the plugin work you have to add the following Java libraries to the $CRATE_HOME/plugins/crate-azure-discovery folder:\n\nactivation-1.1.jar\nadal4j-1.0.0.jar\nazure-core-0.9.3.jar\nazure-mgmt-compute-0.9.3.jar\nazure-mgmt-network-0.9.3.jar\nazure-mgmt-resources-0.9.3.jar\nazure-mgmt-storage-0.9.3.jar\nazure-mgmt-utility-0.9.3.jar\nbcprov-jdk15on-1.51.jar\ncommons-io-2.4.jar\ncommons-lang-2.6.jar\ncommons-lang3-3.3.1.jar\ngson-2.2.4.jar\njackson-core-asl-1.9.2.jar\njackson-jaxrs-1.9.2.jar\njackson-mapper-asl-1.9.2.jar\njackson-xc-1.9.2.jar\njavax.inject-1.jar\njaxb-api-2.2.2.jar\njaxb-impl-2.2.3-1.jar\njcip-annotations-1.0.jar\njersey-client-1.13.jar\njersey-core-1.13.jar\njersey-json-1.13.jar\njettison-1.1.jar\njson-smart-1.1.1.jar\nlang-tag-1.4.jar\nmail-1.4.7.jar\nnimbus-jose-jwt-3.1.2.jar\noauth2-oidc-sdk-4.5.jar\nstax-api-1.0-2.jar\n\n\nYou can download the libraries using a simple build.gradle file:\n\napply plugin: \"java\"\n\ndependencies {\n    compile('com.microsoft.azure:azure-mgmt-utility:0.9.3') {\n        exclude group: 'stax', module: 'stax-api'\n        exclude group: 'org.slf4j', module: 'slf4j-api'\n        exclude group: 'commons-logging', module: 'commons-logging'\n        exclude group: 'commons-codec', module: 'commons-codec'\n        exclude group: 'com.fasterxml.jackson.core', module: 'jackson-core'\n        exclude group: 'org.apache.httpcomponents', module: 'httpclient'\n    }\n}\n\ntask azureLibs (type: Copy, dependsOn: ['compileJava']) {\n  from configurations.testRuntime\n  into \"libs\"\n}\n\n\nRunning gradle azureLibs will fetch the required jars and put them into the libs/ folder from where you can copy them into the plugin folder.\n\nBasic configuration\n\nTo enable Azure discovery simply change the discovery.seed_providers setting to azure:\n\ndiscovery.seed_providers: azure\n\n\nThe discovery mechanism can discover CrateDB instances within the same vnet or the same subnet of the same resource group. By default it will the vnet.\n\nYou can change the behaviour using the discovery.azure.method setting:\n\ndiscovery.azure.method: subnet\n\n\nThe used resource group also needs to be provided:\n\ncloud.azure.management.resourcegroup.name: production\n\nAuthentication\n\nThe discovery plugin requires authentication as service principle. To do so, you have to create an Active Directory application with a password. We recommend to follow the AD Application Guide .\n\nThe configuration settings for authentication are as follows:\n\ncloud.azure.management:\n  subscription.id: my-id\n  tenant.id: my-tenant\n  app:\n    id: my-app\n    secret: my-secret\n\n\nFor a complete list of settings please refer to Discovery on Microsoft Azure."
  },
  {
    "title": "Logical replication — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/admin/logical-replication.html",
    "html": "4.8\nLogical replication\n\nTable of contents\n\nPublication\n\nSubscription\n\nSecurity\n\nMonitoring\n\nLogical replication is a method of data replication across multiple clusters. CrateDB uses a publish and subscribe model where subscribers pull data from the publications of the publisher they subscribed to.\n\nReplicated tables on a subscriber can again be published further to other clusters and thus chaining subscriptions is possible.\n\nNote\n\nA replicated index on a subscriber is read-only.\n\nLogical replication is useful for the following use cases:\n\nConsolidating data from multiple clusters into a single one for aggregated reports.\n\nEnsure high availability if one cluster becomes unavailable.\n\nReplicating between different compatible versions of CrateDB.\n\nSee Also\n\nreplication.logical.ops_batch_size replication.logical.reads_poll_duration replication.logical.recovery.chunk_size replication.logical.recovery.max_concurrent_file_chunks\n\nPublication\n\nA publication is the upstream side of logical replication and it’s created on the cluster which acts as a data source.\n\nEach table can be added to multiple publications if needed. Publications can only contain tables. All operation types (INSERT, UPDATE, DELETE and schema changes) are replicated.\n\nEvery publication can have multiple subscribers.\n\nA publication is created using the CREATE PUBLICATION command. The individual tables can be added or removed dynamically using ALTER PUBLICATION. Publications can be removed using the DROP PUBLICATION command.\n\nCaution\n\nThe publishing cluster must have soft_deletes.enabled set to true so that a subscribing cluster can catch up with all changes made during replication pauses caused by network issues or explicitly done by a user.\n\nAlso, soft_deletes.retention_lease.period should be greater than or equal to replication.logical.reads_poll_duration.\n\nSubscription\n\nA subscription is the downstream side of logical replication. A subscription defines the connection to another database and set of publications to which it wants to subscribe. By default, the subscription creation triggers the replication process on the subscriber cluster. The subscriber cluster behaves in the same way as any other CrateDB cluster and can be used as a publisher for other clusters by defining its own publications.\n\nA cluster can have multiple subscriptions. It is also possible for a cluster to have both subscriptions and publications. A cluster cannot subscribe to locally already existing tables, therefore it is not possible to setup a bi-directional replication (both sides subscribing to ALL TABLES leads to a cluster trying to replicate its own tables from another cluster). However, two clusters still can cross-subscribe to each other if one cluster subscribes to locally non-existing tables of another cluster and vice versa.\n\nA subscription is added using the CREATE SUBSCRIPTION command and can be removed using the DROP SUBSCRIPTION command. A subscription starts replicating on its creation and stops on its removal (if no failure happen in-between).\n\nPublished tables must not exist on the subscriber. A cluster cannot subscribe to a table on another cluster if it exists already on its side, therefore it’s not possible to drop and re-create a subscription without starting from scratch i.e removing all replicated tables.\n\nOnly regular tables (including partitions) may be the target of a replication. For example, you can not replicate system tables or views.\n\nThe tables are matched between the publisher and the subscriber using the fully qualified table name. Replication to differently-named tables on the subscriber is not supported.\n\nSecurity\n\nTo create, alter or drop a publication, a user must have the AL privilege on the cluster. Only the owner (the user who created the publication) or a superuser is allowed to ALTER or DROP a publication. To add tables to a publication, the user must have DQL, DML, and DDL privileges on the table. When a user creates a publication that publishes all tables automatically, only those tables where the user has DQL, DML, and DDL privileges will be published. The user a subscriber uses to connect to the publisher must have DQL privileges on the published tables. Tables, included into a publication but not available for a subscriber due to lack of DQL privilege, will not be replicated.\n\nTo create or drop a subscription, a user must have the AL privilege on the cluster. Only the owner (the user who created the subscription) or a superuser is allowed to DROP a subscription.\n\nCaution\n\nA network setup that allows the two clusters to communicate is a pre-requisite for a working publication/subscription setup. See HBA.\n\nMonitoring\n\nAll publications are listed in the pg_publication table. More details for a publication are available in the pg_publication_tables table. It lists the replicated tables for a specific publication.\n\nAll subscriptions are listed in the pg_subscription table. More details for a subscription are available in the pg_subscription_rel table. The table contains detailed information about the replication state per table, including error messages if there was an error."
  },
  {
    "title": "Snapshots — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/admin/snapshots.html",
    "html": "4.8\nSnapshots\n\nTable of contents\n\nSnapshot\n\nCreating a repository\n\nCreating a snapshot\n\nRestore\n\nRestore data granularity\n\nCleanup\n\nDropping snapshots\n\nDropping repositories\n\nSnapshot\n\nIn CrateDB, backups are called Snapshots. They represent the state of the tables in a CrateDB cluster at the time the Snapshot was created. A Snapshot is always stored in a Repository which has to be created first.\n\nCaution\n\nYou cannot snapshot BLOB tables.\n\nCreating a repository\n\nRepositories are used to store, manage and restore snapshots.\n\nThey are created using the CREATE REPOSITORY statement:\n\ncr> CREATE REPOSITORY where_my_snapshots_go TYPE fs\n... WITH (location='repo_path', compress=true);\nCREATE OK, 1 row affected (... sec)\n\n\nRepositories are uniquely identified by their name. Every repository has a specific type which determines how snapshots are stored.\n\nCrateDB supports different repository types, see Types.\n\nThe creation of a repository configures it inside the CrateDB cluster. In general no data is written, no snapshots inside repositories changed or deleted. This way you can tell the CrateDB cluster about existing repositories which already contain snapshots.\n\nCreating a repository with the same name will result in an error:\n\ncr> CREATE REPOSITORY where_my_snapshots_go TYPE fs\n... WITH (location='another_repo_path', compress=false);\nRepositoryAlreadyExistsException[Repository 'where_my_snapshots_go' already exists]\n\nCreating a snapshot\n\nSnapshots are created inside a repository and can contain any number of tables. The CREATE SNAPSHOT statement is used to create a snapshots:\n\ncr> CREATE SNAPSHOT where_my_snapshots_go.snapshot1 ALL\n... WITH (wait_for_completion=true, ignore_unavailable=true);\nCREATE OK, 1 row affected (... sec)\n\n\nA snapshot is referenced by the name of the repository and the snapshot name, separated by a dot. If ALL is used, all user created tables of the cluster (except blob tables) are stored inside the snapshot.\n\nIt’s possible to only save a specific subset of tables in the snapshot by listing them explicitly:\n\ncr> CREATE SNAPSHOT where_my_snapshots_go.snapshot2 TABLE quotes, doc.locations\n... WITH (wait_for_completion=true);\nCREATE OK, 1 row affected (... sec)\n\n\nEven single partition of Partitioned tables can be selected for backup. This is especially useful if old partitions need to be deleted but it should be possible to restore them if needed:\n\ncr> CREATE SNAPSHOT where_my_snapshots_go.snapshot3 TABLE\n...    locations,\n...    parted_table PARTITION (date='1970-01-01')\n... WITH (wait_for_completion=true);\nCREATE OK, 1 row affected (... sec)\n\n\nSnapshots are incremental. Snapshots of the same cluster created later only store data not already contained in the repository.\n\nAll examples above are used with the argument wait_for_completion set to true. As described in the CREATE REPOSITORY reference documentation, by doing this, the statement will only respond (successfully or not) when the snapshot is fully created. Otherwise the snapshot will be created in the background and the statement will immediately respond as successful. The status of a created snapshot can be retrieved by querying the sys.snapshots system table.\n\nRestore\n\nCaution\n\nIf you are restoring a snapshot into a newer version of CrateDB, be sure to check the Release Notes for upgrade instructions.\n\nCaution\n\nIf you try to restore a table that already exists, CrateDB will return an error. However, if you try to restore metadata or cluster settings that already exist, they will be overwritten.\n\nOnce a snapshot is created, it can be used to restore its tables to the state when the snapshot was created.\n\nTo get basic information about snapshots the sys.snapshots table can be queried:\n\ncr> SELECT repository, name, state, concrete_indices\n... FROM sys.snapshots\n... ORDER BY repository, name;\n+-----------------------+-----------+---------+--------------------...-+\n| repository            | name      | state   | concrete_indices       |\n+-----------------------+-----------+---------+--------------------...-+\n| where_my_snapshots_go | snapshot1 | SUCCESS | [...]                  |\n| where_my_snapshots_go | snapshot2 | SUCCESS | [...]                  |\n| where_my_snapshots_go | snapshot3 | SUCCESS | [...]                  |\n+-----------------------+-----------+---------+--------------------...-+\nSELECT 3 rows in set (... sec)\n\n\nTo restore a table from a snapshot we have to drop it beforehand:\n\ncr> DROP TABLE quotes;\nDROP OK, 1 row affected (... sec)\n\n\nRestoring a snapshot using the RESTORE SNAPSHOT statement.:\n\ncr> RESTORE SNAPSHOT where_my_snapshots_go.snapshot2\n... TABLE quotes\n... WITH (wait_for_completion=true);\nRESTORE OK, 1 row affected (... sec)\n\n\nIn this case only the quotes table from snapshot where_my_snapshots_go.snapshot2 is restored.\n\nIt’s not possible to restore tables that exist in the current cluster:\n\ncr> RESTORE SNAPSHOT where_my_snapshots_go.snapshot2 TABLE quotes;\nRelationAlreadyExists[Relation 'doc.quotes' already exists.]\n\n\nSingle partitions can be either imported into an existing partitioned table the partition belongs to.\n\nTo monitor the progress of RESTORE SNAPSHOT operations please query the sys.snapshot_restore table.\n\ncr> RESTORE SNAPSHOT where_my_snapshots_go.snapshot3 TABLE\n...    parted_table PARTITION (date='1970-01-01')\n... WITH (wait_for_completion=true);\nRESTORE OK, 1 row affected (... sec)\n\n\nOr if no matching partition table exists, it will be implicitly created during restore.\n\nCaution\n\nThis is only possible with CrateDB version 0.55.5 or greater!\n\nSnapshots of single partitions that have been created with earlier versions of CrateDB may be restored, but lead to orphaned partitions!\n\nWhen using CrateDB prior to 0.55.5 you will have to create the table schema first before restoring.\n\ncr> RESTORE SNAPSHOT where_my_snapshots_go.snapshot3 TABLE\n...    parted_table PARTITION (date=0)\n... WITH (wait_for_completion=true);\nRESTORE OK, 1 row affected (... sec)\n\nRestore data granularity\n\nYou are not limited to only being able to restore individual tables (or table partitions). For example:\n\nYou can use ALL instead of listing all tables to restore the whole snapshot, including all metadata and settings.\n\nYou can use TABLES to restore all tables but no metadata or settings. On the other hand, you can use METADATA to restore everything but tables.\n\nYou can use USERS to restore database users only. Or, you can use USERS, PRIVILIGES to restore both database users and privileges.\n\nSee the RESTORE SNAPSHOT documentation for all possible options.\n\nCleanup\nDropping snapshots\n\nDropping a snapshot deletes all files inside the repository that are only referenced by this snapshot. Due to its incremental nature this might be very few files (e.g. for intermediate snapshots). Snapshots are dropped using the DROP SNAPSHOT command:\n\ncr> DROP SNAPSHOT where_my_snapshots_go.snapshot3;\nDROP OK, 1 row affected (... sec)\n\nDropping repositories\n\nIf a repository is not needed anymore, it can be dropped using the DROP REPOSITORY statement:\n\ncr> DROP REPOSITORY \"OldRepository\";\nDROP OK, 1 row affected (... sec)\n\n\nThis statement, like CREATE REPOSITORY, does not manipulate repository contents but only deletes stored configuration for this repository in the cluster state, so it’s not accessible any more."
  },
  {
    "title": "JMX monitoring — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/admin/monitoring.html",
    "html": "4.8\nJMX monitoring\n\nThe JMX monitoring feature exposes query metrics via the JMX API.\n\nTable of contents\n\nSetup\n\nEnable collecting stats\n\nEnable the JMX API\n\nUsing Docker\n\nJMX Beans\n\nQueryStats MBean\n\nNodeStatus MBean\n\nNodeInfo MXBean\n\nConnections MBean\n\nThreadPools MXBean\n\nCircuitBreakers MXBean\n\nExposing JMX via HTTP\n\nSetup\nEnable collecting stats\n\nBy default, Collecting stats is enabled. You can disable collecting stats via the CrateDB configuration file or by running this statement:\n\ncr> SET GLOBAL \"stats.enabled\" = FALSE;\n\nEnable the JMX API\n\nTo monitor CrateDB using the JMX API, you must set the following system properties before you start CrateDB:\n\ncom.sun.management.jmxremote\ncom.sun.management.jmxremote.port=<JMX_PORT>\ncom.sun.management.jmxremote.ssl=false\ncom.sun.management.jmxremote.authenticate=false\n\n\nHere, <JMX_PORT> sets the port number of your JMX server. JMX SSL and authentication are currently not supported.\n\nMore information about the JMX monitoring properties can be found in the JMX documentation.\n\nYou can set the Java system properties with the -D option:\n\nsh$ ./bin/crate -Dcom.sun.management.jmxremote \\\n...             -Dcom.sun.management.jmxremote.port=7979 \\\n...             -Dcom.sun.management.jmxremote.ssl=false \\\n...             -Dcom.sun.management.jmxremote.authenticate=false\n\n\nHowever, the recommended way to set system properties is via the CRATE_JAVA_OPTS environment variable, like so:\n\nsh$ export CRATE_JAVA_OPTS=\"$CRATE_JAVA_OPTS \\\n      -Dcom.sun.management.jmxremote \\\n      -Dcom.sun.management.jmxremote.port=7979 \\\n      -Dcom.sun.management.jmxremote.ssl=false \\\n      -Dcom.sun.management.jmxremote.authenticate=false\"\nsh$ ./bin/crate\n\n\nIf you’re using the CrateDB Debian or RPM packages, you can set this environment variable via the /etc/default/crate configuration file.\n\nUsing Docker\n\nTo enable JMX monitoring when running CrateDB in a Docker container you have to set the following additional Java system properties:\n\n-Djava.rmi.server.hostname=<RMI_HOSTNAME>\n-Dcom.sun.management.jmxremote.rmi.port=<RMI_PORT>\n\n\nHere, <RMI_HOSTNAME> is the IP address or hostname of the Docker host and <RMI_PORT> is the statically assigned port of the RMI server. For convenience, <RMI_PORT> can be set to the same port the JMX server listens on.\n\nThe <RMI_HOSTNAME> and <RMI_PORT> can be used by JMX clients (e.g. JConsole or VisualVM) to connect to the JMX server.\n\nHere’s an example Docker command:\n\nsh> docker run -d -e CRATE_JAVA_OPTS=\"\\\n      -Dcom.sun.management.jmxremote\n      -Dcom.sun.management.jmxremote.port=7979 \\\n      -Dcom.sun.management.jmxremote.ssl=false \\\n      -Dcom.sun.management.jmxremote.authenticate=false \\\n      -Dcom.sun.management.jmxremote.rmi.port=7979 \\\n      -Djava.rmi.server.hostname=<RMI_HOSTNAME>\" \\\n      -p 7979:7979 crate -Cnetwork.host=_site_\n\n\nHere, again, <RMI_HOSTNAME> is the IP address or hostname of the Docker host.\n\nJMX Beans\nQueryStats MBean\n\nThe QueryStats MBean exposes the sum of durations, in milliseconds, total and failed count of all statements executed since the node was started, grouped by type, for SELECT, UPDATE, DELETE, INSERT, MANAGEMENT, DDL, COPY and UNDEFINED queries.\n\nMetrics can be accessed using the JMX MBean object name io.crate.monitoring:type=QueryStats and the following attributes:\n\nStatements total count since the node was started:\n\nSelectQueryTotalCount\n\nInsertQueryTotalCount\n\nUpdateQueryTotalCount\n\nDeleteQueryTotalCount\n\nManagementQueryTotalCount\n\nDDLQueryTotalCount\n\nCopyQueryTotalCount\n\nUndefinedQueryTotalCount\n\nStatements failed count since the node was started:\n\nSelectQueryFailedCount\n\nInsertQueryFailedCount\n\nUpdateQueryFailedCount\n\nDeleteQueryFailedCount\n\nManagementQueryFailedCount\n\nDDLQueryFailedCount\n\nCopyQueryFailedCount\n\nUndefinedQueryFailedCount\n\nThe sum of the durations, in milliseconds, since the node was started, of all statement executions grouped by type:\n\nSelectQuerySumOfDurations\n\nInsertQuerySumOfDurations\n\nUpdateQuerySumOfDurations\n\nDeleteQuerySumOfDurations\n\nManagementQuerySumOfDurations\n\nDDLQuerySumOfDurations\n\nCopyQuerySumOfDurations\n\nUndefinedQuerySumOfDurations\n\nNodeStatus MBean\n\nThe NodeStatus JMX MBean exposes the status of the current node as boolean values.\n\nNodeStatus can be accessed using the JMX MBean object name io.crate.monitoring:type=NodeStatus and the following attributes:\n\nReady\n\nDefines if the node is able to process SQL statements.\n\nNodeInfo MXBean\n\nThe NodeInfo JMX MXBean exposes information about the current node.\n\nNodeInfo can be accessed using the JMX MXBean object name io.crate.monitoring:type=NodeInfo and the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nNodeId\n\n\t\n\nProvides the unique identifier of the node in the cluster.\n\n\n\n\nNodeName\n\n\t\n\nProvides the human friendly name of the node.\n\n\n\n\nClusterStateVersion\n\n\t\n\nProvides the version of the current applied cluster state.\n\n\n\n\nShardStats\n\n\t\n\nStatistics about the number of shards located on the node.\n\n\n\n\nShardInfo\n\n\t\n\nDetailed information about the shards located on the node.\n\nShardStats returns a CompositeData object containing statistics about the number of shards located on the node with the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nTotal\n\n\t\n\nThe number of shards located on the node.\n\n\n\n\nPrimaries\n\n\t\n\nThe number of primary shards located on the node.\n\n\n\n\nReplicas\n\n\t\n\nThe number of replica shards located on the node.\n\n\n\n\nUnassigned\n\n\t\n\nThe number of unassigned shards in the cluster. If the node is the elected master node in the cluster, this will show the total number of unassigned shards in the cluster, otherwise 0.\n\nShardInfo returns an Array of CompositeData objects containing detailed information about the shards located on the node with the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nId\n\n\t\n\nThe shard id. This shard id is managed by the system, ranging from 0 up to the number of configured shards of the table.\n\n\n\n\nTable\n\n\t\n\nThe name of the table this shard belongs to.\n\n\n\n\nPartitionIdent\n\n\t\n\nThe partition ident of a partitioned table. Empty for non-partitioned tables.\n\n\n\n\nRoutingState\n\n\t\n\nThe current state of the shard in the routing table. Possible states are:\n\nUNASSIGNED\n\nINITIALIZING\n\nSTARTED\n\nRELOCATING\n\n\n\n\nState\n\n\t\n\nThe current state of the shard. Possible states are:\n\nCREATED\n\nRECOVERING\n\nPOST_RECOVERY\n\nSTARTED\n\nRELOCATED\n\nCLOSED\n\nINITIALIZING\n\nUNASSIGNED\n\n\n\n\nSize\n\n\t\n\nThe estimated cumulated size in bytes of all files of this shard.\n\nConnections MBean\n\nThe Connections MBean exposes information about any open connections to a CrateDB node.\n\nIt can be accessed using the io.crate.monitoring:type=Connections object name and has the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nHttpOpen\n\n\t\n\nThe number of currently established connections via HTTP\n\n\n\n\nHttpTotal\n\n\t\n\nThe number of total connections established via HTTP over the life time of a node\n\n\n\n\nPsqlOpen\n\n\t\n\nThe number of currently established connections via the PostgreSQL protocol\n\n\n\n\nPsqlTotal\n\n\t\n\nThe number of total connections established via the PostgreSQL protocol over the life time of a node\n\n\n\n\nTransportOpen\n\n\t\n\nThe number of currently established connections via the transport protocol\n\nThreadPools MXBean\n\nThe ThreadPools MXBean exposes statistical information about the used thread pools of a CrateDB node.\n\nIt can be accessed using the io.crate.monitoring:type=ThreadPools object name and has following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nGeneric\n\n\t\n\nThread pool statistics of the generic thread pool.\n\n\n\n\nSearch\n\n\t\n\nThread pool statistics of the search thread pool used by read statements on user generated tables.\n\n\n\n\nWrite\n\n\t\n\nThread pool statistics of the write thread pool used for writing and deleting data.\n\n\n\n\nManagement\n\n\t\n\nThread pool statistics of the management thread pool used by management tasks like stats collecting, repository information, shard allocations, etc.\n\n\n\n\nFlush\n\n\t\n\nThread pool statistics of the flush thread pool used for fsyncing to disk and merging segments in the storage engine.\n\n\n\n\nRefresh\n\n\t\n\nThread pool statistics of the refresh thread pool used for automatic and on-demand refreshing of tables\n\n\n\n\nSnapshot\n\n\t\n\nThread pool statistics of the snapshot thread pool used for creating and restoring snapshots.\n\n\n\n\nForceMerge\n\n\t\n\nThread pool statistics of the force_merge thread pool used when running an optimize statement.\n\n\n\n\nListener\n\n\t\n\nThread pool statistics of the listener thread pool used on client nodes for asynchronous result listeners.\n\n\n\n\nGet\n\n\t\n\nThread pool statistics of the get thread pool used when querying sys.nodes or sys.shards.\n\n\n\n\nFetchShardStarted\n\n\t\n\nThread pool statistics of the fetch_shard_started thread pool used on shard allocation .\n\n\n\n\nFetchShardStore\n\n\t\n\nThread pool statistics of the fetch_shard_store used on shard replication.\n\nEach of them returns a CompositeData object containing detailed statistics of each thread pool with the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\npoolSize\n\n\t\n\nThe current number of threads in the pool.\n\n\n\n\nlargestPoolSize\n\n\t\n\nThe largest number of threads that have ever simultaneously been in the pool.\n\n\n\n\nqueueSize\n\n\t\n\nThe current number of tasks in the queue.\n\n\n\n\nactive\n\n\t\n\nThe approximate number of threads that are actively executing tasks.\n\n\n\n\ncompleted\n\n\t\n\nThe approximate total number of tasks that have completed execution.\n\n\n\n\nrejected\n\n\t\n\nThe number of rejected executions.\n\nCircuitBreakers MXBean\n\nThe CircuitBreaker MXBean exposes statistical information about all available circuit breakers of a CrateDB node.\n\nIt can be accessed using the io.crate.monitoring:type=CircuitBreakers object name and has following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nParent\n\n\t\n\nStatistics of the parent circuit breaker containing summarized counters across all circuit breakers.\n\n\n\n\nQuery\n\n\t\n\nStatistics of the query circuit breaker used to account memory usage of SQL execution including intermediate states e.g. on aggregation and resulting rows.\n\n\n\n\nJobsLog\n\n\t\n\nStatistics of the jobs_log circuit breaker used to account memory usage of the sys.jobs_log table.\n\n\n\n\nOperationsLog\n\n\t\n\nStatistics of the operations_log circuit breaker used to account memory usage of the sys.operations_log table.\n\n\n\n\nFieldData\n\n\t\n\nStatistics of the field_data circuit breaker used for estimating the amount of memory a field will require to be loaded into memory.\n\n\n\n\nInFlightRequests\n\n\t\n\nStatistics of the in_flight_requests circuit breaker used to account memory usage of all incoming requests on transport or HTTP level.\n\n\n\n\nRequest\n\n\t\n\nStatistics of the request circuit breaker used to account memory usage of per-request data structure.\n\nEach of them returns a CompositeData object containing detailed statistics of each circuit breaker with the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nname\n\n\t\n\nThe circuit breaker name this statistic belongs to.\n\n\n\n\nused\n\n\t\n\nThe currently accounted used memory estimations.\n\n\n\n\nlimit\n\n\t\n\nThe configured limit when to trip.\n\n\n\n\noverhead\n\n\t\n\nThe configured overhead used to account estimations.\n\n\n\n\ntrippedCount\n\n\t\n\nThe total number of occurred trips.\n\nExposing JMX via HTTP\n\nThe JMX metrics and a readiness endpoint can be exposed via HTTP (e.g. to be used by Prometheus) by using the Crate JMX HTTP Exporter Java agent. See the README in the Crate JMX HTTP Exporter repository for more information."
  },
  {
    "title": "Optimization — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/admin/optimization.html",
    "html": "4.8\nOptimization\n\nTable of contents\n\nIntroduction\n\nMultiple table optimization\n\nPartition optimization\n\nSegments upgrade\n\nIntroduction\n\nIn CrateDB every table (or if partitioned every partition) consists of segments. When inserting/deleting/updating data new segments are created following as an append-only strategy, which gives the advantage of fast writes but on the other hand can result into a big number of segments. As the number of segments increases the read operations become slower since more segments need to be visited. Moreover each segment consumes file handles, memory and CPU. CrateDB solves this problem by merging segments automatically in the background. Small segments are merged into bigger segments, which, in turn, are merged into even bigger segments. Furthermore any deleted rows and documents are not copied to the new bigger segment during this process.\n\nIf required one or more tables or table partitions can be optimized explicitly in order to improve performance. A few parameters can also be configured for the optimization process, like the max number of segments you wish to have when optimization is completed, or if you only wish to merge segments with deleted data, etc. See OPTIMIZE for detailed description of parameters.\n\ncr> OPTIMIZE table locations;\nOPTIMIZE OK, 1 row affected (... sec)\n\n\nNote\n\nSystem tables cannot be optimized.\n\nMultiple table optimization\n\nIf needed, multiple tables can be defined comma-separated in a single SQL request. The result message is printed if the request on every given table is completed.\n\ncr> OPTIMIZE TABLE locations, parted_table;\nOPTIMIZE OK, 2 rows affected (... sec)\n\n\nNote\n\nIf one or more tables or partitions do not exist, none of the given tables/partitions are optimized and an error is returned. The error returns only the first non-existent table/partition.\n\nPartition optimization\n\nAdditionally it is possible to define a specific PARTITION of a partitioned table which should be optimized (see Partitioned tables).\n\nBy using the PARTITION clause in the optimize statement a separate request for a given partition can be performed. That means that only specific partitions of a partitioned table are optimized. For further details on how to create an optimize request on partitioned tables see the SQL syntax and its synopsis (see OPTIMIZE).\n\ncr> OPTIMIZE TABLE parted_table PARTITION (day='2014-04-08');\nOPTIMIZE OK, 1 row affected (... sec)\n\n\nIn case the PARTITION clause is omitted all partitions will be optimized. If a table has many partitions this should be avoided due to performance reasons.\n\nSegments upgrade\n\nIn case that some or all of the segments of a table or a table partition are created with an older version of the storage engine, then with the use of OPTIMIZE, these segments can be upgraded to the current version of the storage engine.\n\ncr> OPTIMIZE TABLE locations WITH (upgrade_segments=true);\nOPTIMIZE OK, 1 row affected (... sec)\n"
  },
  {
    "title": "Jobs management — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/admin/jobs-management.html",
    "html": "4.8\nJobs management\n\nEach executed SQL statement results in a corresponding job. Jobs that are currently executing are logged in the system table sys.jobs (see Jobs, operations, and logs).\n\nTo obtain the UUID of a job, stats needs to be enabled (see Collecting stats). Job logging can be disabled by setting the queue size to zero.\n\nKilling an active job forces CrateDB to stop its execution on the cluster immediately. There are two different SQL commands available for killing jobs.\n\nThe KILL ALL statement stops every single job on each node that is running. It returns the total number of contexts of all jobs that have been killed. A job can have contexts on multiple nodes.\n\ncr> kill all;\nKILL OK, ... rows affected (... sec)\n\n\nKILL job_id kills one single job with the specified job_id. Like KILL ALL it returns the total number of contexts of that job killed on all nodes.\n\ncr> kill '175011ce-9bbc-45f2-a86a-5b7f993a93a6';\nKILL OK, ... rows affected (... sec)\n\n\nSee KILL for detailed syntax information on KILL statements."
  },
  {
    "title": "Secured communications (SSL/TLS) — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/admin/ssl.html",
    "html": "4.8\nSecured communications (SSL/TLS)\n\nYou can encrypt the internal communication between CrateDB nodes and the external communication with HTTP and PostgreSQL clients. When you configure encryption, CrateDB secures connections using Transport Layer Security (TLS).\n\nYou can enable SSL on a per-protocol basis:\n\nIf you enable SSL for HTTP, all connections will require HTTPS.\n\nBy default, if you enable SSL for the PostgreSQL wire protocol, clients can negotiate on a per-connection basis whether to use SSL. However, you can enforce SSL via Host-Based Authentication.\n\nIf you enable SSL for the CrateDB transport protocol (used for intra-node communication), nodes only accept SSL connections (ssl.transport.mode set to on).\n\nTip\n\nYou can use on SSL mode to configure a multi-zone cluster to ensure encryption for nodes communicating between zones. Please note, that SSL has to be on in all nodes as communication is point-2-point, and intra-zone communication will also be encrypted.\n\nTable of contents\n\nSSL/TLS configuration\n\nConfiguring the Keystore\n\nConfiguring a separate Truststore\n\nConnecting to a CrateDB node using HTTPS\n\nConnect to a CrateDB node using the Admin UI\n\nConnect to a CrateDB node using Crash\n\nConnect to a CrateDB node using REST\n\nConnecting to a CrateDB node using the PostgreSQL wire protocol with SSL/TLS\n\nConnect to a CrateDB node using JDBC\n\nConnect to a CrateDB node using psql\n\nSetting up a Keystore/Truststore with a certificate chain\n\nGenerate Keystore with a private key\n\nGenerate a certificate signing request\n\nOptional: Use a self-signed certificate to act as a Certificate Authority (CA)\n\nGenerate a self-signed certificate\n\nGenerate a signed cert\n\nImport the CA certificate into the Keystore\n\nImport CA into Truststore\n\nImport the signed certificate\n\nConfiguring CrateDB\n\nSSL/TLS configuration\n\nTo enable SSL a keystore and a few configuration changes are necessary. These changes need to be made in the crate.yml file on each node that should have secure communications enabled.\n\nSkip to Generate Keystore with a private key for a step-by-step instruction on how to create a keystore.\n\nOnce the keystore (and optional truststore) is created, continue with the following steps:\n\nSet ssl.psql.enabled or ssl.http.enabled to true.\n\nSet ssl.transport.mode to on.\n\nConfiguring the Keystore\n\n(Optional) Configuring a separate Truststore\n\nNote\n\nCrateDB monitors SSL files such as keystore and truststore that are configured as values of the node settings. If any of these files are updated CrateDB dynamically reloads them. The polling frequency of the files is set via the ssl.resource_poll_interval setting.\n\nConfiguring the Keystore\n\nSSL/TLS needs a keystore. The keystore holds the node certificate(s) which should be signed by a certificate authority (CA). A third-party CA or your organization’s existing CA can be used.\n\nWhen a client connects to a node using SSL/TLS, the client receives the certificate provided by the node and will determine if the node’s certificate is valid, trusted, and matches the hostname or IP address it is trying to connect to.\n\nCaution\n\nTechnically, it’s possible to disable CA checks for certificates on the client. It is strongly recommended however to use certificates signed by an official CA or by a private CA (company PKI) that is also known to the client. This will help to ensure that establishing trust is as painless as possible.\n\nSee Generate Keystore with a private key for information about how to create a keystore.\n\nOnce the keystore is prepared, define the absolute file path to the keystore .jks file on the node using ssl.keystore_filepath setting.\n\nNote\n\nMake sure that the keystore file has the right permissions and is accessible by the system user crate.\n\nAlso, define the password needed to decrypt the keystore by using the ssl.keystore_password setting.\n\nUse ssl.keystore_key_password setting to define the key password used when creating the Keystore.\n\nFor a full list of the settings needed to configure SSL/TLS, refer to SSL configuration reference.\n\nConfiguring a separate Truststore\n\nTrusted CA certificates can be stored in a node’s keystore or a separate truststore can be used to store them.\n\nIf you want to use a separate truststore, create a node truststore and import the CA certificate(s) you want to trust. Once the truststore is prepared, define the absolute file path of the truststore .jks file on the node using the ssl.truststore_filepath setting.\n\nNote\n\nMake sure that the truststore file has the right permissions and is accessible by the system user crate.\n\nAlso define the password needed to decrypt the keystore by using the ssl.truststore_password setting.\n\nFor a full list of the settings needed to configure SSL/TLS, refer to SSL configuration reference.\n\nConnecting to a CrateDB node using HTTPS\nConnect to a CrateDB node using the Admin UI\n\nCrate’s HTTP endpoint remains unchanged. When you have turned on secure communication, it will use HTTPS instead of plain HTTP. Simply point your browser to the same URL you used before but changing the protocol to HTTPS:\n\nFor example, https://localhost:4200 becomes https://localhost:4200. If you have not configured the CrateDB node’s keystore with a signed certificate from a Certificate Authority (CA), then you will get something like the following: NET::ERR_CERT_AUTHORITY_INVALID. You either need to get your certificate signed from one of the CAs included in your browser or import your owned certificates into the browser. A third option is storing an exception for the CrateDB node certification in your browser after verifying that this is indeed a certificate you trust.\n\nConnect to a CrateDB node using Crash\n\nYou can connect to a CrateDB node using a secure communication:\n\ncrash --hosts https://localhost:4200\n\n\nTo validate the provided certificates, please see the options --verify-ssl and --key-file.\n\nConnect to a CrateDB node using REST\n\nIssue your REST requests to the node using the https:// protocol. You may have to configure your client to validate the received certificate accordingly.\n\nConnecting to a CrateDB node using the PostgreSQL wire protocol with SSL/TLS\nConnect to a CrateDB node using JDBC\n\nJDBC needs to validate the CrateDB node’s identity by checking that the node certificate is signed by a trusted authority. If the certificate is signed by a certificate authority (CA) that is known to the Java runtime, there is nothing further to do (as Java comes with copies of the most common CA’s certificates).\n\nIf you have a certificate that is signed by a CA not known to the Java runtime, you need to configure a truststore which contains the node’s certificate and provide the path to the truststore file along with the password when starting your Java application:\n\njava -Djavax.net.ssl.trustStore=mystore -Djavax.net.ssl.trustStorePassword=mypassword com.mycompany.MyApp\n\n\nIn case you face any issues extra debugging information is available by adding -Djavax.net.debug=ssl to your command line.\n\nLast but not least, the connection parameter ssl=true must be added to the connection URL so that the JDBC driver will try and establish an SSL connection.\n\nFor further information, visit JDBC SSL documentation.\n\nConnect to a CrateDB node using psql\n\nBy default, psql attempts to use SSL if available on the node. For further information including the different SSL modes please visit the PSQL documentation.\n\nSetting up a Keystore/Truststore with a certificate chain\n\nIn case you need to setup a Keystore or a Truststore, here are the commands to get you started. All the commands use a validity of 36500 days (about 100 years). You might want to use less.\n\nGenerate Keystore with a private key\n\nThe first step is to create a Keystore with a private key using the RSA algorithm. The “first and last name” is the common name (CN) which should overlap with the URL the service it is used with.\n\nCommand:\n\nkeytool -keystore keystore -genkey -keyalg RSA -alias server -validity 36500\n\n\nOutput:\n\nEnter keystore password:\nRe-enter new password:\nWhat is your first and last name?\n  [Unknown]:  ssl.crate.io\nWhat is the name of your organizational unit?\n  [Unknown]:  Cryptography Department\nWhat is the name of your organization?\n  [Unknown]:  Crate.io GmbH\nWhat is the name of your City or Locality?\n  [Unknown]:  Berlin\nWhat is the name of your State or Province?\n  [Unknown]:  Berlin\nWhat is the two-letter country code for this unit?\n  [Unknown]:  DE\nIs CN=ssl.crate.io, OU=Cryptography Department, O=Crate.io GmbH, L=Berlin, ST=Berlin, C=DE correct?\n  [no]:  yes\n\nEnter key password for <server>\n    (RETURN if same as keystore password):\nRe-enter new password:\n\nGenerate a certificate signing request\n\nTo establish trust for this key, we need to sign it. This is done by generating a certificate signing request.\n\nIf you have access to a certificate authority (CA), you can skip the next steps and get the signed certificate from the CA using the signing request which we will generate with the command below. If you don’t have access to a CA, then follow the optional steps after this step to establish your own CA.\n\nCommand:\n\nkeytool -keystore keystore -certreq -alias server -keyalg RSA -file server.csr\n\n\nOutput:\n\nEnter keystore password:\nEnter key password for <server>\n\nOptional: Use a self-signed certificate to act as a Certificate Authority (CA)\n\nNote\n\nOnly follow these optional steps if you want to create your own Certificate Authority (CA). Otherwise, please request a signed certificate from one of the CAs bundled with Java.\n\nGenerate a self-signed certificate\n\nIf you don’t get your certificate signed from one of the official CAs, you might want to create your own CA with a self-signed certificate. The common name (CN) should overlap with the CN of the server key generated in the first step. For example, ssl.crate.io overlaps with *.crate.io.\n\nNote\n\nIn this step by step guide it is shown how to create a server certificate. If you want to create a client certificate the steps are almost the same with the exception of providing a common name that is equivalent to the crate username as described in client certificate authentication method.\n\nCommand:\n\nopenssl req -x509 -sha256 -nodes -days 36500 -newkey rsa:2048 \\\n    -keyout rootCA.key -out rootCA.crt\n\n\nOutput:\n\nGenerating a 2048 bit RSA private key\n.......................................................................+++\n.............................................................+++\nwriting new private key to 'rootCA.key'\n-----\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n-----\nCountry Name (2 letter code) [AU]:AT\nState or Province Name (full name) [Some-State]:Vorarlberg\nLocality Name (eg, city) []:Dornbirn\nOrganization Name (eg, company) [Internet Widgits Pty Ltd]:Crate.io\nOrganizational Unit Name (eg, section) []:Cryptography Department\nCommon Name (e.g. server FQDN or YOUR name) []:*.crate.io\nEmail Address []:info@crate.io\n\nGenerate a signed cert\n\nIn order that the server can prove itself to have a valid and trusted domain it is required that the server certificate contains subjectAltName.\n\nCreate a file called ssl.ext with the following content. In section [alt_names] list valid domain names of the server:\n\nauthorityKeyIdentifier=keyid,issuer\nbasicConstraints=CA:FALSE\nkeyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment\nsubjectAltName = @alt_names\n\n[alt_names]\nDNS.1 = www.example.com\n\n\nNow you can generate a signed cert from our certificate signing request.\n\nCommand:\n\nopenssl x509 -req -in server.csr -CA rootCA.crt -CAkey rootCA.key \\\n    -CAcreateserial -out server.crt -sha256 -days 36500 -extfile ssl.ext\n\n\nOutput:\n\nSignature ok\nsubject=/C=DE/ST=Berlin/L=Berlin/O=Crate.io GmbH/OU=Cryptography Department/CN=ssl.crate.io\nGetting CA Private Key\n\nImport the CA certificate into the Keystore\n\nThe CA needs to be imported to the Keystore for the certificate chain to be available when we import our signed certificate.\n\nCommand:\n\nkeytool -import -keystore keystore -file rootCA.crt -alias theCARoot\n\n\nOutput:\n\nEnter keystore password:\nOwner: EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT\nIssuer: EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT\nSerial number: f13562ec6184401e\nValid from: Mon Jun 12 13:09:17 CEST 2017 until: Wed May 19 13:09:17 CEST 2117\nCertificate fingerprints:\n     MD5:  BB:A1:79:53:FE:71:EC:61:2A:19:81:E8:0E:E8:C9:81\n     SHA1: 96:66:C1:01:49:17:D1:19:FB:DB:83:86:50:3D:3D:AD:DA:F7:C6:A9\n     SHA256: 69:82:C5:24:9A:A1:AE:DF:80:29:7A:26:92:C1:A5:9F:AF:7D:03:56:CC:C3:E9:73:3B:FD:85:66:35:D6:8A:9B\n     Signature algorithm name: SHA256withRSA\n     Version: 3\n\nExtensions:\n\n#1: ObjectId: 2.5.29.35 Criticality=false\nAuthorityKeyIdentifier [\nKeyIdentifier [\n0000: CD 29 4E 07 3D C3 7C D0   16 45 FB 0A CE 8D B4 98  .)N.=....E......\n0010: B7 A8 4C 79                                        ..Ly\n]\n[EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT]\nSerialNumber: [    f13562ec 6184401e]\n]\n\n#2: ObjectId: 2.5.29.19 Criticality=false\nBasicConstraints:[\n  CA:true\n  PathLen:2147483647\n]\n\n#3: ObjectId: 2.5.29.14 Criticality=false\nSubjectKeyIdentifier [\nKeyIdentifier [\n0000: CD 29 4E 07 3D C3 7C D0   16 45 FB 0A CE 8D B4 98  .)N.=....E......\n0010: B7 A8 4C 79                                        ..Ly\n]\n]\n\nTrust this certificate? [no]:  yes\nCertificate was added to keystore\n\nImport CA into Truststore\n\nIf we are using our own CA, we should also import the certificate to the Truststore, such that it is available for clients which want to verify signatures.\n\nCommand:\n\nkeytool -import -keystore truststore -file rootCA.crt -alias theCARoot\n\n\nOutput:\n\nEnter keystore password:\nRe-enter new password:\nOwner: EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT\nIssuer: EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT\nSerial number: f13562ec6184401e\nValid from: Mon Jun 12 13:09:17 CEST 2017 until: Wed May 19 13:09:17 CEST 2117\nCertificate fingerprints:\n     MD5:  BB:A1:79:53:FE:71:EC:61:2A:19:81:E8:0E:E8:C9:81\n     SHA1: 96:66:C1:01:49:17:D1:19:FB:DB:83:86:50:3D:3D:AD:DA:F7:C6:A9\n     SHA256: 69:82:C5:24:9A:A1:AE:DF:80:29:7A:26:92:C1:A5:9F:AF:7D:03:56:CC:C3:E9:73:3B:FD:85:66:35:D6:8A:9B\n     Signature algorithm name: SHA256withRSA\n     Version: 3\n\nExtensions:\n\n#1: ObjectId: 2.5.29.35 Criticality=false\nAuthorityKeyIdentifier [\nKeyIdentifier [\n0000: CD 29 4E 07 3D C3 7C D0   16 45 FB 0A CE 8D B4 98  .)N.=....E......\n0010: B7 A8 4C 79                                        ..Ly\n]\n[EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT]\nSerialNumber: [    f13562ec 6184401e]\n]\n\n#2: ObjectId: 2.5.29.19 Criticality=false\nBasicConstraints:[\n  CA:true\n  PathLen:2147483647\n]\n\n#3: ObjectId: 2.5.29.14 Criticality=false\nSubjectKeyIdentifier [\nKeyIdentifier [\n0000: CD 29 4E 07 3D C3 7C D0   16 45 FB 0A CE 8D B4 98  .)N.=....E......\n0010: B7 A8 4C 79                                        ..Ly\n]\n]\n\nTrust this certificate? [no]:  yes\nCertificate was added to keystore\n\nImport the signed certificate\n\nNow we have a signed certificate, signed by either from a official CA or from our own CA. Let’s import it to the Keystore.\n\nCommand:\n\nkeytool -import -keystore keystore -file server.crt -alias server\n\n\nOutput:\n\nEnter keystore password:\nEnter key password for <server>\nCertificate reply was installed in keystore\n\nConfiguring CrateDB\n\nFinally, you want to supply the Keystore/Truststore in the CrateDB configuration, see Secured communications (SSL/TLS)."
  },
  {
    "title": "Authentication — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/admin/auth/index.html",
    "html": "4.8\nAuthentication\n\nTable of contents\n\nAuthentication Methods\nTrust method\nPassword authentication method\nClient certificate authentication method\nHost-Based Authentication (HBA)\nAuthentication against CrateDB\nAuthenticating as a superuser\nAuthenticating to Admin UI\nNode-to-node communication"
  },
  {
    "title": "Privileges — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/admin/privileges.html",
    "html": "4.8\nPrivileges\n\nTo execute statements, a user needs to have the required privileges.\n\nTable of contents\n\nIntroduction\n\nPrivilege types\n\nDQL\n\nDML\n\nDDL\n\nAL\n\nHierarchical inheritance of privileges\n\nBehavior of GRANT, DENY and REVOKE\n\nGRANT\n\nDENY\n\nREVOKE\n\nList privileges\n\nIntroduction\n\nCrateDB has a superuser (crate) which has the privilege to do anything. The privileges of other users have to be managed using the GRANT, DENY or REVOKE statements.\n\nThe privileges that can be granted, denied or revoked are:\n\nDQL\n\nDML\n\nDDL\n\nAL\n\nSkip to Privilege types for details.\n\nThese privileges can be granted on different levels:\n\nCLUSTER\n\nSCHEMA\n\nTABLE and VIEW\n\nSkip to Hierarchical inheritance of privileges for details.\n\nA user with AL on level CLUSTER can grant privileges they themselves have to other users as well.\n\nPrivilege types\nDQL\n\nGranting Data Query Language (DQL) privilege to a user, indicates that this user is allowed to execute SELECT, SHOW, REFRESH and COPY TO statements, as well as using the available user-defined functions, on the object for which the privilege applies.\n\nDML\n\nGranting Data Manipulation Language (DML) privilege to a user, indicates that this user is allowed to execute INSERT, COPY FROM, UPDATE and DELETE statements, on the object for which the privilege applies.\n\nDDL\n\nGranting Data Definition Language (DDL) privilege to a user, indicates that this user is allowed to execute the following statements on objects for which the privilege applies:\n\nCREATE TABLE\n\nDROP TABLE\n\nCREATE VIEW\n\nDROP VIEW\n\nCREATE FUNCTION\n\nDROP FUNCTION\n\nCREATE REPOSITORY\n\nDROP REPOSITORY\n\nCREATE SNAPSHOT\n\nDROP SNAPSHOT\n\nRESTORE SNAPSHOT\n\nALTER TABLE\n\nAL\n\nGranting Administration Language (AL) privilege to a user, enables the user to execute the following statements:\n\nCREATE USER\n\nDROP USER\n\nSET GLOBAL\n\nAll statements enabled via the AL privilege operate on a cluster level. So granting this on a schema or table level will have no effect.\n\nHierarchical inheritance of privileges\n\nPrivileges can be managed on three different levels, namely: CLUSTER, SCHEMA, and TABLE/VIEW.\n\nWhen a privilege is assigned on a certain level, the privilege will propagate down the hierarchy. Privileges defined on a lower level will always override those from a higher level:\n\n  cluster\n    ||\n  schema\n   /  \\\ntable view\n\n\nThis statement will grant DQL privilege to user riley on all the tables and functions of the doc schema:\n\ncr> GRANT DQL ON SCHEMA doc TO riley;\nGRANT OK, 1 row affected (... sec)\n\n\nThis statement will deny DQL privilege to user riley on the doc schema table doc.accounting. However, riley will still have DQL privilege on all the other tables of the doc schema:\n\ncr> DENY DQL ON TABLE doc.accounting TO riley;\nDENY OK, 1 row affected (... sec)\n\n\nNote\n\nIn CrateDB, schemas are just namespaces that are created and dropped implicitly. Therefore, when GRANT, DENY or REVOKE are invoked on a schema level, CrateDB takes the schema name provided without further validation.\n\nPrivileges can be managed on all schemas and tables of the cluster, except the information_schema.\n\nViews are on the same hierarchy with tables, i.e. a privilege on a view is gained through a GRANT on either the view itself, the schema the view belongs to, or a cluster-wide privilege. Privileges on relations which are referenced in the view do not grant any privileges on the view itself. On the contrary, even if the user does not have any privileges on a view’s referenced relations but on the view itself, the user can still access the relations through the view. For example:\n\ncr> CREATE VIEW first_customer as SELECT * from doc.accounting ORDER BY id LIMIT 1\nCREATE OK, 1 row affected (... sec)\n\n\nPreviously we had issued a DENY for user riley on doc.accounting but we can still access it through the view because we have access to it through the doc schema:\n\ncr> SELECT id from first_customer;\n+----+\n| id |\n+----+\n|  1 |\n+----+\nSELECT 1 row in set (... sec)\n\n\nSee Also\n\nViews: Privileges\n\nBehavior of GRANT, DENY and REVOKE\n\nNote\n\nYou can only grant, deny, or revoke privileges for an existing user. You must create a user and then configure privileges.\n\nGRANT\n\nTo grant a privilege to an existing user on the whole cluster, we use the GRANT SQL statement, for example:\n\ncr> GRANT DML TO wolfgang;\nGRANT OK, 1 row affected (... sec)\n\n\nDQL privilege can be granted on the sys schema to user wolfgang, like this:\n\ncr> GRANT DQL ON SCHEMA sys TO wolfgang;\nGRANT OK, 1 row affected (... sec)\n\n\nThe following statement will grant all privileges on table doc.books to user wolfgang:\n\ncr> GRANT ALL PRIVILEGES ON TABLE doc.books TO wolfgang;\nGRANT OK, 4 rows affected (... sec)\n\n\nUsing “ALL PRIVILEGES” is a shortcut to grant all the currently grantable privileges to a user.\n\nNote\n\nIf no schema is specified in the table ident, the table will be looked up in the current schema.\n\nIf a user with the username specified in the SQL statement does not exist the statement returns an error:\n\ncr> GRANT DQL TO layla;\nUserUnknownException[User 'layla' does not exist]\n\n\nTo grant ALL PRIVILEGES to user will on the cluster, we can use the following syntax:\n\ncr> GRANT ALL PRIVILEGES TO will;\nGRANT OK, 4 rows affected (... sec)\n\n\nUsing ALL PRIVILEGES is a shortcut to grant all the currently grantable privileges to a user, namely DQL, DML and DDL.\n\nPrivileges can be granted to multiple users in the same statement, like so:\n\ncr> GRANT DDL ON TABLE doc.books TO wolfgang, will;\nGRANT OK, 1 row affected (... sec)\n\nDENY\n\nTo deny a privilege to an existing user on the whole cluster, use the DENY SQL statement, for example:\n\ncr> DENY DDL TO will;\nDENY OK, 1 row affected (... sec)\n\n\nDQL privilege can be denied on the sys schema to user wolfgang like this:\n\ncr> DENY DQL ON SCHEMA sys TO wolfgang;\nDENY OK, 1 row affected (... sec)\n\n\nThe following statement will deny DQL privilege on table doc.books to user wolfgang:\n\ncr> DENY DQL ON TABLE doc.books TO wolfgang;\nDENY OK, 1 row affected (... sec)\n\n\nDENY ALL or DENY ALL PRIVILEGES will deny all privileges to a user, on the cluster it can be used like this:\n\ncr> DENY ALL TO will;\nDENY OK, 3 rows affected (... sec)\n\nREVOKE\n\nTo revoke a privilege that was previously granted or denied to a user use the REVOKE SQL statement, for example the DQL privilege that was previously denied to user wolfgang on the sys schema, can be revoked like this:\n\ncr> REVOKE DQL ON SCHEMA sys FROM wolfgang;\nREVOKE OK, 1 row affected (... sec)\n\n\nThe privileges that were granted and denied to user wolfgang on doc.books can be revoked like this:\n\ncr> REVOKE ALL ON TABLE doc.books FROM wolfgang;\nREVOKE OK, 4 rows affected (... sec)\n\n\nThe privileges that were granted to user will on the cluster can be revoked like this:\n\ncr> REVOKE ALL FROM will;\nREVOKE OK, 4 rows affected (... sec)\n\n\nNote\n\nThe REVOKE statement can remove only privileges that have been granted or denied through the GRANT or DENY statements. If the privilege on a specific object was not explicitly granted, the REVOKE statement has no effect. The effect of the REVOKE statement will be reflected in the row count.\n\nList privileges\n\nCrateDB exposes privileges sys.privileges system table.\n\nBy querying the sys.privileges table you can get all information regarding the existing privileges. E.g.:\n\ncr> SELECT * FROM sys.privileges order by grantee, class, ident;\n+---------+----------+---------+----------------+-------+------+\n| class   | grantee  | grantor | ident          | state | type |\n+---------+----------+---------+----------------+-------+------+\n| SCHEMA  | riley    | crate   | doc            | GRANT | DQL  |\n| TABLE   | riley    | crate   | doc.accounting | DENY  | DQL  |\n| TABLE   | will     | crate   | doc.books      | GRANT | DDL  |\n| CLUSTER | wolfgang | crate   | NULL           | GRANT | DML  |\n+---------+----------+---------+----------------+-------+------+\nSELECT 4 rows in set (... sec)\n\n\nThe column grantor shows the user who granted or denied the privilege, the column grantee shows the user for whom the privilege was granted or denied. The column class identifies on which type of context the privilege applies. ident stands for the ident of the object that the privilege is set on and finally type stands for the type of privileges that was granted or denied."
  },
  {
    "title": "Runtime configuration — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/admin/runtime-config.html",
    "html": "4.8\nRuntime configuration\n\nThe CrateDB cluster can be configured at runtime using the SET and RESET statement. See the Cluster Settings configuration section for details about the supported settings.\n\nIf SET is used with PERSISTENT the change will survive a cluster restart, if used with TRANSIENT the value will reset to the default value or to the value in the configuration file on a restart.\n\ncr> SET GLOBAL PERSISTENT stats.enabled = false;\nSET OK, 1 row affected (... sec)\n\ncr> select sys.cluster.settings['stats']['enabled'] from sys.cluster;\n+------------------------------+\n| settings['stats']['enabled'] |\n+------------------------------+\n| FALSE                        |\n+------------------------------+\nSELECT 1 row in set (... sec)\n\n\nYou can change multiple values at once:\n\ncr> SET GLOBAL TRANSIENT stats.enabled = true,\n... stats.jobs_log_size = 1024, stats.operations_log_size = 4096;\nSET OK, 1 row affected (... sec)\n\ncr> select settings['stats']['enabled'],\n...   settings['stats']['jobs_log_size'],\n...   settings['stats']['operations_log_size']\n... from sys.cluster;\n+-...------------+-...------------------+-...------------------------+\n| ...['enabled'] | ...['jobs_log_size'] | ...['operations_log_size'] |\n+-...------------+-...------------------+-...------------------------+\n| TRUE           |                 1024 |                       4096 |\n+-...------------+-...------------------+-...------------------------+\nSELECT 1 row in set (... sec)\n\n\nIts also possible to save a complete nested object of settings:\n\ncr> SET GLOBAL TRANSIENT stats = {\n...   jobs_log_size = 2048,\n...   operations_log_size = 8192\n... };\nSET OK, 1 row affected (... sec)\n\ncr> SELECT\n...   settings['stats']['jobs_log_size'] AS jobs_size,\n...   settings['stats']['operations_log_size'] AS op_size\n... FROM sys.cluster;\n+-----------+---------+\n| jobs_size | op_size |\n+-----------+---------+\n|      2048 |    8192 |\n+-----------+---------+\nSELECT 1 row in set (... sec)\n\n\nUsing the RESET statement, a setting will be reset to either on node startup defined configuration file value or to its default value:\n\ncr> RESET GLOBAL stats.enabled, stats.operations_log_size;\nRESET OK, 1 row affected (... sec)\n\ncr> SELECT\n...   settings['stats']['jobs_log_size'] AS jobs_size,\n...   settings['stats']['operations_log_size'] AS op_size\n... FROM sys.cluster;\n+-----------+---------+\n| jobs_size | op_size |\n+-----------+---------+\n|      2048 |   10000 |\n+-----------+---------+\nSELECT 1 row in set (... sec)\n\n\nRESET can also be done on objects:\n\ncr> RESET GLOBAL stats;\nRESET OK, 1 row affected (... sec)\n\ncr> SELECT\n...   settings['stats']['jobs_log_size'] AS jobs_size,\n...   settings['stats']['operations_log_size'] AS op_size\n... FROM sys.cluster;\n+-----------+---------+\n| jobs_size | op_size |\n+-----------+---------+\n|     10000 |   10000 |\n+-----------+---------+\nSELECT 1 row in set (... sec)\n"
  },
  {
    "title": "System information — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/admin/system-information.html",
    "html": "4.8\nSystem information\n\nCrateDB provides the sys schema which contains virtual tables. These tables are read-only and can be queried to get statistical real-time information about the cluster, its nodes and their shards:\n\nTable of contents\n\nCluster\n\nCluster license\n\nlicense\n\nCluster settings\n\nNodes\n\nid\n\nname\n\nhostname\n\nrest_url\n\nport\n\nload\n\nmem\n\nheap\n\nversion\n\ncluster_state_version\n\nfs\n\nthread_pools\n\nos\n\ncgroup limitations\n\nUptime limitations\n\nos_info\n\nnetwork\n\nconnections\n\nprocess\n\nNode checks\n\nAcknowledge failed checks\n\nDescription of checked node settings\n\nRecovery expected data nodes\n\nRecovery after data nodes\n\nRecovery after time\n\nRouting allocation disk watermark high\n\nRouting allocation disk watermark low\n\nMaximum shards per node\n\nShards\n\nTable schema\n\nExample\n\nSegments\n\nJobs, operations, and logs\n\nJobs\n\nTable schema\n\nJobs metrics\n\nsys.jobs_metrics Table schema\n\nClassification\n\nOperations\n\nTable schema\n\nLogs\n\nsys.jobs_log Table schema\n\nsys.operations_log Table schema\n\nCluster checks\n\nCurrent Checks\n\nNumber of partitions\n\nTables need to be recreated\n\nCrateDB table version compatibility scheme\n\nAvoiding reindex using partitioned tables\n\nHow to reindex\n\nLicense check\n\nHealth\n\nHealth definition\n\nRepositories\n\nSnapshots\n\nSnapshot Restore\n\nSummits\n\nUsers\n\nAllocations\n\nShard table permissions\n\nsys jobs tables permissions\n\npg_stats\n\npg_publication\n\npg_publication_tables\n\npg_subscription\n\npg_subscription_rel\n\nCluster\n\nBasic information about the CrateDB cluster can be retrieved from the sys.cluster table:\n\nName\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nA unique ID generated by the system.\n\n\t\n\nTEXT\n\n\n\n\nlicense\n\n\t\n\nThe current CrateDB license information. Always NULL. This exists for backward compatibility\n\n\t\n\nOBJECT\n\n\n\n\nname\n\n\t\n\nThe cluster name.\n\n\t\n\nTEXT\n\n\n\n\nmaster_node\n\n\t\n\nNode ID of the node which currently operates as master\n\n\t\n\nTEXT\n\n\n\n\nsettings\n\n\t\n\nThe cluster settings.\n\n\t\n\nOBJECT\n\nThe result has at most 1 row:\n\ncr> select name from sys.cluster;\n+-----------------+\n| name            |\n+-----------------+\n| Testing-CrateDB |\n+-----------------+\nSELECT 1 row in set (... sec)\n\nCluster license\n\nThe sys.cluster.license expression returns information about the currently registered license.\n\nNote\n\nLicenses were removed in CrateDB 4.5. Accordingly, these values are deprecated and return NULL in CrateDB 4.5 and higher.\n\nlicense\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nlicense\n\n\t\nThe current CrateDB license information\n\nor NULL on CrateDB CE.\n\n\t\n\nOBJECT\n\n\n\n\nlicense['expiry_date']\n\n\t\n\nThe Dates and times on which the license expires.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nlicense['issued_to']\n\n\t\n\nThe organisation for which the license is issued.\n\n\t\n\nTEXT\n\n\n\n\nlicense['max_nodes']\n\n\t\n\nThe maximum number of nodes the license is valid for.\n\n\t\n\nINTEGER\n\nCluster settings\n\nThe sys.cluster.settings expression returns information about the currently applied cluster settings.\n\ncr> select settings from sys.cluster;\n+-------------------------------------------------------...-+\n| settings                                                  |\n+-------------------------------------------------------...-+\n| {\"bulk\": {...}, \"cluster\": {...}, \"discovery\": {...}, ... |\n+-------------------------------------------------------...-+\nSELECT 1 row in set (... sec)\n\ncr> select column_name, data_type from information_schema.columns\n... where column_name like 'settings%'\n... and table_name = 'cluster';\n+-----------------------------------------------------------------------------------+------------------+\n| column_name                                                                       | data_type        |\n+-----------------------------------------------------------------------------------+------------------+\n| settings                                                                          | object           |\n| settings['bulk']                                                                  | object           |\n| settings['bulk']['request_timeout']                                               | text             |\n| settings['cluster']                                                               | object           |\n| settings['cluster']['graceful_stop']                                              | object           |\n| settings['cluster']['graceful_stop']['force']                                     | boolean          |\n| settings['cluster']['graceful_stop']['min_availability']                          | text             |\n| settings['cluster']['graceful_stop']['timeout']                                   | text             |\n| settings['cluster']['info']                                                       | object           |\n| settings['cluster']['info']['update']                                             | object           |\n| settings['cluster']['info']['update']['interval']                                 | text             |\n| settings['cluster']['max_shards_per_node']                                        | integer          |\n| settings['cluster']['routing']                                                    | object           |\n| settings['cluster']['routing']['allocation']                                      | object           |\n| settings['cluster']['routing']['allocation']['allow_rebalance']                   | text             |\n| settings['cluster']['routing']['allocation']['balance']                           | object           |\n| settings['cluster']['routing']['allocation']['balance']['index']                  | real             |\n| settings['cluster']['routing']['allocation']['balance']['shard']                  | real             |\n| settings['cluster']['routing']['allocation']['balance']['threshold']              | real             |\n| settings['cluster']['routing']['allocation']['cluster_concurrent_rebalance']      | integer          |\n| settings['cluster']['routing']['allocation']['disk']                              | object           |\n| settings['cluster']['routing']['allocation']['disk']['threshold_enabled']         | boolean          |\n| settings['cluster']['routing']['allocation']['disk']['watermark']                 | object           |\n| settings['cluster']['routing']['allocation']['disk']['watermark']['flood_stage']  | text             |\n| settings['cluster']['routing']['allocation']['disk']['watermark']['high']         | text             |\n| settings['cluster']['routing']['allocation']['disk']['watermark']['low']          | text             |\n| settings['cluster']['routing']['allocation']['enable']                            | text             |\n| settings['cluster']['routing']['allocation']['exclude']                           | object           |\n| settings['cluster']['routing']['allocation']['exclude']['_host']                  | text             |\n| settings['cluster']['routing']['allocation']['exclude']['_id']                    | text             |\n| settings['cluster']['routing']['allocation']['exclude']['_ip']                    | text             |\n| settings['cluster']['routing']['allocation']['exclude']['_name']                  | text             |\n| settings['cluster']['routing']['allocation']['include']                           | object           |\n| settings['cluster']['routing']['allocation']['include']['_host']                  | text             |\n| settings['cluster']['routing']['allocation']['include']['_id']                    | text             |\n| settings['cluster']['routing']['allocation']['include']['_ip']                    | text             |\n| settings['cluster']['routing']['allocation']['include']['_name']                  | text             |\n| settings['cluster']['routing']['allocation']['node_concurrent_recoveries']        | integer          |\n| settings['cluster']['routing']['allocation']['node_initial_primaries_recoveries'] | integer          |\n| settings['cluster']['routing']['allocation']['require']                           | object           |\n| settings['cluster']['routing']['allocation']['require']['_host']                  | text             |\n| settings['cluster']['routing']['allocation']['require']['_id']                    | text             |\n| settings['cluster']['routing']['allocation']['require']['_ip']                    | text             |\n| settings['cluster']['routing']['allocation']['require']['_name']                  | text             |\n| settings['cluster']['routing']['allocation']['total_shards_per_node']             | integer          |\n| settings['cluster']['routing']['rebalance']                                       | object           |\n| settings['cluster']['routing']['rebalance']['enable']                             | text             |\n| settings['discovery']                                                             | object           |\n| settings['discovery']['zen']                                                      | object           |\n| settings['discovery']['zen']['publish_timeout']                                   | text             |\n| settings['gateway']                                                               | object           |\n| settings['gateway']['expected_data_nodes']                                        | integer          |\n| settings['gateway']['expected_nodes']                                             | integer          |\n| settings['gateway']['recover_after_data_nodes']                                   | integer          |\n| settings['gateway']['recover_after_nodes']                                        | integer          |\n| settings['gateway']['recover_after_time']                                         | text             |\n| settings['indices']                                                               | object           |\n| settings['indices']['breaker']                                                    | object           |\n| settings['indices']['breaker']['fielddata']                                       | object           |\n| settings['indices']['breaker']['fielddata']['limit']                              | text             |\n| settings['indices']['breaker']['fielddata']['overhead']                           | double precision |\n| settings['indices']['breaker']['query']                                           | object           |\n| settings['indices']['breaker']['query']['limit']                                  | text             |\n| settings['indices']['breaker']['query']['overhead']                               | double precision |\n| settings['indices']['breaker']['request']                                         | object           |\n| settings['indices']['breaker']['request']['limit']                                | text             |\n| settings['indices']['breaker']['request']['overhead']                             | double precision |\n| settings['indices']['breaker']['total']                                           | object           |\n| settings['indices']['breaker']['total']['limit']                                  | text             |\n| settings['indices']['recovery']                                                   | object           |\n| settings['indices']['recovery']['internal_action_long_timeout']                   | text             |\n| settings['indices']['recovery']['internal_action_timeout']                        | text             |\n| settings['indices']['recovery']['max_bytes_per_sec']                              | text             |\n| settings['indices']['recovery']['recovery_activity_timeout']                      | text             |\n| settings['indices']['recovery']['retry_delay_network']                            | text             |\n| settings['indices']['recovery']['retry_delay_state_sync']                         | text             |\n| settings['indices']['replication']                                                | object           |\n| settings['indices']['replication']['retry_timeout']                               | text             |\n| settings['logger']                                                                | object_array     |\n| settings['logger']['level']                                                       | text_array       |\n| settings['logger']['name']                                                        | text_array       |\n| settings['memory']                                                                | object           |\n| settings['memory']['allocation']                                                  | object           |\n| settings['memory']['allocation']['type']                                          | text             |\n| settings['overload_protection']                                                   | object           |\n| settings['overload_protection']['dml']                                            | object           |\n| settings['overload_protection']['dml']['initial_concurrency']                     | integer          |\n| settings['overload_protection']['dml']['max_concurrency']                         | integer          |\n| settings['overload_protection']['dml']['min_concurrency']                         | integer          |\n| settings['overload_protection']['dml']['queue_size']                              | integer          |\n| settings['replication']                                                           | object           |\n| settings['replication']['logical']                                                | object           |\n| settings['replication']['logical']['ops_batch_size']                              | integer          |\n| settings['replication']['logical']['reads_poll_duration']                         | text             |\n| settings['replication']['logical']['recovery']                                    | object           |\n| settings['replication']['logical']['recovery']['chunk_size']                      | text             |\n| settings['replication']['logical']['recovery']['max_concurrent_file_chunks']      | integer          |\n| settings['stats']                                                                 | object           |\n| settings['stats']['breaker']                                                      | object           |\n| settings['stats']['breaker']['log']                                               | object           |\n| settings['stats']['breaker']['log']['jobs']                                       | object           |\n| settings['stats']['breaker']['log']['jobs']['limit']                              | text             |\n| settings['stats']['breaker']['log']['jobs']['overhead']                           | double precision |\n| settings['stats']['breaker']['log']['operations']                                 | object           |\n| settings['stats']['breaker']['log']['operations']['limit']                        | text             |\n| settings['stats']['breaker']['log']['operations']['overhead']                     | double precision |\n| settings['stats']['enabled']                                                      | boolean          |\n| settings['stats']['jobs_log_expiration']                                          | text             |\n| settings['stats']['jobs_log_filter']                                              | text             |\n| settings['stats']['jobs_log_persistent_filter']                                   | text             |\n| settings['stats']['jobs_log_size']                                                | integer          |\n| settings['stats']['operations_log_expiration']                                    | text             |\n| settings['stats']['operations_log_size']                                          | integer          |\n| settings['stats']['service']                                                      | object           |\n| settings['stats']['service']['interval']                                          | text             |\n| settings['udc']                                                                   | object           |\n| settings['udc']['enabled']                                                        | boolean          |\n| settings['udc']['initial_delay']                                                  | text             |\n| settings['udc']['interval']                                                       | text             |\n| settings['udc']['url']                                                            | text             |\n+-----------------------------------------------------------------------------------+------------------+\nSELECT ... rows in set (... sec)\n\n\nFor further details, see the Cluster Settings configuration section.\n\nNodes\n\nTo get information about the nodes query for sys.nodes.\n\nThis table can be queried for one, multiple or all nodes within a cluster.\n\nThe table schema is as follows:\n\nid\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nA unique ID within the cluster generated by the system.\n\n\t\n\nTEXT\n\nname\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nname\n\n\t\n\nThe node name within a cluster. The system will choose a random name. You can also customize the node name, see Node-specific settings.\n\n\t\n\nTEXT\n\nhostname\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nhostname\n\n\t\n\nThe specified host name of the machine the node is running on.\n\n\t\n\nTEXT\n\nrest_url\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nrest_url\n\n\t\n\nFull HTTP(s) address where the REST API of the node is exposed, including schema, hostname (or IP) and port.\n\n\t\n\nTEXT\n\nport\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nport\n\n\t\n\nThe specified ports for both HTTP and binary transport interfaces. You can also customize the ports setting, see Ports.\n\n\t\n\nOBJECT\n\n\n\n\nport['http']\n\n\t\n\nCrateDB’s HTTP port.\n\n\t\n\nINTEGER\n\n\n\n\nport['transport']\n\n\t\n\nCrateDB’s binary transport port.\n\n\t\n\nINTEGER\n\n\n\n\nport['psql']\n\n\t\n\nThe PostgreSQL wire protocol port.\n\n\t\n\nINTEGER\n\nload\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nload\n\n\t\n\nSystem load statistics\n\n\t\n\nOBJECT\n\n\n\n\nload['1']\n\n\t\n\nAverage load over the last 1 minute.\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\nload['5']\n\n\t\n\nAverage load over the last 5 minutes.\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\nload['15']\n\n\t\n\nAverage load over the last 15 minutes.\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\nload['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the load probe.\n\n\t\n\nBIGINT\n\nmem\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nmem\n\n\t\n\nMemory utilization statistics of the host.\n\n\t\n\nOBJECT\n\n\n\n\nmem['used']\n\n\t\n\nCurrently used memory in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nmem['used_percent']\n\n\t\n\nCurrently used memory in percent of total.\n\n\t\n\nSMALLINT\n\n\n\n\nmem['free']\n\n\t\n\nCurrently available memory in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nmem['free_percent']\n\n\t\n\nCurrently available memory in percent of total.\n\n\t\n\nSMALLINT\n\n\n\n\nmem['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the memory probe.\n\n\t\n\nBIGINT\n\nheap\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nheap\n\n\t\n\nHeap memory utilization statistics.\n\n\t\n\nOBJECT\n\n\n\n\nheap['used']\n\n\t\n\nCurrently used heap memory in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nheap['max']\n\n\t\n\nMaximum available heap memory. You can specify the max heap memory CrateDB should use in the Configuration.\n\n\t\n\nBIGINT\n\n\n\n\nheap['free']\n\n\t\n\nCurrently available heap memory in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nheap['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the heap probe.\n\n\t\n\nBIGINT\n\nversion\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nversion\n\n\t\n\nCrateDB version information.\n\n\t\n\nOBJECT\n\n\n\n\nversion['number']\n\n\t\n\nVersion string in format \"major.minor.hotfix\"\n\n\t\n\nTEXT\n\n\n\n\nversion['build_hash']\n\n\t\n\nSHA hash of the GitHub commit which this build was built from.\n\n\t\n\nTEXT\n\n\n\n\nversion['build_snapshot']\n\n\t\n\nIndicates whether this build is a snapshot build.\n\n\t\n\nBOOLEAN\n\n\n\n\nversion['minimum_index_compatibility_version']\n\n\t\n\nIndicates the minimum compatible index version which is supported.\n\n\t\n\nTEXT\n\n\n\n\nversion['minimum_wire_compatibility_version']\n\n\t\n\nIndicates the minimum compatible wire protocol version which is supported.\n\n\t\n\nTEXT\n\ncluster_state_version\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\ncluster_state_version\n\n\t\n\nThe current version of the cluster state. The cluster state is an immutable structure and that is recreated when a change is published.\n\n\t\n\nBIGINT\n\nfs\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nfs\n\n\t\n\nUtilization statistics about the file system.\n\n\t\n\nOBJECT\n\n\n\n\nfs['total']\n\n\t\n\nAggregated usage statistic of all disks on the host.\n\n\t\n\nOBJECT\n\n\n\n\nfs['total']['size']\n\n\t\n\nTotal size of all disks in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['used']\n\n\t\n\nTotal used space of all disks in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['available']\n\n\t\n\nTotal available space of all disks in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['reads']\n\n\t\n\nTotal number of reads on all disks.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['bytes_read']\n\n\t\n\nTotal size of reads on all disks in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['writes']\n\n\t\n\nTotal number of writes on all disks.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['bytes_written']\n\n\t\n\nTotal size of writes on all disks in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['disks']\n\n\t\n\nUsage statistics of individual disks on the host.\n\n\t\n\nARRAY\n\n\n\n\nfs['disks']['dev']\n\n\t\n\nDevice name\n\n\t\n\nTEXT\n\n\n\n\nfs['disks']['size']\n\n\t\n\nTotal size of the disk in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['disks']['used']\n\n\t\n\nUsed space of the disk in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['disks']['available']\n\n\t\n\nAvailable space of the disk in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['data']\n\n\t\n\nInformation about data paths used by the node.\n\n\t\n\nARRAY\n\n\n\n\nfs['data']['dev']\n\n\t\n\nDevice name\n\n\t\n\nTEXT\n\n\n\n\nfs['data']['path']\n\n\t\n\nFile path where the data of the node resides.\n\n\t\n\nTEXT\n\nthread_pools\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nthread_pools\n\n\t\n\nUsage statistics of Java thread pools.\n\n\t\n\nARRAY\n\n\n\n\nthread_pools['name']\n\n\t\n\nName of the pool.\n\n\t\n\nTEXT\n\n\n\n\nthread_pools['active']\n\n\t\n\nNumber of currently running thread in the thread pool.\n\n\t\n\nINTEGER\n\n\n\n\nthread_pools['rejected']\n\n\t\n\nTotal number of rejected threads in the thread pool.\n\n\t\n\nBIGINT\n\n\n\n\nthread_pools['largest']\n\n\t\n\nLargest number of threads that have ever simultaneously been in the pool.\n\n\t\n\nINTEGER\n\n\n\n\nthread_pools['completed']\n\n\t\n\nTotal number of completed thread in the thread pool.\n\n\t\n\nBIGINT\n\n\n\n\nthread_pools['threads']\n\n\t\n\nSize of the thread pool.\n\n\t\n\nINTEGER\n\n\n\n\nthread_pools['queue']\n\n\t\n\nNumber of thread currently in the queue.\n\n\t\n\nINTEGER\n\nos\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nos\n\n\t\n\nOperating system stats\n\n\t\n\nOBJECT\n\n\n\n\nos['uptime']\n\n\t\n\nSystem uptime in milliseconds\n\nRequires allowing system calls on Windows and macOS. See notes in Uptime limitations.\n\n\t\n\nBIGINT\n\n\n\n\nos['timestamp']\n\n\t\n\nUNIX timestamp in millisecond resolution\n\n\t\n\nBIGINT\n\n\n\n\nos['cpu']\n\n\t\n\nInformation about CPU utilization\n\n\t\n\nOBJECT\n\n\n\n\nos['cpu']['used']\n\n\t\n\nSystem CPU usage as percentage\n\n\t\n\nSMALLINT\n\n\n\n\nos['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the OS probe.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']\n\n\t\n\nInformation about cgroups (Linux only)\n\n\t\n\nOBJECT\n\n\n\n\nos['cgroup']['cpuacct']\n\n\t\n\nInformation about CPU accounting\n\n\t\n\nOBJECT\n\n\n\n\nos['cgroup']['cpuacct']['control_group']\n\n\t\n\nThe path to the CPU accounting cgroup\n\n\t\n\nTEXT\n\n\n\n\nos['cgroup']['cpuacct']['usage_nanos']\n\n\t\n\nThe total CPU time (in nanoseconds) consumed by all tasks in this cgroup.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['cpu']\n\n\t\n\nInformation about the CPU subsystem\n\n\t\n\nOBJECT\n\n\n\n\nos['cgroup']['cpu']['control_group']\n\n\t\n\nThe path to the CPU cgroup\n\n\t\n\nTEXT\n\n\n\n\nos['cgroup']['cpu']['cfs_period_micros']\n\n\t\n\nThe period of time (in microseconds) the cgroup access to the CPU gets reallocated.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['cpu']['cfs_quota_micros']\n\n\t\n\nThe total amount of time (in microseconds) for which all tasks in the cgroup can run during one period (cfs_period_micros).\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['cpu']['num_elapsed_periods']\n\n\t\n\nThe nr. of period intervals (cfs_period_micros) that have elapsed.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['cpu']['num_times_throttled']\n\n\t\n\nThe nr. of times tasks in the cgroup have been throttled.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['cpu']['time_throttled_nanos']\n\n\t\n\nThe total time (in nanoseconds) for which tasks in the cgroup have been throttled.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['mem']\n\n\t\n\nInformation about memory resources used by tasks in a cgroup.\n\n\t\n\nOBJECT\n\n\n\n\nos['cgroup']['mem']['control_group']\n\n\t\n\nThe path to the memory cgroup\n\n\t\n\nTEXT\n\n\n\n\nos['cgroup']['mem']['usage_bytes']\n\n\t\n\nThe total current memory usage by processes in the cgroup.\n\n\t\n\nTEXT\n\n\n\n\nos['cgroup']['mem']['limit_bytes']\n\n\t\n\nThe max. amount of user memory in the cgroup.\n\n\t\n\nTEXT\n\nThe CPU information values are cached for 1s. They might differ from the actual values at query time. Use the probe timestamp to get the time of collection. When analyzing the CPU usage over time, always use os['probe_timestamp'] to calculate the time difference between 2 probes.\n\ncgroup limitations\n\nNote\n\ncgroup metrics only work if the stats are available from /sys/fs/cgroup/cpu and /sys/fs/cgroup/cpuacct.\n\nUptime limitations\n\nNote\n\nos[‘uptime’] required a system call when running CrateDB on Windows or macOS, however, system calls are not permitted by default. If you require this metric you need to allow system calls by setting bootstrap.seccomp to false. This setting must be set in the crate.yml or via command line argument and cannot be changed at runtime.\n\nos_info\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nos_info\n\n\t\n\nOperating system information\n\n\t\n\nOBJECT\n\n\n\n\nos_info['available_processors']\n\n\t\n\nNumber of processors that are available in the JVM. This is usually equal to the number of cores of the CPU.\n\n\t\n\nINTEGER\n\n\n\n\nos_info['name']\n\n\t\n\nName of the operating system (ex: Linux, Windows, macOS)\n\n\t\n\nTEXT\n\n\n\n\nos_info['arch']\n\n\t\n\nName of the JVM architecture (ex: amd64, x86)\n\n\t\n\nTEXT\n\n\n\n\nos_info['version']\n\n\t\n\nVersion of the operating system\n\n\t\n\nTEXT\n\n\n\n\nos_info['jvm']\n\n\t\n\nInformation about the JVM (Java Virtual Machine)\n\n\t\n\nOBJECT\n\n\n\n\nos_info['jvm']['version']\n\n\t\n\nThe JVM version\n\n\t\n\nTEXT\n\n\n\n\nos_info['jvm']['vm_name']\n\n\t\n\nThe name of the JVM (e.g. OpenJDK, Java HotSpot(TM) )\n\n\t\n\nTEXT\n\n\n\n\nos_info['jvm']['vm_vendor']\n\n\t\n\nThe vendor name of the JVM\n\n\t\n\nTEXT\n\n\n\n\nos_info['jvm']['vm_version']\n\n\t\n\nThe version of the JVM\n\n\t\n\nTEXT\n\nnetwork\n\nNetwork statistics are deprecated in CrateDB 2.3 and may completely be removed in subsequent versions. All BIGINT columns always return 0.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nnetwork\n\n\t\n\nStatistics about network activity on the host.\n\n\t\n\nOBJECT\n\n\n\n\nnetwork['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the network probe.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']\n\n\t\n\nTCP network activity on the host.\n\n\t\n\nOBJECT\n\n\n\n\nnetwork['tcp']['connections']\n\n\t\n\nInformation about TCP network connections.\n\n\t\n\nOBJECT\n\n\n\n\nnetwork['tpc']['connections']['initiated']\n\n\t\n\nTotal number of initiated TCP connections.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tpc']['connections']['accepted']\n\n\t\n\nTotal number of accepted TCP connections.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tpc']['connections']['curr_established']\n\n\t\n\nTotal number of currently established TCP connections.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']['connections']['dropped']\n\n\t\n\nTotal number of dropped TCP connections.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']['connections']['embryonic_dropped']\n\n\t\n\nTotal number of TCP connections that have been dropped before they were accepted.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']['packets']\n\n\t\n\nInformation about TCP packets.\n\n\t\n\nOBJECT\n\n\n\n\nnetwork['tpc']['packets']['sent']\n\n\t\n\nTotal number of TCP packets sent.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']['packets']['received']\n\n\t\n\nTotal number of TCP packets received.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tpc']['packets']['retransmitted']\n\n\t\n\nTotal number of TCP packets retransmitted due to an error.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']['packets']['errors_received']\n\n\t\n\nTotal number of TCP packets that contained checksum errors, had a bad offset, were dropped because of a lack of memory or were too short.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']]['packets']['rst_sent']\n\n\t\n\nTotal number of RST packets sent due to left unread data in queue when socket is closed. See tools.ietf.org.\n\n\t\n\nBIGINT\n\nconnections\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nhttp\n\n\t\n\nNumber of connections established via HTTP\n\n\t\n\nOBJECT\n\n\n\n\nhttp['open']\n\n\t\n\nThe currently open connections established via HTTP\n\n\t\n\nBIGINT\n\n\n\n\nhttp['total']\n\n\t\n\nThe total number of connections that have been established via HTTP over the life time of a CrateDB node\n\n\t\n\nBIGINT\n\n\n\n\npsql\n\n\t\n\nNumber of connections established via PostgreSQL protocol\n\n\t\n\nOBJECT\n\n\n\n\npsql['open']\n\n\t\n\nThe currently open connections established via PostgreSQL protocol\n\n\t\n\nBIGINT\n\n\n\n\npsql['total']\n\n\t\n\nThe total number of connections that have been established via PostgreSQL protocol over the life time of a CrateDB node\n\n\t\n\nBIGINT\n\n\n\n\ntransport\n\n\t\n\nNumber of connections established via Transport protocol\n\n\t\n\nOBJECT\n\n\n\n\ntransport['open']\n\n\t\n\nThe currently open connections established via Transport protocol\n\n\t\n\nBIGINT\n\nprocess\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nprocess\n\n\t\n\nStatistics about the CrateDB process.\n\n\t\n\nOBJECT\n\n\n\n\nprocess['open_file_descriptors']\n\n\t\n\nNumber of currently open file descriptors used by the CrateDB process.\n\n\t\n\nBIGINT\n\n\n\n\nprocess['max_open_file_descriptors']\n\n\t\n\nThe maximum number of open file descriptors CrateDB can use.\n\n\t\n\nBIGINT\n\n\n\n\nprocess['probe_timestamp']\n\n\t\n\nThe system UNIX timestamp at the moment of the probe collection.\n\n\t\n\nBIGINT\n\n\n\n\nprocess['cpu']\n\n\t\n\nInformation about the CPU usage of the CrateDB process.\n\n\t\n\nOBJECT\n\n\n\n\nprocess['cpu']['percent']\n\n\t\n\nThe CPU usage of the CrateDB JVM process given in percent.\n\n\t\n\nSMALLINT\n\nThe CPU information values are cached for 1s. They might differ from the actual values at query time. Use the probe timestamp to get the time of the collect. When analyzing the CPU usage over time, always use process['probe_timestamp'] to calculate the time difference between 2 probes.\n\nNote\n\nIf one of the queried nodes is not responding within three seconds it returns null every column except id and name. This behaviour could be used to detect hanging nodes.\n\nNode checks\n\nThe table sys.node_checks exposes a list of internal node checks and results of their validation.\n\nThe table schema is the following:\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe unique check ID.\n\n\t\n\nINTEGER\n\n\n\n\nnode_id\n\n\t\n\nThe unique node ID.\n\n\t\n\nTEXT\n\n\n\n\nseverity\n\n\t\n\nThe level of severity. The higher the value of the field the higher severity.\n\n\t\n\nINTEGER\n\n\n\n\ndescription\n\n\t\n\nThe description message for the setting check.\n\n\t\n\nTEXT\n\n\n\n\npassed\n\n\t\n\nThe flag determines whether the check for the setting has passed.\n\n\t\n\nBOOLEAN\n\n\n\n\nacknowledged\n\n\t\n\nThe flag determines whether the check for this setting has been acknowledged by the user in order to ignored the value of passed column. This column can be updated.\n\n\t\n\nBOOLEAN\n\nExample query:\n\ncr> select id, node_id, description from sys.node_checks order by id, node_id;\n+----+---------...-+--------------------------------------------------------------...-+\n| id | node_id     | description                                                      |\n+----+---------...-+--------------------------------------------------------------...-+\n|  1 | ...         | The value of the cluster setting 'gateway.expected_data_nodes... |\n|  2 | ...         | The value of the cluster setting 'gateway.recover_after_data_... |\n|  3 | ...         | If any of the \"expected data nodes\" recovery settings are set... |\n|  5 | ...         | The high disk watermark is exceeded on the node. The cluster ... |\n|  6 | ...         | The low disk watermark is exceeded on the node. The cluster w... |\n|  7 | ...         | The flood stage disk watermark is exceeded on the node. Table... |\n|  8 | ...         | The amount of shards on the node reached 90 % of the limit of... |\n+----+---------...-+--------------------------------------------------------------...-+\nSELECT 7 rows in set (... sec)\n\nAcknowledge failed checks\n\nIt is possible to acknowledge every check by updating the acknowledged column. By doing this, specially CrateDB’s built-in Admin UI won’t complain anymore about failing checks.\n\nImagine we’ve added a new node to our cluster, but as the gateway.expected_data_nodes column can only be set via config-file or command-line argument, the check for this setting will not pass on the already running nodes until the config-file or command-line argument on these nodes is updated and the nodes are restarted (which is not what we want on a healthy well running cluster).\n\nIn order to make the Admin UI accept a failing check (so the checks label goes green again), we must acknowledge this check by updating it’s acknowledged flag:\n\ncr> update sys.node_checks set acknowledged = true where id = 1;\nUPDATE OK, 1 row affected (... sec)\n\n\nCaution\n\nUpdates on this column are transient, so changed values are lost after the affected node is restarted.\n\nDescription of checked node settings\nRecovery expected data nodes\n\nThe check for the gateway.expected_data_nodes setting checks that the number of data nodes that should be waited for the immediate cluster state recovery. This value must be equal to the maximum number of data nodes in the cluster.\n\nNote\n\nFor backward compatibility, setting the deprecated gateway.expected_nodes instead is still supported. It must then be equal to the maximum of data and master nodes.\n\nRecovery after data nodes\n\nThe check for the gateway.recover_after_data_nodes verifies that the number of started nodes before the cluster starts must be greater than the half of the expected number of data nodes and equal to or less than number of data nodes in the cluster.\n\nNote\n\nFor backward compatibility, setting the deprecated gateway.recover_after_nodes instead is still supported. It must then be equal to less than the number data and master nodes.\n\n(E / 2) < R <= E\n\n\nHere, R is the number of recovery nodes and E is the number of expected nodes.\n\nRecovery after time\n\nIf gateway.recover_after_data_nodes is set, then gateway.recover_after_time must not be set to 0s, otherwise the gateway.recover_after_data_nodes setting wouldn’t have any effect.\n\nNote\n\nFor backward compatibility, setting the deprecated gateway.recover_after_nodes instead is still supported.\n\nRouting allocation disk watermark high\n\nThe check for the cluster.routing.allocation.disk.watermark.high setting verifies that the high watermark is not exceeded on the current node. The usage of each disk for configured CrateDB data paths is verified against the threshold setting. If one or more verification fails the check is marked as not passed.\n\nRouting allocation disk watermark low\n\nThe check for the cluster.routing.allocation.disk.watermark.low which controls the low watermark for the node disk usage. The check verifies that the low watermark is not exceeded on the current node. The verification is done against each disk for configured CrateDB data paths. The check is not passed if the verification for one or more disk fails.\n\nMaximum shards per node\n\nThe check verifies that the amount of shards on the current node is less than 90 percent of cluster.max_shards_per_node. Creating new tables or partitions which would push the number of shards beyond 100 % of the limit will be rejected.\n\nShards\n\nThe table sys.shards contains real-time statistics for all shards of all (non-system) tables.\n\nTable schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nnode\n\n\t\n\nInformation about the node the shard is located at.\n\n\t\n\nOBJECT\n\n\n\n\nnode['name']\n\n\t\n\nThe name of the node the shard is located at.\n\n\t\n\nTEXT\n\n\n\n\nnode['id']\n\n\t\n\nThe id of the node the shard is located at.\n\n\t\n\nTEXT\n\n\n\n\nblob_path\n\n\t\n\nPath to the directory which contains the blob files of the shard, or null if the shard is not a blob shard.\n\n\t\n\nTEXT\n\n\n\n\nid\n\n\t\n\nThe shard id. This shard id is managed by the system, ranging from 0 up to the number of configured shards of the table.\n\n\t\n\nINTEGER\n\n\n\n\nmin_lucene_version\n\n\t\n\nShows the oldest Lucene segment version used in this shard.\n\n\t\n\nTEXT\n\n\n\n\nnum_docs\n\n\t\n\nThe total amount of documents within a shard.\n\n\t\n\nBIGINT\n\n\n\n\noprhan_partition\n\n\t\n\nTrue if this shard belongs to an orphaned partition which doesn’t belong to any table anymore.\n\n\t\n\nBOOLEAN\n\n\n\n\npartition_ident\n\n\t\n\nThe partition ident of a partitioned table. Empty for non-partitioned tables.\n\n\t\n\nTEXT\n\n\n\n\npath\n\n\t\n\nPath to the shard directory on the filesystem. This directory contains state and index files.\n\n\t\n\nTEXT\n\n\n\n\nprimary\n\n\t\n\nIndicates if this shard is the primary shard.\n\n\t\n\nBOOLEAN\n\n\n\n\nrecovery\n\n\t\n\nRecovery statistics for a shard.\n\n\t\n\nOBJECT\n\n\n\n\nrecovery['files']\n\n\t\n\nFile recovery statistics\n\n\t\n\nOBJECT\n\n\n\n\nrecovery['files']['percent']\n\n\t\n\nPercentage of files already recovered.\n\n\t\n\nREAL\n\n\n\n\nrecovery['files']['recovered']\n\n\t\n\nNumber of files recovered in the shard. Includes both existing and reused files.\n\n\t\n\nINTEGER\n\n\n\n\nrecovery['files']['reused']\n\n\t\n\nTotal number of files reused from a local copy while recovering the shard.\n\n\t\n\nINTEGER\n\n\n\n\nrecovery['files']['used']\n\n\t\n\nTotal number of files in the shard.\n\n\t\n\nINTEGER\n\n\n\n\nrecovery['size']\n\n\t\n\nRecovery statistics for the shard in bytes\n\n\t\n\nOBJECT\n\n\n\n\nrecovery['size']['percent']\n\n\t\n\nPercentage of bytes already recovered\n\n\t\n\nREAL\n\n\n\n\nrecovery['size']['recovered']\n\n\t\n\nNumber of bytes recovered. Includes both existing and re-used bytes.\n\n\t\n\nBIGINT\n\n\n\n\nrecovery['size']['reused']\n\n\t\n\nNumber of bytes re-used from a local copy while recovering the shard.\n\n\t\n\nBIGINT\n\n\n\n\nrecovery['size']['used']\n\n\t\n\nTotal number of bytes in the shard.\n\n\t\n\nBIGINT\n\n\n\n\nrecovery['stage']\n\n\t\n\nRecovery stage:\n\ninit: Recovery has not started\n\nindex: Reading the Lucene index meta-data and copying bytes from source to destination\n\nstart: Starting the engine, opening the index for use\n\ntranslog: Replaying transaction log\n\nfinalize: Cleanup\n\ndone: Complete\n\n\t\n\nTEXT\n\n\n\n\nrecovery['total_time']\n\n\t\n\nReturns elapsed time from the start of the shard recovery.\n\n\t\n\nBIGINT\n\n\n\n\nrecovery['type']\n\n\t\n\nRecovery type:\n\ngateway\n\nsnapshot\n\nreplica\n\nrelocating\n\n\t\n\nTEXT\n\n\n\n\nrelocating_node\n\n\t\n\nThe id of the node to which the shard is getting relocated to.\n\n\t\n\nTEXT\n\n\n\n\nrouting_state\n\n\t\n\nThe current state of the shard in the routing table. Possible states are:\n\nUNASSIGNED\n\nINITIALIZING\n\nSTARTED\n\nRELOCATING\n\n\t\n\nTEXT\n\n\n\n\nschema_name\n\n\t\n\nThe schema name of the table the shard belongs to\n\n\t\n\nTEXT\n\n\n\n\nsize\n\n\t\n\nThe current size in bytes. This value is cached for a short period and may return slightly outdated values.\n\n\t\n\nBIGINT\n\n\n\n\nstate\n\n\t\n\nThe current state of the shard. Possible states are:\n\nCREATED\n\nRECOVERING\n\nPOST_RECOVERY\n\nSTARTED\n\nRELOCATED\n\nCLOSED\n\nINITIALIZING\n\nUNASSIGNED\n\n\t\n\nTEXT\n\n\n\n\nclosed\n\n\t\n\nThe state of the table associated with the shard.\n\n\t\n\nBOOLEAN\n\n\n\n\ntable_name\n\n\t\n\nThe name of the table this shard belongs to\n\n\t\n\nTEXT\n\n\n\n\nseq_no_stats\n\n\t\n\nContains information about internal sequence numbering and checkpoints for these sequence numbers.\n\n\t\n\nOBJECT\n\n\n\n\nseq_no_stats['max_seq_no']\n\n\t\n\nThe highest sequence number that has been issued so far on the shard.\n\n\t\n\nBIGINT\n\n\n\n\nseq_no_stats['local_checkpoint']\n\n\t\n\nThe highest sequence number for which all lower sequence number of been processed on this shard. Due to concurrent indexing this can be lower than max_seq_no.\n\n\t\n\nBIGINT\n\n\n\n\nseq_no_stats['global_checkpoint']\n\n\t\n\nThe highest sequence number for which the local shard can guarantee that all lower sequence numbers have been processed on all active shard copies.\n\n\t\n\nBIGINT\n\n\n\n\ntranslog_stats\n\n\t\n\nContains information for the translog of the shard.\n\n\t\n\nOBJECT\n\n\n\n\ntranslog_stats['size']\n\n\t\n\nThe current size of the translog file in bytes.\n\n\t\n\nBIGINT\n\n\n\n\ntranslog_stats['uncommitted_size']\n\n\t\n\nThe size in bytes of the translog that has not been committed to Lucene yet.\n\n\t\n\nBIGINT\n\n\n\n\ntranslog_stats['number_of_operations']\n\n\t\n\nThe number of operations recorded in the translog.\n\n\t\n\nINTEGER\n\n\n\n\ntranslog_stats['uncommitted_operations']\n\n\t\n\nThe number of operations in the translog which have not been committed to Lucene yet.\n\n\t\n\nINTEGER\n\n\n\n\nretention_leases\n\n\t\n\nVersioned collection of retention leases.\n\n\t\n\nOBJECT\n\n\n\n\nflush_stats\n\n\t\n\nFlush information. Shard relocation resets this information.\n\n\t\n\nOBJECT\n\n\n\n\nflush_stats['count']\n\n\t\n\nThe total amount of flush operations that happened on the shard.\n\n\t\n\nBIGINT\n\n\n\n\nflush_stats['periodic_count']\n\n\t\n\nThe number of periodic flushes. Each periodic flush also counts as a regular flush. A periodic flush can happen after writes depending on settings like the translog flush threshold.\n\n\t\n\nBIGINT\n\n\n\n\nflush_stats['total_time_ns']\n\n\t\n\nThe total time spent on flush operations on the shard.\n\n\t\n\nBIGINT\n\nNote\n\nThe sys.shards table is subject to Shard table permissions.\n\nExample\n\nFor example, you can query shards like this:\n\ncr> select schema_name as schema,\n...   table_name as t,\n...   id,\n...   partition_ident as p_i,\n...   num_docs as docs,\n...   primary,\n...   relocating_node as r_n,\n...   routing_state as r_state,\n...   state,\n...   orphan_partition as o_p\n... from sys.shards where table_name = 'locations' and id = 1;\n+--------+-----------+----+-----+------+---------+------+---------+---------+-------+\n| schema | t         | id | p_i | docs | primary | r_n  | r_state |  state  | o_p   |\n+--------+-----------+----+-----+------+---------+------+---------+---------+-------+\n| doc    | locations |  1 |     |    8 | TRUE    | NULL | STARTED | STARTED | FALSE |\n+--------+-----------+----+-----+------+---------+------+---------+---------+-------+\nSELECT 1 row in set (... sec)\n\nSegments\n\nThe sys.segments table contains information about the Lucene segments of the shards.\n\nThe segment information is useful to understand the behaviour of the underlying Lucene file structures for troubleshooting and performance optimization of shards.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nsegment_name\n\n\t\n\nName of the segment, derived from the segment generation and used internally to create file names in the directory of the shard.\n\n\t\n\nTEXT\n\n\n\n\nshard_id\n\n\t\n\nID of the effected shard.\n\n\t\n\nINTEGER\n\n\n\n\ntable_schema\n\n\t\n\nSchema name of the table of the shard.\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nTable name of the shard.\n\n\t\n\nTEXT\n\n\n\n\npartition_ident\n\n\t\n\nThe partition ident of a partitioned table. Empty for non-partitioned tables.\n\n\t\n\nTEXT\n\n\n\n\nnode\n\n\t\n\nInformation about the node the shard is located at.\n\n\t\n\nOBJECT\n\n\n\n\nnode['name']\n\n\t\n\nThe name of the node the shard is located at.\n\n\t\n\nTEXT\n\n\n\n\nnode['id']\n\n\t\n\nThe id of the node the shard is located at.\n\n\t\n\nTEXT\n\n\n\n\ngeneration\n\n\t\n\nGeneration number of the segment, increments for each segment written.\n\n\t\n\nLONG\n\n\n\n\nnum_docs\n\n\t\n\nNumber of non-deleted Lucene documents in this segment.\n\n\t\n\nINTEGER\n\n\n\n\ndeleted_docs\n\n\t\n\nNumber of deleted Lucene documents in this segment.\n\n\t\n\nINTEGER\n\n\n\n\nsize\n\n\t\n\nDisk space used by the segment in bytes.\n\n\t\n\nLONG\n\n\n\n\nmemory\n\n\t\n\nSegment data stored in memory for efficient search, -1 if it is unavailable.\n\n\t\n\nLONG\n\n\n\n\ncommitted\n\n\t\n\nIndicates if the segments are synced to disk. Segments that are synced can survive a hard reboot.\n\n\t\n\nBOOLEAN\n\n\n\n\nprimary\n\n\t\n\nDescribes if this segment is part of a primary shard.\n\n\t\n\nBOOLEAN\n\n\n\n\nsearch\n\n\t\n\nIndicates if the segment is searchable. If false, the segment has most likely been written to disk but needs a refresh to be searchable.\n\n\t\n\nBOOLEAN\n\n\n\n\nversion\n\n\t\n\nVersion of Lucene used to write the segment.\n\n\t\n\nTEXT\n\n\n\n\ncompound\n\n\t\n\nIf true, Lucene merges all files from the segment into a single file to save file descriptors.\n\n\t\n\nBOOLEAN\n\n\n\n\nattributes\n\n\t\n\nContains information about whether high compression was enabled.\n\n\t\n\nOBJECT\n\nNote\n\nThe information in the sys.segments table is expensive to calculate and therefore this information should be retrieved with awareness that it can have performance implications on the cluster.\n\nNote\n\nThe sys.shards table is subject to Shard table permissions.\n\nJobs, operations, and logs\n\nTo let you inspect the activities currently taking place in a cluster, CrateDB provides system tables that let you track current cluster jobs and operations. See Jobs Table and Operations Table.\n\nJobs and operations that finished executing are additionally recorded in memory. There are two retention policies available to control how many records should be kept.\n\nOne option is to configure the maximum number of records which should be kept. Once the configured table size is reached, the older log records are deleted as newer records are added. This is configurable using stats.jobs_log_size and stats.operations_log_size.\n\nAnother option is to configure an expiration time for the records. In this case, the records in the logs tables are periodically cleared if they are older than the expiry time. This behaviour is configurable using stats.jobs_log_expiration and stats.operations_log_expiration.\n\nIn addition to these retention policies, there is a memory limit in place preventing these tables from taking up too much memory. The amount of memory that can be used to store the jobs can be configured using stats.breaker.log.jobs.limit and stats.breaker.log.operations.limit. If the memory limit is reached, an error message will be logged and the log table will be cleared completely.\n\nIt is also possible to define a filter which must match for jobs to be recorded after they finished executing. This can be useful to only record slow queries or queries that failed due to an error. This filter can be configured using the stats.jobs_log_filer setting.\n\nFurthermore, there is a second filter setting which also results in a log entry in the regular CrateDB log file for all finished jobs that match this filter. This can be configured using stats.jobs_log_persistent_filter. This could be used to create a persistent slow query log.\n\nJobs\n\nThe sys.jobs table is a constantly updated view of all jobs that are currently being executed in the cluster.\n\nTable schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe job UUID.\n\nThis job ID is generated by the system.\n\n\t\n\nTEXT\n\n\n\n\nnode\n\n\t\n\nInformation about the node that created the job.\n\n\t\n\nOBJECT\n\n\n\n\nnode['id']\n\n\t\n\nThe id of the node.\n\n\t\n\nTEXT\n\n\n\n\nnode['name']\n\n\t\n\nThe name of the node.\n\n\t\n\nTEXT\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the job started.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nstmt\n\n\t\n\nShows the data query or manipulation statement represented by this job.\n\n\t\n\nTEXT\n\n\n\n\nusername\n\n\t\n\nThe user who is executing the statement.\n\n\t\n\nTEXT\n\nThe field username corresponds to the SESSION_USER that is performing the query:\n\ncr> select stmt, username, started from sys.jobs where stmt like 'sel% from %jobs%';\n+---------------------------------------------------------------------------------+----------+-...-----+\n| stmt                                                                            | username | started |\n+---------------------------------------------------------------------------------+----------+-...-----+\n| select stmt, username, started from sys.jobs where stmt like 'sel% from %jobs%' | crate    | ...     |\n+---------------------------------------------------------------------------------+----------+-...-----+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nIf the user management module is not available, the username is given as crate.\n\nEvery request that queries data or manipulates data is considered a “job” if it is a valid query. Requests that are not valid queries (for example, a request that tries to query a non-existent table) will not show up as jobs.\n\nNote\n\nThe sys.jobs table is subject to sys jobs tables permissions.\n\nJobs metrics\n\nThe sys.jobs_metrics table provides an overview of the query latency in the cluster. Jobs metrics are not persisted across node restarts.\n\nThe metrics are aggregated for each node and each unique classification of the statements.\n\nNote\n\nIn order to reduce the memory requirements for these metrics, the times are statistically sampled and therefore may have slight inaccuracies. In addition, durations are only tracked up to 10 minutes. Statements taking longer than that are capped to 10 minutes.\n\nsys.jobs_metrics Table schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nnode\n\n\t\n\nAn object containing the id and name of the node on which the metrics have been sampled.\n\n\t\n\nOBJECT\n\n\n\n\nclassification\n\n\t\n\nAn object containing the statement classification.\n\n\t\n\nOBJECT\n\n\n\n\nclassification['type']\n\n\t\n\nThe general type of the statement. Types are: INSERT, SELECT, UPDATE, DELETE, COPY, DDL, and MANAGEMENT.\n\n\t\n\nTEXT\n\n\n\n\nclassification['labels']\n\n\t\n\nLabels are only available for certain statement types that can be classified more accurately than just by their type.\n\n\t\n\nTEXT_ARRAY\n\n\n\n\ntotal_count\n\n\t\n\nTotal number of queries executed\n\n\t\n\nBIGINT\n\n\n\n\nfailed_count\n\n\t\n\nTotal number of queries that failed to complete successfully.\n\n\t\n\nBIGINT\n\n\n\n\nsum_of_durations\n\n\t\n\nSum of durations in ms of all executed queries per statement type.\n\n\t\n\nBIGINT\n\n\n\n\nstdev\n\n\t\n\nThe standard deviation of the query latencies\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\nmean\n\n\t\n\nThe mean query latency in ms\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\nmax\n\n\t\n\nThe maximum query latency in ms\n\n\t\n\nBIGINT\n\n\n\n\nmin\n\n\t\n\nThe minimum query latency in ms\n\n\t\n\nBIGINT\n\n\n\n\npercentiles\n\n\t\n\nAn object containing different percentiles\n\n\t\n\nOBJECT\n\nClassification\n\nCertain statement types (such as SELECT statements) have additional labels in their classification. These labels are the names of the logical plan operators that are involved in the query.\n\nFor example, the following UNION statement:\n\nSELECT name FROM t1 where id = 1\nUNION ALL\nSELECT name FROM t2 where id < 2\n\n\nwould result in the following labels:\n\nUnion` for the UNION ALL\n\nGet for the left SELECT\n\nCollect for the right SELECT\n\nNote\n\nLabels may be subject to change as they only represent internal properties of the statement!\n\nOperations\n\nThe sys.operations table is a constantly updated view of all operations that are currently being executed in the cluster:\n\ncr> select node['name'], job_id, name, used_bytes from sys.operations\n... order by name limit 1;\n+--------------+--------...-+-----...-+------------+\n| node['name'] | job_id     | name    | used_bytes |\n+--------------+--------...-+-----...-+------------+\n| crate        | ...        | ...     | ...        |\n+--------------+--------...-+-----...-+------------+\nSELECT 1 row in set (... sec)\n\n\nAn operation is a node-specific sub-component of a job (for when a job involves multi-node processing). Jobs that do not require multi-node processing will not produce any operations.\n\nTable schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe operation UUID.\n\nThis operation ID is generated by the system.\n\n\t\n\nTEXT\n\n\n\n\njob_id\n\n\t\n\nThe job id this operation belongs to.\n\n\t\n\nTEXT\n\n\n\n\nname\n\n\t\n\nThe name of the operation.\n\n\t\n\nTEXT\n\n\n\n\nnode\n\n\t\n\nInformation about the node that created the operation.\n\n\t\n\nOBJECT\n\n\n\n\nnode['id']\n\n\t\n\nThe id of the node.\n\n\t\n\nTEXT\n\n\n\n\nnode['name']\n\n\t\n\nThe name of the node.\n\n\t\n\nTEXT\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the operation started.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nused_bytes\n\n\t\n\nCurrently loaded amount of data by the operation.\n\n\t\n\nBIGINT\n\nNote\n\nIn some cases, operations are generated for internal CrateDB work that does not directly correspond to a user request. These entries do not have corresponding entries in sys.jobs.\n\nLogs\n\nThe sys.jobs and sys.operations tables have corresponding log tables: sys.jobs_log and sys.operations_log.\n\nsys.jobs_log Table schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe job ID.\n\n\t\n\nTEXT\n\n\n\n\nended\n\n\t\n\nThe point in time when the job finished.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nerror\n\n\t\n\nIf the job encountered an error, this will hold the error message.\n\n\t\n\nTEXT\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the job started.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nstmt\n\n\t\n\nShows the data query or manipulation statement executed by the job.\n\n\t\n\nTEXT\n\n\n\n\nusername\n\n\t\n\nThe user who executed the statement.\n\n\t\n\nTEXT\n\n\n\n\nclassification\n\n\t\n\nAn object containing the statement classification.\n\n\t\n\nOBJECT\n\n\n\n\nclassification['type']\n\n\t\n\nThe general type of the statement. Types are: INSERT, SELECT, UPDATE, DELETE,``COPY``, DDL, and MANAGEMENT.\n\n\t\n\nTEXT\n\n\n\n\nclassification['labels']\n\n\t\n\nLabels are only available for certain statement types that can be classified more accurately than just by their type.\n\n\t\n\nTEXT_ARRAY\n\n\n\n\nnode\n\n\t\n\nInformation about the node that created the job.\n\n\t\n\nOBJECT\n\n\n\n\nnode['id']\n\n\t\n\nThe id of the node.\n\n\t\n\nTEXT\n\n\n\n\nnode['name']\n\n\t\n\nThe name of the node.\n\n\t\n\nTEXT\n\nNote\n\nYou can control which jobs are recorded using the stats.jobs_log_filter\n\nNote\n\nThe sys.jobs_log table is subject to sys jobs tables permissions.\n\nsys.operations_log Table schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe operation ID.\n\n\t\n\nTEXT\n\n\n\n\njob_id\n\n\t\n\nThe job id.\n\n\t\n\nTEXT\n\n\n\n\nended\n\n\t\n\nThe point in time when the operation finished.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nerror\n\n\t\n\nIf the operation encountered an error, this will hold the error message.\n\n\t\n\nTEXT\n\n\n\n\nname\n\n\t\n\nThe name of the operation.\n\n\t\n\nTEXT\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the operation started.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nused_bytes\n\n\t\n\nThe amount of data loaded by the operation.\n\n\t\n\nBIGINT\n\nAfter a job or operation finishes, the corresponding entry will be moved into the corresponding log table:\n\ncr> select id, stmt, username, started, ended, error\n... from sys.jobs_log order by ended desc limit 2;\n+-...+----------------------------------------------...-+----------+-...-----+-...---+-------+\n| id | stmt                                             | username | started | ended | error |\n+-...+----------------------------------------------...-+----------+-...-----+-...---+-------+\n| ...| select node['name'], ...                         | crate    | ...     | ...   |  NULL |\n| ...| select stmt, username, started from sys.jobs ... | crate    | ...     | ...   |  NULL |\n+-...+----------------------------------------------...-+----------+-...-----+-...---+-------+\nSELECT 2 rows in set (... sec)\n\n\nInvalid queries are also logged in the sys.jobs_log table, i.e. queries that never make it to the sys.jobs table because they could not be executed.\n\nThe log tables are bound by a fixed size (stats.jobs_log_size) or by an expiration time (stats.jobs_log_expiration)\n\nSee Collecting stats for information on how to configure logs.\n\nCaution\n\nIf you deactivate statistics tracking, the logs tables will be truncated.\n\nCluster checks\n\nThe table sys.checks exposes a list of internal cluster checks and results of their validation.\n\nThe sys.checks table looks like this:\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe unique check id.\n\n\t\n\nINTEGER\n\n\n\n\nseverity\n\n\t\n\nThe level of severity. The higher the value of the field the higher severity.\n\n\t\n\nINTEGER\n\n\n\n\ndescription\n\n\t\n\nThe description message for the setting check.\n\n\t\n\nTEXT\n\n\n\n\npassed\n\n\t\n\nThe flag determines whether the check for the setting has passed.\n\n\t\n\nBOOLEAN\n\nHere’s an example query:\n\ncr> select id, description from sys.checks order by id;\n+----+--------------------------------------------------------------...-+\n| id | description                                                      |\n+----+--------------------------------------------------------------...-+\n|  2 | The total number of partitions of one or more partitioned tab... |\n|  3 | The following tables need to be recreated for compatibility w... |\n+----+--------------------------------------------------------------...-+\nSELECT 2 rows in set (... sec)\n\n\nCluster checks are also indicated in the CrateDB admin console. When all cluster checks (and all Node checks) pass, the Checks icon will be green. Here’s what it looks like when some checks are failing at the CRITICAL severity level:\n\nCurrent Checks\nNumber of partitions\n\nThis check warns if any partitioned table has more than 1000 partitions to detect the usage of a high cardinality field for partitioning.\n\nTables need to be recreated\n\nWarning\n\nDo not attempt to upgrade your cluster to a newer major version if this cluster check is failing. Follow the instructions below to get this cluster check passing.\n\nThis check warns you if your cluster contains tables that you need to reindex before you can upgrade to a future major version of CrateDB.\n\nIf you try to upgrade to a later major CrateDB version without reindexing the tables, CrateDB will refuse to start.\n\nCrateDB table version compatibility scheme\n\nCrateDB maintains backward compatibility for tables created in majorVersion - 1:\n\nTable Origin\n\n\t\n\nCurrent Version\n\n\t\n\nCurrent Version\n\n\t\n\nCurrent Version\n\n\n\t\n\n3.x\n\n\t\n\n4.x\n\n\t\n\n5.x\n\n\n\n\n3.x\n\n\t\n\n✔️\n\n\t\n\n✔️\n\n\t\n\n❌\n\n\n\n\n4.x\n\n\t\n\n❌\n\n\t\n\n✔️\n\n\t\n\n✔️\n\n\n\n\n5.x\n\n\t\n\n❌\n\n\t\n\n❌\n\n\t\n\n✔️\n\nAvoiding reindex using partitioned tables\n\nReindexing tables is an expensive operation which can take a long time. If you are storing time series data for a certain retention period and intend to delete old data, it is possible to use the partitioned tables to avoid reindex operations.\n\nYou will have to use a partition column that denotes time. For example, if you have a retention period of nine months, you could partition a table by a month column. Then, every month, the system will create a new partition. This new partition is created using the active CrateDB version and is compatible with the next major CrateDB version. Now to achieve your goal of avoiding a reindex, you must manually delete any partition older than nine months. If you do that, then after nine months you rolled through all partitions and the remaining nine are compatible with the next major CrateDB version.\n\nHow to reindex\n\nUse SHOW CREATE TABLE to get the schema required to create an empty copy of the table to recreate:\n\ncr> SHOW CREATE TABLE rx.metrics;\n+-----------------------------------------------------+\n| SHOW CREATE TABLE rx.metrics                        |\n+-----------------------------------------------------+\n| CREATE TABLE IF NOT EXISTS \"rx\".\"metrics\" (         |\n|    \"id\" TEXT NOT NULL,                                       |\n|    \"temperature\" REAL,                              |\n|    PRIMARY KEY (\"id\")                               |\n| )                                                   |\n| CLUSTERED BY (\"id\") INTO 4 SHARDS                   |\n| WITH (                                              |\n|    \"allocation.max_retries\" = 5,                    |\n|    \"blocks.metadata\" = false,                       |\n|    \"blocks.read\" = false,                           |\n|    \"blocks.read_only\" = false,                      |\n|    \"blocks.read_only_allow_delete\" = false,         |\n|    \"blocks.write\" = false,                          |\n|    codec = 'default',                               |\n|    column_policy = 'strict',                        |\n|    \"mapping.total_fields.limit\" = 1000,             |\n|    max_ngram_diff = 1,                              |\n|    max_shingle_diff = 3,                            |\n|    number_of_replicas = '0-1',                      |\n|    \"routing.allocation.enable\" = 'all',             |\n|    \"routing.allocation.total_shards_per_node\" = -1, |\n|    \"store.type\" = 'fs',                             |\n|    \"translog.durability\" = 'REQUEST',               |\n|    \"translog.flush_threshold_size\" = 536870912,     |\n|    \"translog.sync_interval\" = 5000,                 |\n|    \"unassigned.node_left.delayed_timeout\" = 60000,  |\n|    \"write.wait_for_active_shards\" = '1'             |\n| )                                                   |\n+-----------------------------------------------------+\nSHOW 1 row in set (... sec)\n\n\nCreate a new temporary table, using the schema returned from SHOW CREATE TABLE:\n\ncr> CREATE TABLE rx.tmp_metrics (id TEXT PRIMARY KEY, temperature REAL);\nCREATE OK, 1 row affected (... sec)\n\n\nCopy the data:\n\ncr> INSERT INTO rx.tmp_metrics (id, temperature) (SELECT id, temperature FROM rx.metrics);\nINSERT OK, 2 rows affected (... sec)\n\n\nSwap the tables:\n\ncr> ALTER CLUSTER SWAP TABLE rx.tmp_metrics TO rx.metrics;\nALTER OK, 1 row affected  (... sec)\n\n\nConfirm the new your_table contains all data and has the new version:\n\ncr> SELECT count(*) FROM rx.metrics;\n+----------+\n| count(*) |\n+----------+\n|        2 |\n+----------+\nSELECT 1 row in set (... sec)\n\ncr> SELECT version['created'] FROM information_schema.tables\n... WHERE table_schema = 'rx' AND table_name = 'metrics';\n+--------------------+\n| version['created'] |\n+--------------------+\n| 4.8.5              |\n+--------------------+\nSELECT 1 row in set (... sec)\n\n\nDrop the old table, as it is now obsolete:\n\ncr> DROP TABLE rx.tmp_metrics;\nDROP OK, 1 row affected  (... sec)\n\n\nAfter you reindexed all tables, this cluster check will pass.\n\nNote\n\nSnapshots of your tables created prior to them being upgraded will not work with future versions of CrateDB. For this reason, you should create a new snapshot for each of your tables. (See Snapshots.)\n\nLicense check\n\nNote\n\nThis check was removed in version 4.5 because CrateDB no longer requires an enterprise license, see also Farewell to the CrateDB Enterprise License.\n\nThis check warns you when your license is close to expiration, is already expired, or if the cluster contains more nodes than allowed by your license. It will yield a MEDIUM alert when your license is valid for less than 15 days and a HIGH alert when your license is valid for less than a day. All other cases, like already expired or max-nodes-violation, it will result in a HIGH alert. We recommend that you request a new license when this check triggers, in order to avoid the situation where operations are rejected due to an invalid license.\n\nHealth\n\nThe sys.health table lists the health of each table and table partition. The health is computed by checking the states of the shard of each table/partition.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\ntable_name\n\n\t\n\nThe table name.\n\n\t\n\nTEXT\n\n\n\n\ntable_schema\n\n\t\n\nThe schema of the table.\n\n\t\n\nTEXT\n\n\n\n\npartition_ident\n\n\t\n\nThe ident of the partition. NULL for non-partitioned tables.\n\n\t\n\nTEXT\n\n\n\n\nhealth\n\n\t\n\nThe health label. Can be RED, YELLOW or GREEN.\n\n\t\n\nTEXT\n\n\n\n\nseverity\n\n\t\n\nThe health as a smallint value. Useful when ordering on health.\n\n\t\n\nSMALLINT\n\n\n\n\nmissing_shards\n\n\t\n\nThe number of not assigned or started shards.\n\n\t\n\nINTEGER\n\n\n\n\nunderreplicated_shards\n\n\t\n\nThe number of shards which are not fully replicated.\n\n\t\n\nINTEGER\n\nBoth missing_shards and underreplicated_shards might return -1 if the cluster is in an unhealthy state that prevents the exact number from being calculated. This could be the case when the cluster can’t elect a master, because there are not enough eligible nodes available.\n\ncr> select * from sys.health order by severity desc, table_name;\n+--------+----------------+-----------------+----------+------------+--------------+------------------------+\n| health | missing_shards | partition_ident | severity | table_name | table_schema | underreplicated_shards |\n+--------+----------------+-----------------+----------+------------+--------------+------------------------+\n| GREEN  |              0 |            NULL |        1 | locations  | doc          |                      0 |\n| GREEN  |              0 |            NULL |        1 | quotes     | doc          |                      0 |\n+--------+----------------+-----------------+----------+------------+--------------+------------------------+\nSELECT 2 rows in set (... sec)\n\n\nThe health with the highest severity will always define the health of the query scope.\n\nExample of getting a cluster health (health of all tables):\n\ncr> select health from sys.health order by severity desc limit 1;\n+--------+\n| health |\n+--------+\n| GREEN  |\n+--------+\nSELECT 1 row in set (... sec)\n\nHealth definition\n\nHealth\n\n\t\n\nDescription\n\n\n\n\nRED\n\n\t\n\nAt least one primary shard is missing (primary shard not started or unassigned).\n\n\n\n\nYELLOW\n\n\t\n\nAt least one shard is underreplicated (replica shard not started or unassigned).\n\n\n\n\nGREEN\n\n\t\n\nAll primary and replica shards have been started.\n\nNote\n\nThe sys.health table is subject to Shard table permissions as it will expose a summary of table shard states.\n\nRepositories\n\nThe table sys.repositories lists all configured repositories that can be used to create, manage and restore snapshots (see Snapshots).\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nname\n\n\t\n\nThe repository name\n\n\t\n\nTEXT\n\n\n\n\ntype\n\n\t\n\nThe type of the repository determining how and where the repository stores its snapshots.\n\n\t\n\nTEXT\n\n\n\n\nsettings\n\n\t\n\nThe configuration settings the repository has been created with. The specific settings depend on the repository type, see CREATE REPOSITORY.\n\n\t\n\nOBJECT\n\ncr> SELECT name, type, settings FROM sys.repositories\n... ORDER BY name;\n+---------+------+---------------------------------------------------...--+\n| name    | type | settings                                               |\n+---------+------+---------------------------------------------------...--+\n| my_repo | fs   | {\"compress\": \"true\", \"location\": \"repo_location\", ...} |\n+---------+------+---------------------------------------------------...--+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nSensitive user account information will be masked and thus not visible to the user.\n\nSnapshots\n\nThe table sys.snapshots lists all existing snapshots in all configured repositories (see Snapshots).\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nname\n\n\t\n\nThe name of the snapshot\n\n\t\n\nTEXT\n\n\n\n\nrepository\n\n\t\n\nThe name of the repository that contains this snapshot.\n\n\t\n\nTEXT\n\n\n\n\nconcrete_indices\n\n\t\n\nContains the names of all tables and partitions that are contained in this snapshot how they are represented as ES index names.\n\n\t\n\nARRAY(TEXT)\n\n\n\n\ntables\n\n\t\n\nContains the fully qualified names of all tables within the snapshot.\n\n\t\n\nARRAY(TEXT)\n\n\n\n\ntable_partitions\n\n\t\n\nContains the table schema, table name and partition values of partitioned tables within the snapshot.\n\n\t\n\nARRAY(OBJECT)\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the creation of the snapshot started. Changes made after that are not stored in this snapshot.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nfinished\n\n\t\n\nThe point in time when the snapshot creation finished.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nstate\n\n\t\n\nThe current state of the snapshot. One of: IN_PROGRESS, SUCCESS, PARTIAL, or FAILED.\n\n\t\n\nTEXT\n\n\n\n\nversion\n\n\t\n\nAn internal version this snapshot was created with.\n\n\t\n\nTEXT\n\n\n\n\nfailures\n\n\t\n\nA list of failures that occurred while taking the snapshot. If taking the snapshot was successful this is empty.\n\n\t\n\nARRAY(TEXT)\n\nSnapshot/Restore operates on a per-shard basis. Hence, the state column indicates whether all (SUCCESS), some (PARTIAL), or no shards(FAILED) have been backed up. PARTIAL snapshots are the result of some primaries becoming unavailable while taking the snapshot when there are no replicas at hand (cluster state is RED). If there are replicas of the (now unavailable) primaries (cluster state is YELLOW) the snapshot succeeds and all shards are included (state SUCCESS). Building on a PARTIAL snapshot will include all primaries again.\n\nWarning\n\nIn case of a PARTIAL state another snapshot should be created in order to guarantee a full backup! Only SUCCESS includes all shards.\n\nThe concrete_indices column contains the names of all Elasticsearch indices that were stored in the snapshot. A normal CrateDB table maps to one Elasticsearch index, a partitioned table maps to one Elasticsearch index per partition. The mapping follows the following pattern:\n\nCrateDB table / partition name\n\n\t\n\nconcrete_indices entry\n\n\n\n\ndoc.my_table\n\n\t\n\nmy_table\n\n\n\n\nmy_schema.my_table\n\n\t\n\nmy_schema.my_table\n\n\n\n\ndoc.parted_table (value=null)\n\n\t\n\n.partitioned.my_table.0400\n\n\n\n\nmy_schema.parted_table (value=null)\n\n\t\n\nmy_schema..partitioned.my_table.0400\n\ncr> SELECT \"repository\", name, state, concrete_indices\n... FROM sys.snapshots order by \"repository\", name;\n+------------+-------------+---------+-----------------...-+\n| repository | name        | state   | concrete_indices    |\n+------------+-------------+---------+-----------------...-+\n| my_repo    | my_snapshot | SUCCESS | [...]               |\n+------------+-------------+---------+-----------------...-+\nSELECT 1 row in set (... sec)\n\nSnapshot Restore\n\nThe sys.snapshot_restore table contains information about the current state of snapshot restore operations.\n\npg_stats schema\n\nName\n\n\t\n\nDescription\n\n\t\n\nType\n\n\n\n\nid\n\n\t\n\nThe UUID of the restore snapshot operation.\n\n\t\n\nTEXT\n\n\n\n\nrepository\n\n\t\n\nThe name of the repository that contains the snapshot.\n\n\t\n\nTEXT\n\n\n\n\nsnapshot\n\n\t\n\nThe name of the snapshot.\n\n\t\n\nTEXT\n\n\n\n\nstate\n\n\t\n\nThe current state of the snapshot restore operations. Possible states are: INIT, STARTED, SUCCESS, and FAILURE.\n\n\t\n\nTEXT\n\n\n\n\nshards['table_schema']\n\n\t\n\nThe schema name of the table of the shard.\n\n\t\n\nTEXT\n\n\n\n\nshards['table_name']\n\n\t\n\nThe table name of the shard.\n\n\t\n\nTEXT\n\n\n\n\nshards['partition_ident']\n\n\t\n\nThe identifier of the partition of the shard. NULL if the is not partitioned.\n\n\t\n\nTEXT\n\n\n\n\nshards['shard_id']\n\n\t\n\nThe ID of the shard.\n\n\t\n\nINTEGER\n\n\n\n\nshards['state']\n\n\t\n\nThe restore state of the shard. Possible states are: INIT, STARTED, SUCCESS, and FAILURE.\n\n\t\n\nTEXT\n\nTo get more information about the restoring snapshots and shards one can join the sys.snapshot_restore with sys.shards or sys.snapshots table.\n\nSummits\n\nThe sys.summits table contains the information about the mountains in the Alps higher than 2000m. The mountain names from the table are also used to generate random nodes names.\n\nUsers\n\nThe sys.users table contains all existing database users in the cluster.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nname\n\n\t\n\nThe name of the database user.\n\n\t\n\nTEXT\n\n\n\n\nsuperuser\n\n\t\n\nFlag to indicate whether the user is a superuser.\n\n\t\n\nBOOLEAN\n\nAllocations\n\nThe sys.allocations table contains information about shards and their allocation state. The table contains:\n\nshards that are unassigned and why they are unassigned\n\nshards that are assigned but cannot be moved or rebalanced and why they remain on their current node\n\nIt can help to identify problems if shard allocations behave different than expected, e.g. when a shard stays unassigned or a shard does not move off a node.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\ntable_schema\n\n\t\n\nSchema name of the table of the shard.\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nTable name of the shard.\n\n\t\n\nTEXT\n\n\n\n\npartition_ident\n\n\t\n\nIdentifier of the partition of the shard. NULL if the table is not partitioned.\n\n\t\n\nTEXT\n\n\n\n\nshard_id\n\n\t\n\nID of the effected shard.\n\n\t\n\nINTEGER\n\n\n\n\nnode_id\n\n\t\n\nID of the node on which the shard resides. NULL if the shard is unassigned.\n\n\t\n\nTEXT\n\n\n\n\nprimary\n\n\t\n\nWhether the shard is a primary shard.\n\n\t\n\nBOOLEAN\n\n\n\n\ncurrent_state\n\n\t\n\nCurrent state of the shard. Possible states are: UNASSIGNED, INITIALIZING, STARTED, RELOCATING\n\n\t\n\nTEXT\n\n\n\n\nexplanation\n\n\t\n\nExplanation why the shard cannot be allocated, moved or rebalanced.\n\n\t\n\nTEXT\n\n\n\n\ndecisions\n\n\t\n\nA list of decisions that describe in detail why the shard in the current state.\n\n\t\n\nARRAY\n\n\n\n\ndecisions['node_id']\n\n\t\n\nID of the node of the decision.\n\n\t\n\nTEXT\n\n\n\n\ndecisions['node_name']\n\n\t\n\nName of the node of the decision.\n\n\t\n\nTEXT\n\n\n\n\ndecisions['explanations']\n\n\t\n\nDetailed list of human readable explanations why the node decided whether to allocate or rebalance the shard. Returns NULL if there is no need to rebalance the shard.\n\n\t\n\nARRAY\n\nNote\n\nThe sys.allocations table is subject to Shard table permissions.\n\nShard table permissions\n\nAccessing tables that return shards (sys.shards, sys.allocations) is subjected to the same privileges constraints as the other tables. Namely, in order to query them, the connected user needs to have the DQL privilege on that particular table, either directly or inherited from the SCHEMA or CLUSTER (for more information on privileges inheritance see Hierarchical Inheritance of Privileges).\n\nHowever, being able to query shard returning system tables will not allow the user to retrieve all the rows in the table, as they may contain information related to tables, which the connected user does not have any privileges for. The only rows that will be returned will be the ones the user is allowed to access.\n\nFor example, if the user john has any privilege on the doc.books table but no privilege at all on doc.locations, when john issues a SELECT * FROM sys.shards statement, the shards information related to the doc.locations table will not be returned.\n\nsys jobs tables permissions\n\nAccessing sys.jobs and sys.jobs_log tables is subjected to the same privileges constraints as other tables. To query them, the current user needs to have the DQL privilege on that particular table, either directly or inherited from the SCHEMA or CLUSTER.\n\nA user that doesn’t have superuser privileges is allowed to retrieve only their own job logs entries, while a user with superuser privileges has access to all.\n\npg_stats\n\nThe pg_stats table in the pg_catalog system schema contains statistical data about the contents of the CrateDB cluster.\n\nEntries are periodically created or updated in the interval configured with the stats.service.interval setting.\n\nAlternatively the statistics can also be updated using the ANALYZE command.\n\nThe table contains 1 entry per column for each table in the cluster which has been analyzed.\n\npg_stats schema\n\nName\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\nschemaname\n\n\t\n\ntext\n\n\t\n\nName of the schema containing the table.\n\n\n\n\ntablename\n\n\t\n\ntext\n\n\t\n\nName of the table.\n\n\n\n\nattname\n\n\t\n\ntext\n\n\t\n\nName of the column.\n\n\n\n\ninherited\n\n\t\n\nbool\n\n\t\n\nAlways false in CrateDB; For compatibility with PostgreSQL.\n\n\n\n\nnull_frac\n\n\t\n\nreal\n\n\t\n\nFraction of column entries that are null.\n\n\n\n\navg_width\n\n\t\n\ninteger\n\n\t\n\nAverage size in bytes of column’s entries.\n\n\n\n\nn_distinct\n\n\t\n\nreal\n\n\t\n\nAn approximation of the number of distinct values in a column.\n\n\n\n\nmost_common_vals\n\n\t\n\nstring[]\n\n\t\n\nA list of the most common values in the column. null if no values seem. more common than others.\n\n\n\n\nmost_common_freqs\n\n\t\n\nreal[]\n\n\t\n\nA list of the frequencies of the most common values. The size of the array always matches most_common_vals. If most_common_vals is null this is null as well.\n\n\n\n\nhistogram_bounds\n\n\t\n\nstring[]\n\n\t\n\nA list of values that divide the column’s values into groups of approximately equal population. The values in most_common_vals, if present, are omitted from this histogram calculation.\n\n\n\n\ncorrelation\n\n\t\n\nreal\n\n\t\n\nAlways 0.0. This column exists for PostgreSQL compatibility.\n\n\n\n\nmost_common_elems\n\n\t\n\nstring[]\n\n\t\n\nAlways null. Exists for PostgreSQL compatibility.\n\n\n\n\nmost_common_elem_freqs\n\n\t\n\nreal[]\n\n\t\n\nAlways null. Exists for PostgreSQL compatibility.\n\n\n\n\nelem_count_histogram\n\n\t\n\nreal[]\n\n\t\n\nAlways null. Exists for PostgreSQL compatibility.\n\nNote\n\nNot all data types support creating statistics. So some columns may not show up in the table.\n\npg_publication\n\nThe pg_publication table in the pg_catalog system schema contains all publications created in the cluster.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\noid\n\n\t\n\nRow identifier.\n\n\t\n\nINTEGER\n\n\n\n\npubname\n\n\t\n\nName of the publication.\n\n\t\n\nTEXT\n\n\n\n\npubowner\n\n\t\n\noid of the owner of the publication.\n\n\t\n\nINTEGER\n\n\n\n\npuballtables\n\n\t\n\nWhether this publication includes all tables in the cluster, including tables created in the future.\n\n\t\n\nBOOLEAN\n\n\n\n\npubinsert\n\n\t\n\nWhether INSERT operations are replicated for tables in the publication. Always true.\n\n\t\n\nBOOLEAN\n\n\n\n\npubupdate\n\n\t\n\nWhether UPDATE operations are replicated for tables in the publication. Always true.\n\n\t\n\nBOOLEAN\n\n\n\n\npubdelete\n\n\t\n\nWhether DELETE operations are replicated for tables in the publication. Always true.\n\n\t\n\nBOOLEAN\n\npg_publication_tables\n\nThe pg_publication_tables table in the pg_catalog system schema contains tables replicated by a publication.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\npubname\n\n\t\n\nName of the publication.\n\n\t\n\nTEXT\n\n\n\n\nschemaname\n\n\t\n\nName of the schema containing table.\n\n\t\n\nTEXT\n\n\n\n\ntablename\n\n\t\n\nName of the table.\n\n\t\n\nTEXT\n\npg_subscription\n\nThe pg_subscription table in the pg_catalog system schema contains all subscriptions created in the cluster.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\noid\n\n\t\n\nRow identifier.\n\n\t\n\nINTEGER\n\n\n\n\nsubdbid\n\n\t\n\nnoop value, always 0.\n\n\t\n\nINTEGER\n\n\n\n\nsubname\n\n\t\n\nName of the subscription.\n\n\t\n\nTEXT\n\n\n\n\nsubowner\n\n\t\n\noid of the owner of the subscription.\n\n\t\n\nINTEGER\n\n\n\n\nsubenabled\n\n\t\n\nWhether the subscription is enabled, always true.\n\n\t\n\nBOOLEAN\n\n\n\n\nsubbinary\n\n\t\n\nNoop value, always true.\n\n\t\n\nBOOLEAN\n\n\n\n\nsubstream\n\n\t\n\nNoop value, always true.\n\n\t\n\nBOOLEAN\n\n\n\n\nsubconninfo\n\n\t\n\nConnection string to the publishing cluster.\n\n\t\n\nTEXT\n\n\n\n\nsubslotname\n\n\t\n\nNoop value, always NULL.\n\n\t\n\nTEXT\n\n\n\n\nsubsynccommit\n\n\t\n\nNoop value, always NULL.\n\n\t\n\nTEXT\n\n\n\n\nsubpublications\n\n\t\n\nArray of subscribed publication names. These publications are defined in the publishing cluster.\n\n\t\n\nARRAY\n\npg_subscription_rel\n\nThe pg_subscription_rel table in the pg_catalog system schema contains the state for each replicated relation in each subscription.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nsrsubid\n\n\t\n\nReference to subscription.\n\n\t\n\nINTEGER\n\n\n\n\nsrrelid\n\n\t\n\nReference to relation.\n\n\t\n\nREGCLASS\n\n\n\n\nsrsubstate\n\n\t\n\nReplication state of the relation. State code: i - initializing; d - restoring; r - monitoring, i.e. waiting for new changes; e - error.\n\n\t\n\nTEXT\n\n\n\n\nsrsubstate_reason\n\n\t\n\nError message if there was a replication error for the relation or NULL.\n\n\t\n\nTEXT\n\n\n\n\nsrsublsn\n\n\t\n\nNoop value, always NULL.\n\n\t\n\nLONG"
  },
  {
    "title": "User management — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/admin/user-management.html",
    "html": "4.8\nUser management\n\nUser account information is stored in the cluster metadata of CrateDB and supports the following statements to create, alter and drop users:\n\nCREATE USER\n\nALTER USER\n\nDROP USER\n\nThese statements are database management statements that can be invoked by superusers that already exist in the CrateDB cluster. The CREATE USER and DROP USER statements can also be invoked by users with the AL privilege.\n\nWhen CrateDB is started, the cluster contains one predefined superuser. This user is called crate. It is not possible to create any other superusers\n\nUsers cannot be backed up or restored.\n\nTable of contents\n\nCREATE USER\n\nALTER USER\n\nDROP USER\n\nList users\n\nCREATE USER\n\nTo create a new user for the CrateDB database cluster use the CREATE USER SQL statement:\n\ncr> CREATE USER user_a;\nCREATE OK, 1 row affected (... sec)\n\n\nTip\n\nNewly created users do not have any privileges. After creating a user, you should configure user privileges.\n\nFor example, to grant all privileges to the user_a user, run:\n\ncr> GRANT ALL PRIVILEGES TO user_a;\nGRANT OK, 4 rows affected (... sec)\n\n\nIt can be used to connect to the database cluster using available authentication methods. You can specify the user’s password in the WITH clause of the CREATE statement. This is required if you want to use the Password authentication method:\n\ncr> CREATE USER user_b WITH (password = 'a_secret_password');\nCREATE OK, 1 row affected (... sec)\n\n\nThe username parameter of the statement follows the principles of an identifier which means that it must be double-quoted if it contains special characters (e.g. whitespace) or if the case needs to be maintained:\n\ncr> CREATE USER \"Custom User\";\nCREATE OK, 1 row affected (... sec)\n\n\nIf a user with the username specified in the SQL statement already exists the statement returns an error:\n\ncr> CREATE USER \"Custom User\";\nUserAlreadyExistsException[User 'Custom User' already exists]\n\nALTER USER\n\nTo alter the password for an existing user from the CrateDB database cluster use the ALTER USER SQL statement:\n\ncr> ALTER USER user_a SET (password = 'pass');\nALTER OK, 1 row affected (... sec)\n\n\nThe password can be reset (cleared) if specified as NULL:\n\ncr> ALTER USER user_a SET (password = NULL);\nALTER OK, 1 row affected (... sec)\n\n\nNote\n\nThe built-in superuser crate has no password and it is not possible to set a new password for this user.\n\nDROP USER\n\nTo remove an existing user from the CrateDB database cluster use the DROP USER SQL statement:\n\ncr> DROP USER user_c;\nDROP OK, 1 row affected (... sec)\n\n\nIf a user with the username specified in the SQL statement does not exist the statement returns an error:\n\ncr> DROP USER user_c;\nUserUnknownException[User 'user_c' does not exist]\n\n\nNote\n\nIt is not possible to drop the built-in superuser crate.\n\nList users\n\nCrateDB exposes database users via the read-only sys.users system table. The sys.users table shows all users in the cluster which can be used for authentication. The initial superuser crate which is available for all CrateDB clusters is also part of that list.\n\nTo list all existing users query that table:\n\ncr> SELECT * FROM sys.users order by name;\n+-------------+----------+-----------+\n| name        | password | superuser |\n+-------------+----------+-----------+\n| Custom User |     NULL | FALSE     |\n| crate       |     NULL | TRUE      |\n| user_a      |     NULL | FALSE     |\n| user_b      | ******** | FALSE     |\n+-------------+----------+-----------+\nSELECT 4 rows in set (... sec)\n\n\nThe column name shows the unique name of the user, the column superuser shows whether the user has superuser privileges or not.\n\nNote\n\nCrateDB also supports retrieving the current connected user using the system information functions: CURRENT_USER, USER and SESSION_USER."
  },
  {
    "title": "Information schema — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/general/information-schema.html",
    "html": "4.8\nInformation schema\n\ninformation_schema is a special schema that contains virtual tables which are read-only and can be queried to get information about the state of the cluster.\n\nTable of contents\n\nAccess\n\nVirtual tables\n\ntables\n\nsettings\n\nviews\n\ncolumns\n\ntable_constraints\n\nkey_column_usage\n\ntable_partitions\n\nroutines\n\nschemata\n\nsql_features\n\ncharacter_sets\n\nAccess\n\nWhen the user management is enabled, accessing the information_schema is open to all users and it does not require any privileges.\n\nHowever, being able to query information_schema tables will not allow the user to retrieve all the rows in the table, as it can contain information related to tables over which the connected user does not have any privileges. The only rows that will be returned will be the ones the user is allowed to access.\n\nFor example, if the user john has any privilege on the doc.books table but no privilege at all on doc.locations, when john issues a SELECT * FROM information_schema.tables statement, the tables information related to the doc.locations table will not be returned.\n\nVirtual tables\ntables\n\nThe information_schema.tables virtual table can be queried to get a list of all available tables and views and their settings, such as number of shards or number of replicas.\n\ncr> SELECT table_schema, table_name, table_type, number_of_shards, number_of_replicas\n... FROM information_schema.tables\n... ORDER BY table_schema ASC, table_name ASC;\n+--------------------+-------------------------+------------+------------------+--------------------+\n| table_schema       | table_name              | table_type | number_of_shards | number_of_replicas |\n+--------------------+-------------------------+------------+------------------+--------------------+\n| doc                | galaxies                | VIEW       |             NULL | NULL               |\n| doc                | locations               | BASE TABLE |                2 | 0                  |\n| doc                | partitioned_table       | BASE TABLE |                4 | 0-1                |\n| doc                | quotes                  | BASE TABLE |                2 | 0                  |\n| information_schema | character_sets          | BASE TABLE |             NULL | NULL               |\n| information_schema | columns                 | BASE TABLE |             NULL | NULL               |\n| information_schema | key_column_usage        | BASE TABLE |             NULL | NULL               |\n| information_schema | referential_constraints | BASE TABLE |             NULL | NULL               |\n| information_schema | routines                | BASE TABLE |             NULL | NULL               |\n| information_schema | schemata                | BASE TABLE |             NULL | NULL               |\n| information_schema | sql_features            | BASE TABLE |             NULL | NULL               |\n| information_schema | table_constraints       | BASE TABLE |             NULL | NULL               |\n| information_schema | table_partitions        | BASE TABLE |             NULL | NULL               |\n| information_schema | tables                  | BASE TABLE |             NULL | NULL               |\n| information_schema | views                   | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_am                   | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_attrdef              | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_attribute            | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_class                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_constraint           | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_database             | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_description          | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_enum                 | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_index                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_indexes              | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_locks                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_namespace            | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_proc                 | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_publication          | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_publication_tables   | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_range                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_roles                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_settings             | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_stats                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_subscription         | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_subscription_rel     | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_tablespace           | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_type                 | BASE TABLE |             NULL | NULL               |\n| sys                | allocations             | BASE TABLE |             NULL | NULL               |\n| sys                | checks                  | BASE TABLE |             NULL | NULL               |\n| sys                | cluster                 | BASE TABLE |             NULL | NULL               |\n| sys                | health                  | BASE TABLE |             NULL | NULL               |\n| sys                | jobs                    | BASE TABLE |             NULL | NULL               |\n| sys                | jobs_log                | BASE TABLE |             NULL | NULL               |\n| sys                | jobs_metrics            | BASE TABLE |             NULL | NULL               |\n| sys                | node_checks             | BASE TABLE |             NULL | NULL               |\n| sys                | nodes                   | BASE TABLE |             NULL | NULL               |\n| sys                | operations              | BASE TABLE |             NULL | NULL               |\n| sys                | operations_log          | BASE TABLE |             NULL | NULL               |\n| sys                | privileges              | BASE TABLE |             NULL | NULL               |\n| sys                | repositories            | BASE TABLE |             NULL | NULL               |\n| sys                | segments                | BASE TABLE |             NULL | NULL               |\n| sys                | shards                  | BASE TABLE |             NULL | NULL               |\n| sys                | snapshot_restore        | BASE TABLE |             NULL | NULL               |\n| sys                | snapshots               | BASE TABLE |             NULL | NULL               |\n| sys                | summits                 | BASE TABLE |             NULL | NULL               |\n| sys                | users                   | BASE TABLE |             NULL | NULL               |\n+--------------------+-------------------------+------------+------------------+--------------------+\nSELECT 57 rows in set (... sec)\n\n\nThe table also contains additional information such as the specified routing column and partition columns:\n\ncr> SELECT table_name, clustered_by, partitioned_by\n... FROM information_schema.tables\n... WHERE table_schema = 'doc'\n... ORDER BY table_schema ASC, table_name ASC;\n+-------------------+--------------+----------------+\n| table_name        | clustered_by | partitioned_by |\n+-------------------+--------------+----------------+\n| galaxies          | NULL         | NULL           |\n| locations         | id           | NULL           |\n| partitioned_table | _id          | [\"date\"]       |\n| quotes            | id           | NULL           |\n+-------------------+--------------+----------------+\nSELECT 4 rows in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nDescription\n\n\t\n\nData Type\n\n\n\n\nblobs_path\n\n\t\n\nThe data path of the blob table\n\n\t\n\nTEXT\n\n\n\n\nclosed\n\n\t\n\nThe state of the table\n\n\t\n\nBOOLEAN\n\n\n\n\nclustered_by\n\n\t\n\nThe routing column used to cluster the table\n\n\t\n\nTEXT\n\n\n\n\ncolumn_policy\n\n\t\n\nDefines whether the table uses a STRICT or a DYNAMIC Column policy\n\n\t\n\nTEXT\n\n\n\n\nnumber_of_replicas\n\n\t\n\nThe number of replicas the table currently has\n\n\t\n\nINTEGER\n\n\n\n\nnumber_of_shards\n\n\t\n\nThe number of shards the table is currently distributed across\n\n\t\n\nINTEGER\n\n\n\n\npartitioned_by\n\n\t\n\nThe partition columns (used to partition the table)\n\n\t\n\nTEXT\n\n\n\n\nreference_generation\n\n\t\n\nSpecifies how values in the self-referencing column are generated\n\n\t\n\nTEXT\n\n\n\n\nrouting_hash_function\n\n\t\n\nThe name of the hash function used for internal routing\n\n\t\n\nTEXT\n\n\n\n\nself_referencing_column_name\n\n\t\n\nThe name of the column that uniquely identifies each row (always _id)\n\n\t\n\nTEXT\n\n\n\n\nsettings\n\n\t\n\nWITH\n\n\t\n\nOBJECT\n\n\n\n\ntable_catalog\n\n\t\n\nRefers to the table_schema\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nThe name of the table\n\n\t\n\nTEXT\n\n\n\n\ntable_schema\n\n\t\n\nThe name of the schema the table belongs to\n\n\t\n\nTEXT\n\n\n\n\ntable_type\n\n\t\n\nThe type of the table (BASE TABLE for tables, VIEW for views)\n\n\t\n\nTEXT\n\n\n\n\nversion\n\n\t\n\nA collection of version numbers relevant to the table\n\n\t\n\nOBJECT\n\nsettings\n\nTable settings specify configuration parameters for tables. Some settings can be set during Cluster runtime and others are only applied on cluster restart.\n\nThis list of table settings in WITH shows detailed information of each parameter.\n\nTable parameters can be applied with CREATE TABLE on creation of a table. With ALTER TABLE they can be set on already existing tables.\n\nThe following statement creates a new table and sets the refresh interval of shards to 500 ms and sets the shard allocation for primary shards only:\n\ncr> create table parameterized_table (id integer, content text)\n... with (\"refresh_interval\"=500, \"routing.allocation.enable\"='primaries');\nCREATE OK, 1 row affected (... sec)\n\n\nThe settings can be verified by querying information_schema.tables:\n\ncr> select settings['routing']['allocation']['enable'] as alloc_enable,\n...   settings['refresh_interval'] as refresh_interval\n... from information_schema.tables\n... where table_name='parameterized_table';\n+--------------+------------------+\n| alloc_enable | refresh_interval |\n+--------------+------------------+\n| primaries    |              500 |\n+--------------+------------------+\nSELECT 1 row in set (... sec)\n\n\nOn existing tables this needs to be done with ALTER TABLE statement:\n\ncr> alter table parameterized_table\n... set (\"routing.allocation.enable\"='none');\nALTER OK, -1 rows affected (... sec)\n\nviews\n\nThe table information_schema.views contains the name, definition and options of all available views.\n\ncr> SELECT table_schema, table_name, view_definition\n... FROM information_schema.views\n... ORDER BY table_schema ASC, table_name ASC;\n+--------------+------------+-------------------------+\n| table_schema | table_name | view_definition         |\n+--------------+------------+-------------------------+\n| doc          | galaxies   | SELECT                  |\n|              |            |   \"id\"                  |\n|              |            | , \"name\"                |\n|              |            | , \"description\"         |\n|              |            | FROM \"locations\"        |\n|              |            | WHERE \"kind\" = 'Galaxy' |\n+--------------+------------+-------------------------+\nSELECT 1 row in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nDescription\n\n\t\n\nData Type\n\n\n\n\ntable_catalog\n\n\t\n\nThe catalog of the table of the view (refers to table_schema)\n\n\t\n\nTEXT\n\n\n\n\ntable_schema\n\n\t\n\nThe schema of the table of the view\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nThe name of the table of the view\n\n\t\n\nTEXT\n\n\n\n\nview_definition\n\n\t\n\nThe SELECT statement that defines the view\n\n\t\n\nTEXT\n\n\n\n\ncheck_option\n\n\t\n\nNot applicable for CrateDB, always return NONE\n\n\t\n\nTEXT\n\n\n\n\nis_updatable\n\n\t\n\nWhether the view is updatable. Not applicable for CrateDB, always returns FALSE\n\n\t\n\nBOOLEAN\n\n\n\n\nowner\n\n\t\n\nThe user that created the view\n\n\t\n\nTEXT\n\nNote\n\nIf you drop the table of a view, the view will still exist and show up in the information_schema.tables and information_schema.views tables.\n\ncolumns\n\nThis table can be queried to get a list of all available columns of all tables and views and their definition like data type and ordinal position inside the table:\n\ncr> select table_name, column_name, ordinal_position as pos, data_type\n... from information_schema.columns\n... where table_schema = 'doc' and table_name not like 'my_table%'\n... order by table_name asc, column_name asc;\n+-------------------+--------------------------------+-----+--------------------------+\n| table_name        | column_name                    | pos | data_type                |\n+-------------------+--------------------------------+-----+--------------------------+\n| locations         | date                           |   3 | timestamp with time zone |\n| locations         | description                    |   6 | text                     |\n| locations         | id                             |   1 | integer                  |\n| locations         | information                    |  11 | object_array             |\n| locations         | information['evolution_level'] |  13 | smallint                 |\n| locations         | information['population']      |  12 | bigint                   |\n| locations         | inhabitants                    |   7 | object                   |\n| locations         | inhabitants['description']     |   9 | text                     |\n| locations         | inhabitants['interests']       |   8 | text_array               |\n| locations         | inhabitants['name']            |  10 | text                     |\n| locations         | kind                           |   4 | text                     |\n| locations         | landmarks                      |  14 | text_array               |\n| locations         | name                           |   2 | text                     |\n| locations         | position                       |   5 | integer                  |\n| partitioned_table | date                           |   3 | timestamp with time zone |\n| partitioned_table | id                             |   1 | bigint                   |\n| partitioned_table | title                          |   2 | text                     |\n| quotes            | id                             |   1 | integer                  |\n| quotes            | quote                          |   2 | text                     |\n+-------------------+--------------------------------+-----+--------------------------+\nSELECT 19 rows in set (... sec)\n\n\nYou can even query this table’s own columns (attention: this might lead to infinite recursion of your mind, beware!):\n\ncr> select column_name, data_type, ordinal_position\n... from information_schema.columns\n... where table_schema = 'information_schema'\n... and table_name = 'columns' order by column_name asc;\n+--------------------------+------------+------------------+\n| column_name              | data_type  | ordinal_position |\n+--------------------------+------------+------------------+\n| character_maximum_length | integer    |                1 |\n| character_octet_length   | integer    |                2 |\n| character_set_catalog    | text       |                3 |\n| character_set_name       | text       |                4 |\n| character_set_schema     | text       |                5 |\n| check_action             | integer    |                6 |\n| check_references         | text       |                7 |\n| collation_catalog        | text       |                8 |\n| collation_name           | text       |                9 |\n| collation_schema         | text       |               10 |\n| column_default           | text       |               11 |\n| column_details           | object     |               12 |\n| column_details['name']   | text       |               13 |\n| column_details['path']   | text_array |               14 |\n| column_name              | text       |               15 |\n| data_type                | text       |               16 |\n| datetime_precision       | integer    |               17 |\n| domain_catalog           | text       |               18 |\n| domain_name              | text       |               19 |\n| domain_schema            | text       |               20 |\n| generation_expression    | text       |               21 |\n| interval_precision       | integer    |               22 |\n| interval_type            | text       |               23 |\n| is_generated             | text       |               24 |\n| is_nullable              | boolean    |               25 |\n| numeric_precision        | integer    |               26 |\n| numeric_precision_radix  | integer    |               27 |\n| numeric_scale            | integer    |               28 |\n| ordinal_position         | integer    |               29 |\n| table_catalog            | text       |               30 |\n| table_name               | text       |               31 |\n| table_schema             | text       |               32 |\n| udt_catalog              | text       |               33 |\n| udt_name                 | text       |               34 |\n| udt_schema               | text       |               35 |\n+--------------------------+------------+------------------+\nSELECT 35 rows in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nDescription\n\n\t\n\nData Type\n\n\n\n\ntable_catalog\n\n\t\n\nRefers to the table_schema\n\n\t\n\nTEXT\n\n\n\n\ntable_schema\n\n\t\n\nSchema name containing the table\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nTable Name\n\n\t\n\nTEXT\n\n\n\n\ncolumn_name\n\n\t\n\nColumn Name For fields in object columns this is not an identifier but a path and therefore must not be double quoted when programmatically obtained.\n\n\t\n\nTEXT\n\n\n\n\nordinal_position\n\n\t\n\nThe position of the column within the table\n\n\t\n\nINTEGER\n\n\n\n\nis_nullable\n\n\t\n\nWhether the column is nullable\n\n\t\n\nBOOLEAN\n\n\n\n\ndata_type\n\n\t\n\nThe data type of the column\n\nFor further information see Data types\n\n\t\n\nTEXT\n\n\n\n\ncolumn_default\n\n\t\n\nThe default expression of the column\n\n\t\n\nTEXT\n\n\n\n\ncharacter_maximum_length\n\n\t\n\nIf the data type is a character type then return the declared length limit; otherwise NULL.\n\n\t\n\nINTEGER\n\n\n\n\ncharacter_octet_length\n\n\t\n\nNot implemented (always returns NULL)\n\nPlease refer to TEXT type\n\n\t\n\nINTEGER\n\n\n\n\nnumeric_precision\n\n\t\n\nIndicates the number of significant digits for a numeric data_type. For all other data types this column is NULL.\n\n\t\n\nINTEGER\n\n\n\n\nnumeric_precision_radix\n\n\t\n\nIndicates in which base the value in the column numeric_precision for a numeric data_type is exposed. This can either be 2 (binary) or 10 (decimal). For all other data types this column is NULL.\n\n\t\n\nINTEGER\n\n\n\n\nnumeric_scale\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nINTEGER\n\n\n\n\ndatetime_precision\n\n\t\n\nContains the fractional seconds precision for a timestamp data_type. For all other data types this column is null.\n\n\t\n\nINTEGER\n\n\n\n\ninterval_type\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ninterval_precision\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nINTEGER\n\n\n\n\ncharacter_set_catalog\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncharacter_set_schema\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncharacter_set_name\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncollation_catalog\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncollation_schema\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncollation_name\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ndomain_catalog\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ndomain_schema\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ndomain_name\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nudt_catalog\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nudt_schema\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nudt_name\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncheck_references\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncheck_action\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nINTEGER\n\n\n\n\ngeneration_expression\n\n\t\n\nThe expression used to generate ad column. If the column is not generated NULL is returned.\n\n\t\n\nTEXT\n\n\n\n\nis_generated\n\n\t\n\nReturns ALWAYS or NEVER wether the column is generated or not.\n\n\t\n\nTEXT\n\ntable_constraints\n\nThis table can be queried to get a list of all defined table constraints, their type, name and which table they are defined in.\n\nNote\n\nCurrently only PRIMARY_KEY constraints are supported.\n\ncr> select table_schema, table_name, constraint_name, constraint_type as type\n... from information_schema.table_constraints\n... where table_name = 'tables'\n...   or table_name = 'quotes'\n...   or table_name = 'documents'\n...   or table_name = 'tbl'\n... order by table_schema desc, table_name asc limit 10;\n+--------------------+------------+------------------------+-------------+\n| table_schema       | table_name | constraint_name        | type        |\n+--------------------+------------+------------------------+-------------+\n| information_schema | tables     | tables_pk              | PRIMARY KEY |\n| doc                | quotes     | quotes_pk              | PRIMARY KEY |\n| doc                | quotes     | doc_quotes_id_not_null | CHECK       |\n| doc                | tbl        | doc_tbl_col_not_null   | CHECK       |\n+--------------------+------------+------------------------+-------------+\nSELECT 4 rows in set (... sec)\n\nkey_column_usage\n\nThis table may be queried to retrieve primary key information from all user tables:\n\ncr> select constraint_name, table_name, column_name, ordinal_position\n... from information_schema.key_column_usage\n... where table_name = 'students'\n+-----------------+------------+-------------+------------------+\n| constraint_name | table_name | column_name | ordinal_position |\n+-----------------+------------+-------------+------------------+\n| students_pk     | students   | id          |                1 |\n| students_pk     | students   | department  |                2 |\n+-----------------+------------+-------------+------------------+\nSELECT 2 rows in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nDescription\n\n\t\n\nData Type\n\n\n\n\nconstraint_catalog\n\n\t\n\nRefers to table_catalog\n\n\t\n\nTEXT\n\n\n\n\nconstraint_schema\n\n\t\n\nRefers to table_schema\n\n\t\n\nTEXT\n\n\n\n\nconstraint_name\n\n\t\n\nName of the constraint\n\n\t\n\nTEXT\n\n\n\n\ntable_catalog\n\n\t\n\nRefers to table_schema\n\n\t\n\nTEXT\n\n\n\n\ntable_schema\n\n\t\n\nName of the schema that contains the table that contains the constraint\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nName of the table that contains the constraint\n\n\t\n\nTEXT\n\n\n\n\ncolumn_name\n\n\t\n\nName of the column that contains the constraint\n\n\t\n\nTEXT\n\n\n\n\nordinal_position\n\n\t\n\nPosition of the column within the constraint (starts with 1)\n\n\t\n\nINTEGER\n\ntable_partitions\n\nThis table can be queried to get information about all partitioned tables, Each partition of a table is represented as one row. The row contains the information table name, schema name, partition ident, and the values of the partition. values is a key-value object with the partition column (or columns) as key(s) and the corresponding value as value(s).\n\ncr> insert into a_partitioned_table (id, content) values (1, 'content_a');\nINSERT OK, 1 row affected (... sec)\n\ncr> alter table a_partitioned_table set (number_of_shards=5);\nALTER OK, -1 rows affected (... sec)\n\ncr> insert into a_partitioned_table (id, content) values (2, 'content_b');\nINSERT OK, 1 row affected (... sec)\n\n\nThe following example shows a table where the column content of table a_partitioned_table has been used to partition the table. The table has two partitions. The partitions are introduced when data is inserted where content is content_a, and content_b.:\n\ncr> select table_name, table_schema as schema, partition_ident, \"values\"\n... from information_schema.table_partitions\n... order by table_name, partition_ident;\n+---------------------+--------+--------------------+--------------------------+\n| table_name          | schema | partition_ident    | values                   |\n+---------------------+--------+--------------------+--------------------------+\n| a_partitioned_table | doc    | 04566rreehimst2vc4 | {\"content\": \"content_a\"} |\n| a_partitioned_table | doc    | 04566rreehimst2vc8 | {\"content\": \"content_b\"} |\n+---------------------+--------+--------------------+--------------------------+\nSELECT 2 rows in set (... sec)\n\n\nThe second partition has been created after the number of shards for future partitions have been changed on the partitioned table, so they show 5 instead of 4:\n\ncr> select table_name, partition_ident,\n... number_of_shards, number_of_replicas\n... from information_schema.table_partitions\n... order by table_name, partition_ident;\n+---------------------+--------------------+------------------+--------------------+\n| table_name          | partition_ident    | number_of_shards | number_of_replicas |\n+---------------------+--------------------+------------------+--------------------+\n| a_partitioned_table | 04566rreehimst2vc4 |                4 | 0-1                |\n| a_partitioned_table | 04566rreehimst2vc8 |                5 | 0-1                |\n+---------------------+--------------------+------------------+--------------------+\nSELECT 2 rows in set (... sec)\n\nroutines\n\nThe routines table contains tokenizers, token-filters, char-filters, custom analyzers created by CREATE ANALYZER statements (see Creating a custom analyzer), and functions created by CREATE FUNCTION statements:\n\ncr> select routine_name, routine_type\n... from information_schema.routines\n... group by routine_name, routine_type\n... order by routine_name asc limit 5;\n+----------------------+--------------+\n| routine_name         | routine_type |\n+----------------------+--------------+\n| PathHierarchy        | TOKENIZER    |\n| apostrophe           | TOKEN_FILTER |\n| arabic               | ANALYZER     |\n| arabic_normalization | TOKEN_FILTER |\n| arabic_stem          | TOKEN_FILTER |\n+----------------------+--------------+\nSELECT 5 rows in set (... sec)\n\n\nFor example you can use this table to list existing tokenizers like this:\n\ncr> select routine_name\n... from information_schema.routines\n... where routine_type='TOKENIZER'\n... order by routine_name asc limit 10;\n+----------------+\n| routine_name   |\n+----------------+\n| PathHierarchy  |\n| char_group     |\n| classic        |\n| edge_ngram     |\n| keyword        |\n| letter         |\n| lowercase      |\n| ngram          |\n| path_hierarchy |\n| pattern        |\n+----------------+\nSELECT 10 rows in set (... sec)\n\n\nOr get an overview of how many routines and routine types are available:\n\ncr> select count(*), routine_type\n... from information_schema.routines\n... group by routine_type\n... order by routine_type;\n+----------+--------------+\n| count(*) | routine_type |\n+----------+--------------+\n|       45 | ANALYZER     |\n|        3 | CHAR_FILTER  |\n|       16 | TOKENIZER    |\n|       62 | TOKEN_FILTER |\n+----------+--------------+\nSELECT 4 rows in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nData Type\n\n\n\n\nroutine_name\n\n\t\n\nTEXT\n\n\n\n\nroutine_type\n\n\t\n\nTEXT\n\n\n\n\nroutine_body\n\n\t\n\nTEXT\n\n\n\n\nroutine_schema\n\n\t\n\nTEXT\n\n\n\n\ndata_type\n\n\t\n\nTEXT\n\n\n\n\nis_deterministic\n\n\t\n\nBOOLEAN\n\n\n\n\nroutine_definition\n\n\t\n\nTEXT\n\n\n\n\nspecific_name\n\n\t\n\nTEXT\n\nroutine_name\n\nName of the routine (might be duplicated in case of overloading)\n\nroutine_type\n\nType of the routine. Can be FUNCTION, ANALYZER, CHAR_FILTER, TOKEN_FILTER or TOKEN_FILTER.\n\nroutine_schema\n\nThe schema where the routine was defined. If it doesn’t apply, then NULL.\n\nroutine_body\n\nThe language used for the routine implementation. If it doesn’t apply, then NULL.\n\ndata_type\n\nThe return type of the function. If it doesn’t apply, then NULL.\n\nis_deterministic\n\nIf the routine is deterministic then True, else False (NULL if it doesn’t apply).\n\nroutine_definition\n\nThe function definition (NULL if it doesn’t apply).\n\nspecific_name\n\nUsed to uniquely identify the function in a schema, even if the function is overloaded. Currently the specific name contains the types of the function arguments. As the format might change in the future, it should be only used to compare it to other instances of specific_name.\n\nschemata\n\nThe schemata table lists all existing schemas. The blob, information_schema, and sys schemas are always available. The doc schema is available after the first user table is created.\n\ncr> select schema_name from information_schema.schemata order by schema_name;\n+--------------------+\n| schema_name        |\n+--------------------+\n| blob               |\n| doc                |\n| information_schema |\n| pg_catalog         |\n| sys                |\n+--------------------+\nSELECT 5 rows in set (... sec)\n\nsql_features\n\nThe sql_features table outlines supported and unsupported SQL features of CrateDB based to the current SQL standard (see SQL standard compliance):\n\ncr> select feature_name, is_supported, sub_feature_id, sub_feature_name\n... from information_schema.sql_features\n... where feature_id='F501';\n+--------------------------------+--------------+----------------+--------------------+\n| feature_name                   | is_supported | sub_feature_id | sub_feature_name   |\n+--------------------------------+--------------+----------------+--------------------+\n| Features and conformance views | FALSE        |                |                    |\n| Features and conformance views | TRUE         | 1              | SQL_FEATURES view  |\n| Features and conformance views | FALSE        | 2              | SQL_SIZING view    |\n| Features and conformance views | FALSE        | 3              | SQL_LANGUAGES view |\n+--------------------------------+--------------+----------------+--------------------+\nSELECT 4 rows in set (... sec)\n\n\nName\n\n\t\n\nData Type\n\n\t\n\nNullable\n\n\n\n\nfeature_id\n\n\t\n\nTEXT\n\n\t\n\nNO\n\n\n\n\nfeature_name\n\n\t\n\nTEXT\n\n\t\n\nNO\n\n\n\n\nsub_feature_id\n\n\t\n\nTEXT\n\n\t\n\nNO\n\n\n\n\nsub_feature_name\n\n\t\n\nTEXT\n\n\t\n\nNO\n\n\n\n\nis_supported\n\n\t\n\nTEXT\n\n\t\n\nNO\n\n\n\n\nis_verified_by\n\n\t\n\nTEXT\n\n\t\n\nYES\n\n\n\n\ncomments\n\n\t\n\nTEXT\n\n\t\n\nYES\n\nfeature_id\n\nIdentifier of the feature\n\nfeature_name\n\nDescriptive name of the feature by the Standard\n\nsub_feature_id\n\nIdentifier of the sub feature; If it has zero-length, this is a feature\n\nsub_feature_name\n\nDescriptive name of the sub feature by the Standard; If it has zero-length, this is a feature\n\nis_supported\n\nYES if the feature is fully supported by the current version of CrateDB, NO if not\n\nis_verified_by\n\nIdentifies the conformance test used to verify the claim;\n\nAlways NULL since the CrateDB development group does not perform formal testing of feature conformance\n\ncomments\n\nEither NULL or shows a comment about the supported status of the feature\n\ncharacter_sets\n\nThe character_sets table identifies the character sets available in the current database.\n\nIn CrateDB there is always a single entry listing UTF8:\n\ncr> SELECT character_set_name, character_repertoire FROM information_schema.character_sets;\n+--------------------+----------------------+\n| character_set_name | character_repertoire |\n+--------------------+----------------------+\n| UTF8               | UCS                  |\n+--------------------+----------------------+\nSELECT 1 row in set (... sec)\n\n\nColumn Name\n\n\t\n\nReturn Type\n\n\t\n\nDescription\n\n\n\n\ncharacter_set_catalog\n\n\t\n\nTEXT\n\n\t\n\nNot implemented, this column is always null.\n\n\n\n\ncharacter_set_schema\n\n\t\n\nTEXT\n\n\t\n\nNot implemented, this column is always null.\n\n\n\n\ncharacter_set_name\n\n\t\n\nTEXT\n\n\t\n\nName of the character set\n\n\n\n\ncharacter_repertoire\n\n\t\n\nTEXT\n\n\t\n\nCharacter repertoire\n\n\n\n\nform_of_use\n\n\t\n\nTEXT\n\n\t\n\nCharacter encoding form, same as character_set_name\n\n\n\n\ndefault_collate_catalog\n\n\t\n\nTEXT\n\n\t\n\nName of the database containing the default collation (Always crate)\n\n\n\n\ndefault_collate_schema\n\n\t\n\nTEXT\n\n\t\n\nName of the schema containing the default collation (Always NULL)\n\n\n\n\ndefault_collate_name\n\n\t\n\nTEXT\n\n\t\n\nName of the default collation (Always NULL)"
  },
  {
    "title": "Optimistic Concurrency Control — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/general/occ.html",
    "html": "4.8\nOptimistic Concurrency Control\n\nTable of contents\n\nIntroduction\n\nOptimistic update\n\nOptimistic delete\n\nKnown limitations\n\nIntroduction\n\nEven though CrateDB does not support transactions, Optimistic Concurrency Control can be achieved by using the internal system columns _seq_no and _primary_term.\n\nEvery new primary shard row has an initial sequence number of 0. This value is increased by 1 on every insert, delete or update operation the primary shard executes. The primary term will be incremented when a shard is promoted to primary so the user can know if they are executing an update against the most up to date cluster configuration.\n\nIt’s possible to fetch the _seq_no and _primary_term by selecting them:\n\ncr> SELECT id, type, _seq_no, _primary_term FROM sensors ORDER BY 1;\n+-----+-------+---------+---------------+\n| id  | type  | _seq_no | _primary_term |\n+-----+-------+---------+---------------+\n| ID1 | DHT11 |       0 |             1 |\n| ID2 | DHT21 |       0 |             1 |\n+-----+-------+---------+---------------+\nSELECT 2 rows in set (... sec)\n\n\nThese _seq_no and _primary_term values can now be used on updates and deletes.\n\nNote\n\nOptimistic concurrency control only works using the = operator, checking for the exact _seq_no and _primary_term your update or delete is based on.\n\nOptimistic update\n\nQuerying for the correct _seq_no and _primary_term ensures that no concurrent update and cluster configuration change has taken place:\n\ncr> UPDATE sensors SET last_verification = '2020-01-10 09:40'\n... WHERE\n...   id = 'ID1'\n...   AND \"_seq_no\" = 0\n...   AND \"_primary_term\" = 1;\nUPDATE OK, 1 row affected (... sec)\n\n\nUpdating a row with a wrong or outdated sequence number or primary term will not execute the update and results in 0 affected rows:\n\ncr> UPDATE sensors SET last_verification = '2020-01-10 09:40'\n... WHERE\n...   id = 'ID1'\n...   AND \"_seq_no\" = 42\n...   AND \"_primary_term\" = 5;\nUPDATE OK, 0 rows affected (... sec)\n\nOptimistic delete\n\nThe same can be done when deleting a row:\n\ncr> DELETE FROM sensors WHERE id = 'ID2'\n...   AND \"_seq_no\" = 0\n...   AND \"_primary_term\" = 1;\nDELETE OK, 1 row affected (... sec)\n\nKnown limitations\n\nThe _seq_no and _primary_term columns can only be used when specifying the whole primary key in a query. For example, the query below is not possible with our used testing data because type is not declared as a primary key and results in an error:\n\ncr> DELETE FROM sensors WHERE type = 'DHT11'\n...   AND \"_seq_no\" = 3\n...   AND \"_primary_term\" = 1;\nUnsupportedFeatureException[\"_seq_no\" and \"_primary_term\" columns can only be used together in the WHERE clause with equals comparisons and if there are also equals comparisons on primary key columns]\n\n\nIn order to use the optimistic concurrency control mechanism both the _seq_no and _primary_term columns need to be specified. It is not possible to only specify one of them. For example, the query below will result in an error:\n\ncr> DELETE FROM sensors WHERE id = 'ID1' AND \"_seq_no\" = 3;\nVersioningValidationException[\"_seq_no\" and \"_primary_term\" columns can only be used together in the WHERE clause with equals comparisons and if there are also equals comparisons on primary key columns]\n\n\nNote\n\nBoth, DELETE and UPDATE, commands will return a row count of 0 if the given required version does not match the actual version of the relevant row."
  },
  {
    "title": "Blobs — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/general/blobs.html",
    "html": "4.8\nBlobs\n\nCrateDB includes support to store binary large objects. By utilizing CrateDB’s cluster features the files can be replicated and sharded just like regular data.\n\nTable of contents\n\nCreating a table for blobs\n\nCustom location for storing blob data\n\nGlobal by configuration\n\nPer blob table setting\n\nList\n\nAltering a blob table\n\nDeleting a blob table\n\nUsing blob tables\n\nUploading\n\nDownloading\n\nDeleting\n\nCreating a table for blobs\n\nBefore adding blobs a blob table must be created. Blob tables can be sharded. This makes it possible to distribute binaries over multiple nodes. Lets use the CrateDB shell crash to issue the SQL statement:\n\nsh$ crash -c \"create blob table myblobs clustered into 3 shards with (number_of_replicas=0)\"\nCREATE OK, 1 row affected (... sec)\n\n\nNow CrateDB is configured to allow blobs to be management under the /_blobs/myblobs endpoint.\n\nCustom location for storing blob data\n\nIt is possible to define a custom directory path for storing blob data which can be completely different than the normal data path. Best use case for this is storing normal data on a fast SSD and blob data on a large cheap spinning disk.\n\nThe custom blob data path can be set either globally by configuration or while creating a blob table. The path can be either absolute or relative and must be creatable/writable by the user CrateDB is running as. A relative path value is relative to CRATE_HOME.\n\nBlob data will be stored under this path with the following layout:\n\n/<blobs.path>/nodes/<NODE_NO>/indices/<INDEX_UUID>/<SHARD_ID>/blobs\n\nGlobal by configuration\n\nJust uncomment or add following entry at the CrateDB configuration in order to define a custom path globally for all blob tables:\n\nblobs.path: /path/to/blob/data\n\n\nAlso see Configuration.\n\nPer blob table setting\n\nIt is also possible to define a custom blob data path per table instead of global by configuration. Also per table setting take precedence over the configuration setting.\n\nSee CREATE BLOB TABLE for details.\n\nCreating a blob table with a custom blob data path:\n\nsh$ crash -c \"create blob table myblobs clustered into 3 shards with (blobs_path='/tmp/crate_blob_data')\" # doctest: +SKIP\nCREATE OK, 1 row affected (... sec)\n\nList\n\nTo list all blobs inside a blob table a SELECT statement can be used:\n\nsh$ crash -c \"select digest, last_modified from blob.myblobs\"\n+------------------------------------------+---------------+\n| digest                                   | last_modified |\n+------------------------------------------+---------------+\n| 4a756ca07e9487f482465a99e8286abc86ba4dc7 | ...           |\n+------------------------------------------+---------------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nTo query blob tables it is necessary to always specify the schema name blob.\n\nAltering a blob table\n\nThe number of replicas a blob table has can be changed using the ALTER BLOB TABLE clause:\n\nsh$ crash -c \"alter blob table myblobs set (number_of_replicas=0)\"\nALTER OK, -1 rows affected (... sec)\n\nDeleting a blob table\n\nBlob tables can be deleted similar to normal tables:\n\nsh$ crash -c \"drop blob table myblobs\"\nDROP OK, 1 row affected (... sec)\n\nUsing blob tables\n\nThe usage of Blob Tables is only supported using the HTTP/HTTPS protocol. This section describes how binaries can be stored, fetched and deleted.\n\nNote\n\nFor the reason of internal optimization any successful request could lead to a 307 Temporary Redirect response.\n\nUploading\n\nTo upload a blob the SHA1 hash of the blob has to be known upfront since this has to be used as the ID of the new blob. For this example we use a fancy Python one-liner to compute the SHA hash:\n\nsh$ python3 -c 'import hashlib;print(hashlib.sha1(\"contents\".encode(\"utf-8\")).hexdigest())'\n4a756ca07e9487f482465a99e8286abc86ba4dc7\n\n\nThe blob can now be uploaded by issuing a PUT request:\n\nsh$ curl -isSX PUT '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7' -d 'contents'\nHTTP/1.1 201 Created\ncontent-length: 0\n\n\nIf a blob already exists with the given hash a 409 Conflict is returned:\n\nsh$ curl -isSX PUT '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7' -d 'contents'\nHTTP/1.1 409 Conflict\ncontent-length: 0\n\nDownloading\n\nTo download a blob simply use a GET request:\n\nsh$ curl -sS '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7'\ncontents\n\n\nIf the blob doesn’t exist a 404 Not Found error is returned:\n\nsh$ curl -isS '127.0.0.1:4200/_blobs/myblobs/e5fa44f2b31c1fb553b6021e7360d07d5d91ff5e'\nHTTP/1.1 404 Not Found\ncontent-length: 0\n\n\nTo determine if a blob exists without downloading it, a HEAD request can be used:\n\nsh$ curl -sS -I '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7'\nHTTP/1.1 200 OK\ncontent-length: 8\naccept-ranges: bytes\nexpires: Thu, 31 Dec 2037 23:59:59 GMT\ncache-control: max-age=315360000\n\n\nNote\n\nThe cache headers for blobs are static and basically allows clients to cache the response forever since the blob is immutable.\n\nDeleting\n\nTo delete a blob simply use a DELETE request:\n\nsh$ curl -isS -XDELETE '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7'\nHTTP/1.1 204 No Content\n\n\nIf the blob doesn’t exist a 404 Not Found error is returned:\n\nsh$ curl -isS -XDELETE '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7'\nHTTP/1.1 404 Not Found\ncontent-length: 0\n"
  },
  {
    "title": "User-defined functions — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/general/user-defined-functions.html",
    "html": "4.8\nUser-defined functions\n\nTable of contents\n\nCREATE OR REPLACE\n\nSupported types\n\nOverloading\n\nDeterminism\n\nDROP FUNCTION\n\nSupported languages\n\nJavaScript\n\nJavaScript supported types\n\nWorking with NUMBERS\n\nCREATE OR REPLACE\n\nCrateDB supports user-defined functions. See CREATE FUNCTION for a full syntax description.\n\nCREATE FUNCTION defines a new function:\n\ncr> CREATE FUNCTION my_subtract_function(integer, integer)\n... RETURNS integer\n... LANGUAGE JAVASCRIPT\n... AS 'function my_subtract_function(a, b) { return a - b; }';\nCREATE OK, 1 row affected  (... sec)\n\ncr> SELECT doc.my_subtract_function(3, 1) AS col;\n+-----+\n| col |\n+-----+\n|   2 |\n+-----+\nSELECT 1 row in set (... sec)\n\n\nCREATE OR REPLACE FUNCTION will either create a new function or replace an existing function definition:\n\ncr> CREATE OR REPLACE FUNCTION log10(bigint)\n... RETURNS double precision\n... LANGUAGE JAVASCRIPT\n... AS 'function log10(a) {return Math.log(a)/Math.log(10); }';\nCREATE OK, 1 row affected  (... sec)\n\ncr> SELECT doc.log10(10) AS col;\n+-----+\n| col |\n+-----+\n| 1.0 |\n+-----+\nSELECT 1 row in set (... sec)\n\n\nIt is possible to use named function arguments in the function signature. For example, the calculate_distance function signature has two geo_point arguments named start and end:\n\ncr> CREATE OR REPLACE FUNCTION calculate_distance(\"start\" geo_point, \"end\" geo_point)\n... RETURNS real\n... LANGUAGE JAVASCRIPT\n... AS 'function calculate_distance(start, end) {\n...       return Math.sqrt(\n...            Math.pow(end[0] - start[0], 2),\n...            Math.pow(end[1] - start[1], 2));\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\n\nNote\n\nArgument names are used for query documentation purposes only. You cannot reference arguments by name in the function body.\n\nOptionally, a schema-qualified function name can be defined. If you omit the schema, the current session schema is used:\n\ncr> CREATE OR REPLACE FUNCTION my_schema.log10(bigint)\n... RETURNS double precision\n... LANGUAGE JAVASCRIPT\n... AS 'function log10(a) { return Math.log(a)/Math.log(10); }';\nCREATE OK, 1 row affected  (... sec)\n\n\nNote\n\nIn order to improve the PostgreSQL server compatibility CrateDB allows the creation of user defined functions against the pg_catalog schema. However, the creation of user defined functions against the read-only System information and Information schema schemas is prohibited.\n\nSupported types\n\nFunction arguments and return values can be any of the supported data types. The values passed into a function must strictly correspond to the specified argument data types.\n\nNote\n\nThe value returned by the function will be casted to the return type provided in the definition if required. An exception will be thrown if the cast is not successful.\n\nOverloading\n\nWithin a specific schema, you can overload functions by defining functions with the same name but a different set of arguments:\n\ncr> CREATE FUNCTION my_schema.my_multiply(integer, integer)\n... RETURNS integer\n... LANGUAGE JAVASCRIPT\n... AS 'function my_multiply(a, b) { return a * b; }';\nCREATE OK, 1 row affected  (... sec)\n\n\nThis would overload the my_multiply function with different argument types:\n\ncr> CREATE FUNCTION my_schema.my_multiply(bigint, bigint)\n... RETURNS bigint\n... LANGUAGE JAVASCRIPT\n... AS 'function my_multiply(a, b) { return a * b; }';\nCREATE OK, 1 row affected  (... sec)\n\n\nThis would overload the my_multiply function with more arguments:\n\ncr> CREATE FUNCTION my_schema.my_multiply(bigint, bigint, bigint)\n... RETURNS bigint\n... LANGUAGE JAVASCRIPT\n... AS 'function my_multiply(a, b, c) { return a * b * c; }';\nCREATE OK, 1 row affected  (... sec)\n\n\nCaution\n\nIt is considered bad practice to create functions that have the same name as the CrateDB built-in functions!\n\nNote\n\nIf you call a function without a schema name, CrateDB will look it up in the built-in functions first and only then in the user-defined functions available in the search_path.\n\nTherefore a built-in function with the same name as a user-defined function will hide the latter, even if it contains a different set of arguments! However, such functions can still be called if the schema name is explicitly provided.\n\nDeterminism\n\nCaution\n\nUser-defined functions need to be deterministic, meaning that they must always return the same result value when called with the same argument values, because CrateDB might cache the returned values and reuse the value if the function is called multiple times with the same arguments.\n\nDROP FUNCTION\n\nFunctions can be dropped like this:\n\ncr> DROP FUNCTION doc.log10(bigint);\nDROP OK, 1 row affected  (... sec)\n\n\nAdding IF EXISTS prevents from raising an error if the function doesn’t exist:\n\ncr> DROP FUNCTION IF EXISTS doc.log10(integer);\nDROP OK, 1 row affected  (... sec)\n\n\nOptionally, argument names can be specified within the drop statement:\n\ncr> DROP FUNCTION IF EXISTS doc.calculate_distance(start_point geo_point, end_point geo_point);\nDROP OK, 1 row affected  (... sec)\n\n\nOptionally, you can provide a schema:\n\ncr> DROP FUNCTION my_schema.log10(bigint);\nDROP OK, 1 row affected  (... sec)\n\nSupported languages\n\nCurrently, CrateDB only supports JavaScript for user-defined functions.\n\nJavaScript\n\nThe user defined function JavaScript is compatible with the ECMAScript 2019 specification.\n\nCrateDB uses the GraalVM JavaScript engine as a JavaScript (ECMAScript) language execution runtime. The GraalVM JavaScript engine is a Java application that works on the stock Java Virtual Machines (VMs). The interoperability between Java code (host language) and JavaScript user-defined functions (guest language) is guaranteed by the GraalVM Polyglot API.\n\nPlease note: CrateDB does not use the GraalVM JIT compiler as optimizing compiler. However, the stock host Java VM JIT compilers can JIT-compile, optimize, and execute the GraalVM JavaScript codebase to a certain extent.\n\nThe execution context for guest JavaScript is created with restricted privileges to allow for the safe execution of less trusted guest language code. The guest language application context for each user-defined function is created with default access modifiers, so any access to managed resources is denied. The only exception is the host language interoperability configuration which explicitly allows access to Java lists and arrays. Please refer to GraalVM Security Guide for more detailed information.\n\nAlso, even though user-defined functions implemented with ECMA-compliant JavaScript, objects that are normally accessible with a web browser (e.g. window, console, and so on) are not available.\n\nNote\n\nGraalVM treats objects provided to JavaScript user-defined functions as close as possible to their respective counterparts and therefore by default only a subset of prototype functions are available in user-defined functions. For CrateDB 4.6 and earlier the object prototype was disabled.\n\nPlease refer to the GraalVM JavaScript Compatibility FAQ to learn more about the compatibility.\n\nJavaScript supported types\n\nJavaScript functions can handle all CrateDB data types. However, for some return types the function output must correspond to the certain format.\n\nIf a function requires geo_point as a return type, then the JavaScript function must return a double precision array of size 2, WKT string or GeoJson object.\n\nHere is an example of a JavaScript function returning a double array:\n\ncr> CREATE FUNCTION rotate_point(point geo_point, angle real)\n... RETURNS geo_point\n... LANGUAGE JAVASCRIPT\n... AS 'function rotate_point(point, angle) {\n...       var cos = Math.cos(angle);\n...       var sin = Math.sin(angle);\n...       var x = cos * point[0] - sin * point[1];\n...       var y = sin * point[0] + cos * point[1];\n...       return [x, y];\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\n\nBelow is an example of a JavaScript function returning a WKT string, which will be cast to geo_point:\n\ncr> CREATE FUNCTION symmetric_point(point geo_point)\n... RETURNS geo_point\n... LANGUAGE JAVASCRIPT\n... AS 'function symmetric_point (point, angle) {\n...       var x = - point[0],\n...           y = - point[1];\n...       return \"POINT (\\\" + x + \\\", \\\" + y +\\\")\";\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\n\nSimilarly, if the function specifies the geo_shape return data type, then the JavaScript function should return a GeoJson object or WKT string:\n\ncr> CREATE FUNCTION line(\"start\" array(double precision), \"end\" array(double precision))\n... RETURNS object\n... LANGUAGE JAVASCRIPT\n... AS 'function line(start, end) {\n...        return { \"type\": \"LineString\", \"coordinates\" : [start_point, end_point] };\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\n\nNote\n\nIf the return value of the JavaScript function is undefined, it is converted to NULL.\n\nWorking with NUMBERS\n\nThe JavaScript engine interprets numbers as java.lang.Double, java.lang.Long, or java.lang.Integer, depending on the computation performed. In most cases, this is not an issue, since the return type of the JavaScript function will be cast to the return type specified in the CREATE FUNCTION statement, although cast might result in a loss of precision.\n\nHowever, when you try to cast DOUBLE PRECISION to TIMESTAMP WITH TIME ZONE, it will be interpreted as UTC seconds and will result in a wrong value:\n\ncr> CREATE FUNCTION utc(bigint, bigint, bigint)\n... RETURNS TIMESTAMP WITH TIME ZONE\n... LANGUAGE JAVASCRIPT\n... AS 'function utc(year, month, day) {\n...       return Date.UTC(year, month, day, 0, 0, 0);\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\ncr> SELECT date_format(utc(2016,04,6)) as epoque;\n+------------------------------+\n| epoque                       |\n+------------------------------+\n| 48314-07-22T00:00:00.000000Z |\n+------------------------------+\nSELECT 1 row in set (... sec)\n\n\nTo avoid this behavior, the numeric value should be divided by 1000 before it is returned:\n\ncr> CREATE FUNCTION utc(bigint, bigint, bigint)\n... RETURNS TIMESTAMP WITH TIME ZONE\n... LANGUAGE JAVASCRIPT\n... AS 'function utc(year, month, day) {\n...       return Date.UTC(year, month, day, 0, 0, 0)/1000;\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\ncr> SELECT date_format(utc(2016,04,6)) as epoque;\n+-----------------------------+\n| epoque                      |\n+-----------------------------+\n| 2016-05-06T00:00:00.000000Z |\n+-----------------------------+\nSELECT 1 row in set (... sec)\n"
  },
  {
    "title": "Built-in functions and operators — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/general/builtins/index.html",
    "html": "4.8\nBuilt-in functions and operators\n\nThis chapter provides an overview of built-in functions and operators.\n\nTable of contents\n\nScalar functions\nString functions\nDate and time functions\nGeo functions\nMathematical functions\nRegular expression functions\nArray functions\nConditional functions and expressions\nSystem information functions\nSpecial functions\nAggregation\nAggregate expressions\nAggregate functions\nLimitations\nArithmetic operators\nTable functions\nScalar functions\nempty_row( )\nunnest( array [ array , ] )\npg_catalog.generate_series(start, stop, [step])\npg_catalog.generate_subscripts(array, dim, [reverse])\nregexp_matches(source, pattern [, flags])\npg_catalog.pg_get_keywords()\ninformation_schema._pg_expandarray(array)\nComparison operators\nBasic operators\nWHERE clause operators\nArray comparisons\nIN (value [, ...])\nANY/SOME (array expression)\nALL (array_expression)\nSubquery expressions\nIN (subquery)\nANY/SOME (subquery)\nALL (subquery)\nWindow functions\nWindow function call\nWindow definition\nGeneral-purpose window functions\nAggregate window functions"
  },
  {
    "title": "Querying — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/general/dql/index.html",
    "html": "4.8\nQuerying\n\nThis section provides an overview of how to query CrateDB.\n\nSee Also\n\nGeneral use: Data definition\n\nGeneral use: Data manipulation\n\nTable of contents\n\nSelecting data\nIntroduction\nFROM clause\nJoins\nDISTINCT clause\nWHERE clause\nComparison operators\nArray comparisons\nContainer data types\nAggregation\nWindow functions\nGROUP BY\nJoins\nCross joins\nInner joins\nOuter joins\nJoin conditions\nAvailable join algorithms\nLimitations\nUnion\nUnion All\nUnion Distinct\nRefresh\nIntroduction\nMultiple Table Refresh\nPartition Refresh\nFulltext search\nMATCH Predicate\nUsage\nSearching On Multiple Columns\nNegative Search\nFilter By _score\nGeo search\nIntroduction\nMATCH predicate\nExact queries"
  },
  {
    "title": "Data definition — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/general/ddl/index.html",
    "html": "4.8\nData definition\n\nThis section provides an overview of how to create tables and perform other data-definition related operations with CrateDB.\n\nSee Also\n\nGeneral use: Data manipulation\n\nGeneral use: Querying\n\nTable of contents\n\nCreating tables\nTable definition\nTable configuration\nData types\nOverview\nPrimitive types\nContainer types\nGeographic types\nType casting\nPostgreSQL compatibility\nSystem columns\nGenerated columns\nGeneration expressions\nLast modified dates\nPartitioning\nConstraints\nPrimary key\nNot null\nCheck\nStorage\nColumn store\nPartitioned tables\nIntroduction\nCreation\nInformation schema\nInsert\nUpdate\nDelete\nQuerying\nAlter\nLimitations\nConsistency notes related to concurrent DML statement\nSharding\nIntroduction\nNumber of shards\nRouting\nReplication\nTable configuration\nShard recovery\nUnderreplication\nShard allocation filtering\nSettings\nSpecial attributes\nColumn policy\nstrict\ndynamic\nFulltext indices\nIndex definition\nDisable indexing\nPlain index (default)\nCreating a custom analyzer\nExtending a built-in analyzer\nFulltext analyzers\nOverview\nBuilt-in analyzers\nBuilt-in tokenizers\nBuilt-in token filters\nBuilt-in char filter\nShow Create Table\nViews\nCreating views\nQuerying views\nDropping views\nAltering tables\nUpdating parameters\nAdding columns\nClosing and opening tables\nRenaming tables\nReroute shards"
  },
  {
    "title": "Data manipulation — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/general/dml.html",
    "html": "4.8\nData manipulation\n\nThis section provides an overview of how to manipulate data (e.g., inserting rows) with CrateDB.\n\nSee Also\n\nGeneral use: Data definition\n\nGeneral use: Querying\n\nTable of contents\n\nInserting data\n\nInserting data by query\n\nUpserts (ON CONFLICT DO UPDATE SET)\n\nUpdating data\n\nDeleting data\n\nImport and export\n\nImporting data\n\nExample\n\nDetailed error reporting\n\nExporting data\n\nInserting data\n\nInserting data to CrateDB is done by using the SQL INSERT statement.\n\nNote\n\nThe column list is always ordered based on the column position in the CREATE TABLE statement of the table. If the insert columns are omitted, the values in the VALUES clauses must correspond to the table columns in that order.\n\nInserting a row:\n\ncr> insert into locations (id, date, description, kind, name, position)\n... values (\n...   '14',\n...   '2013-09-12T21:43:59.000Z',\n...   'Blagulon Kappa is the planet to which the police are native.',\n...   'Planet',\n...   'Blagulon Kappa',\n...   7\n... );\nINSERT OK, 1 row affected (... sec)\n\n\nWhen inserting a single row, if an error occurs an error is returned as a response.\n\nInserting multiple rows at once (aka. bulk insert) can be done by defining multiple values for the INSERT statement:\n\ncr> insert into locations (id, date, description, kind, name, position) values\n... (\n...   '16',\n...   '2013-09-14T21:43:59.000Z',\n...   'Blagulon Kappa II is the planet to which the police are native.',\n...   'Planet',\n...   'Blagulon Kappa II',\n...   19\n... ),\n... (\n...   '17',\n...   '2013-09-13T16:43:59.000Z',\n...   'Brontitall is a planet with a warm, rich atmosphere and no mountains.',\n...   'Planet',\n...   'Brontitall',\n...   10\n... );\nINSERT OK, 2 rows affected (... sec)\n\n\nWhen inserting multiple rows, if an error occurs for some of these rows there is no error returned but instead the number of rows affected would be decreased by the number of rows that failed to be inserted.\n\nWhen inserting into tables containing Generated columns or Base Columns having the Default clause specified, their values can be safely omitted. They are generated upon insert:\n\ncr> CREATE TABLE debit_card (\n...   owner text,\n...   num_part1 integer,\n...   num_part2 integer,\n...   check_sum integer GENERATED ALWAYS AS ((num_part1 + num_part2) * 42),\n...   \"user\" text DEFAULT 'crate'\n... );\nCREATE OK, 1 row affected (... sec)\n\ncr> insert into debit_card (owner, num_part1, num_part2) values\n... ('Zaphod Beeblebrox', 1234, 5678);\nINSERT OK, 1 row affected (... sec)\n\ncr> select * from debit_card;\n+-------------------+-----------+-----------+-----------+-------+\n| owner             | num_part1 | num_part2 | check_sum | user  |\n+-------------------+-----------+-----------+-----------+-------+\n| Zaphod Beeblebrox |      1234 |      5678 |    290304 | crate |\n+-------------------+-----------+-----------+-----------+-------+\nSELECT 1 row in set (... sec)\n\n\nFor Generated columns, if the value is given, it is validated against the generation clause of the column and the currently inserted row:\n\ncr> insert into debit_card (owner, num_part1, num_part2, check_sum) values\n... ('Arthur Dent', 9876, 5432, 642935);\nSQLParseException[Given value 642935 for generated column check_sum does not match calculation ((num_part1 + num_part2) * 42) = 642936]\n\nInserting data by query\n\nIt is possible to insert data using a query instead of values. Column data types of source and target table can differ as long as the values are castable. This gives the opportunity to restructure the tables data, renaming a field, changing a field’s data type or convert a normal table into a partitioned one.\n\nExample of changing a field’s data type, in this case, changing the position data type from integer to smallint:\n\ncr> create table locations2 (\n...     id text primary key,\n...     name text,\n...     date timestamp with time zone,\n...     kind text,\n...     position smallint,\n...     description text\n... ) clustered by (id) into 2 shards with (number_of_replicas = 0);\nCREATE OK, 1 row affected (... sec)\n\ncr> insert into locations2 (id, name, date, kind, position, description)\n... (\n...     select id, name, date, kind, position, description\n...     from locations\n...     where position < 10\n... );\nINSERT OK, 14 rows affected (... sec)\n\n\nExample of creating a new partitioned table out of the locations table with data partitioned by year:\n\ncr> create table locations_parted (\n...     id text primary key,\n...     name text,\n...     year text primary key,\n...     date timestamp with time zone,\n...     kind text,\n...     position integer\n... ) clustered by (id) into 2 shards\n... partitioned by (year) with (number_of_replicas = 0);\nCREATE OK, 1 row affected (... sec)\n\ncr> insert into locations_parted (id, name, year, date, kind, position)\n... (\n...     select\n...         id,\n...         name,\n...         date_format('%Y', date),\n...         date,\n...         kind,\n...         position\n...     from locations\n... );\nINSERT OK, 16 rows affected (... sec)\n\n\nResulting partitions of the last insert by query:\n\ncr> select table_name, partition_ident, values, number_of_shards, number_of_replicas\n... from information_schema.table_partitions\n... where table_name = 'locations_parted'\n... order by partition_ident;\n+------------------+-----------------+------------------+------------------+--------------------+\n| table_name       | partition_ident | values           | number_of_shards | number_of_replicas |\n+------------------+-----------------+------------------+------------------+--------------------+\n| locations_parted | 042j2e9n74      | {\"year\": \"1979\"} |                2 |                  0 |\n| locations_parted | 042j4c1h6c      | {\"year\": \"2013\"} |                2 |                  0 |\n+------------------+-----------------+------------------+------------------+--------------------+\nSELECT 2 rows in set (... sec)\n\n\nNote\n\nlimit, offset and order by are not supported inside the query statement.\n\nUpserts (ON CONFLICT DO UPDATE SET)\n\nThe ON CONFLICT DO UPDATE SET clause is used to update the existing row if inserting is not possible because of a duplicate-key conflict if a document with the same PRIMARY KEY already exists. This is type of operation is commonly referred to as an upsert, being a combination of “update” and “insert”.\n\ncr> SELECT\n...     name,\n...     visits,\n...     extract(year from last_visit) AS last_visit\n... FROM uservisits ORDER BY NAME;\n+----------+--------+------------+\n| name     | visits | last_visit |\n+----------+--------+------------+\n| Ford     |      1 | 2013       |\n| Trillian |      3 | 2013       |\n+----------+--------+------------+\nSELECT 2 rows in set (... sec)\n\ncr> INSERT INTO uservisits (id, name, visits, last_visit) VALUES\n... (\n...     0,\n...     'Ford',\n...     1,\n...     '2015-01-12'\n... ) ON CONFLICT (id) DO UPDATE SET\n...     visits = visits + 1;\nINSERT OK, 1 row affected (... sec)\n\ncr> SELECT\n...     name,\n...     visits,\n...     extract(year from last_visit) AS last_visit\n... FROM uservisits WHERE id = 0;\n+------+--------+------------+\n| name | visits | last_visit |\n+------+--------+------------+\n| Ford |      2 | 2013       |\n+------+--------+------------+\nSELECT 1 row in set (... sec)\n\n\nIt’s possible to refer to values which would be inserted if no duplicate-key conflict occurred, by using the special excluded table. This table is especially useful in multiple-row inserts, to refer to the current rows values:\n\ncr> INSERT INTO uservisits (id, name, visits, last_visit) VALUES\n... (\n...     0,\n...     'Ford',\n...     2,\n...     '2016-01-13'\n... ),\n... (\n...     1,\n...     'Trillian',\n...     5,\n...     '2016-01-15'\n... ) ON CONFLICT (id) DO UPDATE SET\n...     visits = visits + excluded.visits,\n...     last_visit = excluded.last_visit;\nINSERT OK, 2 rows affected (... sec)\n\ncr> SELECT\n...     name,\n...     visits,\n...     extract(year from last_visit) AS last_visit\n... FROM uservisits ORDER BY name;\n+----------+--------+------------+\n| name     | visits | last_visit |\n+----------+--------+------------+\n| Ford     |      4 | 2016       |\n| Trillian |      8 | 2016       |\n+----------+--------+------------+\nSELECT 2 rows in set (... sec)\n\n\nThis can also be done when using a query instead of values:\n\ncr> CREATE TABLE uservisits2 (\n...   id integer primary key,\n...   name text,\n...   visits integer,\n...   last_visit timestamp with time zone\n... ) CLUSTERED BY (id) INTO 2 SHARDS WITH (number_of_replicas = 0);\nCREATE OK, 1 row affected (... sec)\n\ncr> INSERT INTO uservisits2 (id, name, visits, last_visit)\n... (\n...     SELECT id, name, visits, last_visit\n...     FROM uservisits\n... );\nINSERT OK, 2 rows affected (... sec)\n\ncr> INSERT INTO uservisits2 (id, name, visits, last_visit)\n... (\n...     SELECT id, name, visits, last_visit\n...     FROM uservisits\n... ) ON CONFLICT (id) DO UPDATE SET\n...     visits = visits + excluded.visits,\n...     last_visit = excluded.last_visit;\nINSERT OK, 2 rows affected (... sec)\n\ncr> SELECT\n...     name,\n...     visits,\n...     extract(year from last_visit) AS last_visit\n... FROM uservisits ORDER BY name;\n+----------+--------+------------+\n| name     | visits | last_visit |\n+----------+--------+------------+\n| Ford     |      4 | 2016       |\n| Trillian |      8 | 2016       |\n+----------+--------+------------+\nSELECT 2 rows in set (... sec)\n\n\nSee Also\n\nSQL syntax: ON CONFLICT DO UPDATE SET\n\nUpdating data\n\nIn order to update documents in CrateDB the SQL UPDATE statement can be used:\n\ncr> update locations set description = 'Updated description'\n... where name = 'Bartledan';\nUPDATE OK, 1 row affected (... sec)\n\n\nUpdating nested objects is also supported:\n\ncr> update locations set inhabitants['name'] = 'Human' where name = 'Bartledan';\nUPDATE OK, 1 row affected (... sec)\n\n\nIt’s also possible to reference a column within the expression, for example to increment a number like this:\n\ncr> update locations set position = position + 1 where position < 3;\nUPDATE OK, 6 rows affected (... sec)\n\n\nNote\n\nIf the same documents are updated concurrently an VersionConflictException might occur. CrateDB contains a retry logic that tries to resolve the conflict automatically.\n\nDeleting data\n\nDeleting rows in CrateDB is done using the SQL DELETE statement:\n\ncr> delete from locations where position > 3;\nDELETE OK, ... rows affected (... sec)\n\nImport and export\nImporting data\n\nUsing the COPY FROM statement, CrateDB nodes can import data from local files or files that are available over the network.\n\nThe supported data formats are JSON and CSV. The format is inferred from the file extension, if possible. Alternatively the format can also be provided as an option (see WITH). If the format is not provided and cannot be inferred from the file extension, it will be processed as JSON.\n\nJSON files must contain a single JSON object per line.\n\nExample JSON data:\n\n{\"id\": 1, \"quote\": \"Don't panic\"}\n{\"id\": 2, \"quote\": \"Ford, you're turning into a penguin. Stop it.\"}\n\n\nCSV files must contain a header with comma-separated values, which will be added as columns.\n\nExample CSV data:\n\nid,quote\n1,\"Don't panic\"\n2,\"Ford, you're turning into a penguin. Stop it.\"\n\n\nNote\n\nThe COPY FROM statement will convert and validate your data.\n\nValues for generated columns will be computed if the data does not contain them, otherwise they will be imported and validated\n\nFurthermore, column names in your data are considered case sensitive (as if they were quoted in a SQL statement).\n\nFor further information, including how to import data to Partitioned tables, take a look at the COPY FROM reference.\n\nExample\n\nHere’s an example statement:\n\ncr> COPY quotes FROM 'file:///tmp/import_data/quotes.json';\nCOPY OK, 3 rows affected (... sec)\n\n\nThis statement imports data from the /tmp/import_data/quotes.json file into a table named quotes.\n\nNote\n\nThe file you specify must be available on one of the CrateDB nodes. This statement will not work with files that are local to your client.\n\nFor the above statement, every node in the cluster will attempt to import data from a file located at /tmp/import_data/quotes.json relative to the crate process (i.e., if you are running CrateDB inside a container, the file must also be inside the container).\n\nIf you want to import data from a file that on your local computer using COPY FROM, you must first transfer the file to one of the CrateDB nodes.\n\nConsult the COPY FROM reference for additional information.\n\nIf you want to import all files inside the /tmp/import_data directory on every CrateDB node, you can use a wildcard, like so:\n\ncr> COPY quotes FROM '/tmp/import_data/*' WITH (bulk_size = 4);\nCOPY OK, 3 rows affected (... sec)\n\n\nThis wildcard can also be used to only match certain files in a directory:\n\ncr> COPY quotes FROM '/tmp/import_data/qu*.json';\nCOPY OK, 3 rows affected (... sec)\n\nDetailed error reporting\n\nIf the RETURN_SUMMARY clause is specified, a result set containing information about failures and successfully imported records is returned.\n\ncr> COPY locations FROM '/tmp/import_data/locations_with_failure/locations*.json' RETURN SUMMARY;\n +--...--+----------...--------+---------------+-------------+--------------------...-------------------------------------+\n | node  | uri                 | success_count | error_count | errors                                                     |\n +--...--+----------...--------+---------------+-------------+--------------------...-------------------------------------+\n | {...} | .../locations1.json |             6 |           0 | {}                                                         |\n | {...} | .../locations2.json |             5 |           2 | {\"Cannot cast value...{\"count\": ..., \"line_numbers\": ...}} |\n +--...--+----------...--------+---------------+-------------+--------------------...-------------------------------------+\n COPY 2 rows in set (... sec)\n\n\nIf an error happens while processing the URI in general, the error_count and success_count columns will contains NULL values to indicate that no records were processed.\n\ncr> COPY locations FROM '/tmp/import_data/not-existing.json' RETURN SUMMARY;\n +--...--+-----------...---------+---------------+-------------+------------------------...------------------------+\n | node  | uri                   | success_count | error_count | errors                                            |\n +--...--+-----------...---------+---------------+-------------+------------------------...------------------------+\n | {...} | .../not-existing.json |          NULL |        NULL | {\"...not-existing.json (...)\": {\"count\": 1, ...}} |\n +--...--+-----------...---------+---------------+-------------+------------------------...------------------------+\nCOPY 1 row in set (... sec)\n\n\nSee COPY FROM for more information.\n\nExporting data\n\nData can be exported using the COPY TO statement. Data is exported in a distributed way, meaning each node will export its own data.\n\nReplicated data is not exported. So every row of an exported table is stored only once.\n\nThis example shows how to export a given table into files named after the table and shard ID with gzip compression:\n\ncr> REFRESH TABLE quotes;\nREFRESH OK...\n\ncr> COPY quotes TO DIRECTORY '/tmp/' with (compression='gzip');\nCOPY OK, 3 rows affected ...\n\n\nInstead of exporting a whole table, rows can be filtered by an optional WHERE clause condition. This is useful if only a subset of the data needs to be exported:\n\ncr> COPY quotes WHERE match(quote_ft, 'time') TO DIRECTORY '/tmp/' WITH (compression='gzip');\nCOPY OK, 2 rows affected ...\n\n\nFor further details see COPY TO."
  },
  {
    "title": "Environment variables — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/config/environment.html",
    "html": "4.8\nEnvironment variables\n\nCrateDB can be configured with some environment variables.\n\nThere are many different ways to set environment variables, depending on how CrateDB is being deployed.\n\nHere is a trivial example:\n\nsh$ export CRATE_HOME=/tmp/crate\nsh$ ./bin/crate\n\n\nHere, we set CRATE_HOME to /tmp/crate, export it so that sub-processes of the shell have access, and then start CrateDB.\n\nCrateDB supports two kinds of environment variables:\n\nApplication variables\n\nJava Virtual Machine (JVM) variables\n\nWe use “application” here to distinguish between CrateDB running as a Java application and the JVM itself, which runs CrateDB.\n\nTable of contents\n\nApplication variables\n\nJVM variables\n\nGeneral\n\nApplication variables\nCRATE_HOME: directory path\n\nThe home directory of the CrateDB installation.\n\nThis directory is used as the root for the configuration directory, data directory, log directory, and so on.\n\nIf you have installed CrateDB from a package, this variable should be set for you.\n\nIf you are installing manually, in most cases, this should be set to the directory from which you would normally execute bin/crate, i.e. the root directory of the basic installation.\n\nJVM variables\nGeneral\nCRATE_JAVA_OPTS: Java options\n\nThe Java options to use when running CrateDB.\n\nFor example, you could change the stack size like this:\n\nCRATE_JAVA_OPTS=-Xss500k\n\n\nSee Also\n\nFor more information about Java options, consult the documentation for Microsoft Windows or Unix-like operating systems.\n\nCRATE_HEAP_SIZE: size\n\nThe Java heap size, i.e. the amount of memory that can be used.\n\nYou can set the heap size to four gigabytes like this:\n\nCRATE_HEAP_SIZE=4g\n\n\nUse g for gigabytes or m for megabytes.\n\nSee Also\n\nAppropriate memory configuration is important for optimal performance.\n\nCRATE_HEAP_DUMP_PATH: file or directory path (default: varies)\n\nThe directory to be used for heap dumps in the case of a crash.\n\nIf a directory path is configured, new heap dumps will be written to that directory every time CrateDB crashes.\n\nIf a file path is configured (i.e. the last node of the path is non-existent or exists and is a file) CrateDB will overwrite that file with a heap dump every time it crashes.\n\nDefault values are as follows:\n\nFor a basic installation, the process working directory\n\nIf you have installed a CrateDB Linux package, /var/lib/crate\n\nWhen running CrateDB on Docker, /data/data\n\nWarning\n\nMake sure there is enough disk space available for heap dumps."
  },
  {
    "title": "Logging — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/config/logging.html",
    "html": "4.8\nLogging\n\nCrateDB supports two kinds of logging:\n\nApplication logging with Log4j\n\nJava Virtual Machine (JVM) garbage collection logging\n\nWe use “application” here to distinguish between CrateDB running as a Java application and the JVM itself, which runs CrateDB.\n\nBecause garbage collection logging is a native feature of the JVM it behaves differently and is configured differently.\n\nTable of contents\n\nApplication logging\n\nLog4j\n\nConfiguration file\n\nLog levels\n\nRun-time configuration\n\nJVM logging\n\nGarbage collection\n\nEnvironment variables\n\nApplication logging\nLog4j\n\nCrateDB uses Log4j.\n\nConfiguration file\n\nYou can configure Log4j with the log4j2.properties file in the CrateDB configuration directory.\n\nThe log4j2.properties file is formatted using YAML and simplifies Log4j configuration by allowing you to use the PropertyConfigurator but without having to tediously repeat the log4j prefix.\n\nHere’s one example:\n\nrootLogger.level = info\nrootLogger.appenderRef.console.ref = console\n\n# log query execution errors for easier debugging\nlogger.action.name = org.crate.action.sql\nlogger.action.level = debug\n\nappender.console.type = Console\nappender.console.name = console\nappender.console.layout.type = PatternLayout\nappender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c{1.}] %marker%m%n\n\n\nAnd here is a snippet of the generated properties ready for use with log4j. You get the point.\n\nSee Also\n\nConsult the PropertyConfigurator documentation or the configuration section of the Log4j documentation for more information.\n\nLog levels\n\nPossible log levels are the same as for Log4j, in order of increasing importance:\n\nTRACE\n\nDEBUG\n\nINFO\n\nWARN\n\nERROR\n\nLog levels must be provided as string literals in the SET statement.\n\nNote\n\nBe careful using the TRACE log level because it’s extremely verbose, can obscure other important log messages and even fill up entire data disks in some cases.\n\nRun-time configuration\n\nIt’s possible to set the log level of loggers at runtime using SET, like so:\n\nSET GLOBAL TRANSIENT \"logger.action\" = 'INFO';\n\n\nIn this example, the log level INFO is applied to the action logger.\n\nIn addition to being able to configure any of the standard loggers, you can configure the root (i.e. default) logger using logger._root.\n\nAs with any setting, you can inspect the current configuration by querying the sys.cluster table.\n\nTip\n\nRun-time logging configuration is particularly useful if you are debugging a problem and you want to increase the log level without restarting nodes.\n\nRun-time logging configuration is applied across the whole cluster, and overrides the start-up configuration defined in each respective log4j2.properties file.\n\nCaution\n\nThe RESET statement is supported but logging configuration is only reset when the whole cluster is restarted.\n\nJVM logging\n\nCrateDB exposes some native JVM logging functionality.\n\nGarbage collection\n\nCrateDB logs JVM garbage collection times using the built-in garbage collection logging of the JVM.\n\nEnvironment variables\n\nThe following environment variables can be used to configure garbage collection logging.\n\nCRATE_DISABLE_GC_LOGGING: boolean integer (default: 0)\n\nWhether to disable garbage collection logging.\n\nSet to 1 to disable.\n\nNote\n\nSince since CrateDB 3.0, Garbage collection logging is enabled by default.\n\nCRATE_GC_LOG_DIR: path to logs directory (default: varies)\n\nThe log file directory.\n\nFor a basic installation, the logs directory in the CRATE_HOME directory is default.\n\nIf you have installed a CrateDB Linux package, the default directory is /var/log/crate instead.\n\nCRATE_GC_LOG_SIZE: file size (default: 64m)\n\nMaximum file size of log files before they are rotated.\n\nCRATE_GC_LOG_FILES: number (default: 16)\n\nThe amount of files kept in rotation.\n\nCaution\n\nWith the default configuration of 16 rotated 64 megabyte log files, garbage collection logs will grow to occupy one gigabyte on disk.\n\nMake sure you have enough available disk space for configuration."
  },
  {
    "title": "Session settings — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/config/session.html",
    "html": "4.8\nSession settings\n\nTable of contents\n\nUsage\n\nSupported session settings\n\nSession settings only apply to the currently connected client session.\n\nUsage\n\nTo configure a modifiable session setting, use SET, for example:\n\nSET search_path TO myschema, doc;\n\n\nTo retrieve the current value of a session setting, use SHOW e.g:\n\nSHOW search_path;\n\n\nBesides using SHOW, it is also possible to use the current_setting scalar function.\n\nSupported session settings\nsearch_path\nDefault: pg_catalog, doc\nModifiable: yes\n\nThe list of schemas to be searched when a relation is referenced without a schema.\n\nCrateDB will try to resolve an unqualified relation name against the configured search_path by iterating over the configured schemas in the order they were declared. The first matching relation in the search_path is used. CrateDB will report an error if there is no match.\n\nNote\n\nThis setting mirrors the PostgreSQL search_path setting.\n\nSome PostgreSQL clients require access to various tables in the pg_catalog schema. Usually, this is to extract information about built-in data types or functions.\n\nCrateDB implements the system pg_catalog schema and it automatically includes it in the search_path before the configured schemas, unless it is already explicitly in the schema configuration.\n\nenable_hashjoin\nDefault: true\nModifiable: yes\n\nAn experimental setting which enables CrateDB to consider whether a JOIN operation should be evaluated using the HashJoin implementation instead of the Nested-Loops implementation.\n\nerror_on_unknown_object_key\nDefault: true\nModifiable: yes\n\nThis setting controls the behaviour of querying unknown object keys to dynamic objects. CrateDB will throw an error by default if any of the queried object keys are unknown or will return a null if the setting is set to false.\n\nNote\n\nIt is not always possible or efficient to use the HashJoin implementation. Having this setting enabled, will only add the option of considering it, it will not guarantee it. See also the available join algorithms for more insights on this topic.\n\nmax_index_keys\nDefault: 32\nModifiable: no\n\nShows the maximum number of index keys.\n\nNote\n\nThe session setting has no effect in CrateDB and exists for compatibility with PostgreSQL.\n\nserver_version_num\nDefault: 100500\nModifiable: no\n\nShows the emulated PostgreSQL server version.\n\nserver_version\nDefault: 10.5\nModifiable: no\n\nShows the emulated PostgreSQL server version.\n\noptimizer\nDefault: true\nModifiable: yes\n\nThis setting indicates whether a query optimizer rule is activated. The name of the query optimizer rule has to be provided as a suffix as part of the setting e.g. SET optimizer_rewrite_collect_to_get = false.\n\nNote\n\nThe optimizer setting is for advanced use only and can significantly impact the performance behavior of the queries.\n\nWarning\n\nExperimental session settings might be removed in the future even in minor feature releases."
  },
  {
    "title": "Cluster-wide settings — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/config/cluster.html",
    "html": "4.8\nCluster-wide settings\n\nAll current applied cluster settings can be read by querying the sys.cluster.settings column. Most cluster settings can be changed at runtime. This is documented at each setting.\n\nTable of contents\n\nNon-runtime cluster-wide settings\n\nCollecting stats\n\nShard limits\n\nUsage data collector\n\nGraceful stop\n\nBulk operations\n\nDiscovery\n\nUnicast host discovery\n\nDiscovery via DNS\n\nDiscovery on Amazon EC2\n\nDiscovery on Microsoft Azure\n\nRouting allocation\n\nShard balancing\n\nAttribute-based shard allocation\n\nCluster-wide attribute awareness\n\nCluster-wide attribute filtering\n\nDisk-based shard allocation\n\nRecovery\n\nMemory management\n\nQuery circuit breaker\n\nField data circuit breaker\n\nRequest circuit breaker\n\nAccounting circuit breaker\n\nStats circuit breakers\n\nTotal circuit breaker\n\nThread pools\n\nSettings for fixed thread pools\n\nOverload Protection\n\nMetadata\n\nMetadata gateway\n\nLogical Replication\n\nNon-runtime cluster-wide settings\n\nCluster wide settings which cannot be changed at runtime need to be specified in the configuration of each node in the cluster.\n\nCaution\n\nCluster settings specified via node configurations are required to be exactly the same on every node in the cluster for proper operation of the cluster.\n\nCollecting stats\nstats.enabled\nDefault: true\nRuntime: yes\n\nA boolean indicating whether or not to collect statistical information about the cluster.\n\nCaution\n\nThe collection of statistical information incurs a slight performance penalty, as details about every job and operation across the cluster will cause data to be inserted into the corresponding system tables.\n\nstats.jobs_log_size\nDefault: 10000\nRuntime: yes\n\nThe maximum number of job records kept to be kept in the sys.jobs_log table on each node.\n\nA job record corresponds to a single SQL statement to be executed on the cluster. These records are used for performance analytics. A larger job log produces more comprehensive stats, but uses more RAM.\n\nOlder job records are deleted as newer records are added, once the limit is reached.\n\nSetting this value to 0 disables collecting job information.\n\nstats.jobs_log_expiration\nDefault: 0s (disabled)\nRuntime: yes\n\nThe job record expiry time in seconds.\n\nJob records in the sys.jobs_log table are periodically cleared if they are older than the expiry time. This setting overrides stats.jobs_log_size.\n\nIf the value is set to 0, time based log entry eviction is disabled.\n\nNote\n\nIf both the stats.operations_log_size and stats.operations_log_expiration settings are disabled, jobs will not be recorded.\n\nstats.jobs_log_filter\nDefault: true (Include everything)\nRuntime: yes\n\nAn expression to determine if a job should be recorded into sys.jobs_log. The expression must evaluate to a boolean. If it evaluates to true the statement will show up in sys.jobs_log until it’s evicted due to one of the other rules. (expiration or size limit reached).\n\nThe expression may reference all columns contained in sys.jobs_log. A common use case is to include only jobs that took a certain amount of time to execute:\n\ncr> SET GLOBAL \"stats.jobs_log_filter\" = 'ended - started > 100';\n\nstats.jobs_log_persistent_filter\nDefault: false (Include nothing)\nRuntime: yes\n\nAn expression to determine if a job should also be recorded to the regular CrateDB log. Entries that match this filter will be logged under the StatementLog logger with the INFO level.\n\nThis is similar to stats.jobs_log_filter except that these entries are persisted to the log file. This should be used with caution and shouldn’t be set to an expression that matches many queries as the logging operation will block on IO and can therefore affect performance.\n\nA common use case is to use this for slow query logging.\n\nstats.operations_log_size\nDefault: 10000\nRuntime: yes\n\nThe maximum number of operations records to be kept in the sys.operations_log table on each node.\n\nA job consists of one or more individual operations. Operations records are used for performance analytics. A larger operations log produces more comprehensive stats, but uses more RAM.\n\nOlder operations records are deleted as newer records are added, once the limit is reached.\n\nSetting this value to 0 disables collecting operations information.\n\nstats.operations_log_expiration\nDefault: 0s (disabled)\nRuntime: yes\n\nEntries of sys.operations_log are cleared by a periodically job when they are older than the specified expire time. This setting overrides stats.operations_log_size. If the value is set to 0 the time based log entry eviction is disabled.\n\nNote\n\nIf both settings stats.operations_log_size and stats.operations_log_expiration are disabled, no job information will be collected.\n\nstats.service.interval\nDefault: 24h\nRuntime: yes\n\nDefines the refresh interval to refresh tables statistics used to produce optimal query execution plans.\n\nThis field expects a time value either as a bigint or double precision or alternatively as a string literal with a time suffix (ms, s, m, h, d, w).\n\nIf the value provided is 0 then the refresh is disabled.\n\nCaution\n\nUsing a very small value can cause a high load on the cluster.\n\nShard limits\ncluster.max_shards_per_node\nDefault: 1000\nRuntime: yes\n\nThe maximum amount of shards per node.\n\nAny operations that would result in the creation of additional shard copies that would exceed this limit are rejected.\n\nFor example. If you have 999 shards in the current cluster and you try to create a new table, the create table operation will fail.\n\nSimilarly, if a write operation would lead to the creation of a new partition, the statement will fail.\n\nEach shard on a node requires some memory and increases the size of the cluster state. Having too many shards per node will impact the clusters stability and it is therefore discouraged to raise the limit above 1000.\n\nNote\n\nThe maximum amount of shards per node setting is also used for the Maximum shards per node check.\n\nUsage data collector\n\nThe settings of the Usage Data Collector are read-only and cannot be set during runtime. Please refer to Usage Data Collector to get further information about its usage.\n\nudc.enabled\nDefault: true\nRuntime: no\n\ntrue: Enables the Usage Data Collector.\n\nfalse: Disables the Usage Data Collector.\n\nudc.initial_delay\nDefault: 10m\nRuntime: no\n\nThe delay for first ping after start-up.\n\nThis field expects a time value either as a bigint or double precision or alternatively as a string literal with a time suffix (ms, s, m, h, d, w).\n\nudc.interval\nDefault: 24h\nRuntime: no\n\nThe interval a UDC ping is sent.\n\nThis field expects a time value either as a bigint or\n\ndouble precision or alternatively as a string literal with a time suffix (ms, s, m, h, d, w).\n\nudc.url\nDefault: https://udc.crate.io\nRuntime: no\n\nThe URL the ping is sent to.\n\nGraceful stop\n\nBy default, when the CrateDB process stops it simply shuts down, possibly making some shards unavailable which leads to a red cluster state and lets some queries fail that required the now unavailable shards. In order to safely shutdown a CrateDB node, the graceful stop procedure can be used.\n\nThe following cluster settings can be used to change the shutdown behaviour of nodes of the cluster:\n\ncluster.graceful_stop.min_availability\nDefault: primaries\nRuntime: yes\nAllowed values: none | primaries | full\n\nnone: No minimum data availability is required. The node may shut down even if records are missing after shutdown.\n\nprimaries: At least all primary shards need to be available after the node has shut down. Replicas may be missing.\n\nfull: All records and all replicas need to be available after the node has shut down. Data availability is full.\n\nNote\n\nThis option is ignored if there is only 1 node in a cluster!\n\ncluster.graceful_stop.timeout\nDefault: 2h\nRuntime: yes\n\nDefines the maximum waiting time in milliseconds for the reallocation process to finish. The force setting will define the behaviour when the shutdown process runs into this timeout.\n\nThe timeout expects a time value either as a bigint or double precision or alternatively as a string literal with a time suffix (ms, s, m, h, d, w).\n\ncluster.graceful_stop.force\nDefault: false\nRuntime: yes\n\nDefines whether graceful stop should force stopping of the node if it runs into the timeout which is specified with the cluster.graceful_stop.timeout setting.\n\nBulk operations\n\nSQL DML Statements involving a huge amount of rows like COPY FROM, INSERT or UPDATE can take an enormous amount of time and resources. The following settings change the behaviour of those queries.\n\nbulk.request_timeout\nDefault: 1m\nRuntime: yes\n\nDefines the timeout of internal shard-based requests involved in the execution of SQL DML Statements over a huge amount of rows.\n\nDiscovery\n\nData sharding and work splitting are at the core of CrateDB. This is how we manage to execute very fast queries over incredibly large datasets. In order for multiple CrateDB nodes to work together a cluster needs to be formed. The process of finding other nodes with which to form a cluster is called discovery. Discovery runs when a CrateDB node starts and when a node is not able to reach the master node and continues until a master node is found or a new master node is elected.\n\ndiscovery.seed_hosts\nDefault: 127.0.0.1\nRuntime: no\n\nIn order to form a cluster with CrateDB instances running on other nodes a list of seed master-eligible nodes needs to be provided. This setting should normally contain the addresses of all the master-eligible nodes in the cluster. In order to seed the discovery process the nodes listed here must be live and contactable. This setting contains either an array of hosts or a comma-delimited string. By default a node will bind to the available loopback and scan for local ports between 4300 and 4400 to try to connect to other nodes running on the same server. This default behaviour provides local auto clustering without any configuration. Each value should be in the form of host:port or host (where port defaults to the setting transport.tcp.port).\n\nNote\n\nIPv6 hosts must be bracketed.\n\ncluster.initial_master_nodes\nDefault: not set\nRuntime: no\n\nContains a list of node names, full-qualified hostnames or IP addresses of the master-eligible nodes which will vote in the very first election of a cluster that’s bootstrapping for the first time. By default this is not set, meaning it expects this node to join an already formed cluster. In development mode, with no discovery settings configured, this step is performed by the nodes themselves, but this auto-bootstrapping is designed to aim development and is not safe for production. In production you must explicitly list the names or IP addresses of the master-eligible nodes whose votes should be counted in the very first election.\n\ndiscovery.type\nDefault: zen\nRuntime: no\nAllowed values: zen | single-node\n\nSpecifies whether CrateDB should form a multiple-node cluster. By default, CrateDB discovers other nodes when forming a cluster and allows other nodes to join the cluster later. If discovery.type is set to single-node, CrateDB forms a single-node cluster and the node won’t join any other clusters. This can be useful for testing. It is not recommend to use this for production setups. The single-node mode also skips bootstrap checks.\n\nCaution\n\nIf a node is started without any initial_master_nodes or a discovery_type set to single-node (e.g., the default configuration), it will never join a cluster even if the configuration is subsequently changed.\n\nIt is possible to force the node to forget its current cluster state by using the crate-node CLI tool. However, be aware that this may result in data loss.\n\nUnicast host discovery\n\nAs described above, CrateDB has built-in support for statically specifying a list of addresses that will act as the seed nodes in the discovery process using the discovery.seed_hosts setting.\n\nCrateDB also has support for several different mechanisms of seed nodes discovery. Currently there are three other discovery types: via DNS, via EC2 API and via Microsoft Azure mechanisms.\n\nWhen a node starts up with one of these discovery types enabled, it performs a lookup using the settings for the specified mechanism listed below. The hosts and ports retrieved from the mechanism will be used to generate a list of unicast hosts for node discovery.\n\nThe same lookup is also performed by all nodes in a cluster whenever the master is re-elected (see Cluster Meta Data).\n\ndiscovery.seed_providers\nDefault: not set\nRuntime: no\nAllowed values: srv, ec2, azure\n\nSee also: Discovery.\n\nDiscovery via DNS\n\nCrate has built-in support for discovery via DNS. To enable DNS discovery the discovery.seed_providers setting needs to be set to srv.\n\nThe order of the unicast hosts is defined by the priority, weight and name of each host defined in the SRV record. For example:\n\n_crate._srv.example.com. 3600 IN SRV 2 20 4300 crate1.example.com.\n_crate._srv.example.com. 3600 IN SRV 1 10 4300 crate2.example.com.\n_crate._srv.example.com. 3600 IN SRV 2 10 4300 crate3.example.com.\n\n\nwould result in a list of discovery nodes ordered like:\n\ncrate2.example.com:4300, crate3.example.com:4300, crate1.example.com:4300\n\ndiscovery.srv.query\nRuntime: no\n\nThe DNS query that is used to look up SRV records, usually in the format _service._protocol.fqdn If not set, the service discovery will not be able to look up any SRV records.\n\ndiscovery.srv.resolver\nRuntime: no\n\nThe hostname or IP of the DNS server used to resolve DNS records. If this is not set, or the specified hostname/IP is not resolvable, the default (system) resolver is used.\n\nOptionally a custom port can be specified using the format hostname:port.\n\nDiscovery on Amazon EC2\n\nCrateDB has built-in support for discovery via the EC2 API. To enable EC2 discovery the discovery.seed_providers settings needs to be set to ec2.\n\ndiscovery.ec2.access_key\nRuntime: no\n\nThe access key ID to identify the API calls.\n\ndiscovery.ec2.secret_key\nRuntime: no\n\nThe secret key to identify the API calls.\n\nFollowing settings control the discovery:\n\ndiscovery.ec2.groups\nRuntime: no\n\nA list of security groups; either by ID or name. Only instances with the given group will be used for unicast host discovery.\n\ndiscovery.ec2.any_group\nDefault: true\nRuntime: no\n\nDefines whether all (false) or just any (true) security group must be present for the instance to be used for discovery.\n\ndiscovery.ec2.host_type\nDefault: private_ip\nRuntime: no\nAllowed values: private_ip, public_ip, private_dns, public_dns\n\nDefines via which host type to communicate with other instances.\n\ndiscovery.ec2.availability_zones\nRuntime: no\n\nA list of availability zones. Only instances within the given availability zone will be used for unicast host discovery.\n\ndiscovery.ec2.tag.<name>\nRuntime: no\n\nEC2 instances for discovery can also be filtered by tags using the discovery.ec2.tag. prefix plus the tag name.\n\nE.g. to filter instances that have the environment tags with the value dev your setting will look like: discovery.ec2.tag.environment: dev.\n\ndiscovery.ec2.endpoint\nRuntime: no\n\nIf you have your own compatible implementation of the EC2 API service you can set the endpoint that should be used.\n\nDiscovery on Microsoft Azure\n\nCrateDB has built-in support for discovery via the Azure Virtual Machine API. To enable Azure discovery set the discovery.seed_providers setting to azure.\n\ncloud.azure.management.resourcegroup.name\nRuntime: no\n\nThe name of the resource group the CrateDB cluster is running on.\n\nAll nodes need to be started within the same resource group.\n\ncloud.azure.management.subscription.id\nRuntime: no\n\nThe subscription ID of your Azure account.\n\nYou can find the ID on the Azure Portal.\n\ncloud.azure.management.tenant.id\nRuntime: no\n\nThe tenant ID of the Active Directory application.\n\ncloud.azure.management.app.id\nRuntime: no\n\nThe application ID of the Active Directory application.\n\ncloud.azure.management.app.secret\nRuntime: no\n\nThe password of the Active Directory application.\n\ndiscovery.azure.method\nDefault: vnet\nRuntime: no\nAllowed values: vnet | subnet\n\nDefines the scope of the discovery. vnet will discover all VMs within the same virtual network (default), subnet will discover all VMs within the same subnet of the CrateDB instance.\n\nRouting allocation\ncluster.routing.allocation.enable\nDefault: all\nRuntime: yes\nAllowed values: all | none | primaries | new_primaries\n\nall allows all shard allocations, the cluster can allocate all kinds of shards.\n\nnone allows no shard allocations at all. No shard will be moved or created.\n\nprimaries only primaries can be moved or created. This includes existing primary shards.\n\nnew_primaries allows allocations for new primary shards only. This means that for example a newly added node will not allocate any replicas. However it is still possible to allocate new primary shards for new indices. Whenever you want to perform a zero downtime upgrade of your cluster you need to set this value before gracefully stopping the first node and reset it to all after starting the last updated node.\n\nNote\n\nThis allocation setting has no effect on the recovery of primary shards! Even when cluster.routing.allocation.enable is set to none, nodes will recover their unassigned local primary shards immediately after restart.\n\ncluster.routing.rebalance.enable\nDefault: all\nRuntime: yes\nAllowed values: all | none | primaries | replicas\n\nEnables or disables rebalancing for different types of shards:\n\nall allows shard rebalancing for all types of shards.\n\nnone disables shard rebalancing for any types.\n\nprimaries allows shard rebalancing only for primary shards.\n\nreplicas allows shard rebalancing only for replica shards.\n\ncluster.routing.allocation.allow_rebalance\nDefault: indices_all_active\nRuntime: yes\nAllowed values: always | indices_primary_active | indices_all_active\n\nDefines when rebalancing will happen based on the total state of all the indices shards in the cluster.\n\nDefaults to indices_all_active to reduce chatter during initial recovery.\n\ncluster.routing.allocation.cluster_concurrent_rebalance\nDefault: 2\nRuntime: yes\n\nDefines how many concurrent rebalancing tasks are allowed across all nodes.\n\ncluster.routing.allocation.node_initial_primaries_recoveries\nDefault: 4\nRuntime: yes\n\nDefines how many concurrent primary shard recoveries are allowed on a node.\n\nSince primary recoveries use data that is already on disk (as opposed to inter-node recoveries), recovery should be fast and so this setting can be higher than node_concurrent_recoveries.\n\ncluster.routing.allocation.node_concurrent_recoveries\nDefault: 2\nRuntime: yes\n\nDefines how many concurrent recoveries are allowed on a node.\n\nShard balancing\n\nYou can configure how CrateDB attempts to balance shards across a cluster by specifying one or more property weights. CrateDB will consider a cluster to be balanced when no further allowed action can bring the weighted properties of each node closer together.\n\nNote\n\nBalancing may be restricted by other settings (e.g., attribute-based and disk-based shard allocation).\n\ncluster.routing.allocation.balance.shard\nDefault: 0.45f\nRuntime: yes\n\nDefines the weight factor for shards allocated on a node (float). Raising this raises the tendency to equalize the number of shards across all nodes in the cluster.\n\ncluster.routing.allocation.balance.index\nDefault: 0.55f\nRuntime: yes\n\nDefines a factor to the number of shards per index allocated on a specific node (float). Increasing this value raises the tendency to equalize the number of shards per index across all nodes in the cluster.\n\ncluster.routing.allocation.balance.threshold\nDefault: 1.0f\nRuntime: yes\n\nMinimal optimization value of operations that should be performed (non negative float). Increasing this value will cause the cluster to be less aggressive about optimising the shard balance.\n\nAttribute-based shard allocation\n\nYou can control how shards are allocated to specific nodes by setting custom attributes on each node (e.g., server rack ID or node availability zone). After doing this, you can define cluster-wide attribute awareness and then configure cluster-wide attribute filtering.\n\nSee Also\n\nFor an in-depth example of using custom node attributes, check out the multi-zone setup how-to guide.\n\nCluster-wide attribute awareness\n\nTo make use of custom attributes for attribute-based shard allocation, you must configure cluster-wide attribute awareness.\n\ncluster.routing.allocation.awareness.attributes\nRuntime: no\n\nYou may define custom node attributes which can then be used to do awareness based on the allocation of a shard and its replicas.\n\nFor example, let’s say we want to use an attribute named rack_id. We start two nodes with node.attr.rack_id set to rack_one. Then we create a single table with five shards and one replica. The table will be fully deployed on the current nodes (five shards and one replica each, making a total of 10 shards).\n\nNow, if we start two more nodes with node.attr.rack_id set to rack_two, CrateDB will relocate shards to even out the number of shards across the nodes. However, a shard and its replica will not be allocated to nodes sharing the same rack_id value.\n\nThe awareness.attributes setting supports using several values.\n\ncluster.routing.allocation.awareness.force.*.values\nRuntime: no\n\nAttributes on which shard allocation will be forced. Here, * is a placeholder for the awareness attribute, which can be configured using the cluster.routing.allocation.awareness.attributes setting.\n\nFor example, let’s say we configured forced shard allocation for an awareness attribute named zone with values set to zone1, zone2. Start two nodes with node.attr.zone set to zone1. Then, create a table with five shards and one replica. The table will be created, but only five shards will be allocated (with no replicas). The replicas will only be allocated when we start one or more nodes with node.attr.zone set to zone2.\n\nCluster-wide attribute filtering\n\nTo control how CrateDB uses custom attributes for attribute-based shard allocation, you must configure cluster-wide attribute filtering.\n\nNote\n\nCrateDB will retroactively enforce filter definitions. If a new filter would prevent newly created matching shards from being allocated to a node, CrateDB would also move any existing matching shards away from that node.\n\ncluster.routing.allocation.include.*\nRuntime: yes\n\nOnly allocate shards on nodes where one of the specified values matches the attribute.\n\nFor example:\n\ncluster.routing.allocation.include.zone: \"zone1,zone2\"`\n\ncluster.routing.allocation.exclude.*\nRuntime: yes\n\nOnly allocate shards on nodes where none of the specified values matches the attribute.\n\nFor example:\n\ncluster.routing.allocation.exclude.zone: \"zone1\"\n\ncluster.routing.allocation.require.*\nRuntime: yes\n\nUsed to specify a number of rules, which all MUST match for a node in order to allocate a shard on it. This is in contrast to include which will include a node if ANY rule matches.\n\nDisk-based shard allocation\ncluster.routing.allocation.disk.threshold_enabled\nDefault: true\nRuntime: yes\n\nPrevent shard allocation on nodes depending of the disk usage.\n\ncluster.routing.allocation.disk.watermark.low\nDefault: 85%\nRuntime: yes\n\nDefines the lower disk threshold limit for shard allocations. New shards will not be allocated on nodes with disk usage greater than this value. It can also be set to an absolute bytes value (like e.g. 500mb) to prevent the cluster from allocating new shards on node with less free disk space than this value.\n\ncluster.routing.allocation.disk.watermark.high\nDefault: 90%\nRuntime: yes\n\nDefines the higher disk threshold limit for shard allocations. The cluster will attempt to relocate existing shards to another node if the disk usage on a node rises above this value. It can also be set to an absolute bytes value (like e.g. 500mb) to relocate shards from nodes with less free disk space than this value.\n\ncluster.routing.allocation.disk.watermark.flood_stage\nDefault: 95%\nRuntime: yes\n\nDefines the threshold on which CrateDB enforces a read-only block on every index that has at least one shard allocated on a node with at least one disk exceeding the flood stage.\n\nNote\n\nRead-only blocks are not automatically removed from the indices if the disk space is freed and the threshold is undershot. To remove the block, execute ALTER TABLE ... SET (\"blocks.read_only_allow_delete\" = FALSE) for affected tables (see blocks.read_only_allow_delete).\n\ncluster.routing.allocation.disk.watermark settings may be defined as percentages or bytes values. However, it is not possible to mix the value types.\n\nBy default, the cluster will retrieve information about the disk usage of the nodes every 30 seconds. This can also be changed by setting the cluster.info.update.interval setting.\n\nNote\n\nThe watermark settings are also used for the Routing allocation disk watermark low and Routing allocation disk watermark high node check.\n\nSetting cluster.routing.allocation.disk.threshold_enabled to false will disable the allocation decider, but the node checks will still be active and warn users about running low on disk space.\n\ncluster.routing.allocation.total_shards_per_node\nDefault: -1\nRuntime: yes\n\nLimits the number of shards that can be allocated per node. A value of -1 means unlimited.\n\nSetting this to 1000, for example, will prevent CrateDB from assigning more than 1000 shards per node. A node with 1000 shards would be excluded from allocation decisions and CrateDB would attempt to allocate shards to other nodes, or leave shards unassigned if no suitable node can be found.\n\nRecovery\nindices.recovery.max_bytes_per_sec\nDefault: 40mb\nRuntime: yes\n\nSpecifies the maximum number of bytes that can be transferred during shard recovery per seconds. Limiting can be disabled by setting it to 0. This setting allows to control the network usage of the recovery process. Higher values may result in higher network utilization, but also faster recovery process.\n\nindices.recovery.retry_delay_state_sync\nDefault: 500ms\nRuntime: yes\n\nDefines the time to wait after an issue caused by cluster state syncing before retrying to recover.\n\nindices.recovery.retry_delay_network\nDefault: 5s\nRuntime: yes\n\nDefines the time to wait after an issue caused by the network before retrying to recover.\n\nindices.recovery.internal_action_timeout\nDefault: 15m\nRuntime: yes\n\nDefines the timeout for internal requests made as part of the recovery.\n\nindices.recovery.internal_action_long_timeout\nDefault: 30m\nRuntime: yes\n\nDefines the timeout for internal requests made as part of the recovery that are expected to take a long time. Defaults to twice internal_action_timeout.\n\nindices.recovery.recovery_activity_timeout\nDefault: 30m\nRuntime: yes\n\nRecoveries that don’t show any activity for more then this interval will fail. Defaults to internal_action_long_timeout.\n\nindices.recovery.max_concurrent_file_chunks\nDefault: 2\nRuntime: yes\n\nControls the number of file chunk requests that can be sent in parallel per recovery. As multiple recoveries are already running in parallel, controlled by cluster.routing.allocation.node_concurrent_recoveries, increasing this expert-level setting might only help in situations where peer recovery of a single shard is not reaching the total inbound and outbound peer recovery traffic as configured by indices.recovery.max_bytes_per_sec, but is CPU-bound instead, typically when using transport-level security or compression.\n\nMemory management\nmemory.allocation.type\nDefault: on-heap\nRuntime: yes\n\nSupported values are on-heap and off-heap. This influences if memory is preferably allocated in the heap space or in the off-heap/direct memory region.\n\nSetting this to off-heap doesn’t imply that the heap won’t be used anymore. Most allocations will still happen in the heap space but some operations will be allowed to utilize off heap buffers.\n\nWarning\n\nUsing off-heap is considered experimental.\n\nQuery circuit breaker\n\nThe Query circuit breaker will keep track of the used memory during the execution of a query. If a query consumes too much memory or if the cluster is already near its memory limit it will terminate the query to ensure the cluster keeps working.\n\nindices.breaker.query.limit\nDefault: 60%\nRuntime: yes\n\nSpecifies the limit for the query breaker. Provided values can either be absolute values (interpreted as a number of bytes), byte sizes (like 1mb) or percentage of the heap size (like 12%). A value of -1 disables breaking the circuit while still accounting memory usage.\n\nindices.breaker.query.overhead\nDefault: 1.00\nRuntime: no\n\nCaution\n\nThis setting is deprecated and has no effect.\n\nField data circuit breaker\n\nThese settings are deprecated and will be removed in CrateDB 5.0. They don’t have any effect anymore.\n\nindices.breaker.fielddata.limit\nDefault: 60%\nRuntime: yes\nindices.breaker.fielddata.overhead\nDefault: 1.03\nRuntime: yes\nRequest circuit breaker\n\nThe request circuit breaker allows an estimation of required heap memory per request. If a single request exceeds the specified amount of memory, an exception is raised.\n\nindices.breaker.request.limit\nDefault: 60%\nRuntime: yes\n\nSpecifies the JVM heap limit for the request circuit breaker.\n\nindices.breaker.request.overhead\nDefault: 1.0\nRuntime: yes\n\nCaution\n\nThis setting is deprecated and has no effect.\n\nAccounting circuit breaker\n\nTracks things that are held in memory independent of queries. For example the memory used by Lucene for segments.\n\nindices.breaker.accounting.limit\nDefault: 100%\nRuntime: yes\n\nSpecifies the JVM heap limit for the accounting circuit breaker\n\nindices.breaker.accounting.overhead\nDefault: 1.0\nRuntime: yes\n\nCaution\n\nThis setting is deprecated and has no effect.\n\nStats circuit breakers\n\nSettings that control the behaviour of the stats circuit breaker. There are two breakers in place, one for the jobs log and one for the operations log. For each of them, the breaker limit can be set.\n\nstats.breaker.log.jobs.limit\nDefault: 5%\nRuntime: yes\n\nThe maximum memory that can be used from CRATE_HEAP_SIZE for the sys.jobs_log table on each node.\n\nWhen this memory limit is reached, the job log circuit breaker logs an error message and clears the sys.jobs_log table completely.\n\nstats.breaker.log.operations.limit\nDefault: 5%\nRuntime: yes\n\nThe maximum memory that can be used from CRATE_HEAP_SIZE for the sys.operations_log table on each node.\n\nWhen this memory limit is reached, the operations log circuit breaker logs an error message and clears the sys.operations_log table completely.\n\nTotal circuit breaker\nindices.breaker.total.limit\nDefault: 95%\nRuntime: yes\n\nThe maximum memory that can be used by all aforementioned circuit breakers together.\n\nEven if an individual circuit breaker doesn’t hit its individual limit, queries might still get aborted if several circuit breakers together would hit the memory limit configured in indices.breaker.total.limit.\n\nThread pools\n\nEvery node holds several thread pools to improve how threads are managed within a node. There are several pools, but the important ones include:\n\nwrite: For index, update and delete operations, defaults to fixed\n\nsearch: For count/search operations, defaults to fixed\n\nget: For queries on sys.shards and sys.nodes, defaults to fixed.\n\nrefresh: For refresh operations, defaults to cache\n\nlogical_replication: For operations used by the logical replication, defaults to fixed.\n\nthread_pool.<name>.type\nRuntime: no\nAllowed values: fixed | scaling\n\nfixed holds a fixed size of threads to handle the requests. It also has a queue for pending requests if no threads are available.\n\nscaling ensures that a thread pool holds a dynamic number of threads that are proportional to the workload.\n\nSettings for fixed thread pools\n\nIf the type of a thread pool is set to fixed there are a few optional settings.\n\nthread_pool.<name>.size\nRuntime: no\n\nNumber of threads. The default size of the different thread pools depend on the number of available CPU cores.\n\nthread_pool.<name>.queue_size\nDefault write: 200\nDefault search: 1000\nDefault get: 100\nRuntime: no\n\nSize of the queue for pending requests. A value of -1 sets it to unbounded.\n\nOverload Protection\n\nOverload protection settings control how many resources operations like INSERT INTO FROM QUERY or COPY can use.\n\nThe values here serve as a starting point for an algorithm that dynamically adapts the effective concurrency limit based on the round-trip time of requests. Whenever one of these settings is updated, the previously calculated effective concurrency is reset.\n\nChanging settings will only effect new operations, already running operations will continue with the previous settings.\n\noverload_protection.dml.initial_concurrency\nDefault: 5\nRuntime: yes\n\nThe initial number of concurrent operations allowed per target node.\n\noverload_protection.dml.min_concurrency\nDefault: 1\nRuntime: yes\n\nThe minimum number of concurrent operations allowed per target node.\n\noverload_protection.dml.max_concurrency\nDefault: 2000\nRuntime: yes\n\nThe maximum number of concurrent operations allowed per target node.\n\noverload_protection.dml.queue_size\nDefault: 200\nRuntime: yes\n\nHow many operations are allowed to queue up.\n\nMetadata\ncluster.info.update.interval\nDefault: 30s\nRuntime: yes\n\nDefines how often the cluster collect metadata information (e.g. disk usages etc.) if no concrete event is triggered.\n\nMetadata gateway\n\nThe following settings can be used to configure the behavior of the metadata gateway.\n\ngateway.expected_nodes\nDefault: -1\nRuntime: no\n\nThe setting gateway.expected_nodes defines the number of nodes that should be waited for until the cluster state is recovered immediately. The value of the setting should be equal to the number of nodes in the cluster, because you only want the cluster state to be recovered after all nodes are started.\n\nCaution\n\nThis setting is deprecated and will be removed in CrateDB 5.0. Use gateway.expected_data_nodes instead.\n\ngateway.expected_data_nodes\nDefault: -1\nRuntime: no\n\nThe setting gateway.expected_data_nodes defines the number of data nodes that should be waited for until the cluster state is recovered immediately. The value of the setting should be equal to the number of data nodes in the cluster, because you only want the cluster state to be recovered after all data nodes are started.\n\ngateway.recover_after_time\nDefault: 0ms\nRuntime: no\n\nThe gateway.recover_after_time setting defines the time to wait before starting the recovery once the number of nodes defined in gateway.recover_after_nodes are started. The setting is relevant if gateway.recover_after_nodes is less than gateway.expected_nodes.\n\ngateway.recover_after_nodes\nDefault: -1\nRuntime: no\n\nThe gateway.recover_after_nodes setting defines the number of nodes that need to be started before the cluster state recovery will start. Ideally the value of the setting should be equal to the number of nodes in the cluster, because you only want the cluster state to be recovered once all nodes are started. However, the value must be bigger than the half of the expected number of nodes in the cluster.\n\nCaution\n\nThis setting is deprecated and will be removed in CrateDB 5.0. Use gateway.recover_after_data_nodes instead.\n\ngateway.recover_after_data_nodes\nDefault: -1\nRuntime: no\n\nThe gateway.recover_after_data_nodes setting defines the number of data nodes that need to be started before the cluster state recovery will start. Ideally the value of the setting should be equal to the number of data nodes in the cluster, because you only want the cluster state to be recovered once all data nodes are started. However, the value must be bigger than the half of the expected number of data nodes in the cluster.\n\nLogical Replication\n\nReplication process can be configured by the following settings. Settings are dynamic and can be changed in runtime.\n\nreplication.logical.ops_batch_size\nDefault: 50000\nMin value: 16\nRuntime: yes\n\nMaximum number of operations to replicate from the publisher cluster per poll. Represents a number to advance a sequence.\n\nreplication.logical.reads_poll_duration\nDefault: 50\nRuntime: yes\n\nThe maximum time (in milliseconds) to wait for changes per poll operation. When a subscriber makes another one request to a publisher, it has reads_poll_duration milliseconds to harvest changes from the publisher.\n\nreplication.logical.recovery.chunk_size\nDefault: 1MB\nMin value: 1KB\nMax value: 1GB\nRuntime: yes\n\nChunk size to transfer files during the initial recovery of a replicating table.\n\nreplication.logical.recovery.max_concurrent_file_chunks\nDefault: 2\nMin value: 1\nMax value: 5\nRuntime: yes\n\nControls the number of file chunk requests that can be sent in parallel between clusters during the recovery."
  },
  {
    "title": "Node-specific settings — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/config/node.html",
    "html": "4.8\nNode-specific settings\n\nTable of contents\n\nBasics\n\nNode types\n\nRead-only node\n\nHosts\n\nPorts\n\nPaths\n\nPlug-ins\n\nCPU\n\nMemory\n\nGarbage collection\n\nAuthentication\n\nTrust authentication\n\nHost-based authentication\n\nHBA entries\n\nSecured communications (SSL/TLS)\n\nCross-origin resource sharing (CORS)\n\nBlobs\n\nRepositories\n\nQueries\n\nJavaScript language\n\nCustom attributes\n\nBasics\ncluster.name\nDefault: crate\nRuntime: no\n\nThe name of the CrateDB cluster the node should join to.\n\nnode.name\nRuntime: no\n\nThe name of the node. If no name is configured a random one will be generated.\n\nNote\n\nNode names must be unique in a CrateDB cluster.\n\nnode.max_local_storage_nodes\nDefault: 1\nRuntime: no\n\nDefines how many nodes are allowed to be started on the same machine using the same configured data path defined via path.data.\n\nThis setting is deprecated and will be removed with CrateDB 5.0. You should instead set different path.data values if you intend to run multiple CrateDB processes on the same machine.\n\nnode.store.allow_mmap\nDefault: true\nRuntime: no\n\nThe setting indicates whether or not memory-mapping is allowed.\n\nNode types\n\nCrateDB supports different types of nodes.\n\nThe following settings can be used to differentiate nodes upon startup:\n\nnode.master\nDefault: true\nRuntime: no\n\nWhether or not this node is able to get elected as master node in the cluster.\n\nnode.data\nDefault: true\nRuntime: no\n\nWhether or not this node will store data.\n\nUsing different combinations of these two settings, you can create four different types of node. Each type of node is differentiated by what types of load it will handle.\n\nTabulating the truth values for node.master and node.data produces a truth table outlining the four different types of node:\n\n\t\n\nMaster\n\n\t\n\nNo master\n\n\n\n\nData\n\n\t\n\nHandle all loads.\n\n\t\n\nHandles client requests and query execution.\n\n\n\n\nNo data\n\n\t\n\nHandles cluster management.\n\n\t\n\nHandles client requests.\n\nNodes marked as node.master will only handle cluster management if they are elected as the cluster master. All other loads are shared equally.\n\nRead-only node\nnode.sql.read_only\nDefault: false\nRuntime: no\n\nIf set to true, the node will only allow SQL statements which are resulting in read operations.\n\nHosts\nnetwork.host\nDefault: _local_\nRuntime: no\n\nThe IP address CrateDB will bind itself to. This setting sets both the network.bind_host and network.publish_host values.\n\nnetwork.bind_host\nDefault: _local_\nRuntime: no\n\nThis setting determines to which address CrateDB should bind itself to.\n\nnetwork.publish_host\nDefault: _local_\nRuntime: no\n\nThis setting is used by a CrateDB node to publish its own address to the rest of the cluster.\n\nTip\n\nApart from IPv4 and IPv6 addresses there are some special values that can be used for all above settings:\n\n_local_\n\n\t\n\nAny loopback addresses on the system, for example 127.0.0.1.\n\n\n\n\n_site_\n\n\t\n\nAny site-local addresses on the system, for example 192.168.0.1.\n\n\n\n\n_global_\n\n\t\n\nAny globally-scoped addresses on the system, for example 8.8.8.8.\n\n\n\n\n_[INTERFACE]_\n\n\t\n\nAddresses of a network interface, for example _en0_.\n\nPorts\nhttp.port\nRuntime: no\n\nThis defines the TCP port range to which the CrateDB HTTP service will be bound to. It defaults to 4200-4300. Always the first free port in this range is used. If this is set to an integer value it is considered as an explicit single port.\n\nThe HTTP protocol is used for the REST endpoint which is used by all clients except the Java client.\n\nhttp.publish_port\nRuntime: no\n\nThe port HTTP clients should use to communicate with the node. It is necessary to define this setting if the bound HTTP port (http.port) of the node is not directly reachable from outside, e.g. running it behind a firewall or inside a Docker container.\n\ntransport.tcp.port\nRuntime: no\n\nThis defines the TCP port range to which the CrateDB transport service will be bound to. It defaults to 4300-4400. Always the first free port in this range is used. If this is set to an integer value it is considered as an explicit single port.\n\nThe transport protocol is used for internal node-to-node communication.\n\ntransport.publish_port\nRuntime: no\n\nThe port that the node publishes to the cluster for its own discovery. It is necessary to define this setting when the bound tranport port (transport.tcp.port) of the node is not directly reachable from outside, e.g. running it behind a firewall or inside a Docker container.\n\npsql.port\nRuntime: no\n\nThis defines the TCP port range to which the CrateDB Postgres service will be bound to. It defaults to 5432-5532. Always the first free port in this range is used. If this is set to an integer value it is considered as an explicit single port.\n\nPaths\n\nNote\n\nRelative paths are relative to CRATE_HOME. Absolute paths override this behavior.\n\npath.conf\nDefault: config\nRuntime: no\n\nFilesystem path to the directory containing the configuration files crate.yml and log4j2.properties.\n\npath.data\nDefault: data\nRuntime: no\n\nFilesystem path to the directory where this CrateDB node stores its data (table data and cluster metadata).\n\nMultiple paths can be set by using a comma separated list and each of these paths will hold full shards (instead of striping data across them). For example:\n\npath.data: /path/to/data1,/path/to/data2\n\n\nWhen CrateDB finds striped shards at the provided locations (from CrateDB <0.55.0), these shards will be migrated automatically on startup.\n\npath.logs\nDefault: logs\nRuntime: no\n\nFilesystem path to a directory where log files should be stored.\n\nCan be used as a variable inside log4j2.properties.\n\nFor example:\n\nappender:\n  file:\n    file: ${path.logs}/${cluster.name}.log\n\npath.repo\nRuntime: no\n\nA list of filesystem or UNC paths where repositories of type fs may be stored.\n\nWithout this setting a CrateDB user could write snapshot files to any directory that is writable by the CrateDB process. To safeguard against this security issue, the possible paths have to be whitelisted here.\n\nSee also location setting of repository type fs.\n\nSee Also\n\nblobs.path\n\nPlug-ins\nplugin.mandatory\nRuntime: no\n\nA list of plug-ins that are required for a node to startup.\n\nIf any plug-in listed here is missing, the CrateDB node will fail to start.\n\nCPU\nprocessors\nRuntime: no\n\nThe number of processors is used to set the size of the thread pools CrateDB is using appropriately. If not set explicitly, CrateDB will infer the number from the available processors on the system.\n\nIn environments where the CPU amount can be restricted (like Docker) or when multiple CrateDB instances are running on the same hardware, the inferred number might be too high. In such a case, it is recommended to set the value explicitly.\n\nMemory\nbootstrap.memory_lock\nDefault: false\nRuntime: no\n\nCrateDB performs poorly when the JVM starts swapping: you should ensure that it never swaps. If set to true, CrateDB will use the mlockall system call on startup to ensure that the memory pages of the CrateDB process are locked into RAM.\n\nGarbage collection\n\nCrateDB logs if JVM garbage collection on different memory pools takes too long. The following settings can be used to adjust these timeouts:\n\nmonitor.jvm.gc.collector.young.warn\nDefault: 1000ms\nRuntime: no\n\nCrateDB will log a warning message if it takes more than the configured timespan to collect the Eden Space (heap).\n\nmonitor.jvm.gc.collector.young.info\nDefault: 700ms\nRuntime: no\n\nCrateDB will log an info message if it takes more than the configured timespan to collect the Eden Space (heap).\n\nmonitor.jvm.gc.collector.young.debug\nDefault: 400ms\nRuntime: no\n\nCrateDB will log a debug message if it takes more than the configured timespan to collect the Eden Space (heap).\n\nmonitor.jvm.gc.collector.old.warn\nDefault: 10000ms\nRuntime: no\n\nCrateDB will log a warning message if it takes more than the configured timespan to collect the Old Gen / Tenured Gen (heap).\n\nmonitor.jvm.gc.collector.old.info\nDefault: 5000ms\nRuntime: no\n\nCrateDB will log an info message if it takes more than the configured timespan to collect the Old Gen / Tenured Gen (heap).\n\nmonitor.jvm.gc.collector.old.debug\nDefault: 2000ms\nRuntime: no\n\nCrateDB will log a debug message if it takes more than the configured timespan to collect the Old Gen / Tenured Gen (heap).\n\nAuthentication\nTrust authentication\nauth.trust.http_default_user\nDefault: crate\nRuntime: no\n\nThe default user that should be used for authentication when clients connect to CrateDB via HTTP protocol and they do not specify a user via the Authorization request header.\n\nHost-based authentication\n\nAuthentication settings (auth.host_based.*) are node settings, which means that their values apply only to the node where they are applied and different nodes may have different authentication settings.\n\nauth.host_based.enabled\nDefault: false\nRuntime: no\n\nSetting to enable or disable Host Based Authentication (HBA). It is disabled by default.\n\nHBA entries\n\nThe auth.host_based.config. setting is a group setting that can have zero, one or multiple groups that are defined by their group key (${order}) and their fields (user, address, method, protocol, ssl).\n\n${order}:\nAn identifier that is used as a natural order key when looking up the host\nbased configuration entries. For example, an order key of a will be\nlooked up before an order key of b. This key guarantees that the entry\nlookup order will remain independent from the insertion order of the\nentries.\n\nThe Host-Based Authentication (HBA) setting is a list of predicates that users can specify to restrict or allow access to CrateDB.\n\nThe meaning of the fields of the are as follows:\n\nauth.host_based.config.${order}.user\nRuntime: no\nSpecifies an existing CrateDB username, only crate user (superuser) is\navailable. If no user is specified in the entry, then all existing users\ncan have access.\nauth.host_based.config.${order}.address\nRuntime: no\nThe client machine addresses that the client matches, and which are allowed\nto authenticate. This field may contain an IPv4 address, an IPv6 address or\nan IPv4 CIDR mask. For example: 127.0.0.1 or 127.0.0.1/32. It also\nmay contain a hostname or the special _local_ notation which will match\nboth IPv4 and IPv6 connections from localhost. A hostname specification\nthat starts with a dot (.) matches a suffix of the actual hostname.\nSo .crate.io would match foo.crate.io but not just crate.io. If no address\nis specified in the entry, then access to CrateDB is open for all hosts.\nauth.host_based.config.${order}.method\nRuntime: no\nThe authentication method to use when a connection matches this entry.\nValid values are trust, cert, and password. If no method is\nspecified, the trust method is used by default.\nSee Trust method, Client certificate authentication method and Password authentication method for more\ninformation about these methods.\nauth.host_based.config.${order}.protocol\nRuntime: no\nSpecifies the protocol for which the authentication entry should be used.\nIf no protocol is specified, then this entry will be valid for all\nprotocols that rely on host based authentication see Trust method).\nauth.host_based.config.${order}.ssl\nDefault: optional\nRuntime: no\nSpecifies whether the client must use SSL/TLS to connect to the cluster.\nIf set to on then the client must be connected through SSL/TLS\notherwise is not authenticated. If set to off then the client must\nnot be connected via SSL/TLS otherwise is not authenticated. Finally\noptional, which is the value when the option is completely skipped,\nmeans that the client can be authenticated regardless of SSL/TLS is used\nor not.\n\nExample of config groups:\n\nauth.host_based.config:\n  entry_a:\n    user: crate\n    address: 127.16.0.0/16\n  entry_b:\n    method: trust\n  entry_3:\n    user: crate\n    address: 172.16.0.0/16\n    method: trust\n    protocol: pg\n    ssl: on\n\nSecured communications (SSL/TLS)\n\nSecured communications via SSL allows you to encrypt traffic between CrateDB nodes and clients connecting to them. Connections are secured using Transport Layer Security (TLS).\n\nssl.http.enabled\nDefault: false\nRuntime: no\n\nSet this to true to enable secure communication between the CrateDB node and the client through SSL via the HTTPS protocol.\n\nssl.psql.enabled\nDefault: false\nRuntime: no\n\nSet this to true to enable secure communication between the CrateDB node and the client through SSL via the PostgreSQL wire protocol.\n\nssl.transport.mode\nDefault: legacy\nRuntime: no\n\nFor communication between nodes, choose:\n\noff\n\nSSL cannot be used\n\nlegacy\n\nSSL is not used. If HBA is enabled, transport connections won’t be verified Any reachable host can establish a connection.\n\non\n\nSSL must be used\n\nssl.keystore_filepath\nRuntime: no\n\nThe full path to the node keystore file.\n\nssl.keystore_password\nRuntime: no\n\nThe password used to decrypt the keystore file defined with ssl.keystore_filepath.\n\nssl.keystore_key_password\nRuntime: no\n\nThe password entered at the end of the keytool -genkey command.\n\nNote\n\nOptionally trusted CA certificates can be stored separately from the node’s keystore into a truststore for CA certificates.\n\nssl.truststore_filepath\nRuntime: no\n\nThe full path to the node truststore file. If not defined, then only a keystore will be used.\n\nssl.truststore_password\nRuntime: no\n\nThe password used to decrypt the truststore file defined with ssl.truststore_filepath.\n\nssl.resource_poll_interval\nDefault: 5m\nRuntime: no\n\nThe frequency at which SSL files such as keystore and truststore are polled for changes.\n\nCross-origin resource sharing (CORS)\n\nMany browsers support the same-origin policy which requires web applications to explicitly allow requests across origins. The cross-origin resource sharing settings in CrateDB allow for configuring these.\n\nhttp.cors.enabled\nDefault: false\nRuntime: no\n\nEnable or disable cross-origin resource sharing.\n\nhttp.cors.allow-origin\nDefault: <empty>\nRuntime: no\n\nDefine allowed origins of a request. * allows any origin (which can be a substantial security risk) and by prepending a / the string will be treated as a regular expression. For example /https?:\\/\\/crate.io/ will allow requests from https://crate.io and https://crate.io. This setting disallows any origin by default.\n\nhttp.cors.max-age\nDefault: 1728000 (20 days)\nRuntime: no\n\nMax cache age of a preflight request in seconds.\n\nhttp.cors.allow-methods\nDefault: OPTIONS, HEAD, GET, POST, PUT, DELETE\nRuntime: no\n\nAllowed HTTP methods.\n\nhttp.cors.allow-headers\nDefault: X-Requested-With, Content-Type, Content-Length\nRuntime: no\n\nAllowed HTTP headers.\n\nhttp.cors.allow-credentials\nDefault: false\nRuntime: no\n\nAdd the Access-Control-Allow-Credentials header to responses.\n\nBlobs\nblobs.path\nRuntime: no\n\nPath to a filesystem directory where to store blob data allocated for this node.\n\nBy default blobs will be stored under the same path as normal data. A relative path value is interpreted as relative to CRATE_HOME.\n\nRepositories\n\nRepositories are used to backup a CrateDB cluster.\n\nrepositories.url.allowed_urls\nRuntime: no\n\nThis setting only applies to repositories of type url.\n\nWith this setting a list of urls can be specified which are allowed to be used if a repository of type url is created.\n\nWildcards are supported in the host, path, query and fragment parts.\n\nThis setting is a security measure to prevent access to arbitrary resources.\n\nIn addition, the supported protocols can be restricted using the repositories.url.supported_protocols setting.\n\nrepositories.url.supported_protocols\nDefault: http, https, ftp, file and jar\nRuntime: no\n\nA list of protocols that are supported by repositories of type url.\n\nThe jar protocol is used to access the contents of jar files. For more info, see the java JarURLConnection documentation.\n\nSee also the path.repo Setting.\n\nQueries\nindices.query.bool.max_clause_count\nDefault: 8192\nRuntime: no\n\nThis setting defines the maximum number of elements an array can have so that the != ANY(), LIKE ANY(), ILIKE ANY(), NOT LIKE ANY() and the NOT ILIKE ANY() operators can be applied on it.\n\nNote\n\nIncreasing this value to a large number (e.g. 10M) and applying those ANY operators on arrays of that length can lead to heavy memory, consumption which could cause nodes to crash with OutOfMemory exceptions.\n\nJavaScript language\nlang.js.enabled\nDefault: true\nRuntime: no\n\nSetting to enable or disable JavaScript UDF support.\n\nCustom attributes\n\nThe node.attr namespace is a bag of custom attributes. Custom attributes can be used to control shard allocation.\n\nYou can create any attribute you want under this namespace, like node.attr.key: value. These attributes use the node.attr namespace to distinguish them from core node attribute like node.name.\n\nCustom attributes are not validated by CrateDB, unlike core node attributes."
  },
  {
    "title": "Resiliency — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/concepts/resiliency.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n4.8\nResiliency\n\nDistributed systems are tricky. All sorts of things can go wrong that are beyond your control. The network can go away, disks can fail, hosts can be terminated unexpectedly. CrateDB tries very hard to cope with these sorts of issues while maintaining availability, consistency, and durability.\n\nHowever, as with any distributed system, sometimes, rarely, things can go wrong.\n\nThankfully, for most use-cases, if you follow best practices, you are extremely unlikely to experience resiliency issues with CrateDB.\n\nSee Also\n\nAppendix: Resiliency Issues\n\nTable of contents\n\nMonitoring cluster status\n\nStorage and consistency\n\nDeployment strategies\n\nMonitoring cluster status\n\nThe Admin UI in CrateDB has a status indicator which can be used to determine the stability and health of a cluster.\n\nA green status indicates that all shards have been replicated, are available, and are not being relocated. This is the lowest risk status for a cluster. The status will turn yellow when there is an elevated risk of encountering issues, due to a network failure or the failure of a node in the cluster.\n\nThe status is updated every few seconds (variable on your cluster ping configuration).\n\nStorage and consistency\n\nCode that expects the behavior of an ACID compliant database like MySQL may not always work as expected with CrateDB.\n\nCrateDB does not support ACID transactions, but instead has atomic operations and eventual consistency at the row level. See also Clustering.\n\nEventual consistency is the trade-off that CrateDB makes in exchange for high-availability that can tolerate most hardware and network failures. So you may observe data from different cluster nodes temporarily falling very briefly out-of-sync with each other, although over time they will become consistent.\n\nFor example, you know a row has been written as soon as you get the INSERT OK message. But that row might not be read back by a subsequent SELECT on a different node until after a table refresh (which typically occurs within one second).\n\nYour applications should be designed to work this storage and consistency model.\n\nDeployment strategies\n\nWhen deploying CrateDB you should carefully weigh your need for high-availability and disaster recovery against operational complexity and expense.\n\nWhich strategy you pick is going to depend on the specifics of your situation.\n\nHere are some considerations:\n\nCrateDB is designed to scale horizontally. Make sure that your machines are fit for purpose, i.e. use SSDs, increase RAM up to 64 GB, and use multiple CPU cores when you can. But if you want to dynamically increase (or decrease) the capacity of your cluster, add (or remove) nodes.\n\nIf availability is a concern, you can add nodes across multiple zones (e.g. different data centers or geographical regions). The more available your CrateDB cluster is, the more likely it is to withstand external failures like a zone going down.\n\nIf data durability or read performance is a concern, you can increase the number of table replicas. More table replicas means a smaller chance of permanent data loss due to hardware failures, in exchange for the use of more disk space and more intra-cluster network traffic.\n\nIf disaster recovery is important, you can take regular snapshots and store those snapshots in cold storage. This safeguards data that has already been successfully written and replicated across the cluster.\n\nCrateDB works well as part of a data pipeline, especially if you’re working with high-volume data. If you have a message queue in front of CrateDB, you can configure it with backups and replay the data flow for a specific timeframe. This can be used to recover from issues that affect your data before it has been successfully written and replicated across the cluster.\n\nIndeed, this is the generally recommended way to recover from any of the rare consistency or data-loss issues you might encounter when CrateDB experiences network or hardware failures (see next section)."
  },
  {
    "title": "Storage and consistency — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/concepts/storage-consistency.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n4.8\nStorage and consistency\n\nThis document provides an overview on how CrateDB stores and distributes state across the cluster and what consistency and durability guarantees are provided.\n\nNote\n\nSince CrateDB heavily relies on Elasticsearch and Lucene for storage and cluster consensus, concepts shown here might look familiar to Elasticsearch users, since the implementation is actually reused from the Elasticsearch code.\n\nTable of contents\n\nData storage\n\nAtomicity at document level\n\nDurability\n\nAddressing documents\n\nConsistency\n\nCluster meta data\n\nData storage\n\nEvery table in CrateDB is sharded, which means that tables are divided and distributed across the nodes of a cluster. Each shard in CrateDB is a Lucene index broken down into segments getting stored on the filesystem. Physically the files reside under one of the configured data directories of a node.\n\nLucene only appends data to segment files, which means that data written to the disc will never be mutated. This makes it easy for replication and recovery, since syncing a shard is simply a matter of fetching data from a specific marker.\n\nAn arbitrary number of replica shards can be configured per table. Every operational replica holds a full synchronized copy of the primary shard.\n\nWith read operations, there is no difference between executing the operation on the primary shard or on any of the replicas. CrateDB randomly assigns a shard when routing an operation. It is possible to configure this behavior if required, see our best practice guide on multi zone setups for more details.\n\nWrite operations are handled differently than reads. Such operations are synchronous over all active replicas with the following flow:\n\nThe primary shard and the active replicas are looked up in the cluster state for the given operation. The primary shard and a quorum of the configured replicas need to be available for this step to succeed.\n\nThe operation is routed to the according primary shard for execution.\n\nThe operation gets executed on the primary shard\n\nIf the operation succeeds on the primary, the operation gets executed on all replicas in parallel.\n\nAfter all replica operations finish the operation result gets returned to the caller.\n\nShould any replica shard fail to write the data or times out in step 5, it’s immediately considered as unavailable.\n\nAtomicity at document level\n\nEach row of a table in CrateDB is a semi structured document which can be nested arbitrarily deep through the use of object and array types.\n\nOperations on documents are atomic. Meaning that a write operation on a document either succeeds as a whole or has no effect at all. This is always the case, regardless of the nesting depth or size of the document.\n\nCrateDB does not provide transactions. Since every document in CrateDB has a version number assigned, which gets increased every time a change occurs, patterns like Optimistic Concurrency Control can help to work around that limitation.\n\nDurability\n\nEach shard has a WAL also known as translog. It guarantees that operations on documents are persisted to disk without having to issue a Lucene-Commit for every write operation. When the translog gets flushed all data is written to the persistent index storage of Lucene and the translog gets cleared.\n\nIn case of an unclean shutdown of a shard, the transactions in the translog are getting replayed upon startup to ensure that all executed operations are permanent.\n\nThe translog is also directly transferred when a newly allocated replica initializes itself from the primary shard. There is no need to flush segments to disc just for replica recovery purposes.\n\nAddressing documents\n\nEvery document has an internal identifier. By default this identifier is derived from the primary key. Documents living in tables without a primary key are assigned a unique auto-generated ID automatically when created.\n\nEach document is routed to one specific shard according to the routing column. All rows that have the same routing column row value are stored in the same shard. The routing column can be specified with the CLUSTERED clause when creating the table. If a primary key has been defined, it will be used as the default routing column, otherwise the internal document ID is used.\n\nWhile transparent to the user, internally there are two ways how CrateDB accesses documents:\n\nget\n\nDirect access by identifier. Only applicable if the routing key and the identifier can be computed from the given query specification. (e.g: the full primary key is defined in the where clause).\n\nThis is the most efficient way to access a document, since only a single shard gets accessed and only a simple index lookup on the _id field has to be done.\n\nsearch\n\nQuery by matching against fields of documents across all candidate shards of the table.\n\nConsistency\n\nCrateDB is eventual consistent for search operations. Search operations are performed on shared IndexReaders which besides other functionality, provide caching and reverse lookup capabilities for shards. An IndexReader is always bound to the Lucene segment it was started from, which means it has to be refreshed in order to see new changes, this is done on a time based manner, but can also be done manually (see refresh). Therefore a search only sees a change if the according IndexReader was refreshed after that change occurred.\n\nIf a query specification results in a get operation, changes are visible immediately. This is achieved by looking up the document in the translog first, which will always have the most recent version of the document. The common update and fetch use-case is therefore possible. If a client updates a row and that row is looked up by its primary key after that update the changes will always be visible, since the information will be retrieved directly from the translog.\n\nNote\n\nDirty reads can occur if the primary shard becomes isolated. The primary will only realize it is isolated once it tries to communicate with its replicas or the master. At that point, a write operation is already committed into the primary and can be read by a concurrent read operation. In order to minimise the window of opportunity for this phenomena, the CrateDB nodes communicate with the master every second (by default) and once they realise no master is known, they will start rejecting write operations.\n\nEvery replica shard is updated synchronously with its primary and always carries the same information. Therefore it does not matter if the primary or a replica shard is accessed in terms of consistency. Only the refresh of the IndexReader affects consistency.\n\nCaution\n\nSome outage conditions can affect these consistency claims. See the resiliency documentation for details.\n\nCluster meta data\n\nCluster meta data is held in the so called “Cluster State”, which contains the following information:\n\nTables schemas.\n\nPrimary and replica shard locations. Basically just a mapping from shard number to the storage node.\n\nStatus of each shard, which tells if a shard is currently ready for use or has any other state like “initializing”, “recovering” or cannot be assigned at all.\n\nInformation about discovered nodes and their status.\n\nConfiguration information.\n\nEvery node has its own copy of the cluster state. However there is only one node allowed to change the cluster state at runtime. This node is called the “master” node and gets auto-elected. The “master” node has no special configuration at all, any node in the cluster can be elected as a master. There is also an automatic re-election if the current master node goes down for some reason.\n\nNote\n\nTo avoid a scenario where two masters are elected due to network partitioning it’s required to define a quorum of nodes with which it’s possible to elect a master. For details in how to do this and further information see Master Node Election.\n\nTo explain the flow of events for any cluster state change, here is an example flow for an ALTER TABLE statement which changes the schema of a table:\n\nA node in the cluster receives the ALTER TABLE request.\n\nThe node sends out a request to the current master node to change the table definition.\n\nThe master node applies the changes locally to the cluster state and sends out a notification to all affected nodes about the change.\n\nThe nodes apply the change, so that they are now in sync with the master.\n\nEvery node might take some local action depending on the type of cluster state change."
  },
  {
    "title": "Clustering — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/concepts/clustering.html",
    "html": "4.8\nClustering\n\nThe aim of this document is to describe, on a high level, how the distributed SQL database CrateDB uses a shared nothing architecture to form high- availability, resilient database clusters with minimal effort of configuration.\n\nIt will lay out the core concepts of the shared nothing architecture at the heart of CrateDB. The main difference to a primary-secondary architecture is that every node in the CrateDB cluster can perform every operation - hence all nodes are equal in terms of functionality (see Components of a CrateDB Node) and are configured the same.\n\nTable of contents\n\nComponents of a CrateDB Node\n\nSQL Handler\n\nJob Execution Service\n\nCluster State Service\n\nData storage\n\nMulti-node setup: Clusters\n\nCluster state management\n\nSettings, metadata, and routing\n\nMaster Node Election\n\nDiscovery\n\nNetworking\n\nCluster behavior\n\nApplication use case\n\nComponents of a CrateDB Node\n\nTo understand how a CrateDB cluster works it makes sense to first take a look at the components of an individual node of the cluster.\n\nFigure 1\n\nMultiple interconnected instances of CrateDB form a single database cluster. The components of each node are equal.\n\nFigure 1 shows that in CrateDB each node of a cluster contains the same components that (a) interface with each other, (b) with the same component from a different node and/or (c) with the outside world. These four major components are: SQL Handler, Job Execution Service, Cluster State Service, and Data Storage.\n\nSQL Handler\n\nThe SQL Handler part of a node is responsible for three aspects:\n\nhandling incoming client requests,\n\nparsing and analyzing the SQL statement from the request and\n\ncreating an execution plan based on the analyzed statement (abstract syntax tree)\n\nThe SQL Handler is the only of the four components that interfaces with the “outside world”. CrateDB supports three protocols to handle client requests:\n\nHTTP\n\na Binary Transport Protocol\n\nthe PostgreSQL Wire Protocol\n\nA typical request contains a SQL statement and its corresponding arguments.\n\nJob Execution Service\n\nThe Job Execution Service is responsible for the execution of a plan (“job”). The phases of the job and the resulting operations are already defined in the execution plan. A job usually consists of multiple operations that are distributed via the Transport Protocol to the involved nodes, be it the local node and/or one or multiple remote nodes. Jobs maintain IDs of their individual operations. This allows CrateDB to “track” (or for example “kill”) distributed queries.\n\nCluster State Service\n\nThe three main functions of the Cluster State Service are:\n\ncluster state management,\n\nelection of the master node and\n\nnode discovery, thus being the main component for cluster building (as described in section Multi-node setup: Clusters).\n\nIt communicates using the Binary Transport Protocol.\n\nData storage\n\nThe data storage component handles operations to store and retrieve data from disk based on the execution plan.\n\nIn CrateDB, the data stored in the tables is sharded, meaning that tables are divided and (usually) stored across multiple nodes. Each shard is a separate Lucene index that is stored physically on the filesystem. Reads and writes are operating on a shard level.\n\nMulti-node setup: Clusters\n\nA CrateDB cluster is a set of two or more CrateDB instances (referred to as nodes) running on different hosts which form a single, distributed database.\n\nFor inter-node communication, CrateDB uses a software specific transport protocol that utilizes byte-serialized Plain Old Java Objects (POJOs) and operates on a separate port. That so-called “transport port” must be open and reachable from all nodes in the cluster.\n\nCluster state management\n\nThe cluster state is versioned and all nodes in a cluster keep a copy of the latest cluster state. However, only a single node in the cluster – the master node – is allowed to change the state at runtime. This node is elected by all nodes in the cluster.\n\nSettings, metadata, and routing\n\nThe cluster state contains all necessary meta information to maintain the cluster and coordinate operations:\n\nGlobal cluster settings\n\nDiscovered nodes and their status\n\nSchemas of tables\n\nThe status and location of primary and replica shards\n\nWhen the master node updates the cluster state it will publish the new state to all nodes in the cluster and wait for all nodes to respond before processing the next update.\n\nMaster Node Election\n\nThe process of electing a node as the master node in a cluster is called Master Node Election. There must be only one master node per cluster at any single time. In a CrateDB cluster any node is eligible to be elected as a master node, although this could also be restricted to a subset of nodes if required. These nodes will then elect a single node as master to coordinate the cluster state across the cluster.\n\nNote\n\nThe following only applies to CrateDB versions 3.x and below. CrateDB versions 4.x and above determine quorum size automatically.\n\nA minimum number of nodes (referred as a quorum) needs to configured (using the discovery.zen.minimum_master_nodes setting) to ensure that in case of a network partition (when some nodes become unavailable) the cluster can elect a master node.\n\nIf the quorum is smaller than half the expected nodes in the cluster, and the cluster is split in half by a network partition, neither partition will be able to elect a new master node.\n\nIf the quorum is exactly half the expected nodes in the cluster, and the cluster is split in half, both sides of the partition will be able to elect a master node. This is known as a split-brain scenario, and can lead to data loss because the master nodes may disagree with each other when the full cluster is restored.\n\nTo avoid both of these problems, the quorum must be greater than half the expected nodes in the cluster:\n\nq = FLOOR(n / 2) + 1\n\n\nIt also helps if your cluster has an odd nodes. That way, no matter how the cluster gets split, one side of the split will be able to elect a master node.\n\nFor example: a five node cluster should have a quorum set at three. The largest network partition would split the cluster into three nodes and two nodes. In this scenario, the three node cluster would elect a master node and the two node cluster would not.\n\nDiscovery\n\nThe process of finding, adding and removing nodes is done in the discovery module.\n\nFigure 2\n\nPhases of the node discovery process. n1 and n2 already form a cluster where n1 is the elected master node, n3 joins the cluster. The cluster state update happens in parallel!\n\nNode discovery happens in multiple steps:\n\nCrateDB requires a list of potential host addresses for other CrateDB nodes when it is starting up. That list can either be provided by a static configuration or can be dynamically generated, for example by fetching DNS SRV records, querying the Amazon EC2 API, and so on.\n\nAll potential host addresses are pinged. Nodes which receive the request respond to it with information about the cluster it belongs to, the current master node, and its own node name.\n\nNow that the node knows the master node, it sends a join request. The Primary verifies the incoming request and adds the new node to the cluster state that now contains the complete list of all nodes in the cluster.\n\nThe cluster state is then published across the cluster. This guarantees the common knowledge of the node addition.\n\nCaution\n\nIf a node is started without any initial_master_nodes or a discovery_type set to single-node (e.g., the default configuration), it will never join a cluster even if the configuration is subsequently changed.\n\nIt is possible to force the node to forget its current cluster state by using the crate-node CLI tool. However, be aware that this may result in data loss.\n\nNetworking\n\nIn a CrateDB cluster all nodes have a direct link to all other nodes; this is known as full mesh topology. Due to simplicity reasons every node maintains a one-way connections to every other node in the network. The network topology of a 5 node cluster looks like this:\n\nFigure 3\n\nNetwork topology of a 5 node CrateDB cluster. Each line represents a one-way connection.\n\nThe advantages of a fully connected network are that it provides a high degree of reliability and the paths between nodes are the shortest possible. However, there are limitations in the size of such networked applications because the number of connections (c) grows quadratically with the number of nodes (n):\n\nc = n * (n - 1)\n\nCluster behavior\n\nThe fact that each CrateDB node in a cluster is equal allows applications and users to connect to any node and get the same response for the same operations. As already described in section Components of a CrateDB Node, the SQL handler is responsible handling incoming client SQL requests, either using the HTTP, transport protocol or PostgreSQL wire protocol. The “handler node” that accepts the client request also returns the response to the client. It does neither redirect nor delegate the request to a different nodes. The handler node parses the incoming request into a syntax tree, analyzes it and creates an execution plan locally. Then the operations of the plan are executed in a distributed manner. The upstream of the final phase of the execution is always the handler which then returns the response to the client.\n\nApplication use case\n\nIn a conventional setup of an application using a primary-secondary database the deployed stack looks similar to this:\n\nFigure 4\n\nConventional deployment of an application-database stack.\n\nHowever, this given setup does not scale because all application servers use the same, single entry point to the database for writes (the application can still read from secondaries) and if that entry point is unavailable the complete stack is broken.\n\nChoosing a shared nothing architecture allows DevOps to deploy their applications in an “elastic” manner without SPoF. The idea is to extend the shared nothing architecture from the database to the application which in most cases is stateless already.\n\nFigure 5\n\nElastic deployment making use of the shared nothing architecture.\n\nIf you deploy an instance of CrateDB together with every application server you will be able to dynamically scale up and down your database backend depending on your needs. The application only needs to communicate to its “bound” CrateDB instance on localhost. The load balancer tracks the health of the hosts and if either the application or the database on a single host fails the complete host will taken out of the load balancing."
  },
  {
    "title": "Joins — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/concepts/joins.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n4.8\nJoins\n\nJoins are essential operations in relational databases. They create a link between rows based on common values and allow the meaningful combination of these rows. CrateDB supports joins and due to its distributed nature allows you to work with large amounts of data.\n\nIn this document we will present the following topics. First, an overview of the existing types of joins and algorithms provided. Then a description of how CrateDB implements them along with the necessary optimizations, which allows us to work with huge datasets.\n\nTable of contents\n\nJoin types\n\nCross join\n\nInner join\n\nEqui Join\n\nOuter join\n\nJoin algorithms\n\nNested loop join\n\nPrimitive nested loop\n\nDistributed nested loop\n\nHash join\n\nBasic algorithm\n\nBlock hash join\n\nSwitch tables optimization\n\nDistributed block hash join\n\nJoin optimizations\n\nQuery then fetch\n\nPush-down query optimization\n\nJoin types\n\nA join is a relational operation that merges two data sets based on certain properties. Join Types (Inspired by this article) shows which elements appear in which join.\n\nJoin Types\n\nFrom left to right, top to bottom: left join, right join, inner join, outer join, and cross join of a set L and R.\n\nCross join\n\nA cross join returns the Cartesian product of two or more relations. The result of the Cartesian product on the relation L and R consists of all possible permutations of each tuple of the relation L with every tuple of the relation R.\n\nInner join\n\nAn inner join is a join of two or more relations that returns only tuples that satisfy the join condition.\n\nEqui Join\n\nAn equi join is a subset of an inner join and a comparison-based join, that uses equality comparisons in the join condition. The equi join of the relation L and R combines tuple l of relation L with a tuple r of the relation R if the join attributes of both tuples are identical.\n\nOuter join\n\nAn outer join returns a relation consisting of tuples that satisfy the join condition and dangling tuples from both or one of the relations, respectively to the outer join type.\n\nAn outer join has following types:\n\nLeft outer join returns tuples of the relation L matching tuples of the relation R and dangling tuples of the relation R padded with null values.\n\nRight outer join returns tuples of the relation R matching tuples of the relation L and dangling tuples from the relation L padded with null values.\n\nFull outer join returns matching tuples of both relations and dangling tuples produced by left and right outer joins.\n\nJoin algorithms\n\nCrateDB supports (a) CROSS JOIN, (b) INNER JOIN, (c) EQUI JOIN, (d) LEFT JOIN, (e) RIGHT JOIN and (f) FULL JOIN. All of these join types are executed using the nested loop join algorithm except for the Equi Joins which are executed using the hash join algorithm. Special optimizations, according to the specific use cases, are applied to improve execution performance.\n\nNested loop join\n\nThe nested loop join is the simplest join algorithm. One of the relations is nominated as the inner relation and the other as the outer relation. Each tuple of the outer relation is compared with each tuple of the inner relation and if the join condition is satisfied, the tuples of the relation L and R are concatenated and added into the returned virtual relation:\n\nfor each tuple l ∈ L do\n    for each tuple r ∈ R do\n        if l.a Θ r.b\n            put tuple(l, r) in Q\n\n\nListing 1. Nested loop join algorithm.\n\nPrimitive nested loop\n\nFor joins on some relations, the nested loop operation can be executed directly on the handler node. Specifically for queries involving a CROSS JOIN or joins on system tables /information_schema each shard sends the data to the handler node. Afterwards, this node runs the nested loop, applies limits, etc. and ultimately returns the results. Similarly, joins can be nested, so instead of collecting data from shards the rows can be the result of a previous join or table function.\n\nDistributed nested loop\n\nRelations are usually distributed to different nodes which require the nested loop to acquire the data before being able to join. After finding the locations of the required shards (which is done in the planning stage), the smaller data set (based on the row count) is broadcast amongst all the nodes holding the shards they are joined with. After that, each of the receiving nodes can start running a nested loop on the subset it has just received. Finally, these intermediate results are pushed to the original (handler) node to merge and return the results to the requesting client (see Nodes that are holding the smaller shards broadcast the data to the processing nodes which then return the results to the requesting node.).\n\nNodes that are holding the smaller shards broadcast the data to the processing nodes which then return the results to the requesting node.\n\nQueries can be optimized if they contain (a) ORDER BY, (b) LIMIT, or (c) if INNER/EQUI JOIN. In any of these cases, the nested loop can be terminated earlier:\n\nOrdering allows determining whether there are records left\n\nLimit states the maximum number of rows that are returned\n\nConsequently, the number of rows is significantly reduced allowing the operation to complete much faster.\n\nHash join\n\nThe Hash Join algorithm is used to execute certain types of joins in a more efficient way than Nested Loop.\n\nBasic algorithm\n\nThe operation takes place in one node (the handler node to which the client is connected). The rows of the left relation of the join are read and a hashing algorithm is applied on the fields of the relation which participate in the join condition. The hashing algorithm generates a hash value which is used to store every row of the left relation in the proper position in a hash table.\n\nThen the rows of the right relation are read one-by-one and the same hashing algorithm is applied on the fields that participate in the join condition. The generated hash value is used to make a lookup in the hash table. If no entry is found, the row is skipped and the processing continues with the next row from the right relation. If an entry is found, the join condition is validated (handling hash collisions) and on successful validation the combined tuple of left and right relation is returned.\n\nBasic hash join algorithm\n\nBlock hash join\n\nThe Hash Join algorithm requires a hash table containing all the rows of the left relation to be stored in memory. Therefore, depending on the size of the relation (number of rows) and the size of each row, the size of this hash table might exceed the available memory of the node executing the hash join. To resolve this limitation the rows of the left relation are loaded into the hash table in blocks.\n\nOn every iteration the maximum available size of the hash table is calculated, based on the number of rows and size of each row of the table but also taking into account the available memory for query execution on the node. Once this block-size is calculated the rows of the left relation are processed and inserted into the hash table until the block-size is reached. The operation then starts reading the rows of the right relation, process them one-by-one and performs the lookup and the join condition validation. Once all rows from the right relation are processed the hash table is re-initialized based on a new calculation of the block size and a new iteration starts until all rows of the left relation are processed.\n\nWith this algorithm the memory limitation is handled in expense of having to iterate over the rows of the right table multiple times, and it is the default algorithm used for Hash Join execution by CrateDB.\n\nSwitch tables optimization\n\nSince the right table can be processed multiple times (number of rows from left / block-size) the right table should be the smaller (in number of rows) of the two relations participating in the join. Therefore, if originally the right relation is larger than the left the query planner performs a switch to take advantage of this detail and execute the hash join with better performance.\n\nDistributed block hash join\n\nSince CrateDB is a distributed database and a standard deployment consists of at least three nodes and in most case of much more, the Hash Join algorithm execution can be further optimized (performance-wise) by executing it in a distributed manner across the CrateDB cluster.\n\nThe idea is to have the hash join operation executing in multiple nodes of the cluster in parallel and then merge the intermediate results before returning them to the client.\n\nA hashing algorithm is applied on every row of both the left and right relations. On the integer value generated by this hash, a modulo, by the number of nodes in the cluster, is applied and the resulting number defines the node to which this row should be sent. As a result each node of the cluster receives a subset of the whole data set which is ensured (by the hashing and modulo) to contain all candidate matching rows. Each node in turn performs a block hash join on this subset and sends its result tuples to the handler node (where the client issued the query). Finally, the handler node receives those intermediate results, merges them and applies any pending ORDER BY, LIMIT and OFFSET and sends the final result to the client.\n\nThis algorithm is used by CrateDB for most cases of hash join execution except for joins on complex subqueries that contain LIMIT and/or OFFSET.\n\nDistributed hash join algorithm\n\nJoin optimizations\nQuery then fetch\n\nJoin operations on large relation can be extremely slow especially if the join is executed with a Nested Loop. - which means that the runtime complexity grows quadratically (O(n*m)). Specifically for cross joins this results in large amounts of data sent over the network and loaded into memory at the handler node. CrateDB reduces the volume of data transferred by employing Query Then Fetch: First, filtering and ordering are applied (if possible where the data is located) to obtain the required document IDs. Next, as soon as the final data set is ready, CrateDB fetches the selected fields and returns the data to the client.\n\nPush-down query optimization\n\nComplex queries such as Listing 2 require the planner to decide when to filter, sort, and merge in order to efficiently execute the plan. In this case, the query would be split internally into subqueries before running the join. As shown in Figure 5, first filtering (and ordering) is applied to relations L and R on their shards, then the result is directly broadcast to the nodes running the join. Not only will this behavior reduce the number of rows to work with, it also distributes the workload among the nodes so that the (expensive) join operation can run faster.\n\nSELECT L.a, R.x\nFROM L, R\nWHERE L.id = R.id\n  AND L.b > 100\n  AND R.y < 10\nORDER BY L.a\n\n\nListing 2. An INNER JOIN on ids (effectively an EQUI JOIN) which can be optimized.\n\nFigure 5\n\nComplex queries are broken down into subqueries that are run on their shards before joining."
  },
  {
    "title": "SQL compatibility — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/appendices/compatibility.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n5.5\nSQL compatibility\n\nCrateDB provides a standards-based SQL implementation similar to many other SQL databases. In particular, CrateDB aims for compatibility with PostgreSQL. However, CrateDB’s SQL dialect does have some unique characteristics, documented on this page.\n\nSee Also\n\nSQL: Syntax reference\n\nTable of contents\n\nImplementation notes\n\nData types\n\nCreate table\n\nAlter table\n\nSystem information tables\n\nBLOB support\n\nTransactions (BEGIN, START, COMMIT, and ROLLBACK)\n\nUnsupported features and functions\n\nImplementation notes\nData types\n\nCrateDB supports a set of primitive data types. The following table defines how data types of standard SQL map to CrateDB Data types.\n\nStandard SQL\n\n\t\n\nCrateDB\n\n\n\n\ninteger\n\n\t\n\ninteger, int, int4\n\n\n\n\nbit[8]\n\n\t\n\nbyte, char\n\n\n\n\nboolean, bool\n\n\t\n\nboolean\n\n\n\n\nchar [(n)], varchar [(n)]\n\n\t\n\nstring, text, varchar, character varying\n\n\n\n\ntimestamp with time zone\n\n\t\n\ntimestamp with time zone, timestamptz\n\n\n\n\ntimestamp\n\n\t\n\ntimestamp without time zone\n\n\n\n\nsmallint\n\n\t\n\nshort, int2, smallint\n\n\n\n\nbigint\n\n\t\n\nlong, bigint, int8\n\n\n\n\nreal\n\n\t\n\nfloat, real\n\n\n\n\ndouble precision\n\n\t\n\ndouble, double precision\n\nCreate table\n\nCREATE TABLE supports additional storage and table parameters for sharding, replication and routing of data, and does not support inheritance.\n\nAlter table\n\nALTER COLUMN action is not currently supported (see ALTER TABLE).\n\nSystem information tables\n\nThe read-only System information and Information schema tables have a slightly different schema than specified in standard SQL. They provide schema information and can be queried to get real-time statistical data about the cluster, its nodes, and their shards.\n\nBLOB support\n\nStandard SQL defines a binary string type, called BLOB or BINARY LARGE OBJECT. With CrateDB, Binary Data is instead stored in separate BLOB Tables (see Blobs) which can be sharded and replicated.\n\nTransactions (BEGIN, START, COMMIT, and ROLLBACK)\n\nCrateDB is focused on providing analytical capabilities over supporting traditional transactional use cases, and thus it does not provide transaction control. Every statement commits immediately and is replicated within the cluster.\n\nHowever, every row in CrateDB has a version number that is incremented whenever the record is modified. This version number can be used to implement patterns like Optimistic Concurrency Control, which can be used to solve many of the use cases that would otherwise require traditional transactions.\n\nUnsupported features and functions\n\nThese features of standard SQL are not supported:\n\nStored procedures\n\nTriggers\n\nWITH Queries (Common Table Expressions)\n\nSequences\n\nInheritance\n\nConstraints\n\nUnique\n\nForeign key\n\nExclusion constraints\n\nThese functions of standard SQL are either not supported or only partly supported:\n\nAggregate functions\n\nVarious functions available (see Aggregation)\n\nWindow functions\n\nVarious functions available (see Window functions)\n\nENUM support functions\n\nIS DISTINCT FROM\n\nNetwork address functions and operators\n\nMathematical functions\n\nCertain functions supported (see Mathematical functions)\n\nSet returning functions\n\nTrigger functions\n\nXML functions\n\nNote\n\nThe currently supported and unsupported features in CrateDB are exposed in the Information schema table (see sql_features for usage).\n\nCrateDB also supports the PostgreSQL wire protocol.\n\nIf you have use cases for any missing features, functions, or dialect improvements, let us know on GitHub! We are always improving and extending CrateDB and would love to hear your feedback."
  },
  {
    "title": "Glossary — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/appendices/glossary.html",
    "html": "5.5\nGlossary\n\nThis glossary defines key terms used in the CrateDB reference manual.\n\nTable of contents\n\nTerms\n\nB\n\nC\n\nE\n\nF\n\nM\n\nN\n\nO\n\nP\n\nR\n\nS\n\nU\n\nV\n\nTerms\nB\nBinary operator\n\nSee operation.\n\nC\nCLUSTERED BY column\n\nSee routing column.\n\nE\nEvaluation\n\nSee expression.\n\nExpression\n\nAny valid SQL that produces a value (e.g., column references, comparison operators, and functions) through a process known as evaluation.\n\nContrary to a statement.\n\nSee Also\n\nSQL: Value expressions\n\nBuilt-ins: Subquery expressions\n\nData definition: Generation expressions\n\nScalar functions: Conditional functions and expressions\n\nAggregation: Aggregation expressions\n\nF\nFunction\n\nA token (e.g., replace) that takes zero or more arguments (e.g., three strings), performs a specific task, and may return one or more values (e.g., a modified string). Functions that return more than one value are called multi-valued functions.\n\nFunctions may be called in an SQL statement, like so:\n\ncr> SELECT replace('Hello world!', 'world', 'friend') as result;\n+---------------+\n| result        |\n+---------------+\n| Hello friend! |\n+---------------+\nSELECT 1 row in set (... sec)\n\n\nSee Also\n\nScalar functions\n\nAggregate functions\n\nTable functions\n\nWindow functions\n\nUser-defined functions\n\nM\nMetadata gateway\n\nPersists cluster metadata on disk every time the metadata changes. This data is stored persistently across full cluster restarts and recovered after nodes are started again.\n\nSee Also\n\nCluster configuration: Metadata gateway\n\nMulti-valued function\n\nA function that returns two or more values.\n\nSee Also\n\nTable functions\n\nWindow functions\n\nN\nNonscalar\n\nA data type that can have more than one value (e.g., arrays and objects).\n\nContrary to a scalar.\n\nSee Also\n\nGeographic types\n\nContainer types\n\nO\nOperand\n\nSee operator.\n\nOperation\n\nSee operator.\n\nOperator\n\nA reserved keyword (e.g., IN) or sequence of symbols (e.g., >=) that can be used in an SQL statement to manipulate one or more expressions and return a result (e.g., true or false). This process is known as an operation and the expressions can be called operands or arguments.\n\nAn operator that takes one operand is known as a unary operator and an operator that takes two is known as a binary operator.\n\nSee Also\n\nArithmetic operators\n\nComparison operators\n\nArray comparisons\n\nP\nPartition column\n\nA column used to partition a table. Specified by the PARTITIONED BY clause.\n\nAlso known as a PARTITIONED BY column or partitioned column.\n\nA table may be partitioned by one or more columns:\n\nIf a table is partitioned by one column, a new partition is created for every unique value in that partition column\n\nIf a table is partitioned by multiple columns, a new partition is created for every unique combination of row values in those partition columns\n\nSee Also\n\nData definition: Partitioned tables\n\nGenerated columns: Partitioning\n\nCREATE TABLE: PARTITIONED BY clause\n\nALTER TABLE: PARTITION clause\n\nREFRESH: PARTITION clause\n\nOPTIMIZE: PARTITION clause\n\nCOPY TO: PARTITION clause\n\nCOPY FROM: PARTITION clause\n\nCREATE SNAPSHOT: PARTITION clause\n\nRESTORE SNAPSHOT: PARTITION clause\n\nPARTITIONED BY column\n\nSee partition column.\n\nPartitioned column\n\nSee partition column.\n\nR\nRegular expression\n\nAn expression used to search for patterns in a string.\n\nSee Also\n\nWikipedia: Regular expression\n\nData definition: Fulltext analyzers\n\nQuerying: Regular expressions\n\nScalar functions: Regular expressions\n\nTable functions: regexp_matches\n\nRouting column\n\nValues in this column are used to compute a hash which is then used to route the corresponding row to a specific shard.\n\nAlso known as the CLUSTERED BY column.\n\nAll rows that have the same routing column row value are stored in the same shard.\n\nNote\n\nThe routing of rows to a specific shard is not the same as the routing of shards to a specific node (also known as shard allocation).\n\nSee Also\n\nStorage and consistency: Addressing documents\n\nSharding: Routing\n\nCREATE TABLE: CLUSTERED clause\n\nS\nScalar\n\nA data type with a single value (e.g., numbers and strings).\n\nContrary to a nonscalar.\n\nSee Also\n\nPrimitive types\n\nShard allocation\n\nThe process by which CrateDB allocates shards to a specific nodes.\n\nNote\n\nShard allocation is sometimes referred to as shard routing, which is not to be confused with row routing.\n\nSee Also\n\nShard allocation filtering\n\nCluster configuration: Routing allocation\n\nSharding: Number of shards\n\nAltering tables: Changing the number of shards\n\nAltering tables: Reroute shards\n\nShard recovery\n\nThe process by which CrateDB synchronizes a replica shard from a primary shard.\n\nShard recovery can happen during node startup, after node failure, when replicating a primary shard, when moving a shard to another node (i.e., when rebalancing the cluster), or during snapshot restoration.\n\nA shard that is being recovered cannot be queried until the recovery process is complete.\n\nSee Also\n\nCluster settings: Recovery\n\nSystem information: Checked node settings\n\nShard routing\n\nSee shard allocation.\n\nStatement\n\nAny valid SQL that serves as a database instruction (e.g., CREATE TABLE, INSERT, and SELECT) instead of producing a value.\n\nContrary to an expression.\n\nSee Also\n\nData definition\n\nData manipulation\n\nQuerying\n\nSQL Statements\n\nSubquery\n\nA SELECT statement used as a relation in the FROM clause of a parent SELECT statement.\n\nAlso known as a subselect.\n\nSubselect\n\nSee subquery.\n\nU\nUnary operator\n\nSee operation.\n\nUncorrelated subquery\n\nA scalar subquery that does not reference any relations (e.g., tables) in the parent SELECT statement.\n\nSee Also\n\nBuilt-ins: Subquery expressions\n\nV\nValue expression\n\nSee expression."
  },
  {
    "title": "Resiliency Issues — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/appendices/resiliency.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n5.5\nResiliency Issues\n\nCrateDB uses Elasticsearch for data distribution and replication. Most of the resiliency issues exist in the Elasticsearch layer and can be tested by Jepsen.\n\nTable of contents\n\nKnown issues\n\nRetry of updates causes double execution\n\nFixed issues\n\nRepeated cluster partitions can cause lost cluster updates\n\nVersion number representing ambiguous row versions\n\nReplicas can fall out of sync when a primary shard fails\n\nLoss of rows due to network partition\n\nDirty reads caused by bad primary handover\n\nChanges are overwritten by old data in danger of lost data\n\nMake table creation resilient to closing and full cluster crashes\n\nUnaware master accepts cluster updates\n\nKnown issues\nRetry of updates causes double execution\nStatus\tWork ongoing (More info)\nSeverity\tModerate\nLikelihood\tVery rare\nCause\tNetwork issues, unresponsive nodes\nWorkloads\tNon-Idempotent writes\n\nScenario\n\nA node with a primary shard receives an update, writes it to disk, but goes offline before having sent a confirmation back to the executing node. When the node comes back online, it receives an update retry and executes the update again.\n\nConsequence\n\nIncorrect data for non-idempotent writes.\n\nFor example:\n\nAn double insert on a table without an explicit primary key would be executed twice and would result in duplicate data.\n\nA double update would incorrectly increment the row version number twice.\n\nFixed issues\nRepeated cluster partitions can cause lost cluster updates\nStatus\tFixed in CrateDB v4.0 (#32006, #32171)\nSeverity\tModerate\nLikelihood\tVery rare\nCause\tNetwork issues, unresponsive nodes\nWorkloads\tAll\n\nScenario\n\nA cluster is partitioned and a new master is elected on the side that has quorum. The cluster is repaired and simultaneously a change is made to the cluster state. The cluster is partitioned again before the new master node has a chance to publish the new cluster state and the partition the master lands on does not have quorum.\n\nConsequence\n\nThe node steps down as master and the uncommunicated state changes are lost.\n\nCluster state is very important and contains information like shard location, schemas, and so on. Lost cluster state updates can cause data loss, reset settings, and problems with table structures.\n\nPartially fixed\n\nThis problem is mostly fixed by #20384 (CrateDB v2.0.x), which uses committed cluster state updates during master election process. This does not fully solve this rare problem but considerably reduces the chance of occurrence. The reason is that if the second partition happens concurrently with a cluster state update and blocks the cluster state commit message from reaching a majority of nodes, it may be that the in flight update is lost. If the now-isolated master can still acknowledge the cluster state update to the client this will result to the loss of an acknowledged change.\n\nVersion number representing ambiguous row versions\nStatus\tFixed in CrateDB v4.0 (#19269, #10708)\nSeverity\tSignificant\nLikelihood\tVery rare\nCause\tNetwork issues, unresponsive nodes\nWorkloads\tVersioned reads with replicated tables while writing.\n\nScenario\n\nA client is writing to a primary shard. The node holding the primary shard is partitioned from the cluster. It usually takes between 30 and 60 seconds (depending on ping configuration) before the master node notices the partition. During this time, the same row is updated on both the primary shard (partitioned) and a replica shard (not partitioned).\n\nConsequence\n\nThere are two different versions of the same row using the same version number. When the primary shard rejoins the cluster and its data is replicated, the update that was made on the replicated shard is lost but the new version number matches the lost update. This will break Optimistic Concurrency Control.\n\nReplicas can fall out of sync when a primary shard fails\nStatus\tFixed in CrateDB v4.0 (#10708)\nSeverity\tModest\nLikelihood\tRare\nCause\tPrimary fails and in-flight writes are only written to a subset of its replicas\nWorkloads\tWrites on replicated table\n\nScenario\n\nWhen a primary shard fails, a replica shard will be promoted to be the primary shard. If there is more than one replica shard, it is possible for the remaining replicas to be out of sync with the new primary shard. This is caused by operations that were in-flight when the primary shard failed and may not have been processed on all replica shards. Currently, the discrepancies are not repaired on primary promotion but instead would be repaired if replica shards are relocated (e.g., from hot to cold nodes); this does mean that the length of time which replicas can be out of sync with the primary shard is unbounded.\n\nConsequence\n\nStale data may be read from replicas.\n\nLoss of rows due to network partition\nStatus\tFixed in Crate v2.0.x (#7572, #14252)\nSeverity\tSignificant\nLikelihood\tVery rare\nCause\tSingle node isolation\nWorkloads\tWrites on replicated table\n\nScenario\n\nA node with a primary shard is partitioned from the cluster. The node continues to accept writes until it notices the network partition. In the meantime, another shard has been elected as the primary. Eventually, the partitioned node rejoins the cluster.\n\nConsequence\n\nData that was written to the original primary shard on the partitioned node is lost as data from the newly elected primary shard replaces it when it rejoins the cluster.\n\nThe risk window depends on your ping configuration. The default configuration of a 30 second ping timeout with three retries corresponds to a 90 second risk window. However, it is very rare for a node to lose connectivity within the cluster but maintain connectivity with clients.\n\nDirty reads caused by bad primary handover\nStatus\tFixed in CrateDB v2.0.x (#15900, #12573)\nSeverity\tModerate\nLikelihood\tRare\nCause\tRace Condition\nWorkloads\tReads\n\nScenario\n\nDuring a primary handover, there is a small risk window when a shard can find out it has been elected as the new primary before the old primary shard notices that it is no longer the primary.\n\nA primary handover can happen in the following scenarios:\n\nA shard is relocated and then elected as the new primary, as two separate but sequential actions. Relocating a shard means creating a new shard and then deleting the old shard.\n\nAn existing replica shard gets promoted to primary because the primary shard was partitioned from the cluster.\n\nConsequence\n\nWrites that occur on the new primary during the risk window will not be replicated to the old shard (which still believes it is the primary) so any subsequent reads on the old shard may return incorrect data.\n\nChanges are overwritten by old data in danger of lost data\nStatus\tFixed in CrateDB v2.0.x (#14671)\nSeverity\tSignificant\nLikelihood\tVery rare\nCause\tNetwork problems\nWorkloads\tWrites\n\nScenario\n\nA node with a primary that contains new data is partitioned from the cluster.\n\nConsequence\n\nCrateDB prefers old data over no data, and so promotes an a shard with stale data as a new primary. The data on the original primary shard is lost. Even if the node with the original primary shard rejoins the cluster, CrateDB has no way of distinguishing correct and incorrect data, so that data replaced with data from the new primary shard.\n\nMake table creation resilient to closing and full cluster crashes\nStatus\tThe issue has been fixed with the following issues. Table recovery: #9126 Reopening tables: #14739 Allocation IDs: #15281\nSeverity\tModest\nLikelihood\tVery Rare\nCause\tEither the cluster fails while recovering a table or the table is closed during shard creation.\nWorkloads\tTable creation\n\nScenario\n\nRecovering a table requires a quorum of shard copies to be available to allocate a primary. This means that a primary cannot be assigned if the cluster dies before enough shards have been allocated. The same happens if a table is closed before enough shard copies were started, making it impossible to reopen the table. Allocation IDs solve this issue by tracking allocated shard copies in the cluster. This makes it possible to safely recover a table in the presence of a single shard copy. Allocation IDs can also distinguish the situation where a table has been created but none of the shards have been started. If such an table was inadvertently closed before at least one shard could be started, a fresh shard will be allocated upon reopening the table.\n\nConsequence\n\nThe primary shard of the table cannot be assigned or a closed table cannot be re-opened.\n\nUnaware master accepts cluster updates\nStatus\tFixed in CrateDB v2.0.x (#13062)\nSeverity\tModerate\nLikelihood\tVery rare\nCause\tNetwork problems\nWorkloads\tDDL statements\n\nScenario\n\nIf a master has lost quorum (i.e. the number of nodes it is in communication with has fallen below the configured minimum) it should step down as master and stop answering requests to perform cluster updates. There is a small risk window between losing quorum and noticing that quorum has been lost, depending on your ping configuration.\n\nConsequence\n\nIf a cluster update request is made to the node between losing quorum and noticing the loss of quorum, that request will be confirmed. However, those updates will be lost because the node will not be able to perform a successful cluster update.\n\nCluster state is very important and contains information like shard location, schemas, and so on. Lost cluster state updates can cause data loss, reset settings, and problems with table structures."
  },
  {
    "title": "SQL standard compliance — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/appendices/compliance.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n5.5\nSQL standard compliance\n\nThis page documents the standard SQL (ISO/IEC 9075) features that CrateDB supports, along with implementation notes and any associated caveats.\n\nCaution\n\nThis list is approximate and features that are listed as supported might be nonconforming in their implementation. However, the main reference documentation always contains the most accurate information about the features CrateDB supports and how to use them.\n\nSee Also\n\nSQL compatibility\n\nID\n\n\t\n\nPackage\n\n\t\n\n#\n\n\t\n\nDescription\n\n\t\n\nComments\n\n\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n1\n\n\t\n\nINTEGER and SMALLINT data types\n\n\t\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n2\n\n\t\n\nREAL, DOUBLE PRECISION, and FLOAT data types\n\n\t\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n3\n\n\t\n\nDECIMAL and NUMERIC data types\n\n\t\n\nNot supported in DDL\n\n\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n4\n\n\t\n\nArithmetic operators\n\n\t\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n5\n\n\t\n\nNumeric comparison\n\n\t\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n6\n\n\t\n\nImplicit casting among the numeric data types\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n1\n\n\t\n\nCHARACTER data type\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n2\n\n\t\n\nCHARACTER VARYING data type\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n3\n\n\t\n\nCharacter literals\n\n\t\n\nOnly simple ‘ quoting\n\n\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n4\n\n\t\n\nCHARACTER_LENGTH function\n\n\t\n\nchar_length only\n\n\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n5\n\n\t\n\nOCTET_LENGTH function\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n6\n\n\t\n\nSUBSTRING function\n\n\t\n\nsubstr scalar\n\n\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n7\n\n\t\n\nCharacter concatenation\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n8\n\n\t\n\nUPPER and LOWER functions\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n9\n\n\t\n\nTRIM function\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n10\n\n\t\n\nImplicit casting among the character string types\n\n\t\n\njust one type\n\n\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n12\n\n\t\n\nCharacter comparison\n\n\t\n\n\nE031\n\n\t\n\nIdentifiers\n\n\t\t\t\n\n\nE031\n\n\t\n\nIdentifiers\n\n\t\n\n1\n\n\t\n\nDelimited identifiers\n\n\t\n\n\nE031\n\n\t\n\nIdentifiers\n\n\t\n\n2\n\n\t\n\nLower case identifiers\n\n\t\n\n\nE031\n\n\t\n\nIdentifiers\n\n\t\n\n3\n\n\t\n\nTrailing underscore\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\t\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n1\n\n\t\n\nSELECT DISTINCT\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n2\n\n\t\n\nGROUP BY clause\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n4\n\n\t\n\nGROUP BY can contain columns not in <select list>\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n5\n\n\t\n\nSelect list items can be renamed\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n6\n\n\t\n\nHAVING clause\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n7\n\n\t\n\nQualified * in select list\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n8\n\n\t\n\nCorrelation names in the FROM clause\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n9\n\n\t\n\nRename columns in the FROM clause\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n1\n\n\t\n\nComparison predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n2\n\n\t\n\nBETWEEN predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n3\n\n\t\n\nIN predicate with list of values\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n4\n\n\t\n\nLIKE predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n6\n\n\t\n\nNULL predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n8\n\n\t\n\nEXISTS predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n9\n\n\t\n\nSubqueries in comparison predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n11\n\n\t\n\nSubqueries in IN predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n12\n\n\t\n\nSubqueries in quantified comparison predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n13\n\n\t\n\nCorrelated subqueries\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n14\n\n\t\n\nSearch condition\n\n\t\n\n\nE071\n\n\t\n\nBasic query expressions\n\n\t\n\n1\n\n\t\n\nUNION DISTINCT table operator\n\n\t\n\n\nE071\n\n\t\n\nBasic query expressions\n\n\t\n\n2\n\n\t\n\nUNION ALL table operator\n\n\t\n\n\nE081\n\n\t\n\nBasic Privileges\n\n\t\t\t\n\n\nE081\n\n\t\n\nBasic Privileges\n\n\t\n\n1\n\n\t\n\nSELECT privilege\n\n\t\n\n\nE081\n\n\t\n\nBasic Privileges\n\n\t\n\n2\n\n\t\n\nDELETE privilege\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\t\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n1\n\n\t\n\nAVG\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n2\n\n\t\n\nCOUNT\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n3\n\n\t\n\nMAX\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n4\n\n\t\n\nMIN\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n5\n\n\t\n\nSUM\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n6\n\n\t\n\nALL quantifier\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n7\n\n\t\n\nDISTINCT quantifier\n\n\t\n\n\nE101\n\n\t\n\nBasic data manipulation\n\n\t\t\t\n\n\nE101\n\n\t\n\nBasic data manipulation\n\n\t\n\n1\n\n\t\n\nINSERT statement\n\n\t\n\n\nE101\n\n\t\n\nBasic data manipulation\n\n\t\n\n3\n\n\t\n\nSearched UPDATE statement\n\n\t\n\n\nE101\n\n\t\n\nBasic data manipulation\n\n\t\n\n4\n\n\t\n\nSearched DELETE statement\n\n\t\n\n\nE121\n\n\t\n\nBasic cursor support\n\n\t\n\n1\n\n\t\n\nDECLARE CURSOR\n\n\t\n\n\nE121\n\n\t\n\nBasic cursor support\n\n\t\n\n8\n\n\t\n\nCLOSE statement\n\n\t\n\n\nE121\n\n\t\n\nBasic cursor support\n\n\t\n\n10\n\n\t\n\nFETCH statement implicit NEXT\n\n\t\n\n\nE121\n\n\t\n\nBasic cursor support\n\n\t\n\n17\n\n\t\n\nWITH HOLD cursors\n\n\t\n\n\nE131\n\n\t\n\nNull value support (nulls in lieu of values)\n\n\t\t\t\n\n\nE141\n\n\t\n\nBasic integrity constraints\n\n\t\n\n1\n\n\t\n\nNOT NULL constraints\n\n\t\n\n\nE141\n\n\t\n\nBasic integrity constraints\n\n\t\n\n3\n\n\t\n\nPRIMARY KEY constraints\n\n\t\n\n\nE141\n\n\t\n\nBasic integrity constraints\n\n\t\n\n6\n\n\t\n\nCHECK constraints\n\n\t\n\n\nE141\n\n\t\n\nBasic integrity constraints\n\n\t\n\n7\n\n\t\n\nColumn defaults\n\n\t\n\n\nE141\n\n\t\n\nBasic integrity constraints\n\n\t\n\n8\n\n\t\n\nNOT NULL inferred on PRIMARY KEY\n\n\t\n\n\nE151\n\n\t\n\nTransaction support\n\n\t\n\n1\n\n\t\n\nCOMMIT statement\n\n\t\n\n\nE152\n\n\t\n\nBasic SET TRANSACTION statement\n\n\t\t\t\n\n\nE152\n\n\t\n\nBasic SET TRANSACTION statement\n\n\t\n\n1\n\n\t\n\nSET TRANSACTION statement: ISOLATION LEVEL SERIALIZABLE clause\n\n\t\n\nIs ignored\n\n\n\n\nE152\n\n\t\n\nBasic SET TRANSACTION statement\n\n\t\n\n2\n\n\t\n\nSET TRANSACTION statement: READ ONLY and READ WRITE clauses\n\n\t\n\nIs ignored\n\n\n\n\nE161\n\n\t\n\nSQL comments using leading double minus\n\n\t\t\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n1\n\n\t\n\nCOLUMNS view\n\n\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n2\n\n\t\n\nTABLES view\n\n\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n3\n\n\t\n\nVIEWS view\n\n\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n4\n\n\t\n\nTABLE_CONSTRAINTS view\n\n\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n5\n\n\t\n\nREFERENTIAL_CONSTRAINTS view\n\n\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n6\n\n\t\n\nCHECK_CONSTRAINTS view\n\n\t\n\n\nF031\n\n\t\n\nBasic schema manipulation\n\n\t\n\n1\n\n\t\n\nCREATE TABLE statement to create persistent base tables\n\n\t\n\n\nF031\n\n\t\n\nBasic schema manipulation\n\n\t\n\n2\n\n\t\n\nCREATE VIEW statement\n\n\t\n\n\nF031\n\n\t\n\nBasic schema manipulation\n\n\t\n\n3\n\n\t\n\nGRANT statement\n\n\t\n\n\nF031\n\n\t\n\nBasic schema manipulation\n\n\t\n\n4\n\n\t\n\nALTER TABLE statement: ADD COLUMN clause\n\n\t\n\n\nF033\n\n\t\n\nALTER TABLE statement: DROP COLUMN clause\n\n\t\t\t\n\n\nF034\n\n\t\n\nExtended REVOKE statement\n\n\t\t\t\n\n\nF034\n\n\t\n\nExtended REVOKE statement\n\n\t\n\n1\n\n\t\n\nREVOKE statement performed by other than the owner of a schema object\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\t\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n1\n\n\t\n\nInner join (but not necessarily the INNER keyword)\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n2\n\n\t\n\nINNER keyword\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n3\n\n\t\n\nLEFT OUTER JOIN\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n4\n\n\t\n\nRIGHT OUTER JOIN\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n5\n\n\t\n\nOuter joins can be nested\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n7\n\n\t\n\nThe inner table in a left or right outer join can also be used in an inner join\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n8\n\n\t\n\nAll comparison operators are supported (rather than just =)\n\n\t\n\n\nF051\n\n\t\n\nBasic date and time\n\n\t\n\n1\n\n\t\n\nDATE data type (including support of DATE literal)\n\n\t\n\n\nF051\n\n\t\n\nBasic date and time\n\n\t\n\n3\n\n\t\n\nTIMESTAMP data type (including support of TIMESTAMP literal) with fractional seconds precision of at least 0 and 6\n\n\t\n\n\nF051\n\n\t\n\nBasic date and time\n\n\t\n\n4\n\n\t\n\nComparison predicate on DATE, TIME, and TIMESTAMP data types\n\n\t\n\n\nF051\n\n\t\n\nBasic date and time\n\n\t\n\n5\n\n\t\n\nExplicit CAST between datetime types and character string types\n\n\t\n\n\nF051\n\n\t\n\nBasic date and time\n\n\t\n\n6\n\n\t\n\nCURRENT_DATE\n\n\t\n\n\nF052\n\n\t\n\nIntervals and datetime arithmetic\n\n\t\t\t\n\n\nF111\n\n\t\n\nIsolation levels other than SERIALIZABLE\n\n\t\t\t\n\n\nF111\n\n\t\n\nIsolation levels other than SERIALIZABLE\n\n\t\n\n1\n\n\t\n\nREAD UNCOMMITTED isolation level\n\n\t\n\nIs ignored\n\n\n\n\nF111\n\n\t\n\nIsolation levels other than SERIALIZABLE\n\n\t\n\n2\n\n\t\n\nREAD COMMITTED isolation level\n\n\t\n\nIs ignored\n\n\n\n\nF111\n\n\t\n\nIsolation levels other than SERIALIZABLE\n\n\t\n\n3\n\n\t\n\nREPEATABLE READ isolation level\n\n\t\n\nIs ignored\n\n\n\n\nF131\n\n\t\n\nGrouped operations\n\n\t\n\n1\n\n\t\n\nWHERE, GROUP BY, and HAVING clauses supported in queries with grouped views\n\n\t\n\n\nF131\n\n\t\n\nGrouped operations\n\n\t\n\n3\n\n\t\n\nSet functions supported in queries with grouped views\n\n\t\n\n\nF171\n\n\t\n\nMultiple schemas per user\n\n\t\t\t\n\n\nF201\n\n\t\n\nCAST function\n\n\t\t\t\n\n\nF221\n\n\t\n\nExplicit defaults\n\n\t\t\t\n\n\nF222\n\n\t\n\nINSERT statement: DEFAULT VALUES clause\n\n\t\t\t\n\n\nF261\n\n\t\n\nCASE expression\n\n\t\t\t\n\n\nF261\n\n\t\n\nCASE expression\n\n\t\n\n1\n\n\t\n\nSimple CASE\n\n\t\n\n\nF261\n\n\t\n\nCASE expression\n\n\t\n\n2\n\n\t\n\nSearched CASE\n\n\t\n\n\nF261\n\n\t\n\nCASE expression\n\n\t\n\n3\n\n\t\n\nNULLIF\n\n\t\n\n\nF261\n\n\t\n\nCASE expression\n\n\t\n\n4\n\n\t\n\nCOALESCE\n\n\t\n\n\nF262\n\n\t\n\nExtended CASE expression\n\n\t\t\t\n\n\nF311\n\n\t\n\nSchema definition statement\n\n\t\n\n2\n\n\t\n\nCREATE TABLE for persistent base tables\n\n\t\n\n\nF311\n\n\t\n\nSchema definition statement\n\n\t\n\n3\n\n\t\n\nCREATE VIEW\n\n\t\n\n\nF381\n\n\t\n\nExtended schema manipulation\n\n\t\t\t\n\n\nF381\n\n\t\n\nExtended schema manipulation\n\n\t\n\n1\n\n\t\n\nALTER TABLE statement: ALTER COLUMN clause\n\n\t\n\n\nF381\n\n\t\n\nExtended schema manipulation\n\n\t\n\n2\n\n\t\n\nALTER TABLE statement: ADD CONSTRAINT clause\n\n\t\n\n\nF381\n\n\t\n\nExtended schema manipulation\n\n\t\n\n3\n\n\t\n\nALTER TABLE statement: DROP CONSTRAINT clause\n\n\t\n\n\nF391\n\n\t\n\nLong identifiers\n\n\t\t\t\n\n\nF392\n\n\t\n\nUnicode escapes in identifiers\n\n\t\t\t\n\n\nF401\n\n\t\n\nExtended joined table\n\n\t\n\n2\n\n\t\n\nFULL OUTER JOIN\n\n\t\n\n\nF401\n\n\t\n\nExtended joined table\n\n\t\n\n4\n\n\t\n\nCROSS JOIN\n\n\t\n\n\nF431\n\n\t\n\nRead-only scrollable cursors\n\n\t\t\t\n\n\nF431\n\n\t\n\nRead-only scrollable cursors\n\n\t\n\n1\n\n\t\n\nFETCH with explicit NEXT\n\n\t\n\n\nF431\n\n\t\n\nRead-only scrollable cursors\n\n\t\n\n2\n\n\t\n\nFETCH FIRST\n\n\t\n\n\nF431\n\n\t\n\nRead-only scrollable cursors\n\n\t\n\n3\n\n\t\n\nFETCH LAST\n\n\t\n\n\nF431\n\n\t\n\nRead-only scrollable cursors\n\n\t\n\n4\n\n\t\n\nFETCH PRIOR\n\n\t\n\n\nF431\n\n\t\n\nRead-only scrollable cursors\n\n\t\n\n5\n\n\t\n\nFETCH ABSOLUTE\n\n\t\n\n\nF431\n\n\t\n\nRead-only scrollable cursors\n\n\t\n\n6\n\n\t\n\nFETCH RELATIVE\n\n\t\n\n\nF471\n\n\t\n\nScalar subquery values\n\n\t\t\t\n\n\nF481\n\n\t\n\nExpanded NULL predicate\n\n\t\t\t\n\n\nF501\n\n\t\n\nFeatures and conformance views\n\n\t\n\n1\n\n\t\n\nSQL_FEATURES view\n\n\t\n\n\nF571\n\n\t\n\nTruth value tests\n\n\t\t\t\n\n\nF651\n\n\t\n\nCatalog name qualifiers\n\n\t\t\t\n\n\nF763\n\n\t\n\nCURRENT_SCHEMA\n\n\t\t\t\n\n\nF791\n\n\t\n\nInsensitive cursors\n\n\t\t\t\n\n\nF850\n\n\t\n\nTop-level <order by clause> in <query expression>\n\n\t\t\t\n\n\nF851\n\n\t\n\n<order by clause> in subqueries\n\n\t\t\t\n\n\nF852\n\n\t\n\nTop-level <order by clause> in views\n\n\t\t\t\n\n\nF855\n\n\t\n\nNested <order by clause> in <query expression>\n\n\t\t\t\n\n\nF856\n\n\t\n\nNested <fetch first clause> in <query expression>\n\n\t\t\t\n\n\nF857\n\n\t\n\nTop-level <fetch first clause> in <query expression>\n\n\t\t\t\n\n\nF858\n\n\t\n\n<fetch first clause> in subqueries\n\n\t\t\t\n\n\nF859\n\n\t\n\nTop-level <fetch first clause> in views\n\n\t\t\t\n\n\nF860\n\n\t\n\n<fetch first row count> in <fetch first clause>\n\n\t\t\t\n\n\nS091\n\n\t\n\nBasic array support\n\n\t\t\t\n\nspecial syntax\n\n\n\n\nS091\n\n\t\n\nBasic array support\n\n\t\n\n1\n\n\t\n\nArrays of built-in data types\n\n\t\n\nspecial syntax\n\n\n\n\nS098\n\n\t\n\nARRAY_AGG\n\n\t\t\t\n\n\nT031\n\n\t\n\nBOOLEAN data type\n\n\t\t\t\n\n\nT051\n\n\t\n\nRow types\n\n\t\t\t\n\nLimited to built-in table functions\n\n\n\n\nT054\n\n\t\n\nGREATEST and LEAST\n\n\t\t\t\n\n\nT055\n\n\t\n\nString padding functions\n\n\t\t\t\n\n\nT056\n\n\t\n\nMulti-character trim functions\n\n\t\t\t\n\n\nT071\n\n\t\n\nBIGINT data type\n\n\t\t\t\n\n\nT081\n\n\t\n\nOptional string types maximum length\n\n\t\t\t\n\n\nT121\n\n\t\n\nWITH (excluding RECURSIVE) in query expression\n\n\t\t\t\n\n\nT122\n\n\t\n\nWITH (excluding RECURSIVE) in subquery\n\n\t\t\t\n\n\nT175\n\n\t\n\nGenerated columns\n\n\t\t\t\n\n\nT241\n\n\t\n\nSTART TRANSACTION statement\n\n\t\t\t\n\nIs ignored\n\n\n\n\nT321\n\n\t\n\nBasic SQL-invoked routines\n\n\t\n\n1\n\n\t\n\nUser-defined functions with no overloading\n\n\t\n\n\nT321\n\n\t\n\nBasic SQL-invoked routines\n\n\t\n\n3\n\n\t\n\nFunction invocation\n\n\t\n\n\nT321\n\n\t\n\nBasic SQL-invoked routines\n\n\t\n\n6\n\n\t\n\nROUTINES view\n\n\t\n\n\nT351\n\n\t\n\nBracketed SQL comments (/…/ comments)\n\n\t\t\t\n\n\nT441\n\n\t\n\nABS and MOD functions\n\n\t\t\t\n\n\nT461\n\n\t\n\nSymmetric BETWEEN predicate\n\n\t\t\t\n\n\nT471\n\n\t\n\nResult sets return value\n\n\t\t\t\n\n\nT615\n\n\t\n\nLEAD and LAG functions\n\n\t\t\t\n\n\nT617\n\n\t\n\nFIRST_VALUE and LAST_VALUE function\n\n\t\t\t\n\n\nT618\n\n\t\n\nNTH_VALUE function\n\n\t\t\t\n\n\nT621\n\n\t\n\nEnhanced numeric functions\n\n\t\t\t\n\n\nT626\n\n\t\n\nANY_VALUE aggregation\n\n\t\t\t\n\n\nT631\n\n\t\n\nIN predicate with one list element\n\n\t\t\t\n\n\nT662\n\n\t\n\nUnderscores in numeric literals\n\n\t\t\t"
  },
  {
    "title": "Release Notes — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/appendices/release-notes/index.html",
    "html": "5.5\nRelease Notes\n\nInformation about individual CrateDB releases, typically including upgrade information and changelog.\n\nNote\n\nThe latest stable, testing, and nightly version is always available from the CrateDB download page.\n\nOlder version of CrateDB are available in the release archives.\n\nVersions\n5.x\n5.5.x\nVersion 5.5.5 - Unreleased\nVersion 5.5.4\nVersion 5.5.3\nVersion 5.5.2\nVersion 5.5.1\nVersion 5.5.0\n5.4.x\nVersion 5.4.8\nVersion 5.4.7\nVersion 5.4.6\nVersion 5.4.5\nVersion 5.4.4\nVersion 5.4.3\nVersion 5.4.2\nVersion 5.4.1\nVersion 5.4.0\n5.3.x\nVersion 5.3.9\nVersion 5.3.8\nVersion 5.3.7\nVersion 5.3.6\nVersion 5.3.5\nVersion 5.3.4\nVersion 5.3.3\nVersion 5.3.2\nVersion 5.3.1\nVersion 5.3.0\n5.2.x\nVersion 5.2.11\nVersion 5.2.10\nVersion 5.2.9\nVersion 5.2.8\nVersion 5.2.7\nVersion 5.2.6\nVersion 5.2.5\nVersion 5.2.4\nVersion 5.2.3\nVersion 5.2.2\nVersion 5.2.1\nVersion 5.2.0\n5.1.x\nVersion 5.1.4\nVersion 5.1.3\nVersion 5.1.2\nVersion 5.1.1\nVersion 5.1.0\n5.0.x\nVersion 5.0.3\nVersion 5.0.2\nVersion 5.0.1\nVersion 5.0.0\n4.x\n4.8.x\nVersion 4.8.4\nVersion 4.8.3\nVersion 4.8.2\nVersion 4.8.1\nVersion 4.8.0\n4.7.x\nVersion 4.7.3\nVersion 4.7.2\nVersion 4.7.1\nVersion 4.7.0\n4.6.x\nVersion 4.6.8\nVersion 4.6.7\nVersion 4.6.6\nVersion 4.6.5\nVersion 4.6.4\nVersion 4.6.3\nVersion 4.6.2\nVersion 4.6.1\nVersion 4.6.0\n4.5.x\nVersion 4.5.5\nVersion 4.5.4\nVersion 4.5.3\nVersion 4.5.2\nVersion 4.5.1\nVersion 4.5.0\n4.4.x\nVersion 4.4.3\nVersion 4.4.2\nVersion 4.4.1\nVersion 4.4.0\n4.3.x\nVersion 4.3.4\nVersion 4.3.3\nVersion 4.3.2\nVersion 4.3.1\nVersion 4.3.0\n4.2.x\nVersion 4.2.7\nVersion 4.2.6\nVersion 4.2.5\nVersion 4.2.4\nVersion 4.2.3\nVersion 4.2.2\nVersion 4.2.1\nVersion 4.2.0\n4.1.x\nVersion 4.1.8\nVersion 4.1.7\nVersion 4.1.6\nVersion 4.1.5\nVersion 4.1.4\nVersion 4.1.3\nVersion 4.1.2\nVersion 4.1.1\nVersion 4.1.0\n4.0.x\nVersion 4.0.12\nVersion 4.0.11\nVersion 4.0.10\nVersion 4.0.9\nVersion 4.0.8\nVersion 4.0.7\nVersion 4.0.6\nVersion 4.0.5\nVersion 4.0.4\nVersion 4.0.3\nVersion 4.0.2\nVersion 4.0.1\nVersion 4.0.0\n3.x\n3.3.x\nVersion 3.3.6\nVersion 3.3.5\nVersion 3.3.4\nVersion 3.3.3\nVersion 3.3.2\nVersion 3.3.1\nVersion 3.3.0\n3.2.x\nVersion 3.2.8\nVersion 3.2.7\nVersion 3.2.6\nVersion 3.2.5\nVersion 3.2.4\nVersion 3.2.3\nVersion 3.2.2\nVersion 3.2.1\nVersion 3.2.0\n3.1.x\nVersion 3.1.6\nVersion 3.1.5\nVersion 3.1.4\nVersion 3.1.3\nVersion 3.1.2\nVersion 3.1.1\nVersion 3.1.0\n3.0.x\nVersion 3.0.7\nVersion 3.0.6\nVersion 3.0.5\nVersion 3.0.4\nVersion 3.0.3\nVersion 3.0.2\nVersion 3.0.1\nVersion 3.0.0\n2.x\n2.3.x\nVersion 2.3.11\nVersion 2.3.10\nVersion 2.3.9\nVersion 2.3.8\nVersion 2.3.7\nVersion 2.3.6\nVersion 2.3.5\nVersion 2.3.4\nVersion 2.3.3\nVersion 2.3.2\nVersion 2.3.1\nVersion 2.3.0\n2.2.x\nVersion 2.2.7\nVersion 2.2.6\nVersion 2.2.5\nVersion 2.2.4\nVersion 2.2.3\nVersion 2.2.2\nVersion 2.2.1\nVersion 2.2.0\n2.1.x\nVersion 2.1.10\nVersion 2.1.9\nVersion 2.1.8\nVersion 2.1.7\nVersion 2.1.6\nVersion 2.1.5\nVersion 2.1.4\nVersion 2.1.3\nVersion 2.1.2\nVersion 2.1.1\nVersion 2.1.0\n2.0.x\nVersion 2.0.7\nVersion 2.0.6\nVersion 2.0.5\nVersion 2.0.4\nVersion 2.0.3\nVersion 2.0.2\nVersion 2.0.1\nVersion 2.0.0\n1.x\n1.2.x\nVersion 1.2.0\n1.1.x\nVersion 1.1.6\nVersion 1.1.5\nVersion 1.1.4\nVersion 1.1.3\nVersion 1.1.2\nVersion 1.1.1\nVersion 1.1.0\n1.0.x\nVersion 1.0.6\nVersion 1.0.5\nVersion 1.0.4\nVersion 1.0.3\nVersion 1.0.2\nVersion 1.0.1\nVersion 1.0.0\nOlder Versions\n\nFor older versions, see the 0.57.8 CHANGES.txt file."
  },
  {
    "title": "PostgreSQL wire protocol — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/interfaces/postgres.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n5.5\nPostgreSQL wire protocol\n\nCrateDB supports the PostgreSQL wire protocol v3.\n\nIf a node is started with PostgreSQL wire protocol support enabled it will bind to port 5432 by default. To use a custom port, set the corresponding Ports in the Configuration.\n\nHowever, even though connecting PostgreSQL tools and client libraries is supported, the actual SQL statements have to be supported by CrateDB’s SQL dialect. A notable difference is that CrateDB doesn’t support transactions, which is why clients should generally enable autocommit.\n\nNote\n\nIn order to use setFetchSize in JDBC it is possible to set auto commit to false.\n\nThe client will utilize the fetchSize on SELECT statements and only load up to fetchSize rows into memory.\n\nSee the PostgreSQL JDBC Query docs for more information.\n\nWrite operations will still behave as if auto commit was enabled and commit or rollback calls are ignored.\n\nTable of contents\n\nServer compatibility\n\nStart-up\n\nSSL Support\n\nAuthentication\n\nParameterStatus\n\nDatabase selection\n\nQuery modes\n\nSimple query\n\nExtended query\n\nCopy operations\n\nFunction call\n\nCanceling requests\n\npg_catalog\n\npg_type\n\nOID types\n\nShow transaction isolation\n\nBEGIN, START, and COMMIT statements\n\nClient compatibility\n\nJDBC\n\nLimitations\n\nConnection failover and load balancing\n\nImplementation differences\n\nCopy operations\n\nData types\n\nDates and times\n\nObjects\n\nArrays\n\nDeclaration of arrays\n\nType casts\n\nText search functions and operators\n\nServer compatibility\n\nCrateDB emulates PostgreSQL server version 14.\n\nStart-up\nSSL Support\n\nSSL can be configured using Secured communications (SSL/TLS).\n\nAuthentication\n\nAuthentication methods can be configured using Host-Based Authentication (HBA).\n\nParameterStatus\n\nAfter the authentication succeeded, the server has the possibility to send multiple ParameterStatus messages to the client. These are used to communicate information like server_version (emulates PostgreSQL 9.5) or server_encoding.\n\nCrateDB also sends a message containing the crate_version parameter. This contains the current CrateDB version number.\n\nThis information is useful for clients to detect that they’re connecting to CrateDB instead of a PostgreSQL instance.\n\nDatabase selection\n\nSince CrateDB uses schemas instead of databases, the database parameter sets the default schema name for future queries. If no schema is specified, the schema doc will be used as default. Additionally, the only supported charset is UTF8.\n\nQuery modes\nSimple query\n\nThe PostgreSQL simple query protocol mode is fully implemented.\n\nExtended query\n\nThe PostgreSQL extended query protocol mode is implemented with the following limitations:\n\nThe ParameterDescription message works for the most common use cases except for DDL statements.\n\nTo optimize the execution of bulk operations the execution of statements is delayed until the Sync message is received\n\nCopy operations\n\nCrateDB does not support the COPY sub-protocol, see also Copy operations.\n\nFunction call\n\nThe function call sub-protocol is not supported since it’s a legacy feature.\n\nCanceling requests\n\nPostgreSQL cancelling requests is fully implemented.\n\npg_catalog\n\nFor improved compatibility, the pg_catalog schema is implemented containing following tables:\n\npg_am\n\npg_attrdef\n\npg_attribute\n\npg_class\n\npg_constraint\n\npg_cursors\n\npg_database\n\npg_description\n\npg_enum\n\npg_index\n\npg_indexes\n\npg_locks\n\npg_namespace\n\npg_proc\n\npg_publication\n\npg_publication_tables\n\npg_range\n\npg_roles\n\npg_settings\n\npg_shdescription\n\npg_stats\n\npg_subscription\n\npg_subscription_rel\n\npg_tables\n\npg_tablespace\n\npg_type\n\npg_views\n\npg_event_trigger\n\npg_type\n\nSome clients require the pg_catalog.pg_type in order to be able to stream arrays or other non-primitive types.\n\nFor compatibility reasons, there is a trimmed down pg_type table available in CrateDB:\n\ncr> SELECT oid, typname, typarray, typelem, typlen, typtype, typcategory\n... FROM pg_catalog.pg_type\n... ORDER BY oid;\n+------+--------------+----------+---------+--------+---------+-------------+\n|  oid | typname      | typarray | typelem | typlen | typtype | typcategory |\n+------+--------------+----------+---------+--------+---------+-------------+\n|   16 | bool         |     1000 |       0 |      1 | b       | N           |\n|   18 | char         |     1002 |       0 |      1 | b       | S           |\n|   19 | name         |       -1 |       0 |     64 | b       | S           |\n|   20 | int8         |     1016 |       0 |      8 | b       | N           |\n|   21 | int2         |     1005 |       0 |      2 | b       | N           |\n|   23 | int4         |     1007 |       0 |      4 | b       | N           |\n|   24 | regproc      |     1008 |       0 |      4 | b       | N           |\n|   25 | text         |     1009 |       0 |     -1 | b       | S           |\n|   26 | oid          |     1028 |       0 |      4 | b       | N           |\n|   30 | oidvector    |     1013 |      26 |     -1 | b       | A           |\n|  114 | json         |      199 |       0 |     -1 | b       | U           |\n|  199 | _json        |        0 |     114 |     -1 | b       | A           |\n|  600 | point        |     1017 |       0 |     16 | b       | G           |\n|  700 | float4       |     1021 |       0 |      4 | b       | N           |\n|  701 | float8       |     1022 |       0 |      8 | b       | N           |\n| 1000 | _bool        |        0 |      16 |     -1 | b       | A           |\n| 1002 | _char        |        0 |      18 |     -1 | b       | A           |\n| 1005 | _int2        |        0 |      21 |     -1 | b       | A           |\n| 1007 | _int4        |        0 |      23 |     -1 | b       | A           |\n| 1008 | _regproc     |        0 |      24 |     -1 | b       | A           |\n| 1009 | _text        |        0 |      25 |     -1 | b       | A           |\n| 1014 | _bpchar      |        0 |    1042 |     -1 | b       | A           |\n| 1015 | _varchar     |        0 |    1043 |     -1 | b       | A           |\n| 1016 | _int8        |        0 |      20 |     -1 | b       | A           |\n| 1017 | _point       |        0 |     600 |     -1 | b       | A           |\n| 1021 | _float4      |        0 |     700 |     -1 | b       | A           |\n| 1022 | _float8      |        0 |     701 |     -1 | b       | A           |\n| 1042 | bpchar       |     1014 |       0 |     -1 | b       | S           |\n| 1043 | varchar      |     1015 |       0 |     -1 | b       | S           |\n| 1082 | date         |     1182 |       0 |      8 | b       | D           |\n| 1114 | timestamp    |     1115 |       0 |      8 | b       | D           |\n| 1115 | _timestamp   |        0 |    1114 |     -1 | b       | A           |\n| 1182 | _date        |        0 |    1082 |     -1 | b       | A           |\n| 1184 | timestamptz  |     1185 |       0 |      8 | b       | D           |\n| 1185 | _timestamptz |        0 |    1184 |     -1 | b       | A           |\n| 1186 | interval     |     1187 |       0 |     16 | b       | T           |\n| 1187 | _interval    |        0 |    1186 |     -1 | b       | A           |\n| 1231 | _numeric     |        0 |    1700 |     -1 | b       | A           |\n| 1266 | timetz       |     1270 |       0 |     12 | b       | D           |\n| 1270 | _timetz      |        0 |    1266 |     -1 | b       | A           |\n| 1560 | bit          |     1561 |       0 |     -1 | b       | V           |\n| 1561 | _bit         |        0 |    1560 |     -1 | b       | A           |\n| 1700 | numeric      |     1231 |       0 |     -1 | b       | N           |\n| 2205 | regclass     |     2210 |       0 |      4 | b       | N           |\n| 2210 | _regclass    |        0 |    2205 |     -1 | b       | A           |\n| 2249 | record       |     2287 |       0 |     -1 | p       | P           |\n| 2276 | any          |        0 |       0 |      4 | p       | P           |\n| 2277 | anyarray     |        0 |    2276 |     -1 | p       | P           |\n| 2287 | _record      |        0 |    2249 |     -1 | p       | A           |\n+------+--------------+----------+---------+--------+---------+-------------+\nSELECT 49 rows in set (... sec)\n\n\nNote\n\nThis is just a snapshot of the table.\n\nCheck table information_schema.columns to get information for all supported columns.\n\nOID types\n\nObject Identifiers (OIDs) are used internally by PostgreSQL as primary keys for various system tables.\n\nCrateDB supports the oid type and the following aliases:\n\nName\n\n\t\n\nReference\n\n\t\n\nDescription\n\n\t\n\nExample\n\n\n\n\nregproc\n\n\t\n\npg_proc\n\n\t\n\nA function name\n\n\t\n\nsum\n\n\n\n\nregclass\n\n\t\n\npg_class\n\n\t\n\nA relation name\n\n\t\n\npg_type\n\nCrateDB also supports the oidvector type.\n\nNote\n\nCasting a string or an integer to the regproc type does not result in a function lookup (as it does with PostgreSQL).\n\nInstead:\n\nCasting a string to the regproc type results in an object of the regproc type with a name equal to the string value and an oid equal to an integer hash of the string.\n\nCasting an integer to the regproc type results in an object of the regproc type with a name equal to the string representation of the integer and an oid equal to the integer value.\n\nConsult the CrateDB data types reference for more information about each OID type (including additional type casting behaviour).\n\nShow transaction isolation\n\nFor compatibility with JDBC the SHOW TRANSACTION ISOLATION LEVEL statement is implemented:\n\ncr> show transaction isolation level;\n+-----------------------+\n| transaction_isolation |\n+-----------------------+\n| read uncommitted      |\n+-----------------------+\nSHOW 1 row in set (... sec)\n\nBEGIN, START, and COMMIT statements\n\nFor compatibility with clients that use the PostgresSQL wire protocol (e.g., the Golang lib/pq and pgx drivers), CrateDB will accept the BEGIN, COMMIT, and START TRANSACTION statements. For example:\n\ncr> BEGIN TRANSACTION ISOLATION LEVEL READ UNCOMMITTED,\n...                   READ ONLY,\n...                   NOT DEFERRABLE;\nBEGIN OK, 0 rows affected  (... sec)\n\ncr> COMMIT\nCOMMIT OK, 0 rows affected  (... sec)\n\n\nCrateDB will silently ignore the COMMIT, BEGIN, and START TRANSACTION statements and all respective parameters.\n\nClient compatibility\nJDBC\n\npgjdbc JDBC drivers version 9.4.1209 and above are compatible.\n\nLimitations\n\nReflection methods like conn.getMetaData().getTables(...) won’t work since the required tables are unavailable in CrateDB.\n\nAs a workaround it’s possible to use SHOW TABLES or query the information_schema tables manually using SELECT statements.\n\nOBJECT and GEO_SHAPE columns can be streamed as JSON but require pgjdbc version 9.4.1210 or newer.\n\nMultidimensional arrays will be streamed as JSON encoded string to avoid a protocol limitation where all sub-arrays are required to have the same length.\n\nThe behavior of PreparedStatement.executeBatch in error cases depends on in which stage an error occurs: A BatchUpdateException is thrown if no processing has been done yet, whereas single operations failing after the processing started are indicated by an EXECUTE_FAILED (-3) return value.\n\nTransaction limitations as described above.\n\nHaving escape processing enabled could prevent the usage of Object Literals in case an object key’s starting character clashes with a JDBC escape keyword (see also JDBC escape syntax). Disabling escape processing will remedy this appropriately for pgjdbc version >= 9.4.1212.\n\nConnection failover and load balancing\n\nConnection failover and load balancing is supported as described here: PostgreSQL JDBC connection failover.\n\nNote\n\nIt is not recommended to use the targetServerType parameter since CrateDB has no concept of master-replica nodes.\n\nImplementation differences\n\nThe PostgreSQL Wire Protocol makes it easy to use many PostgreSQL compatible tools and libraries directly with CrateDB. However, many of these tools assume that they are talking to PostgreSQL specifically, and thus rely on SQL extensions and idioms that are unique to PostgreSQL. Because of this, some tools or libraries may not work with other SQL databases such as CrateDB.\n\nCrateDB’s SQL query engine enables real-time search & aggregations for online analytic processing (OLAP) and business intelligence (BI) with the benefit of the ability to scale horizontally. The use-cases of CrateDB are different than those of PostgreSQL, as CrateDB’s specialized storage schema and query execution engine addresses different needs (see Clustering).\n\nThe features listed below cover the main differences in implementation and dialect between CrateDB and PostgreSQL. A detailed comparison between CrateDB’s SQL dialect and standard SQL is outlined in SQL compatibility.\n\nCopy operations\n\nCrateDB does not support the distinct sub-protocol that is used to serve COPY operations and provides another implementation for transferring bulk data using the COPY FROM and COPY TO statements.\n\nData types\nDates and times\n\nAt the moment, CrateDB does not support TIME without a time zone.\n\nAdditionally, CrateDB does not support the INTERVAL input units MILLENNIUM, CENTURY, DECADE, MILLISECOND, or MICROSECOND.\n\nObjects\n\nThe definition of structured values by using JSON types, composite types or HSTORE are not supported. CrateDB alternatively allows the definition of nested documents (of type OBJECT) that store fields containing any CrateDB supported data type, including nested object types.\n\nArrays\nDeclaration of arrays\n\nWhile multidimensional arrays in PostgreSQL must have matching extends for each dimension, CrateDB allows different length nested arrays as this example shows:\n\ncr> select [[1,2,3],[1,2]] from sys.cluster;\n+---------------------+\n| [[1, 2, 3], [1, 2]] |\n+---------------------+\n| [[1, 2, 3], [1, 2]] |\n+---------------------+\nSELECT 1 row in set (... sec)\n\nType casts\n\nCrateDB accepts the Type casting syntax for conversion of one data type to another.\n\nSee Also\n\nPostgreSQL value expressions\n\nCrateDB value expressions\n\nText search functions and operators\n\nThe functions and operators provided by PostgreSQL for full-text search (see PostgreSQL fulltext Search) are not compatible with those provided by CrateDB.\n\nIf you are missing features, functions or dialect improvements and have a great use case for it, let us know on GitHub. We’re always improving and extending CrateDB and we love to hear feedback."
  },
  {
    "title": "SQL Statements — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/sql/statements/index.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n5.5\nSQL Statements\n\nTable of contents\n\nALTER CLUSTER\nALTER PUBLICATION\nALTER TABLE\nALTER USER\nANALYZE\nBEGIN\nCLOSE\nCOMMIT\nCOPY FROM\nCOPY TO\nCREATE ANALYZER\nCREATE BLOB TABLE\nCREATE FUNCTION\nCREATE PUBLICATION\nCREATE REPOSITORY\nCREATE SNAPSHOT\nCREATE SUBSCRIPTION\nCREATE TABLE\nCREATE TABLE AS\nCREATE USER\nCREATE VIEW\nDEALLOCATE\nDECLARE\nDELETE\nDENY\nDISCARD\nDROP ANALYZER\nDROP FUNCTION\nDROP PUBLICATION\nDROP REPOSITORY\nDROP SNAPSHOT\nDROP SUBSCRIPTION\nDROP TABLE\nDROP USER\nDROP VIEW\nEND\nEXPLAIN\nFETCH\nGRANT\nINSERT\nKILL\nOPTIMIZE\nREFRESH\nRESTORE SNAPSHOT\nREVOKE\nSELECT\nSET and RESET\nSET LICENSE\nSET AND RESET SESSION AUTHORIZATION\nSET TRANSACTION\nSHOW (session settings)\nSHOW COLUMNS\nSHOW CREATE TABLE\nSHOW SCHEMAS\nSHOW TABLES\nSTART TRANSACTION\nUPDATE\nVALUES\nWITH"
  },
  {
    "title": "General SQL — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/sql/general/index.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n5.5\nGeneral SQL\n\nTable of contents\n\nConstraints\nValue expressions\nLexical structure"
  },
  {
    "title": "HTTP endpoint — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/interfaces/http.html",
    "html": "5.5\nHTTP endpoint\n\nCrateDB provides a HTTP Endpoint that can be used to submit SQL queries. The endpoint is accessible under <servername:port>/_sql.\n\nSQL statements are sent to the _sql endpoint in json format, whereby the statement is sent as value associated to the key stmt.\n\nSee Also\n\nData manipulation\n\nA simple SELECT statement can be submitted like this:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' \\\n... -d '{\"stmt\":\"select name, position from locations order by id limit 2\"}'\n{\n  \"cols\": [\n    \"name\",\n    \"position\"\n  ],\n  \"rows\": [\n    [\n      \"North West Ripple\",\n      1\n    ],\n    [\n      \"Outer Eastern Rim\",\n      2\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nNote\n\nWe’re using a simple command line invocation of curl here so you can see how to run this by hand in the terminal. For the rest of the examples in this document, we use here documents (i.e. EOF) for multi line readability.\n\nTable of contents\n\nParameter substitution\n\nDefault schema\n\nColumn types\n\nAvailable data types\n\nBulk operations\n\nError handling\n\nError codes\n\nBulk errors\n\nParameter substitution\n\nIn addition to the stmt key the request body may also contain an args key which can be used for SQL parameter substitution.\n\nThe SQL statement has to be changed to use placeholders where the values should be inserted. Placeholders can either be numbered (in the form of $1, $2, etc.) or unnumbered using a question mark ?.\n\nThe placeholders will then be substituted with values from an array that is expected under the args key:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\":\n...     \"select date,position from locations\n...     where date <= \\$1 and position < \\$2 order by position\",\n...   \"args\": [\"1979-10-12\", 3]\n... }\n... EOF\n{\n  \"cols\": [\n    \"date\",\n    \"position\"\n  ],\n  \"rows\": [\n    [\n      308534400000,\n      1\n    ],\n    [\n      308534400000,\n      2\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nNote\n\nIn this example the placeholders start with an backslash due to shell escaping.\n\nWarning\n\nParameter substitution must not be used within subscript notation.\n\nFor example, column[?] is not allowed.\n\nThe same query using question marks as placeholders looks like this:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\":\n...     \"select date,position from locations\n...     where date <= ? and position < ? order by position\",\n...   \"args\": [\"1979-10-12\", 3]\n... }\n... EOF\n{\n  \"cols\": [\n    \"date\",\n    \"position\"\n  ],\n  \"rows\": [\n    [\n      308534400000,\n      1\n    ],\n    [\n      308534400000,\n      2\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nNote\n\nWith some queries the row count is not ascertainable. In this cases rowcount is -1.\n\nDefault schema\n\nIt is possible to set a default schema while querying the CrateDB cluster via _sql end point. In such case the HTTP request should contain the Default-Schema header with the specified schema name:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' \\\n... -H 'Default-Schema: doc' -d@- <<- EOF\n... {\n...   \"stmt\":\"select name, position from locations order by id limit 2\"\n... }\n... EOF\n{\n  \"cols\": [\n    \"name\",\n    \"position\"\n  ],\n  \"rows\": [\n    [\n      \"North West Ripple\",\n      1\n    ],\n    [\n      \"Outer Eastern Rim\",\n      2\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nIf the schema name is not specified in the header, the default doc schema will be used instead.\n\nColumn types\n\nCrateDB can respond a list col_types with the data type ID of every responded column. This way one can know what exact data type a column is holding.\n\nIn order to get the list of column data types, a types query parameter must be passed to the request:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql?types' -d@- <<- EOF\n... {\n...   \"stmt\":\n...     \"select date, position from locations\n...      where date <= \\$1 and position < \\$2 order by position\",\n...   \"args\": [\"1979-10-12\", 3]\n... }\n... EOF\n{\n  \"cols\": [\n    \"date\",\n    \"position\"\n  ],\n  \"col_types\": [\n    11,\n    9\n  ],\n  \"rows\": [\n    [\n      308534400000,\n      1\n    ],\n    [\n      308534400000,\n      2\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nThe Array collection data type is displayed as a list where the first value is the collection type and the second is the inner type. The inner type could also be a collection.\n\nExample of JSON representation of a column list of (String, Integer[]):\n\n\"column_types\": [ 4, [ 100, 9 ] ]\n\nAvailable data types\n\nIDs of all currently available data types:\n\nID\n\n\t\n\nData type\n\n\n\n\n0\n\n\t\n\nNULL\n\n\n\n\n1\n\n\t\n\nNot supported\n\n\n\n\n2\n\n\t\n\nCHAR\n\n\n\n\n3\n\n\t\n\nBOOLEAN\n\n\n\n\n4\n\n\t\n\nTEXT\n\n\n\n\n5\n\n\t\n\nIP\n\n\n\n\n6\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\n7\n\n\t\n\nREAL\n\n\n\n\n8\n\n\t\n\nSMALLINT\n\n\n\n\n9\n\n\t\n\nINTEGER\n\n\n\n\n10\n\n\t\n\nBIGINT\n\n\n\n\n11\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\n12\n\n\t\n\nOBJECT\n\n\n\n\n13\n\n\t\n\nGEO_POINT\n\n\n\n\n14\n\n\t\n\nGEO_SHAPE\n\n\n\n\n15\n\n\t\n\nTIMESTAMP WITHOUT TIME ZONE\n\n\n\n\n16\n\n\t\n\nUnchecked object\n\n\n\n\n19\n\n\t\n\nREGPROC\n\n\n\n\n20\n\n\t\n\nTIME\n\n\n\n\n21\n\n\t\n\nOIDVECTOR\n\n\n\n\n22\n\n\t\n\nNUMERIC\n\n\n\n\n23\n\n\t\n\nREGCLASS\n\n\n\n\n24\n\n\t\n\nDATE\n\n\n\n\n25\n\n\t\n\nBIT\n\n\n\n\n26\n\n\t\n\nJSON\n\n\n\n\n27\n\n\t\n\nCHARACTER\n\n\n\n\n100\n\n\t\n\nARRAY\n\nBulk operations\n\nThe REST endpoint allows to issue bulk operations which are executed as single calls on the back-end site. It can be compared to prepared statement.\n\nA bulk operation can be expressed simply as an SQL statement.\n\nSupported bulk SQL statements are:\n\nInsert\n\nUpdate\n\nDelete\n\nInstead of the args (Parameter substitution) key, use the key bulk_args. This allows to specify a list of lists, containing all the records which shall be processed. The inner lists need to match the specified columns.\n\nThe bulk response contains a results array, with a row count for each bulk operation. Those results are in the same order as the issued operations of the bulk operation.\n\nThe following example describes how to issue an insert bulk operation and insert three records at once:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\": \"INSERT INTO locations (id, name, kind, description)\n...           VALUES (?, ?, ?, ?)\",\n...   \"bulk_args\": [\n...     [1337, \"Earth\", \"Planet\", \"An awesome place to spend some time on.\"],\n...     [1338, \"Sun\", \"Star\", \"An extraordinarily hot place.\"],\n...     [1339, \"Titan\", \"Moon\", \"Titan, where it rains fossil fuels.\"]\n...   ]\n... }\n... EOF\n{\n  \"cols\": [],\n  \"duration\": ...,\n  \"results\": [\n    {\n      \"rowcount\": 1\n    },\n    {\n      \"rowcount\": 1\n    },\n    {\n      \"rowcount\": 1\n    }\n  ]\n}\n\nError handling\n\nQueries that are invalid or cannot be satisfied will result in an error response. The response will contain an error code, an error message and in some cases additional arguments that are specific to the error code.\n\nClient libraries should use the error code to translate the error into an appropriate exception:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\":\"select name, position from foo.locations\"\n... }\n... EOF\n{\n  \"error\": {\n    \"message\": \"SchemaUnknownException[Schema 'foo' unknown]\",\n    \"code\": 4045\n  }\n}\n\n\nTo get more insight into what exactly went wrong an additional error_trace GET parameter can be specified to return the stack trace:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql?error_trace=true' -d@- <<- EOF\n... {\n...   \"stmt\":\"select name, position from foo.locations\"\n... }\n... EOF\n{\n  \"error\": {\n    \"message\": \"SchemaUnknownException[Schema 'foo' unknown]\",\n    \"code\": 4045\n  },\n  \"error_trace\": \"...\"\n}\n\n\nNote\n\nThis parameter is intended for CrateDB developers or for users requesting support for CrateDB. Client libraries shouldn’t make use of this option and not include the stack trace.\n\nError codes\n\nCode\n\n\t\n\nError\n\n\n\n\n4000\n\n\t\n\nThe statement contains an invalid syntax or unsupported SQL statement\n\n\n\n\n4001\n\n\t\n\nThe statement contains an invalid analyzer definition.\n\n\n\n\n4002\n\n\t\n\nThe name of the relation is invalid.\n\n\n\n\n4003\n\n\t\n\nField type validation failed\n\n\n\n\n4004\n\n\t\n\nPossible feature not supported (yet)\n\n\n\n\n4005\n\n\t\n\nAlter table using a table alias is not supported.\n\n\n\n\n4006\n\n\t\n\nThe used column alias is ambiguous.\n\n\n\n\n4007\n\n\t\n\nThe operation is not supported on this relation, as it is not accessible.\n\n\n\n\n4008\n\n\t\n\nThe name of the column is invalid.\n\n\n\n\n4009\n\n\t\n\nCrateDB License is expired. (Deprecated.)\n\n\n\n\n4010\n\n\t\n\nUser is not authorized to perform the SQL statement.\n\n\n\n\n4011\n\n\t\n\nMissing privilege for user.\n\n\n\n\n4031\n\n\t\n\nOnly read operations are allowed on this node.\n\n\n\n\n4041\n\n\t\n\nUnknown relation.\n\n\n\n\n4042\n\n\t\n\nUnknown analyzer.\n\n\n\n\n4043\n\n\t\n\nUnknown column.\n\n\n\n\n4044\n\n\t\n\nUnknown type.\n\n\n\n\n4045\n\n\t\n\nUnknown schema.\n\n\n\n\n4046\n\n\t\n\nUnknown Partition.\n\n\n\n\n4047\n\n\t\n\nUnknown Repository.\n\n\n\n\n4048\n\n\t\n\nUnknown Snapshot.\n\n\n\n\n4049\n\n\t\n\nUnknown user-defined function.\n\n\n\n\n40410\n\n\t\n\nUnknown user.\n\n\n\n\n4091\n\n\t\n\nA document with the same primary key exists already.\n\n\n\n\n4092\n\n\t\n\nA VersionConflict. Might be thrown if an attempt was made to update the same document concurrently.\n\n\n\n\n4093\n\n\t\n\nA relation with the same name exists already.\n\n\n\n\n4094\n\n\t\n\nThe used table alias contains tables with different schema.\n\n\n\n\n4095\n\n\t\n\nA repository with the same name exists already.\n\n\n\n\n4096\n\n\t\n\nA snapshot with the same name already exists in the repository.\n\n\n\n\n4097\n\n\t\n\nA partition for the same values already exists in this table.\n\n\n\n\n4098\n\n\t\n\nA user-defined function with the same signature already exists.\n\n\n\n\n4099\n\n\t\n\nA user with the same name already exists.\n\n\n\n\n5000\n\n\t\n\nUnhandled server error.\n\n\n\n\n5001\n\n\t\n\nThe execution of one or more tasks failed.\n\n\n\n\n5002\n\n\t\n\nOne or more shards are not available.\n\n\n\n\n5003\n\n\t\n\nThe query failed on one or more shards\n\n\n\n\n5004\n\n\t\n\nCreating a snapshot failed\n\n\n\n\n5030\n\n\t\n\nThe query was killed by a kill statement\n\nBulk errors\n\nIf a bulk operation fails, the resulting row count will be -2 and the resulting object may contain an error_message depending on the resulting error:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\": \"INSERT into locations (name, id) values (?,?)\",\n...   \"bulk_args\": [\n...     [\"Mars\", 1341],\n...     [\"Sun\", 1341]\n...   ]\n... }\n... EOF\n{\n  \"cols\": [],\n  \"duration\": ...,\n  \"results\": [\n    {\n      \"rowcount\": 1\n    },\n    {\n      \"rowcount\": -2\n    }\n  ]\n}\n\n\nNote\n\nEvery bulk operation will be executed, independent if one of the operation fails."
  },
  {
    "title": "Cloud discovery — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/admin/discovery.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n5.5\nCloud discovery\n\nTable of contents\n\nAmazon EC2 discovery\n\nAmazon EC2 discovery\n\nCrateDB has native discovery support when running a cluster with Amazon Web Services (AWS). The discovery mechanism uses the Amazon EC2 API to generate the list of hosts for the unicast host discovery (see Unicast host discovery).\n\nThere is a best practice how to configure and run a CrateDB cluster on Amazon EC2."
  },
  {
    "title": "Logical replication — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/admin/logical-replication.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n5.5\nLogical replication\n\nTable of contents\n\nPublication\n\nSubscription\n\nSecurity\n\nMonitoring\n\nLogical replication is a method of data replication across multiple clusters. CrateDB uses a publish and subscribe model where subscribers pull data from the publications of the publisher they subscribed to.\n\nReplicated tables on a subscriber can again be published further to other clusters and thus chaining subscriptions is possible.\n\nNote\n\nA replicated index on a subscriber is read-only.\n\nLogical replication is useful for the following use cases:\n\nConsolidating data from multiple clusters into a single one for aggregated reports.\n\nEnsure high availability if one cluster becomes unavailable.\n\nReplicating between different compatible versions of CrateDB. Replicating tables created on a cluster with higher major/minor version to a cluster with lower major/minor version is not supported.\n\nSee Also\n\nreplication.logical.ops_batch_size replication.logical.reads_poll_duration replication.logical.recovery.chunk_size replication.logical.recovery.max_concurrent_file_chunks\n\nPublication\n\nA publication is the upstream side of logical replication and it’s created on the cluster which acts as a data source.\n\nEach table can be added to multiple publications if needed. Publications can only contain tables. All operation types (INSERT, UPDATE, DELETE and schema changes) are replicated.\n\nEvery publication can have multiple subscribers.\n\nA publication is created using the CREATE PUBLICATION command. The individual tables can be added or removed dynamically using ALTER PUBLICATION. Publications can be removed using the DROP PUBLICATION command.\n\nCaution\n\nThe publishing cluster must have soft_deletes.enabled set to true so that a subscribing cluster can catch up with all changes made during replication pauses caused by network issues or explicitly done by a user.\n\nAlso, soft_deletes.retention_lease.period should be greater than or equal to replication.logical.reads_poll_duration.\n\nSubscription\n\nA subscription is the downstream side of logical replication. A subscription defines the connection to another database and set of publications to which it wants to subscribe. By default, the subscription creation triggers the replication process on the subscriber cluster. The subscriber cluster behaves in the same way as any other CrateDB cluster and can be used as a publisher for other clusters by defining its own publications.\n\nA cluster can have multiple subscriptions. It is also possible for a cluster to have both subscriptions and publications. A cluster cannot subscribe to locally already existing tables, therefore it is not possible to setup a bi-directional replication (both sides subscribing to ALL TABLES leads to a cluster trying to replicate its own tables from another cluster). However, two clusters still can cross-subscribe to each other if one cluster subscribes to locally non-existing tables of another cluster and vice versa.\n\nA subscription is added using the CREATE SUBSCRIPTION command and can be removed using the DROP SUBSCRIPTION command. A subscription starts replicating on its creation and stops on its removal (if no failure happen in-between).\n\nPublished tables must not exist on the subscriber. A cluster cannot subscribe to a table on another cluster if it exists already on its side, therefore it’s not possible to drop and re-create a subscription without starting from scratch i.e removing all replicated tables.\n\nOnly regular tables (including partitions) may be the target of a replication. For example, you can not replicate system tables or views.\n\nThe tables are matched between the publisher and the subscriber using the fully qualified table name. Replication to differently-named tables on the subscriber is not supported.\n\nSecurity\n\nTo create, alter or drop a publication, a user must have the AL privilege on the cluster. Only the owner (the user who created the publication) or a superuser is allowed to ALTER or DROP a publication. To add tables to a publication, the user must have DQL, DML, and DDL privileges on the table. When a user creates a publication that publishes all tables automatically, only those tables where the user has DQL, DML, and DDL privileges will be published. The user a subscriber uses to connect to the publisher must have DQL privileges on the published tables. Tables, included into a publication but not available for a subscriber due to lack of DQL privilege, will not be replicated.\n\nTo create or drop a subscription, a user must have the AL privilege on the cluster. Only the owner (the user who created the subscription) or a superuser is allowed to DROP a subscription.\n\nCaution\n\nA network setup that allows the two clusters to communicate is a pre-requisite for a working publication/subscription setup. See HBA.\n\nMonitoring\n\nAll publications are listed in the pg_publication table. More details for a publication are available in the pg_publication_tables table. It lists the replicated tables for a specific publication.\n\nAll subscriptions are listed in the pg_subscription table. More details for a subscription are available in the pg_subscription_rel table. The table contains detailed information about the replication state per table, including error messages if there was an error."
  },
  {
    "title": "Usage Data Collector — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/admin/udc.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n5.5\nUsage Data Collector\n\nThe CrateDB Usage Data Collector (UDC) is a sub-system that gathers usage data, reporting it to the UDC server at https://udc.crate.io. It is easy to disable, and does not collect any data that is confidential. For more information about what is being sent, see below.\n\nCrateDB uses this information as a form of automatic, effortless feedback from the community. We want to verify that we are doing the right thing by matching download statistics with usage statistics. After each release, we can see if there is a larger retention span of the server software.\n\nThe data collected is clearly stated here. If any future versions of this system collect additional data, we will clearly announce those changes.\n\nCrateDB is concerned about your privacy. We do not disclose any personally identifiable information.\n\nTable of contents\n\nTechnical information\n\nAdmin UI tracking\n\nConfiguration\n\nHow to disable UDC\n\nBy configuration\n\nBy system property\n\nTechnical information\n\nTo gather good statistics about CrateDB usage, UDC collects this information:\n\nName\n\n\t\n\nDescription\n\n\n\n\nKernel Version\n\n\t\n\nThe build number, and if there are any modifications to the kernel.\n\n\n\n\nCluster Id\n\n\t\n\nA randomized globally unique ID created every time the whole cluster is restarted.\n\n\n\n\nMaster\n\n\t\n\nBoolean whether the current node is master.\n\n\n\n\nPing Count\n\n\t\n\nUDC holds an internal counter per node which is incremented for every ping, and reset on every restart of the a node.\n\n\n\n\nCrateDB Version\n\n\t\n\nThe CrateDB version.\n\n\n\n\nJava Version\n\n\t\n\nThe Java version CrateDB is currently running with.\n\n\n\n\nHardware Address\n\n\t\n\nMAC address to uniquely identify instances behind firewalls.\n\n\n\n\nProcessor count\n\n\t\n\nNumber of available CPUs as reported by Runtime.availableProcessors\n\n\n\n\nEnterprise\n\n\t\n\nIdentifies whether the Enterprise Edition is used. 1\n\nAfter startup, UDC waits for 10 minutes before sending the first ping. It does this for two reasons; first, we don’t want the startup to be slower because of UDC, and secondly, we want to keep pings from automatic tests to a minimum. By default, UDC is sending pings every 24 hours. The ping to the UDC servers is done with a HTTP GET.\n\nAdmin UI tracking\n\nSince Admin UI v0.16.0 we are tracking the user ID along with the cluster ID to know how many active users are currently using CrateDB.\n\nConfiguration\n\nThe Usage Data Collector can be configured by adapting the crate.yml configuration file or adding a system property setting. Refer to Usage data collector to see how these settings can be accessed and how they are configured.\n\nHow to disable UDC\n\nBelow are two ways you can disable UDC. However we hope you support us offering the open source edition, and leave UDC on, so we learn how many people use CrateDB.\n\nBy configuration\n\nJust add following to your crate.yml configuration file:\n\nudc.enabled:  false\n\nBy system property\n\nIf you do not want to make any change to the jars or to the configuration, a system property setting like this will also make sure that UDC is never activated:\n\n-Cudc.enabled=false\n\n1\n\nThe “CrateDB Enterprise Edition” has been dissolved starting with CrateDB 4.5.0, see also Farewell to the CrateDB Enterprise License."
  },
  {
    "title": "Snapshots — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/admin/snapshots.html",
    "html": "5.5\nSnapshots\n\nTable of contents\n\nSnapshot\n\nCreating a repository\n\nCreating a snapshot\n\nRestore\n\nRestore data granularity\n\nCleanup\n\nDropping snapshots\n\nDropping repositories\n\nSnapshot\n\nIn CrateDB, backups are called Snapshots. They represent the state of the tables in a CrateDB cluster at the time the Snapshot was created. A Snapshot is always stored in a Repository which has to be created first.\n\nCaution\n\nYou cannot snapshot BLOB tables.\n\nCreating a repository\n\nRepositories are used to store, manage and restore snapshots.\n\nThey are created using the CREATE REPOSITORY statement:\n\ncr> CREATE REPOSITORY where_my_snapshots_go TYPE fs\n... WITH (location='repo_path', compress=true);\nCREATE OK, 1 row affected (... sec)\n\n\nRepositories are uniquely identified by their name. Every repository has a specific type which determines how snapshots are stored.\n\nCrateDB supports different repository types, see Types.\n\nThe creation of a repository configures it inside the CrateDB cluster. In general no data is written, no snapshots inside repositories changed or deleted. This way you can tell the CrateDB cluster about existing repositories which already contain snapshots.\n\nCreating a repository with the same name will result in an error:\n\ncr> CREATE REPOSITORY where_my_snapshots_go TYPE fs\n... WITH (location='another_repo_path', compress=false);\nRepositoryAlreadyExistsException[Repository 'where_my_snapshots_go' already exists]\n\nCreating a snapshot\n\nSnapshots are created inside a repository and can contain any number of tables. The CREATE SNAPSHOT statement is used to create a snapshots:\n\ncr> CREATE SNAPSHOT where_my_snapshots_go.snapshot1 ALL\n... WITH (wait_for_completion=true, ignore_unavailable=true);\nCREATE OK, 1 row affected (... sec)\n\n\nA snapshot is referenced by the name of the repository and the snapshot name, separated by a dot. If ALL is used, all user created tables of the cluster (except blob tables) are stored inside the snapshot.\n\nIt’s possible to only save a specific subset of tables in the snapshot by listing them explicitly:\n\ncr> CREATE SNAPSHOT where_my_snapshots_go.snapshot2 TABLE quotes, doc.locations\n... WITH (wait_for_completion=true);\nCREATE OK, 1 row affected (... sec)\n\n\nEven single partition of Partitioned tables can be selected for backup. This is especially useful if old partitions need to be deleted but it should be possible to restore them if needed:\n\ncr> CREATE SNAPSHOT where_my_snapshots_go.snapshot3 TABLE\n...    locations,\n...    parted_table PARTITION (date='1970-01-01')\n... WITH (wait_for_completion=true);\nCREATE OK, 1 row affected (... sec)\n\n\nSnapshots are incremental. Snapshots of the same cluster created later only store data not already contained in the repository.\n\nAll examples above are used with the argument wait_for_completion set to true. As described in the CREATE REPOSITORY reference documentation, by doing this, the statement will only respond (successfully or not) when the snapshot is fully created. Otherwise the snapshot will be created in the background and the statement will immediately respond as successful. The status of a created snapshot can be retrieved by querying the sys.snapshots system table.\n\nRestore\n\nCaution\n\nIf you are restoring a snapshot into a newer version of CrateDB, be sure to check the Release Notes for upgrade instructions.\n\nCaution\n\nIf you try to restore a table that already exists, CrateDB will return an error. However, if you try to restore metadata or cluster settings that already exist, they will be overwritten.\n\nOnce a snapshot is created, it can be used to restore its tables to the state when the snapshot was created.\n\nTo get basic information about snapshots the sys.snapshots table can be queried:\n\ncr> SELECT repository, name, state, concrete_indices\n... FROM sys.snapshots\n... ORDER BY repository, name;\n+-----------------------+-----------+---------+--------------------...-+\n| repository            | name      | state   | concrete_indices       |\n+-----------------------+-----------+---------+--------------------...-+\n| where_my_snapshots_go | snapshot1 | SUCCESS | [...]                  |\n| where_my_snapshots_go | snapshot2 | SUCCESS | [...]                  |\n| where_my_snapshots_go | snapshot3 | SUCCESS | [...]                  |\n+-----------------------+-----------+---------+--------------------...-+\nSELECT 3 rows in set (... sec)\n\n\nTo restore a table from a snapshot we have to drop it beforehand:\n\ncr> DROP TABLE quotes;\nDROP OK, 1 row affected (... sec)\n\n\nRestoring a snapshot using the RESTORE SNAPSHOT statement.:\n\ncr> RESTORE SNAPSHOT where_my_snapshots_go.snapshot2\n... TABLE quotes\n... WITH (wait_for_completion=true);\nRESTORE OK, 1 row affected (... sec)\n\n\nIn this case only the quotes table from snapshot where_my_snapshots_go.snapshot2 is restored.\n\nIt’s not possible to restore tables that exist in the current cluster:\n\ncr> RESTORE SNAPSHOT where_my_snapshots_go.snapshot2 TABLE quotes;\nRelationAlreadyExists[Relation 'doc.quotes' already exists.]\n\n\nSingle partitions can be either imported into an existing partitioned table the partition belongs to.\n\nTo monitor the progress of RESTORE SNAPSHOT operations please query the sys.snapshot_restore table.\n\ncr> RESTORE SNAPSHOT where_my_snapshots_go.snapshot3 TABLE\n...    parted_table PARTITION (date='1970-01-01')\n... WITH (wait_for_completion=true);\nRESTORE OK, 1 row affected (... sec)\n\n\nOr if no matching partition table exists, it will be implicitly created during restore.\n\nCaution\n\nThis is only possible with CrateDB version 0.55.5 or greater!\n\nSnapshots of single partitions that have been created with earlier versions of CrateDB may be restored, but lead to orphaned partitions!\n\nWhen using CrateDB prior to 0.55.5 you will have to create the table schema first before restoring.\n\ncr> RESTORE SNAPSHOT where_my_snapshots_go.snapshot3 TABLE\n...    parted_table PARTITION (date=0)\n... WITH (wait_for_completion=true);\nRESTORE OK, 1 row affected (... sec)\n\nRestore data granularity\n\nYou are not limited to only being able to restore individual tables (or table partitions). For example:\n\nYou can use ALL instead of listing all tables to restore the whole snapshot, including all metadata and settings.\n\nYou can use TABLES to restore all tables but no metadata or settings. On the other hand, you can use METADATA to restore everything but tables.\n\nYou can use USERS to restore database users only. Or, you can use USERS, PRIVILIGES to restore both database users and privileges.\n\nSee the RESTORE SNAPSHOT documentation for all possible options.\n\nCleanup\nDropping snapshots\n\nDropping a snapshot deletes all files inside the repository that are only referenced by this snapshot. Due to its incremental nature this might be very few files (e.g. for intermediate snapshots). Snapshots are dropped using the DROP SNAPSHOT command:\n\ncr> DROP SNAPSHOT where_my_snapshots_go.snapshot3;\nDROP OK, 1 row affected (... sec)\n\nDropping repositories\n\nIf a repository is not needed anymore, it can be dropped using the DROP REPOSITORY statement:\n\ncr> DROP REPOSITORY \"OldRepository\";\nDROP OK, 1 row affected (... sec)\n\n\nThis statement, like CREATE REPOSITORY, does not manipulate repository contents but only deletes stored configuration for this repository in the cluster state, so it’s not accessible any more."
  },
  {
    "title": "Jobs management — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/admin/jobs-management.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n5.5\nJobs management\n\nEach executed SQL statement results in a corresponding job. Jobs that are currently executing are logged in the system table sys.jobs (see Jobs, operations, and logs).\n\nTo obtain the UUID of a job, stats needs to be enabled (see Collecting stats). Job logging can be disabled by setting the queue size to zero.\n\nKilling an active job forces CrateDB to stop its execution on the cluster immediately. There are two different SQL commands available for killing jobs.\n\nThe KILL ALL statement stops every single job on each node that is running. It returns the total number of contexts of all jobs that have been killed. A job can have contexts on multiple nodes.\n\ncr> kill all;\nKILL OK, ... rows affected (... sec)\n\n\nKILL job_id kills one single job with the specified job_id. Like KILL ALL it returns the total number of contexts of that job killed on all nodes.\n\ncr> kill '175011ce-9bbc-45f2-a86a-5b7f993a93a6';\nKILL OK, ... rows affected (... sec)\n\n\nSee KILL for detailed syntax information on KILL statements."
  },
  {
    "title": "Privileges — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/admin/privileges.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n5.5\nPrivileges\n\nTo execute statements, a user needs to have the required privileges.\n\nTable of contents\n\nIntroduction\n\nPrivilege Classes\n\nPrivilege types\n\nDQL\n\nDML\n\nDDL\n\nAL\n\nHierarchical inheritance of privileges\n\nBehavior of GRANT, DENY and REVOKE\n\nGRANT\n\nDENY\n\nREVOKE\n\nList privileges\n\nIntroduction\n\nCrateDB has a superuser (crate) which has the privilege to do anything. The privileges of other users have to be managed using the GRANT, DENY or REVOKE statements.\n\nThe privileges that can be granted, denied or revoked are:\n\nDQL\n\nDML\n\nDDL\n\nAL\n\nSkip to Privilege types for details.\n\nPrivilege Classes\n\nThe privileges can be granted on different classes:\n\nCLUSTER\n\nSCHEMA\n\nTABLE and VIEW\n\nSkip to Hierarchical inheritance of privileges for details.\n\nA user with AL on level CLUSTER can grant privileges they themselves have to other users as well.\n\nPrivilege types\nDQL\n\nGranting Data Query Language (DQL) privilege to a user, indicates that this user is allowed to execute SELECT, SHOW, REFRESH and COPY TO statements, as well as using the available user-defined functions, on the object for which the privilege applies.\n\nDML\n\nGranting Data Manipulation Language (DML) privilege to a user, indicates that this user is allowed to execute INSERT, COPY FROM, UPDATE and DELETE statements, on the object for which the privilege applies.\n\nDDL\n\nGranting Data Definition Language (DDL) privilege to a user, indicates that this user is allowed to execute the following statements on objects for which the privilege applies:\n\nCREATE TABLE\n\nDROP TABLE\n\nCREATE VIEW\n\nDROP VIEW\n\nCREATE FUNCTION\n\nDROP FUNCTION\n\nCREATE REPOSITORY\n\nDROP REPOSITORY\n\nCREATE SNAPSHOT\n\nDROP SNAPSHOT\n\nRESTORE SNAPSHOT\n\nALTER TABLE\n\nAL\n\nGranting Administration Language (AL) privilege to a user, enables the user to execute the following statements:\n\nCREATE USER\n\nDROP USER\n\nSET GLOBAL\n\nAll statements enabled via the AL privilege operate on a cluster level. So granting this on a schema or table level will have no effect.\n\nHierarchical inheritance of privileges\n\nPrivileges can be managed on three different levels, namely: CLUSTER, SCHEMA, and TABLE/VIEW.\n\nWhen a privilege is assigned on a certain level, the privilege will propagate down the hierarchy. Privileges defined on a lower level will always override those from a higher level:\n\n  cluster\n    ||\n  schema\n   /  \\\ntable view\n\n\nThis statement will grant DQL privilege to user riley on all the tables and functions of the doc schema:\n\ncr> GRANT DQL ON SCHEMA doc TO riley;\nGRANT OK, 1 row affected (... sec)\n\n\nThis statement will deny DQL privilege to user riley on the doc schema table doc.accounting. However, riley will still have DQL privilege on all the other tables of the doc schema:\n\ncr> DENY DQL ON TABLE doc.accounting TO riley;\nDENY OK, 1 row affected (... sec)\n\n\nNote\n\nIn CrateDB, schemas are just namespaces that are created and dropped implicitly. Therefore, when GRANT, DENY or REVOKE are invoked on a schema level, CrateDB takes the schema name provided without further validation.\n\nPrivileges can be managed on all schemas and tables of the cluster, except the information_schema.\n\nViews are on the same hierarchy with tables, i.e. a privilege on a view is gained through a GRANT on either the view itself, the schema the view belongs to, or a cluster-wide privilege. Privileges on relations which are referenced in the view do not grant any privileges on the view itself. On the contrary, even if the user does not have any privileges on a view’s referenced relations but on the view itself, the user can still access the relations through the view. For example:\n\ncr> CREATE VIEW first_customer as SELECT * from doc.accounting ORDER BY id LIMIT 1\nCREATE OK, 1 row affected (... sec)\n\n\nPreviously we had issued a DENY for user riley on doc.accounting but we can still access it through the view because we have access to it through the doc schema:\n\ncr> SELECT id from first_customer;\n+----+\n| id |\n+----+\n|  1 |\n+----+\nSELECT 1 row in set (... sec)\n\n\nSee Also\n\nViews: Privileges\n\nBehavior of GRANT, DENY and REVOKE\n\nNote\n\nYou can only grant, deny, or revoke privileges for an existing user. You must create a user and then configure privileges.\n\nGRANT\n\nTo grant a privilege to an existing user on the whole cluster, we use the GRANT SQL statement, for example:\n\ncr> GRANT DML TO wolfgang;\nGRANT OK, 1 row affected (... sec)\n\n\nDQL privilege can be granted on the sys schema to user wolfgang, like this:\n\ncr> GRANT DQL ON SCHEMA sys TO wolfgang;\nGRANT OK, 1 row affected (... sec)\n\n\nThe following statement will grant all privileges on table doc.books to user wolfgang:\n\ncr> GRANT ALL PRIVILEGES ON TABLE doc.books TO wolfgang;\nGRANT OK, 4 rows affected (... sec)\n\n\nUsing “ALL PRIVILEGES” is a shortcut to grant all the currently grantable privileges to a user.\n\nNote\n\nIf no schema is specified in the table ident, the table will be looked up in the current schema.\n\nIf a user with the username specified in the SQL statement does not exist the statement returns an error:\n\ncr> GRANT DQL TO layla;\nUserUnknownException[User 'layla' does not exist]\n\n\nTo grant ALL PRIVILEGES to user will on the cluster, we can use the following syntax:\n\ncr> GRANT ALL PRIVILEGES TO will;\nGRANT OK, 4 rows affected (... sec)\n\n\nUsing ALL PRIVILEGES is a shortcut to grant all the currently grantable privileges to a user, namely DQL, DML and DDL.\n\nPrivileges can be granted to multiple users in the same statement, like so:\n\ncr> GRANT DDL ON TABLE doc.books TO wolfgang, will;\nGRANT OK, 1 row affected (... sec)\n\nDENY\n\nTo deny a privilege to an existing user on the whole cluster, use the DENY SQL statement, for example:\n\ncr> DENY DDL TO will;\nDENY OK, 1 row affected (... sec)\n\n\nDQL privilege can be denied on the sys schema to user wolfgang like this:\n\ncr> DENY DQL ON SCHEMA sys TO wolfgang;\nDENY OK, 1 row affected (... sec)\n\n\nThe following statement will deny DQL privilege on table doc.books to user wolfgang:\n\ncr> DENY DQL ON TABLE doc.books TO wolfgang;\nDENY OK, 1 row affected (... sec)\n\n\nDENY ALL or DENY ALL PRIVILEGES will deny all privileges to a user, on the cluster it can be used like this:\n\ncr> DENY ALL TO will;\nDENY OK, 3 rows affected (... sec)\n\nREVOKE\n\nTo revoke a privilege that was previously granted or denied to a user use the REVOKE SQL statement, for example the DQL privilege that was previously denied to user wolfgang on the sys schema, can be revoked like this:\n\ncr> REVOKE DQL ON SCHEMA sys FROM wolfgang;\nREVOKE OK, 1 row affected (... sec)\n\n\nThe privileges that were granted and denied to user wolfgang on doc.books can be revoked like this:\n\ncr> REVOKE ALL ON TABLE doc.books FROM wolfgang;\nREVOKE OK, 4 rows affected (... sec)\n\n\nThe privileges that were granted to user will on the cluster can be revoked like this:\n\ncr> REVOKE ALL FROM will;\nREVOKE OK, 4 rows affected (... sec)\n\n\nNote\n\nThe REVOKE statement can remove only privileges that have been granted or denied through the GRANT or DENY statements. If the privilege on a specific object was not explicitly granted, the REVOKE statement has no effect. The effect of the REVOKE statement will be reflected in the row count.\n\nList privileges\n\nCrateDB exposes the privileges of users and roles of the database through the sys.privileges system table.\n\nBy querying the sys.privileges table you can get all information regarding the existing privileges. E.g.:\n\ncr> SELECT * FROM sys.privileges order by grantee, class, ident;\n+---------+----------+---------+----------------+-------+------+\n| class   | grantee  | grantor | ident          | state | type |\n+---------+----------+---------+----------------+-------+------+\n| SCHEMA  | riley    | crate   | doc            | GRANT | DQL  |\n| TABLE   | riley    | crate   | doc.accounting | DENY  | DQL  |\n| TABLE   | will     | crate   | doc.books      | GRANT | DDL  |\n| CLUSTER | wolfgang | crate   | NULL           | GRANT | DML  |\n+---------+----------+---------+----------------+-------+------+\nSELECT 4 rows in set (... sec)\n\n\nThe column grantor shows the user who granted or denied the privilege, the column grantee shows the user for whom the privilege was granted or denied. The column class identifies on which type of context the privilege applies. ident stands for the ident of the object that the privilege is set on and finally type stands for the type of privileges that was granted or denied."
  },
  {
    "title": "Optimization — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/admin/optimization.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n5.5\nOptimization\n\nTable of contents\n\nIntroduction\n\nMultiple table optimization\n\nPartition optimization\n\nIntroduction\n\nIn CrateDB every table (or if partitioned every partition) consists of segments. When inserting/deleting/updating data new segments are created following as an append-only strategy, which gives the advantage of fast writes but on the other hand can result into a big number of segments. As the number of segments increases the read operations become slower since more segments need to be visited. Moreover each segment consumes file handles, memory and CPU. CrateDB solves this problem by merging segments automatically in the background. Small segments are merged into bigger segments, which, in turn, are merged into even bigger segments. Furthermore any deleted rows and documents are not copied to the new bigger segment during this process.\n\nIf required one or more tables or table partitions can be optimized explicitly in order to improve performance. A few parameters can also be configured for the optimization process, like the max number of segments you wish to have when optimization is completed, or if you only wish to merge segments with deleted data, etc. See OPTIMIZE for detailed description of parameters.\n\ncr> OPTIMIZE table locations;\nOPTIMIZE OK, 1 row affected (... sec)\n\n\nNote\n\nSystem tables cannot be optimized.\n\nMultiple table optimization\n\nIf needed, multiple tables can be defined comma-separated in a single SQL request. The result message is printed if the request on every given table is completed.\n\ncr> OPTIMIZE TABLE locations, parted_table;\nOPTIMIZE OK, 2 rows affected (... sec)\n\n\nNote\n\nIf one or more tables or partitions do not exist, none of the given tables/partitions are optimized and an error is returned. The error returns only the first non-existent table/partition.\n\nPartition optimization\n\nAdditionally it is possible to define a specific PARTITION of a partitioned table which should be optimized (see Partitioned tables).\n\nBy using the PARTITION clause in the optimize statement a separate request for a given partition can be performed. That means that only specific partitions of a partitioned table are optimized. For further details on how to create an optimize request on partitioned tables see the SQL syntax and its synopsis (see OPTIMIZE).\n\ncr> OPTIMIZE TABLE parted_table PARTITION (day='2014-04-08');\nOPTIMIZE OK, 1 row affected (... sec)\n\n\nIn case the PARTITION clause is omitted all partitions will be optimized. If a table has many partitions this should be avoided due to performance reasons."
  },
  {
    "title": "JMX monitoring — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/admin/monitoring.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n5.5\nJMX monitoring\n\nThe JMX monitoring feature exposes query metrics via the JMX API.\n\nTable of contents\n\nSetup\n\nEnable collecting stats\n\nEnable the JMX API\n\nUsing Docker\n\nJMX Beans\n\nQueryStats MBean\n\nNodeStatus MBean\n\nNodeInfo MXBean\n\nConnections MBean\n\nThreadPools MXBean\n\nCircuitBreakers MXBean\n\nExposing JMX via HTTP\n\nSetup\nEnable collecting stats\n\nBy default, Collecting stats is enabled. You can disable collecting stats via the CrateDB configuration file or by running this statement:\n\ncr> SET GLOBAL \"stats.enabled\" = FALSE;\n\nEnable the JMX API\n\nTo monitor CrateDB using the JMX API, you must set the following system properties before you start CrateDB:\n\ncom.sun.management.jmxremote\ncom.sun.management.jmxremote.port=<JMX_PORT>\ncom.sun.management.jmxremote.ssl=false\ncom.sun.management.jmxremote.authenticate=false\n\n\nHere, <JMX_PORT> sets the port number of your JMX server. JMX SSL and authentication are currently not supported.\n\nMore information about the JMX monitoring properties can be found in the JMX documentation.\n\nYou can set the Java system properties with the -D option:\n\nsh$ ./bin/crate -Dcom.sun.management.jmxremote \\\n...             -Dcom.sun.management.jmxremote.port=7979 \\\n...             -Dcom.sun.management.jmxremote.ssl=false \\\n...             -Dcom.sun.management.jmxremote.authenticate=false\n\n\nHowever, the recommended way to set system properties is via the CRATE_JAVA_OPTS environment variable, like so:\n\nsh$ export CRATE_JAVA_OPTS=\"$CRATE_JAVA_OPTS \\\n      -Dcom.sun.management.jmxremote \\\n      -Dcom.sun.management.jmxremote.port=7979 \\\n      -Dcom.sun.management.jmxremote.ssl=false \\\n      -Dcom.sun.management.jmxremote.authenticate=false\"\nsh$ ./bin/crate\n\n\nIf you’re using the CrateDB Debian or RPM packages, you can set this environment variable via the /etc/default/crate configuration file.\n\nUsing Docker\n\nTo enable JMX monitoring when running CrateDB in a Docker container you have to set the following additional Java system properties:\n\n-Djava.rmi.server.hostname=<RMI_HOSTNAME>\n-Dcom.sun.management.jmxremote.rmi.port=<RMI_PORT>\n\n\nHere, <RMI_HOSTNAME> is the IP address or hostname of the Docker host and <RMI_PORT> is the statically assigned port of the RMI server. For convenience, <RMI_PORT> can be set to the same port the JMX server listens on.\n\nThe <RMI_HOSTNAME> and <RMI_PORT> can be used by JMX clients (e.g. JConsole or VisualVM) to connect to the JMX server.\n\nHere’s an example Docker command:\n\nsh> docker run -d --env CRATE_HEAP_SIZE=1g -e CRATE_JAVA_OPTS=\"\\\n      -Dcom.sun.management.jmxremote\n      -Dcom.sun.management.jmxremote.port=7979 \\\n      -Dcom.sun.management.jmxremote.ssl=false \\\n      -Dcom.sun.management.jmxremote.authenticate=false \\\n      -Dcom.sun.management.jmxremote.rmi.port=7979 \\\n      -Djava.rmi.server.hostname=<RMI_HOSTNAME>\" \\\n      -p 7979:7979 crate -Cnetwork.host=_site_\n\n\nHere, again, <RMI_HOSTNAME> is the IP address or hostname of the Docker host.\n\nJMX Beans\nQueryStats MBean\n\nThe QueryStats MBean exposes the sum of durations, in milliseconds, total and failed count of all statements executed since the node was started, grouped by type, for SELECT, UPDATE, DELETE, INSERT, MANAGEMENT, DDL, COPY and UNDEFINED queries.\n\nMetrics can be accessed using the JMX MBean object name io.crate.monitoring:type=QueryStats and the following attributes:\n\nStatements total count since the node was started:\n\nSelectQueryTotalCount\n\nInsertQueryTotalCount\n\nUpdateQueryTotalCount\n\nDeleteQueryTotalCount\n\nManagementQueryTotalCount\n\nDDLQueryTotalCount\n\nCopyQueryTotalCount\n\nUndefinedQueryTotalCount\n\nStatements failed count since the node was started:\n\nSelectQueryFailedCount\n\nInsertQueryFailedCount\n\nUpdateQueryFailedCount\n\nDeleteQueryFailedCount\n\nManagementQueryFailedCount\n\nDDLQueryFailedCount\n\nCopyQueryFailedCount\n\nUndefinedQueryFailedCount\n\nThe sum of the durations, in milliseconds, since the node was started, of all statement executions grouped by type:\n\nSelectQuerySumOfDurations\n\nInsertQuerySumOfDurations\n\nUpdateQuerySumOfDurations\n\nDeleteQuerySumOfDurations\n\nManagementQuerySumOfDurations\n\nDDLQuerySumOfDurations\n\nCopyQuerySumOfDurations\n\nUndefinedQuerySumOfDurations\n\nNodeStatus MBean\n\nThe NodeStatus JMX MBean exposes the status of the current node as boolean values.\n\nNodeStatus can be accessed using the JMX MBean object name io.crate.monitoring:type=NodeStatus and the following attributes:\n\nReady\n\nDefines if the node is able to process SQL statements.\n\nNodeInfo MXBean\n\nThe NodeInfo JMX MXBean exposes information about the current node.\n\nNodeInfo can be accessed using the JMX MXBean object name io.crate.monitoring:type=NodeInfo and the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nNodeId\n\n\t\n\nProvides the unique identifier of the node in the cluster.\n\n\n\n\nNodeName\n\n\t\n\nProvides the human friendly name of the node.\n\n\n\n\nClusterStateVersion\n\n\t\n\nProvides the version of the current applied cluster state.\n\n\n\n\nShardStats\n\n\t\n\nStatistics about the number of shards located on the node.\n\n\n\n\nShardInfo\n\n\t\n\nDetailed information about the shards located on the node.\n\nShardStats returns a CompositeData object containing statistics about the number of shards located on the node with the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nTotal\n\n\t\n\nThe number of shards located on the node.\n\n\n\n\nPrimaries\n\n\t\n\nThe number of primary shards located on the node.\n\n\n\n\nReplicas\n\n\t\n\nThe number of replica shards located on the node.\n\n\n\n\nUnassigned\n\n\t\n\nThe number of unassigned shards in the cluster. If the node is the elected master node in the cluster, this will show the total number of unassigned shards in the cluster, otherwise 0.\n\nShardInfo returns an Array of CompositeData objects containing detailed information about the shards located on the node with the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nId\n\n\t\n\nThe shard id. This shard id is managed by the system, ranging from 0 up to the number of configured shards of the table.\n\n\n\n\nTable\n\n\t\n\nThe name of the table this shard belongs to.\n\n\n\n\nPartitionIdent\n\n\t\n\nThe partition ident of a partitioned table. Empty for non-partitioned tables.\n\n\n\n\nRoutingState\n\n\t\n\nThe current state of the shard in the routing table. Possible states are:\n\nUNASSIGNED\n\nINITIALIZING\n\nSTARTED\n\nRELOCATING\n\n\n\n\nState\n\n\t\n\nThe current state of the shard. Possible states are:\n\nCREATED\n\nRECOVERING\n\nPOST_RECOVERY\n\nSTARTED\n\nRELOCATED\n\nCLOSED\n\nINITIALIZING\n\nUNASSIGNED\n\n\n\n\nSize\n\n\t\n\nThe estimated cumulated size in bytes of all files of this shard.\n\nConnections MBean\n\nThe Connections MBean exposes information about any open connections to a CrateDB node.\n\nIt can be accessed using the io.crate.monitoring:type=Connections object name and has the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nHttpOpen\n\n\t\n\nThe number of currently established connections via HTTP\n\n\n\n\nHttpTotal\n\n\t\n\nThe number of total connections established via HTTP over the life time of a node\n\n\n\n\nPsqlOpen\n\n\t\n\nThe number of currently established connections via the PostgreSQL protocol\n\n\n\n\nPsqlTotal\n\n\t\n\nThe number of total connections established via the PostgreSQL protocol over the life time of a node\n\n\n\n\nTransportOpen\n\n\t\n\nThe number of currently established connections via the transport protocol\n\nThreadPools MXBean\n\nThe ThreadPools MXBean exposes statistical information about the used thread pools of a CrateDB node.\n\nIt can be accessed using the io.crate.monitoring:type=ThreadPools object name and has following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nGeneric\n\n\t\n\nThread pool statistics of the generic thread pool.\n\n\n\n\nSearch\n\n\t\n\nThread pool statistics of the search thread pool used by read statements on user generated tables.\n\n\n\n\nWrite\n\n\t\n\nThread pool statistics of the write thread pool used for writing and deleting data.\n\n\n\n\nManagement\n\n\t\n\nThread pool statistics of the management thread pool used by management tasks like stats collecting, repository information, shard allocations, etc.\n\n\n\n\nFlush\n\n\t\n\nThread pool statistics of the flush thread pool used for fsyncing to disk and merging segments in the storage engine.\n\n\n\n\nRefresh\n\n\t\n\nThread pool statistics of the refresh thread pool used for automatic and on-demand refreshing of tables\n\n\n\n\nSnapshot\n\n\t\n\nThread pool statistics of the snapshot thread pool used for creating and restoring snapshots.\n\n\n\n\nForceMerge\n\n\t\n\nThread pool statistics of the force_merge thread pool used when running an optimize statement.\n\n\n\n\nListener\n\n\t\n\nThread pool statistics of the listener thread pool used on client nodes for asynchronous result listeners.\n\n\n\n\nGet\n\n\t\n\nThread pool statistics of the get thread pool used when querying sys.nodes or sys.shards.\n\n\n\n\nFetchShardStarted\n\n\t\n\nThread pool statistics of the fetch_shard_started thread pool used on shard allocation .\n\n\n\n\nFetchShardStore\n\n\t\n\nThread pool statistics of the fetch_shard_store used on shard replication.\n\nEach of them returns a CompositeData object containing detailed statistics of each thread pool with the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\npoolSize\n\n\t\n\nThe current number of threads in the pool.\n\n\n\n\nlargestPoolSize\n\n\t\n\nThe largest number of threads that have ever simultaneously been in the pool.\n\n\n\n\nqueueSize\n\n\t\n\nThe current number of tasks in the queue.\n\n\n\n\nactive\n\n\t\n\nThe approximate number of threads that are actively executing tasks.\n\n\n\n\ncompleted\n\n\t\n\nThe approximate total number of tasks that have completed execution.\n\n\n\n\nrejected\n\n\t\n\nThe number of rejected executions.\n\nCircuitBreakers MXBean\n\nThe CircuitBreaker MXBean exposes statistical information about all available circuit breakers of a CrateDB node.\n\nIt can be accessed using the io.crate.monitoring:type=CircuitBreakers object name and has following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nParent\n\n\t\n\nStatistics of the parent circuit breaker containing summarized counters across all circuit breakers.\n\n\n\n\nQuery\n\n\t\n\nStatistics of the query circuit breaker used to account memory usage of SQL execution including intermediate states e.g. on aggregation and resulting rows.\n\n\n\n\nJobsLog\n\n\t\n\nStatistics of the jobs_log circuit breaker used to account memory usage of the sys.jobs_log table.\n\n\n\n\nOperationsLog\n\n\t\n\nStatistics of the operations_log circuit breaker used to account memory usage of the sys.operations_log table.\n\n\n\n\nFieldData\n\n\t\n\nStatistics of the field_data circuit breaker used for estimating the amount of memory a field will require to be loaded into memory.\n\n\n\n\nInFlightRequests\n\n\t\n\nStatistics of the in_flight_requests circuit breaker used to account memory usage of all incoming requests on transport or HTTP level.\n\n\n\n\nRequest\n\n\t\n\nStatistics of the request circuit breaker used to account memory usage of per-request data structure.\n\nEach of them returns a CompositeData object containing detailed statistics of each circuit breaker with the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nname\n\n\t\n\nThe circuit breaker name this statistic belongs to.\n\n\n\n\nused\n\n\t\n\nThe currently accounted used memory estimations.\n\n\n\n\nlimit\n\n\t\n\nThe configured limit when to trip.\n\n\n\n\ntrippedCount\n\n\t\n\nThe total number of occurred trips.\n\nExposing JMX via HTTP\n\nThe JMX metrics and a readiness endpoint can be exposed via HTTP (e.g. to be used by Prometheus) by using the Crate JMX HTTP Exporter Java agent. See the README in the Crate JMX HTTP Exporter repository for more information."
  },
  {
    "title": "Secured communications (SSL/TLS) — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/admin/ssl.html",
    "html": "5.5\nSecured communications (SSL/TLS)\n\nYou can encrypt the internal communication between CrateDB nodes and the external communication with HTTP and PostgreSQL clients. When you configure encryption, CrateDB secures connections using Transport Layer Security (TLS).\n\nYou can enable SSL on a per-protocol basis:\n\nIf you enable SSL for HTTP, all connections will require HTTPS.\n\nBy default, if you enable SSL for the PostgreSQL wire protocol, clients can negotiate on a per-connection basis whether to use SSL. However, you can enforce SSL via Host-Based Authentication.\n\nIf you enable SSL for the CrateDB transport protocol (used for intra-node communication), nodes only accept SSL connections (ssl.transport.mode set to on).\n\nTip\n\nYou can use on SSL mode to configure a multi-zone cluster to ensure encryption for nodes communicating between zones. Please note, that SSL has to be on in all nodes as communication is point-2-point, and intra-zone communication will also be encrypted.\n\nTable of contents\n\nSSL/TLS configuration\n\nConfiguring the Keystore\n\nConfiguring a separate Truststore\n\nConnecting to a CrateDB node using HTTPS\n\nConnect to a CrateDB node using the Admin UI\n\nConnect to a CrateDB node using Crash\n\nConnect to a CrateDB node using REST\n\nConnecting to a CrateDB node using the PostgreSQL wire protocol with SSL/TLS\n\nConnect to a CrateDB node using JDBC\n\nConnect to a CrateDB node using psql\n\nSetting up a Keystore/Truststore with a certificate chain\n\nGenerate Keystore with a private key\n\nGenerate a certificate signing request\n\nOptional: Use a self-signed certificate to act as a Certificate Authority (CA)\n\nGenerate a self-signed certificate\n\nGenerate a signed cert\n\nImport the CA certificate into the Keystore\n\nImport CA into Truststore\n\nImport the signed certificate\n\nConfiguring CrateDB\n\nSSL/TLS configuration\n\nTo enable SSL a keystore and a few configuration changes are necessary. These changes need to be made in the crate.yml file on each node that should have secure communications enabled.\n\nSkip to Generate Keystore with a private key for a step-by-step instruction on how to create a keystore.\n\nOnce the keystore (and optional truststore) is created, continue with the following steps:\n\nSet ssl.psql.enabled or ssl.http.enabled to true.\n\nSet ssl.transport.mode to on.\n\nConfiguring the Keystore\n\n(Optional) Configuring a separate Truststore\n\nNote\n\nCrateDB monitors SSL files such as keystore and truststore that are configured as values of the node settings. If any of these files are updated CrateDB dynamically reloads them. The polling frequency of the files is set via the ssl.resource_poll_interval setting.\n\nConfiguring the Keystore\n\nSSL/TLS needs a keystore. The keystore holds the node certificate(s) which should be signed by a certificate authority (CA). A third-party CA or your organization’s existing CA can be used.\n\nWhen a client connects to a node using SSL/TLS, the client receives the certificate provided by the node and will determine if the node’s certificate is valid, trusted, and matches the hostname or IP address it is trying to connect to.\n\nCaution\n\nTechnically, it’s possible to disable CA checks for certificates on the client. It is strongly recommended however to use certificates signed by an official CA or by a private CA (company PKI) that is also known to the client. This will help to ensure that establishing trust is as painless as possible.\n\nSee Generate Keystore with a private key for information about how to create a keystore.\n\nOnce the keystore is prepared, define the absolute file path to the keystore .jks file on the node using ssl.keystore_filepath setting.\n\nNote\n\nMake sure that the keystore file has the right permissions and is accessible by the system user crate.\n\nAlso, define the password needed to decrypt the keystore by using the ssl.keystore_password setting.\n\nUse ssl.keystore_key_password setting to define the key password used when creating the Keystore.\n\nFor a full list of the settings needed to configure SSL/TLS, refer to SSL configuration reference.\n\nConfiguring a separate Truststore\n\nTrusted CA certificates can be stored in a node’s keystore or a separate truststore can be used to store them.\n\nIf you want to use a separate truststore, create a node truststore and import the CA certificate(s) you want to trust. Once the truststore is prepared, define the absolute file path of the truststore .jks file on the node using the ssl.truststore_filepath setting.\n\nNote\n\nMake sure that the truststore file has the right permissions and is accessible by the system user crate.\n\nAlso define the password needed to decrypt the keystore by using the ssl.truststore_password setting.\n\nFor a full list of the settings needed to configure SSL/TLS, refer to SSL configuration reference.\n\nConnecting to a CrateDB node using HTTPS\nConnect to a CrateDB node using the Admin UI\n\nCrate’s HTTP endpoint remains unchanged. When you have turned on secure communication, it will use HTTPS instead of plain HTTP. Simply point your browser to the same URL you used before but changing the protocol to HTTPS:\n\nFor example, https://localhost:4200 becomes https://localhost:4200. If you have not configured the CrateDB node’s keystore with a signed certificate from a Certificate Authority (CA), then you will get something like the following: NET::ERR_CERT_AUTHORITY_INVALID. You either need to get your certificate signed from one of the CAs included in your browser or import your owned certificates into the browser. A third option is storing an exception for the CrateDB node certification in your browser after verifying that this is indeed a certificate you trust.\n\nConnect to a CrateDB node using Crash\n\nYou can connect to a CrateDB node using a secure communication:\n\ncrash --hosts https://localhost:4200\n\n\nTo validate the provided certificates, please see the options --verify-ssl and --key-file.\n\nConnect to a CrateDB node using REST\n\nIssue your REST requests to the node using the https:// protocol. You may have to configure your client to validate the received certificate accordingly.\n\nConnecting to a CrateDB node using the PostgreSQL wire protocol with SSL/TLS\nConnect to a CrateDB node using JDBC\n\nJDBC needs to validate the CrateDB node’s identity by checking that the node certificate is signed by a trusted authority. If the certificate is signed by a certificate authority (CA) that is known to the Java runtime, there is nothing further to do (as Java comes with copies of the most common CA’s certificates).\n\nIf you have a certificate that is signed by a CA not known to the Java runtime, you need to configure a truststore which contains the node’s certificate and provide the path to the truststore file along with the password when starting your Java application:\n\njava -Djavax.net.ssl.trustStore=mystore -Djavax.net.ssl.trustStorePassword=mypassword com.mycompany.MyApp\n\n\nIn case you face any issues extra debugging information is available by adding -Djavax.net.debug=ssl to your command line.\n\nLast but not least, the connection parameter ssl=true must be added to the connection URL so that the JDBC driver will try and establish an SSL connection.\n\nFor further information, visit JDBC SSL documentation.\n\nConnect to a CrateDB node using psql\n\nBy default, psql attempts to use SSL if available on the node. For further information including the different SSL modes please visit the PSQL documentation.\n\nSetting up a Keystore/Truststore with a certificate chain\n\nIn case you need to setup a Keystore or a Truststore, here are the commands to get you started. All the commands use a validity of 36500 days (about 100 years). You might want to use less.\n\nGenerate Keystore with a private key\n\nThe first step is to create a Keystore with a private key using the RSA algorithm. The “first and last name” is the common name (CN) which should overlap with the URL the service it is used with.\n\nCommand:\n\nkeytool -keystore keystore -genkey -keyalg RSA -alias server -validity 36500\n\n\nOutput:\n\nEnter keystore password:\nRe-enter new password:\nWhat is your first and last name?\n  [Unknown]:  ssl.crate.io\nWhat is the name of your organizational unit?\n  [Unknown]:  Cryptography Department\nWhat is the name of your organization?\n  [Unknown]:  Crate.io GmbH\nWhat is the name of your City or Locality?\n  [Unknown]:  Berlin\nWhat is the name of your State or Province?\n  [Unknown]:  Berlin\nWhat is the two-letter country code for this unit?\n  [Unknown]:  DE\nIs CN=ssl.crate.io, OU=Cryptography Department, O=Crate.io GmbH, L=Berlin, ST=Berlin, C=DE correct?\n  [no]:  yes\n\nEnter key password for <server>\n    (RETURN if same as keystore password):\nRe-enter new password:\n\nGenerate a certificate signing request\n\nTo establish trust for this key, we need to sign it. This is done by generating a certificate signing request.\n\nIf you have access to a certificate authority (CA), you can skip the next steps and get the signed certificate from the CA using the signing request which we will generate with the command below. If you don’t have access to a CA, then follow the optional steps after this step to establish your own CA.\n\nCommand:\n\nkeytool -keystore keystore -certreq -alias server -keyalg RSA -file server.csr\n\n\nOutput:\n\nEnter keystore password:\nEnter key password for <server>\n\nOptional: Use a self-signed certificate to act as a Certificate Authority (CA)\n\nNote\n\nOnly follow these optional steps if you want to create your own Certificate Authority (CA). Otherwise, please request a signed certificate from one of the CAs bundled with Java.\n\nGenerate a self-signed certificate\n\nIf you don’t get your certificate signed from one of the official CAs, you might want to create your own CA with a self-signed certificate. The common name (CN) should overlap with the CN of the server key generated in the first step. For example, ssl.crate.io overlaps with *.crate.io.\n\nNote\n\nIn this step by step guide it is shown how to create a server certificate. If you want to create a client certificate the steps are almost the same with the exception of providing a common name that is equivalent to the crate username as described in client certificate authentication method.\n\nCommand:\n\nopenssl req -x509 -sha256 -nodes -days 36500 -newkey rsa:2048 \\\n    -keyout rootCA.key -out rootCA.crt\n\n\nOutput:\n\nGenerating a 2048 bit RSA private key\n.......................................................................+++\n.............................................................+++\nwriting new private key to 'rootCA.key'\n-----\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n-----\nCountry Name (2 letter code) [AU]:AT\nState or Province Name (full name) [Some-State]:Vorarlberg\nLocality Name (eg, city) []:Dornbirn\nOrganization Name (eg, company) [Internet Widgits Pty Ltd]:Crate.io\nOrganizational Unit Name (eg, section) []:Cryptography Department\nCommon Name (e.g. server FQDN or YOUR name) []:*.crate.io\nEmail Address []:info@crate.io\n\nGenerate a signed cert\n\nIn order that the server can prove itself to have a valid and trusted domain it is required that the server certificate contains subjectAltName.\n\nCreate a file called ssl.ext with the following content. In section [alt_names] list valid domain names of the server:\n\nauthorityKeyIdentifier=keyid,issuer\nbasicConstraints=CA:FALSE\nkeyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment\nsubjectAltName = @alt_names\n\n[alt_names]\nDNS.1 = www.example.com\n\n\nNow you can generate a signed cert from our certificate signing request.\n\nCommand:\n\nopenssl x509 -req -in server.csr -CA rootCA.crt -CAkey rootCA.key \\\n    -CAcreateserial -out server.crt -sha256 -days 36500 -extfile ssl.ext\n\n\nOutput:\n\nSignature ok\nsubject=/C=DE/ST=Berlin/L=Berlin/O=Crate.io GmbH/OU=Cryptography Department/CN=ssl.crate.io\nGetting CA Private Key\n\nImport the CA certificate into the Keystore\n\nThe CA needs to be imported to the Keystore for the certificate chain to be available when we import our signed certificate.\n\nCommand:\n\nkeytool -import -keystore keystore -file rootCA.crt -alias theCARoot\n\n\nOutput:\n\nEnter keystore password:\nOwner: EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT\nIssuer: EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT\nSerial number: f13562ec6184401e\nValid from: Mon Jun 12 13:09:17 CEST 2017 until: Wed May 19 13:09:17 CEST 2117\nCertificate fingerprints:\n     MD5:  BB:A1:79:53:FE:71:EC:61:2A:19:81:E8:0E:E8:C9:81\n     SHA1: 96:66:C1:01:49:17:D1:19:FB:DB:83:86:50:3D:3D:AD:DA:F7:C6:A9\n     SHA256: 69:82:C5:24:9A:A1:AE:DF:80:29:7A:26:92:C1:A5:9F:AF:7D:03:56:CC:C3:E9:73:3B:FD:85:66:35:D6:8A:9B\n     Signature algorithm name: SHA256withRSA\n     Version: 3\n\nExtensions:\n\n#1: ObjectId: 2.5.29.35 Criticality=false\nAuthorityKeyIdentifier [\nKeyIdentifier [\n0000: CD 29 4E 07 3D C3 7C D0   16 45 FB 0A CE 8D B4 98  .)N.=....E......\n0010: B7 A8 4C 79                                        ..Ly\n]\n[EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT]\nSerialNumber: [    f13562ec 6184401e]\n]\n\n#2: ObjectId: 2.5.29.19 Criticality=false\nBasicConstraints:[\n  CA:true\n  PathLen:2147483647\n]\n\n#3: ObjectId: 2.5.29.14 Criticality=false\nSubjectKeyIdentifier [\nKeyIdentifier [\n0000: CD 29 4E 07 3D C3 7C D0   16 45 FB 0A CE 8D B4 98  .)N.=....E......\n0010: B7 A8 4C 79                                        ..Ly\n]\n]\n\nTrust this certificate? [no]:  yes\nCertificate was added to keystore\n\nImport CA into Truststore\n\nIf we are using our own CA, we should also import the certificate to the Truststore, such that it is available for clients which want to verify signatures.\n\nCommand:\n\nkeytool -import -keystore truststore -file rootCA.crt -alias theCARoot\n\n\nOutput:\n\nEnter keystore password:\nRe-enter new password:\nOwner: EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT\nIssuer: EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT\nSerial number: f13562ec6184401e\nValid from: Mon Jun 12 13:09:17 CEST 2017 until: Wed May 19 13:09:17 CEST 2117\nCertificate fingerprints:\n     MD5:  BB:A1:79:53:FE:71:EC:61:2A:19:81:E8:0E:E8:C9:81\n     SHA1: 96:66:C1:01:49:17:D1:19:FB:DB:83:86:50:3D:3D:AD:DA:F7:C6:A9\n     SHA256: 69:82:C5:24:9A:A1:AE:DF:80:29:7A:26:92:C1:A5:9F:AF:7D:03:56:CC:C3:E9:73:3B:FD:85:66:35:D6:8A:9B\n     Signature algorithm name: SHA256withRSA\n     Version: 3\n\nExtensions:\n\n#1: ObjectId: 2.5.29.35 Criticality=false\nAuthorityKeyIdentifier [\nKeyIdentifier [\n0000: CD 29 4E 07 3D C3 7C D0   16 45 FB 0A CE 8D B4 98  .)N.=....E......\n0010: B7 A8 4C 79                                        ..Ly\n]\n[EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT]\nSerialNumber: [    f13562ec 6184401e]\n]\n\n#2: ObjectId: 2.5.29.19 Criticality=false\nBasicConstraints:[\n  CA:true\n  PathLen:2147483647\n]\n\n#3: ObjectId: 2.5.29.14 Criticality=false\nSubjectKeyIdentifier [\nKeyIdentifier [\n0000: CD 29 4E 07 3D C3 7C D0   16 45 FB 0A CE 8D B4 98  .)N.=....E......\n0010: B7 A8 4C 79                                        ..Ly\n]\n]\n\nTrust this certificate? [no]:  yes\nCertificate was added to keystore\n\nImport the signed certificate\n\nNow we have a signed certificate, signed by either from a official CA or from our own CA. Let’s import it to the Keystore.\n\nCommand:\n\nkeytool -import -keystore keystore -file server.crt -alias server\n\n\nOutput:\n\nEnter keystore password:\nEnter key password for <server>\nCertificate reply was installed in keystore\n\nConfiguring CrateDB\n\nFinally, you want to supply the Keystore/Truststore in the CrateDB configuration, see Secured communications (SSL/TLS)."
  },
  {
    "title": "Authentication — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/admin/auth/index.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n5.5\nAuthentication\n\nTable of contents\n\nAuthentication Methods\nTrust method\nPassword authentication method\nClient certificate authentication method\nHost-Based Authentication (HBA)\nAuthentication against CrateDB\nAuthenticating as a superuser\nAuthenticating to Admin UI\nNode-to-node communication"
  },
  {
    "title": "Information schema — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/general/information-schema.html",
    "html": "5.5\nInformation schema\n\ninformation_schema is a special schema that contains virtual tables which are read-only and can be queried to get information about the state of the cluster.\n\nTable of contents\n\nAccess\n\nVirtual tables\n\ntables\n\nsettings\n\nviews\n\ncolumns\n\ntable_constraints\n\nkey_column_usage\n\ntable_partitions\n\nroutines\n\nschemata\n\nsql_features\n\ncharacter_sets\n\nAccess\n\nWhen the user management is enabled, accessing the information_schema is open to all users and it does not require any privileges.\n\nHowever, being able to query information_schema tables will not allow the user to retrieve all the rows in the table, as it can contain information related to tables over which the connected user does not have any privileges. The only rows that will be returned will be the ones the user is allowed to access.\n\nFor example, if the user john has any privilege on the doc.books table but no privilege at all on doc.locations, when john issues a SELECT * FROM information_schema.tables statement, the tables information related to the doc.locations table will not be returned.\n\nVirtual tables\ntables\n\nThe information_schema.tables virtual table can be queried to get a list of all available tables and views and their settings, such as number of shards or number of replicas.\n\ncr> SELECT table_schema, table_name, table_type, number_of_shards, number_of_replicas\n... FROM information_schema.tables\n... ORDER BY table_schema ASC, table_name ASC;\n+--------------------+-------------------------+------------+------------------+--------------------+\n| table_schema       | table_name              | table_type | number_of_shards | number_of_replicas |\n+--------------------+-------------------------+------------+------------------+--------------------+\n| doc                | galaxies                | VIEW       |             NULL | NULL               |\n| doc                | locations               | BASE TABLE |                2 | 0                  |\n| doc                | partitioned_table       | BASE TABLE |                4 | 0-1                |\n| doc                | quotes                  | BASE TABLE |                2 | 0                  |\n| information_schema | character_sets          | BASE TABLE |             NULL | NULL               |\n| information_schema | columns                 | BASE TABLE |             NULL | NULL               |\n| information_schema | key_column_usage        | BASE TABLE |             NULL | NULL               |\n| information_schema | referential_constraints | BASE TABLE |             NULL | NULL               |\n| information_schema | routines                | BASE TABLE |             NULL | NULL               |\n| information_schema | schemata                | BASE TABLE |             NULL | NULL               |\n| information_schema | sql_features            | BASE TABLE |             NULL | NULL               |\n| information_schema | table_constraints       | BASE TABLE |             NULL | NULL               |\n| information_schema | table_partitions        | BASE TABLE |             NULL | NULL               |\n| information_schema | tables                  | BASE TABLE |             NULL | NULL               |\n| information_schema | views                   | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_am                   | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_attrdef              | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_attribute            | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_class                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_constraint           | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_cursors              | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_database             | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_description          | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_enum                 | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_event_trigger        | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_index                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_indexes              | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_locks                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_namespace            | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_proc                 | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_publication          | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_publication_tables   | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_range                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_roles                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_settings             | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_shdescription        | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_stats                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_subscription         | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_subscription_rel     | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_tables               | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_tablespace           | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_type                 | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_views                | BASE TABLE |             NULL | NULL               |\n| sys                | allocations             | BASE TABLE |             NULL | NULL               |\n| sys                | checks                  | BASE TABLE |             NULL | NULL               |\n| sys                | cluster                 | BASE TABLE |             NULL | NULL               |\n| sys                | health                  | BASE TABLE |             NULL | NULL               |\n| sys                | jobs                    | BASE TABLE |             NULL | NULL               |\n| sys                | jobs_log                | BASE TABLE |             NULL | NULL               |\n| sys                | jobs_metrics            | BASE TABLE |             NULL | NULL               |\n| sys                | node_checks             | BASE TABLE |             NULL | NULL               |\n| sys                | nodes                   | BASE TABLE |             NULL | NULL               |\n| sys                | operations              | BASE TABLE |             NULL | NULL               |\n| sys                | operations_log          | BASE TABLE |             NULL | NULL               |\n| sys                | privileges              | BASE TABLE |             NULL | NULL               |\n| sys                | repositories            | BASE TABLE |             NULL | NULL               |\n| sys                | segments                | BASE TABLE |             NULL | NULL               |\n| sys                | shards                  | BASE TABLE |             NULL | NULL               |\n| sys                | snapshot_restore        | BASE TABLE |             NULL | NULL               |\n| sys                | snapshots               | BASE TABLE |             NULL | NULL               |\n| sys                | summits                 | BASE TABLE |             NULL | NULL               |\n| sys                | users                   | BASE TABLE |             NULL | NULL               |\n+--------------------+-------------------------+------------+------------------+--------------------+\nSELECT 62 rows in set (... sec)\n\n\nThe table also contains additional information such as the specified routing column and partition columns:\n\ncr> SELECT table_name, clustered_by, partitioned_by\n... FROM information_schema.tables\n... WHERE table_schema = 'doc'\n... ORDER BY table_schema ASC, table_name ASC;\n+-------------------+--------------+----------------+\n| table_name        | clustered_by | partitioned_by |\n+-------------------+--------------+----------------+\n| galaxies          | NULL         | NULL           |\n| locations         | id           | NULL           |\n| partitioned_table | _id          | [\"date\"]       |\n| quotes            | id           | NULL           |\n+-------------------+--------------+----------------+\nSELECT 4 rows in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nDescription\n\n\t\n\nData Type\n\n\n\n\nblobs_path\n\n\t\n\nThe data path of the blob table\n\n\t\n\nTEXT\n\n\n\n\nclosed\n\n\t\n\nThe state of the table\n\n\t\n\nBOOLEAN\n\n\n\n\nclustered_by\n\n\t\n\nThe routing column used to cluster the table\n\n\t\n\nTEXT\n\n\n\n\ncolumn_policy\n\n\t\n\nDefines whether the table uses a STRICT or a DYNAMIC Column policy\n\n\t\n\nTEXT\n\n\n\n\nnumber_of_replicas\n\n\t\n\nThe number of replicas the table currently has\n\n\t\n\nINTEGER\n\n\n\n\nnumber_of_shards\n\n\t\n\nThe number of shards the table is currently distributed across\n\n\t\n\nINTEGER\n\n\n\n\npartitioned_by\n\n\t\n\nThe partition columns (used to partition the table)\n\n\t\n\nTEXT\n\n\n\n\nreference_generation\n\n\t\n\nSpecifies how values in the self-referencing column are generated\n\n\t\n\nTEXT\n\n\n\n\nrouting_hash_function\n\n\t\n\nThe name of the hash function used for internal routing\n\n\t\n\nTEXT\n\n\n\n\nself_referencing_column_name\n\n\t\n\nThe name of the column that uniquely identifies each row (always _id)\n\n\t\n\nTEXT\n\n\n\n\nsettings\n\n\t\n\nWITH\n\n\t\n\nOBJECT\n\n\n\n\ntable_catalog\n\n\t\n\nRefers to the table_schema\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nThe name of the table\n\n\t\n\nTEXT\n\n\n\n\ntable_schema\n\n\t\n\nThe name of the schema the table belongs to\n\n\t\n\nTEXT\n\n\n\n\ntable_type\n\n\t\n\nThe type of the table (BASE TABLE for tables, VIEW for views)\n\n\t\n\nTEXT\n\n\n\n\nversion\n\n\t\n\nA collection of version numbers relevant to the table\n\n\t\n\nOBJECT\n\nsettings\n\nTable settings specify configuration parameters for tables. Some settings can be set during Cluster runtime and others are only applied on cluster restart.\n\nThis list of table settings in WITH shows detailed information of each parameter.\n\nTable parameters can be applied with CREATE TABLE on creation of a table. With ALTER TABLE they can be set on already existing tables.\n\nThe following statement creates a new table and sets the refresh interval of shards to 500 ms and sets the shard allocation for primary shards only:\n\ncr> create table parameterized_table (id integer, content text)\n... with (\"refresh_interval\"=500, \"routing.allocation.enable\"='primaries');\nCREATE OK, 1 row affected (... sec)\n\n\nThe settings can be verified by querying information_schema.tables:\n\ncr> select settings['routing']['allocation']['enable'] as alloc_enable,\n...   settings['refresh_interval'] as refresh_interval\n... from information_schema.tables\n... where table_name='parameterized_table';\n+--------------+------------------+\n| alloc_enable | refresh_interval |\n+--------------+------------------+\n| primaries    |              500 |\n+--------------+------------------+\nSELECT 1 row in set (... sec)\n\n\nOn existing tables this needs to be done with ALTER TABLE statement:\n\ncr> alter table parameterized_table\n... set (\"routing.allocation.enable\"='none');\nALTER OK, -1 rows affected (... sec)\n\nviews\n\nThe table information_schema.views contains the name, definition and options of all available views.\n\ncr> SELECT table_schema, table_name, view_definition\n... FROM information_schema.views\n... ORDER BY table_schema ASC, table_name ASC;\n+--------------+------------+-------------------------+\n| table_schema | table_name | view_definition         |\n+--------------+------------+-------------------------+\n| doc          | galaxies   | SELECT                  |\n|              |            |   \"id\"                  |\n|              |            | , \"name\"                |\n|              |            | , \"description\"         |\n|              |            | FROM \"locations\"        |\n|              |            | WHERE \"kind\" = 'Galaxy' |\n+--------------+------------+-------------------------+\nSELECT 1 row in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nDescription\n\n\t\n\nData Type\n\n\n\n\ntable_catalog\n\n\t\n\nThe catalog of the table of the view (refers to table_schema)\n\n\t\n\nTEXT\n\n\n\n\ntable_schema\n\n\t\n\nThe schema of the table of the view\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nThe name of the table of the view\n\n\t\n\nTEXT\n\n\n\n\nview_definition\n\n\t\n\nThe SELECT statement that defines the view\n\n\t\n\nTEXT\n\n\n\n\ncheck_option\n\n\t\n\nNot applicable for CrateDB, always return NONE\n\n\t\n\nTEXT\n\n\n\n\nis_updatable\n\n\t\n\nWhether the view is updatable. Not applicable for CrateDB, always returns FALSE\n\n\t\n\nBOOLEAN\n\n\n\n\nowner\n\n\t\n\nThe user that created the view\n\n\t\n\nTEXT\n\nNote\n\nIf you drop the table of a view, the view will still exist and show up in the information_schema.tables and information_schema.views tables.\n\ncolumns\n\nThis table can be queried to get a list of all available columns of all tables and views and their definition like data type and ordinal position inside the table:\n\ncr> select table_name, column_name, ordinal_position as pos, data_type\n... from information_schema.columns\n... where table_schema = 'doc' and table_name not like 'my_table%'\n... order by table_name asc, column_name asc;\n+-------------------+--------------------------------+-----+--------------------------+\n| table_name        | column_name                    | pos | data_type                |\n+-------------------+--------------------------------+-----+--------------------------+\n| locations         | date                           |   3 | timestamp with time zone |\n| locations         | description                    |   6 | text                     |\n| locations         | id                             |   1 | integer                  |\n| locations         | information                    |  11 | object_array             |\n| locations         | information['evolution_level'] |  13 | smallint                 |\n| locations         | information['population']      |  12 | bigint                   |\n| locations         | inhabitants                    |   7 | object                   |\n| locations         | inhabitants['description']     |   9 | text                     |\n| locations         | inhabitants['interests']       |   8 | text_array               |\n| locations         | inhabitants['name']            |  10 | text                     |\n| locations         | kind                           |   4 | text                     |\n| locations         | landmarks                      |  14 | text_array               |\n| locations         | name                           |   2 | text                     |\n| locations         | position                       |   5 | integer                  |\n| partitioned_table | date                           |   3 | timestamp with time zone |\n| partitioned_table | id                             |   1 | bigint                   |\n| partitioned_table | title                          |   2 | text                     |\n| quotes            | id                             |   1 | integer                  |\n| quotes            | quote                          |   2 | text                     |\n+-------------------+--------------------------------+-----+--------------------------+\nSELECT 19 rows in set (... sec)\n\n\nYou can even query this table’s own columns (attention: this might lead to infinite recursion of your mind, beware!):\n\ncr> select column_name, data_type, ordinal_position\n... from information_schema.columns\n... where table_schema = 'information_schema'\n... and table_name = 'columns' order by column_name asc;\n+--------------------------+------------+------------------+\n| column_name              | data_type  | ordinal_position |\n+--------------------------+------------+------------------+\n| character_maximum_length | integer    |                1 |\n| character_octet_length   | integer    |                2 |\n| character_set_catalog    | text       |                3 |\n| character_set_name       | text       |                4 |\n| character_set_schema     | text       |                5 |\n| check_action             | integer    |                6 |\n| check_references         | text       |                7 |\n| collation_catalog        | text       |                8 |\n| collation_name           | text       |                9 |\n| collation_schema         | text       |               10 |\n| column_default           | text       |               11 |\n| column_details           | object     |               12 |\n| column_details['name']   | text       |               13 |\n| column_details['path']   | text_array |               14 |\n| column_name              | text       |               15 |\n| data_type                | text       |               16 |\n| datetime_precision       | integer    |               17 |\n| domain_catalog           | text       |               18 |\n| domain_name              | text       |               19 |\n| domain_schema            | text       |               20 |\n| generation_expression    | text       |               21 |\n| identity_cycle           | boolean    |               22 |\n| identity_generation      | text       |               23 |\n| identity_increment       | text       |               24 |\n| identity_maximum         | text       |               25 |\n| identity_minimum         | text       |               26 |\n| identity_start           | text       |               27 |\n| interval_precision       | integer    |               28 |\n| interval_type            | text       |               29 |\n| is_generated             | text       |               30 |\n| is_identity              | boolean    |               31 |\n| is_nullable              | boolean    |               32 |\n| numeric_precision        | integer    |               33 |\n| numeric_precision_radix  | integer    |               34 |\n| numeric_scale            | integer    |               35 |\n| ordinal_position         | integer    |               36 |\n| table_catalog            | text       |               37 |\n| table_name               | text       |               38 |\n| table_schema             | text       |               39 |\n| udt_catalog              | text       |               40 |\n| udt_name                 | text       |               41 |\n| udt_schema               | text       |               42 |\n+--------------------------+------------+------------------+\nSELECT 42 rows in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nDescription\n\n\t\n\nData Type\n\n\n\n\ntable_catalog\n\n\t\n\nRefers to the table_schema\n\n\t\n\nTEXT\n\n\n\n\ntable_schema\n\n\t\n\nSchema name containing the table\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nTable Name\n\n\t\n\nTEXT\n\n\n\n\ncolumn_name\n\n\t\n\nColumn Name For fields in object columns this is not an identifier but a path and therefore must not be double quoted when programmatically obtained.\n\n\t\n\nTEXT\n\n\n\n\nordinal_position\n\n\t\n\nThe position of the column within the table\n\n\t\n\nINTEGER\n\n\n\n\nis_nullable\n\n\t\n\nWhether the column is nullable\n\n\t\n\nBOOLEAN\n\n\n\n\ndata_type\n\n\t\n\nThe data type of the column\n\nFor further information see Data types\n\n\t\n\nTEXT\n\n\n\n\ncolumn_default\n\n\t\n\nThe default expression of the column\n\n\t\n\nTEXT\n\n\n\n\ncharacter_maximum_length\n\n\t\n\nIf the data type is a character type then return the declared length limit; otherwise NULL.\n\n\t\n\nINTEGER\n\n\n\n\ncharacter_octet_length\n\n\t\n\nNot implemented (always returns NULL)\n\nPlease refer to TEXT type\n\n\t\n\nINTEGER\n\n\n\n\nnumeric_precision\n\n\t\n\nIndicates the number of significant digits for a numeric data_type. For all other data types this column is NULL.\n\n\t\n\nINTEGER\n\n\n\n\nnumeric_precision_radix\n\n\t\n\nIndicates in which base the value in the column numeric_precision for a numeric data_type is exposed. This can either be 2 (binary) or 10 (decimal). For all other data types this column is NULL.\n\n\t\n\nINTEGER\n\n\n\n\nnumeric_scale\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nINTEGER\n\n\n\n\ndatetime_precision\n\n\t\n\nContains the fractional seconds precision for a timestamp data_type. For all other data types this column is null.\n\n\t\n\nINTEGER\n\n\n\n\ninterval_type\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ninterval_precision\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nINTEGER\n\n\n\n\ncharacter_set_catalog\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncharacter_set_schema\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncharacter_set_name\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncollation_catalog\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncollation_schema\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncollation_name\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ndomain_catalog\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ndomain_schema\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ndomain_name\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nudt_catalog\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nudt_schema\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nudt_name\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncheck_references\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncheck_action\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nINTEGER\n\n\n\n\ngeneration_expression\n\n\t\n\nThe expression used to generate ad column. If the column is not generated NULL is returned.\n\n\t\n\nTEXT\n\n\n\n\nis_generated\n\n\t\n\nReturns ALWAYS or NEVER wether the column is generated or not.\n\n\t\n\nTEXT\n\n\n\n\nis_identity\n\n\t\n\nNot implemented (always returns false)\n\n\t\n\nBOOLEAN\n\n\n\n\nidentity_cycle\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nBOOLEAN\n\n\n\n\nidentity_generation\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nidentity_increment\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nidentity_maximum\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nidentity_minimum\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nidentity_start\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\ntable_constraints\n\nThis table can be queried to get a list of all defined table constraints, their type, name and which table they are defined in.\n\nNote\n\nCurrently only PRIMARY_KEY constraints are supported.\n\ncr> select table_schema, table_name, constraint_name, constraint_type as type\n... from information_schema.table_constraints\n... where table_name = 'tables'\n...   or table_name = 'quotes'\n...   or table_name = 'documents'\n...   or table_name = 'tbl'\n... order by table_schema desc, table_name asc limit 10;\n+--------------------+------------+------------------------+-------------+\n| table_schema       | table_name | constraint_name        | type        |\n+--------------------+------------+------------------------+-------------+\n| information_schema | tables     | tables_pk              | PRIMARY KEY |\n| doc                | quotes     | quotes_pk              | PRIMARY KEY |\n| doc                | quotes     | doc_quotes_id_not_null | CHECK       |\n| doc                | tbl        | doc_tbl_col_not_null   | CHECK       |\n+--------------------+------------+------------------------+-------------+\nSELECT 4 rows in set (... sec)\n\nkey_column_usage\n\nThis table may be queried to retrieve primary key information from all user tables:\n\ncr> select constraint_name, table_name, column_name, ordinal_position\n... from information_schema.key_column_usage\n... where table_name = 'students'\n+-----------------+------------+-------------+------------------+\n| constraint_name | table_name | column_name | ordinal_position |\n+-----------------+------------+-------------+------------------+\n| students_pk     | students   | id          |                1 |\n| students_pk     | students   | department  |                2 |\n+-----------------+------------+-------------+------------------+\nSELECT 2 rows in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nDescription\n\n\t\n\nData Type\n\n\n\n\nconstraint_catalog\n\n\t\n\nRefers to table_catalog\n\n\t\n\nTEXT\n\n\n\n\nconstraint_schema\n\n\t\n\nRefers to table_schema\n\n\t\n\nTEXT\n\n\n\n\nconstraint_name\n\n\t\n\nName of the constraint\n\n\t\n\nTEXT\n\n\n\n\ntable_catalog\n\n\t\n\nRefers to table_schema\n\n\t\n\nTEXT\n\n\n\n\ntable_schema\n\n\t\n\nName of the schema that contains the table that contains the constraint\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nName of the table that contains the constraint\n\n\t\n\nTEXT\n\n\n\n\ncolumn_name\n\n\t\n\nName of the column that contains the constraint\n\n\t\n\nTEXT\n\n\n\n\nordinal_position\n\n\t\n\nPosition of the column within the constraint (starts with 1)\n\n\t\n\nINTEGER\n\ntable_partitions\n\nThis table can be queried to get information about all partitioned tables, Each partition of a table is represented as one row. The row contains the information table name, schema name, partition ident, and the values of the partition. values is a key-value object with the partition column (or columns) as key(s) and the corresponding value as value(s).\n\ncr> insert into a_partitioned_table (id, content) values (1, 'content_a');\nINSERT OK, 1 row affected (... sec)\n\ncr> alter table a_partitioned_table set (number_of_shards=5);\nALTER OK, -1 rows affected (... sec)\n\ncr> insert into a_partitioned_table (id, content) values (2, 'content_b');\nINSERT OK, 1 row affected (... sec)\n\n\nThe following example shows a table where the column content of table a_partitioned_table has been used to partition the table. The table has two partitions. The partitions are introduced when data is inserted where content is content_a, and content_b.:\n\ncr> select table_name, table_schema as schema, partition_ident, \"values\"\n... from information_schema.table_partitions\n... order by table_name, partition_ident;\n+---------------------+--------+--------------------+--------------------------+\n| table_name          | schema | partition_ident    | values                   |\n+---------------------+--------+--------------------+--------------------------+\n| a_partitioned_table | doc    | 04566rreehimst2vc4 | {\"content\": \"content_a\"} |\n| a_partitioned_table | doc    | 04566rreehimst2vc8 | {\"content\": \"content_b\"} |\n+---------------------+--------+--------------------+--------------------------+\nSELECT 2 rows in set (... sec)\n\n\nThe second partition has been created after the number of shards for future partitions have been changed on the partitioned table, so they show 5 instead of 4:\n\ncr> select table_name, partition_ident,\n... number_of_shards, number_of_replicas\n... from information_schema.table_partitions\n... order by table_name, partition_ident;\n+---------------------+--------------------+------------------+--------------------+\n| table_name          | partition_ident    | number_of_shards | number_of_replicas |\n+---------------------+--------------------+------------------+--------------------+\n| a_partitioned_table | 04566rreehimst2vc4 |                4 | 0-1                |\n| a_partitioned_table | 04566rreehimst2vc8 |                5 | 0-1                |\n+---------------------+--------------------+------------------+--------------------+\nSELECT 2 rows in set (... sec)\n\nroutines\n\nThe routines table contains tokenizers, token-filters, char-filters, custom analyzers created by CREATE ANALYZER statements (see Creating a custom analyzer), and functions created by CREATE FUNCTION statements:\n\ncr> select routine_name, routine_type\n... from information_schema.routines\n... group by routine_name, routine_type\n... order by routine_name asc limit 5;\n+----------------------+--------------+\n| routine_name         | routine_type |\n+----------------------+--------------+\n| PathHierarchy        | TOKENIZER    |\n| apostrophe           | TOKEN_FILTER |\n| arabic               | ANALYZER     |\n| arabic_normalization | TOKEN_FILTER |\n| arabic_stem          | TOKEN_FILTER |\n+----------------------+--------------+\nSELECT 5 rows in set (... sec)\n\n\nFor example you can use this table to list existing tokenizers like this:\n\ncr> select routine_name\n... from information_schema.routines\n... where routine_type='TOKENIZER'\n... order by routine_name asc limit 10;\n+----------------+\n| routine_name   |\n+----------------+\n| PathHierarchy  |\n| char_group     |\n| classic        |\n| edge_ngram     |\n| keyword        |\n| letter         |\n| lowercase      |\n| ngram          |\n| path_hierarchy |\n| pattern        |\n+----------------+\nSELECT 10 rows in set (... sec)\n\n\nOr get an overview of how many routines and routine types are available:\n\ncr> select count(*), routine_type\n... from information_schema.routines\n... group by routine_type\n... order by routine_type;\n+----------+--------------+\n| count(*) | routine_type |\n+----------+--------------+\n|       45 | ANALYZER     |\n|        3 | CHAR_FILTER  |\n|       16 | TOKENIZER    |\n|       61 | TOKEN_FILTER |\n+----------+--------------+\nSELECT 4 rows in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nData Type\n\n\n\n\nroutine_name\n\n\t\n\nTEXT\n\n\n\n\nroutine_type\n\n\t\n\nTEXT\n\n\n\n\nroutine_body\n\n\t\n\nTEXT\n\n\n\n\nroutine_schema\n\n\t\n\nTEXT\n\n\n\n\ndata_type\n\n\t\n\nTEXT\n\n\n\n\nis_deterministic\n\n\t\n\nBOOLEAN\n\n\n\n\nroutine_definition\n\n\t\n\nTEXT\n\n\n\n\nspecific_name\n\n\t\n\nTEXT\n\nroutine_name\n\nName of the routine (might be duplicated in case of overloading)\n\nroutine_type\n\nType of the routine. Can be FUNCTION, ANALYZER, CHAR_FILTER, TOKEN_FILTER or TOKEN_FILTER.\n\nroutine_schema\n\nThe schema where the routine was defined. If it doesn’t apply, then NULL.\n\nroutine_body\n\nThe language used for the routine implementation. If it doesn’t apply, then NULL.\n\ndata_type\n\nThe return type of the function. If it doesn’t apply, then NULL.\n\nis_deterministic\n\nIf the routine is deterministic then True, else False (NULL if it doesn’t apply).\n\nroutine_definition\n\nThe function definition (NULL if it doesn’t apply).\n\nspecific_name\n\nUsed to uniquely identify the function in a schema, even if the function is overloaded. Currently the specific name contains the types of the function arguments. As the format might change in the future, it should be only used to compare it to other instances of specific_name.\n\nschemata\n\nThe schemata table lists all existing schemas. The blob, information_schema, and sys schemas are always available. The doc schema is available after the first user table is created.\n\ncr> select schema_name from information_schema.schemata order by schema_name;\n+--------------------+\n| schema_name        |\n+--------------------+\n| blob               |\n| doc                |\n| information_schema |\n| pg_catalog         |\n| sys                |\n+--------------------+\nSELECT 5 rows in set (... sec)\n\nsql_features\n\nThe sql_features table outlines supported and unsupported SQL features of CrateDB based to the current SQL standard (see SQL standard compliance):\n\ncr> select feature_name, is_supported, sub_feature_id, sub_feature_name\n... from information_schema.sql_features\n... where feature_id='F501';\n+--------------------------------+--------------+----------------+--------------------+\n| feature_name                   | is_supported | sub_feature_id | sub_feature_name   |\n+--------------------------------+--------------+----------------+--------------------+\n| Features and conformance views | FALSE        |                |                    |\n| Features and conformance views | TRUE         | 1              | SQL_FEATURES view  |\n| Features and conformance views | FALSE        | 2              | SQL_SIZING view    |\n| Features and conformance views | FALSE        | 3              | SQL_LANGUAGES view |\n+--------------------------------+--------------+----------------+--------------------+\nSELECT 4 rows in set (... sec)\n\n\nName\n\n\t\n\nData Type\n\n\t\n\nNullable\n\n\n\n\nfeature_id\n\n\t\n\nTEXT\n\n\t\n\nNO\n\n\n\n\nfeature_name\n\n\t\n\nTEXT\n\n\t\n\nNO\n\n\n\n\nsub_feature_id\n\n\t\n\nTEXT\n\n\t\n\nNO\n\n\n\n\nsub_feature_name\n\n\t\n\nTEXT\n\n\t\n\nNO\n\n\n\n\nis_supported\n\n\t\n\nTEXT\n\n\t\n\nNO\n\n\n\n\nis_verified_by\n\n\t\n\nTEXT\n\n\t\n\nYES\n\n\n\n\ncomments\n\n\t\n\nTEXT\n\n\t\n\nYES\n\nfeature_id\n\nIdentifier of the feature\n\nfeature_name\n\nDescriptive name of the feature by the Standard\n\nsub_feature_id\n\nIdentifier of the sub feature; If it has zero-length, this is a feature\n\nsub_feature_name\n\nDescriptive name of the sub feature by the Standard; If it has zero-length, this is a feature\n\nis_supported\n\nYES if the feature is fully supported by the current version of CrateDB, NO if not\n\nis_verified_by\n\nIdentifies the conformance test used to verify the claim;\n\nAlways NULL since the CrateDB development group does not perform formal testing of feature conformance\n\ncomments\n\nEither NULL or shows a comment about the supported status of the feature\n\ncharacter_sets\n\nThe character_sets table identifies the character sets available in the current database.\n\nIn CrateDB there is always a single entry listing UTF8:\n\ncr> SELECT character_set_name, character_repertoire FROM information_schema.character_sets;\n+--------------------+----------------------+\n| character_set_name | character_repertoire |\n+--------------------+----------------------+\n| UTF8               | UCS                  |\n+--------------------+----------------------+\nSELECT 1 row in set (... sec)\n\n\nColumn Name\n\n\t\n\nReturn Type\n\n\t\n\nDescription\n\n\n\n\ncharacter_set_catalog\n\n\t\n\nTEXT\n\n\t\n\nNot implemented, this column is always null.\n\n\n\n\ncharacter_set_schema\n\n\t\n\nTEXT\n\n\t\n\nNot implemented, this column is always null.\n\n\n\n\ncharacter_set_name\n\n\t\n\nTEXT\n\n\t\n\nName of the character set\n\n\n\n\ncharacter_repertoire\n\n\t\n\nTEXT\n\n\t\n\nCharacter repertoire\n\n\n\n\nform_of_use\n\n\t\n\nTEXT\n\n\t\n\nCharacter encoding form, same as character_set_name\n\n\n\n\ndefault_collate_catalog\n\n\t\n\nTEXT\n\n\t\n\nName of the database containing the default collation (Always crate)\n\n\n\n\ndefault_collate_schema\n\n\t\n\nTEXT\n\n\t\n\nName of the schema containing the default collation (Always NULL)\n\n\n\n\ndefault_collate_name\n\n\t\n\nTEXT\n\n\t\n\nName of the default collation (Always NULL)"
  },
  {
    "title": "User management — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/admin/user-management.html",
    "html": "5.5\nUser management\n\nUser account information is stored in the cluster metadata of CrateDB and supports the following statements to create, alter and drop users:\n\nCREATE USER\n\nALTER USER\n\nDROP USER\n\nThese statements are database management statements that can be invoked by superusers that already exist in the CrateDB cluster. The CREATE USER and DROP USER statements can also be invoked by users with the AL privilege.\n\nWhen CrateDB is started, the cluster contains one predefined superuser. This user is called crate. It is not possible to create any other superusers\n\nThe definition of all users, including hashes of their passwords, is backed up together with the cluster’s metadata when a snapshot is created, and it is restored when using the ALL, METADATA, or USERS keywords with the RESTORE SNAPSHOT command.\n\nTable of contents\n\nCREATE USER\n\nALTER USER\n\nDROP USER\n\nList users\n\nCREATE USER\n\nTo create a new user for the CrateDB database cluster use the CREATE USER SQL statement:\n\ncr> CREATE USER user_a;\nCREATE OK, 1 row affected (... sec)\n\n\nTip\n\nNewly created users do not have any privileges. After creating a user, you should configure user privileges.\n\nFor example, to grant all privileges to the user_a user, run:\n\ncr> GRANT ALL PRIVILEGES TO user_a;\nGRANT OK, 4 rows affected (... sec)\n\n\nIt can be used to connect to the database cluster using available authentication methods. You can specify the user’s password in the WITH clause of the CREATE statement. This is required if you want to use the Password authentication method:\n\ncr> CREATE USER user_b WITH (password = 'a_secret_password');\nCREATE OK, 1 row affected (... sec)\n\n\nThe username parameter of the statement follows the principles of an identifier which means that it must be double-quoted if it contains special characters (e.g. whitespace) or if the case needs to be maintained:\n\ncr> CREATE USER \"Custom User\";\nCREATE OK, 1 row affected (... sec)\n\n\nIf a user with the username specified in the SQL statement already exists the statement returns an error:\n\ncr> CREATE USER \"Custom User\";\nUserAlreadyExistsException[User 'Custom User' already exists]\n\nALTER USER\n\nTo alter the password for an existing user from the CrateDB database cluster use the ALTER USER SQL statement:\n\ncr> ALTER USER user_a SET (password = 'pass');\nALTER OK, 1 row affected (... sec)\n\n\nThe password can be reset (cleared) if specified as NULL:\n\ncr> ALTER USER user_a SET (password = NULL);\nALTER OK, 1 row affected (... sec)\n\n\nNote\n\nThe built-in superuser crate has no password and it is not possible to set a new password for this user.\n\nDROP USER\n\nTo remove an existing user from the CrateDB database cluster use the DROP USER SQL statement:\n\ncr> DROP USER user_c;\nDROP OK, 1 row affected (... sec)\n\n\nIf a user with the username specified in the SQL statement does not exist the statement returns an error:\n\ncr> DROP USER user_c;\nUserUnknownException[User 'user_c' does not exist]\n\n\nNote\n\nIt is not possible to drop the built-in superuser crate.\n\nList users\n\nCrateDB exposes database users via the read-only Users system table. The sys.users table shows all users in the cluster which can be used for authentication. The initial superuser crate which is available for all CrateDB clusters is also part of that list.\n\nTo list all existing users query that table:\n\ncr> SELECT * FROM sys.users order by name;\n+-------------+----------+-----------+\n| name        | password | superuser |\n+-------------+----------+-----------+\n| Custom User |     NULL | FALSE     |\n| crate       |     NULL | TRUE      |\n| user_a      |     NULL | FALSE     |\n| user_b      | ******** | FALSE     |\n+-------------+----------+-----------+\nSELECT 4 rows in set (... sec)\n\n\nThe column name shows the unique name of the user, the column superuser shows whether the user has superuser privileges or not.\n\nNote\n\nCrateDB also supports retrieving the current connected user using the system information functions: CURRENT_USER, USER and SESSION_USER."
  },
  {
    "title": "Runtime configuration — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/admin/runtime-config.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n5.5\nRuntime configuration\n\nThe CrateDB cluster can be configured at runtime using the SET and RESET statement. See the Cluster Settings configuration section for details about the supported settings.\n\nIf SET is used with PERSISTENT the change will survive a cluster restart, if used with TRANSIENT the value will reset to the default value or to the value in the configuration file on a restart.\n\ncr> SET GLOBAL PERSISTENT stats.enabled = false;\nSET OK, 1 row affected (... sec)\n\ncr> select sys.cluster.settings['stats']['enabled'] from sys.cluster;\n+------------------------------+\n| settings['stats']['enabled'] |\n+------------------------------+\n| FALSE                        |\n+------------------------------+\nSELECT 1 row in set (... sec)\n\n\nYou can change multiple values at once:\n\ncr> SET GLOBAL TRANSIENT stats.enabled = true,\n... stats.jobs_log_size = 1024, stats.operations_log_size = 4096;\nSET OK, 1 row affected (... sec)\n\ncr> select settings['stats']['enabled'],\n...   settings['stats']['jobs_log_size'],\n...   settings['stats']['operations_log_size']\n... from sys.cluster;\n+-...------------+-...------------------+-...------------------------+\n| ...['enabled'] | ...['jobs_log_size'] | ...['operations_log_size'] |\n+-...------------+-...------------------+-...------------------------+\n| TRUE           |                 1024 |                       4096 |\n+-...------------+-...------------------+-...------------------------+\nSELECT 1 row in set (... sec)\n\n\nIts also possible to save a complete nested object of settings:\n\ncr> SET GLOBAL TRANSIENT stats = {\n...   jobs_log_size = 2048,\n...   operations_log_size = 8192\n... };\nSET OK, 1 row affected (... sec)\n\ncr> SELECT\n...   settings['stats']['jobs_log_size'] AS jobs_size,\n...   settings['stats']['operations_log_size'] AS op_size\n... FROM sys.cluster;\n+-----------+---------+\n| jobs_size | op_size |\n+-----------+---------+\n|      2048 |    8192 |\n+-----------+---------+\nSELECT 1 row in set (... sec)\n\n\nUsing the RESET statement, a setting will be reset to either on node startup defined configuration file value or to its default value:\n\ncr> RESET GLOBAL stats.enabled, stats.operations_log_size;\nRESET OK, 1 row affected (... sec)\n\ncr> SELECT\n...   settings['stats']['jobs_log_size'] AS jobs_size,\n...   settings['stats']['operations_log_size'] AS op_size\n... FROM sys.cluster;\n+-----------+---------+\n| jobs_size | op_size |\n+-----------+---------+\n|      2048 |   10000 |\n+-----------+---------+\nSELECT 1 row in set (... sec)\n\n\nRESET can also be done on objects:\n\ncr> RESET GLOBAL stats;\nRESET OK, 1 row affected (... sec)\n\ncr> SELECT\n...   settings['stats']['jobs_log_size'] AS jobs_size,\n...   settings['stats']['operations_log_size'] AS op_size\n... FROM sys.cluster;\n+-----------+---------+\n| jobs_size | op_size |\n+-----------+---------+\n|     10000 |   10000 |\n+-----------+---------+\nSELECT 1 row in set (... sec)\n"
  },
  {
    "title": "System information — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/admin/system-information.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n5.5\nSystem information\n\nCrateDB provides the sys schema which contains virtual tables. These tables are read-only and can be queried to get statistical real-time information about the cluster, its nodes and their shards:\n\nTable of contents\n\nCluster\n\nCluster license\n\nlicense\n\nCluster settings\n\nNodes\n\nid\n\nname\n\nhostname\n\nrest_url\n\nattributes\n\nport\n\nload\n\nmem\n\nheap\n\nversion\n\ncluster_state_version\n\nfs\n\nthread_pools\n\nos\n\ncgroup limitations\n\nUptime limitations\n\nos_info\n\nnetwork\n\nconnections\n\nprocess\n\nNode checks\n\nAcknowledge failed checks\n\nDescription of checked node settings\n\nRecovery expected data nodes\n\nRecovery after data nodes\n\nRecovery after time\n\nRouting allocation disk watermark high\n\nRouting allocation disk watermark low\n\nMaximum shards per node\n\nShards\n\nTable schema\n\nExample\n\nSegments\n\nJobs, operations, and logs\n\nJobs\n\nTable schema\n\nJobs metrics\n\nsys.jobs_metrics Table schema\n\nClassification\n\nOperations\n\nTable schema\n\nLogs\n\nsys.jobs_log Table schema\n\nsys.operations_log Table schema\n\nCluster checks\n\nCurrent Checks\n\nNumber of partitions\n\nTables need to be recreated\n\nCrateDB table version compatibility scheme\n\nAvoiding reindex using partitioned tables\n\nHow to reindex\n\nLicense check\n\nHealth\n\nHealth definition\n\nRepositories\n\nSnapshots\n\nSnapshot Restore\n\nSummits\n\nUsers\n\nPrivileges\n\nAllocations\n\nShard table permissions\n\nsys jobs tables permissions\n\npg_stats\n\npg_publication\n\npg_publication_tables\n\npg_subscription\n\npg_subscription_rel\n\nCluster\n\nBasic information about the CrateDB cluster can be retrieved from the sys.cluster table:\n\nName\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nA unique ID generated by the system.\n\n\t\n\nTEXT\n\n\n\n\nlicense\n\n\t\n\nThe current CrateDB license information. Always NULL. This exists for backward compatibility\n\n\t\n\nOBJECT\n\n\n\n\nname\n\n\t\n\nThe cluster name.\n\n\t\n\nTEXT\n\n\n\n\nmaster_node\n\n\t\n\nNode ID of the node which currently operates as master\n\n\t\n\nTEXT\n\n\n\n\nsettings\n\n\t\n\nThe cluster settings.\n\n\t\n\nOBJECT\n\nThe result has at most 1 row:\n\ncr> select name from sys.cluster;\n+-----------------+\n| name            |\n+-----------------+\n| Testing-CrateDB |\n+-----------------+\nSELECT 1 row in set (... sec)\n\nCluster license\n\nThe sys.cluster.license expression returns information about the currently registered license.\n\nNote\n\nLicenses were removed in CrateDB 4.5. Accordingly, these values are deprecated and return NULL in CrateDB 4.5 and higher.\n\nlicense\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nlicense\n\n\t\nThe current CrateDB license information\n\nor NULL on CrateDB CE.\n\n\t\n\nOBJECT\n\n\n\n\nlicense['expiry_date']\n\n\t\n\nThe Dates and times on which the license expires.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nlicense['issued_to']\n\n\t\n\nThe organisation for which the license is issued.\n\n\t\n\nTEXT\n\n\n\n\nlicense['max_nodes']\n\n\t\n\nThe maximum number of nodes the license is valid for.\n\n\t\n\nINTEGER\n\nCluster settings\n\nThe sys.cluster.settings expression returns information about the currently applied cluster settings.\n\ncr> select settings from sys.cluster;\n+-----------------------------------------------------...-+\n| settings                                                |\n+-----------------------------------------------------...-+\n| {\"bulk\": {...}, \"cluster\": {...}, \"gateway\": {...}, ... |\n+-----------------------------------------------------...-+\nSELECT 1 row in set (... sec)\n\ncr> select column_name, data_type from information_schema.columns\n... where column_name like 'settings%'\n... and table_name = 'cluster';\n+-----------------------------------------------------------------------------------+--------------+\n| column_name                                                                       | data_type    |\n+-----------------------------------------------------------------------------------+--------------+\n| settings                                                                          | object       |\n| settings['bulk']                                                                  | object       |\n| settings['bulk']['request_timeout']                                               | text         |\n| settings['cluster']                                                               | object       |\n| settings['cluster']['graceful_stop']                                              | object       |\n| settings['cluster']['graceful_stop']['force']                                     | boolean      |\n| settings['cluster']['graceful_stop']['min_availability']                          | text         |\n| settings['cluster']['graceful_stop']['timeout']                                   | text         |\n| settings['cluster']['info']                                                       | object       |\n| settings['cluster']['info']['update']                                             | object       |\n| settings['cluster']['info']['update']['interval']                                 | text         |\n| settings['cluster']['max_shards_per_node']                                        | integer      |\n| settings['cluster']['routing']                                                    | object       |\n| settings['cluster']['routing']['allocation']                                      | object       |\n| settings['cluster']['routing']['allocation']['allow_rebalance']                   | text         |\n| settings['cluster']['routing']['allocation']['balance']                           | object       |\n| settings['cluster']['routing']['allocation']['balance']['index']                  | real         |\n| settings['cluster']['routing']['allocation']['balance']['shard']                  | real         |\n| settings['cluster']['routing']['allocation']['balance']['threshold']              | real         |\n| settings['cluster']['routing']['allocation']['cluster_concurrent_rebalance']      | integer      |\n| settings['cluster']['routing']['allocation']['disk']                              | object       |\n| settings['cluster']['routing']['allocation']['disk']['threshold_enabled']         | boolean      |\n| settings['cluster']['routing']['allocation']['disk']['watermark']                 | object       |\n| settings['cluster']['routing']['allocation']['disk']['watermark']['flood_stage']  | text         |\n| settings['cluster']['routing']['allocation']['disk']['watermark']['high']         | text         |\n| settings['cluster']['routing']['allocation']['disk']['watermark']['low']          | text         |\n| settings['cluster']['routing']['allocation']['enable']                            | text         |\n| settings['cluster']['routing']['allocation']['exclude']                           | object       |\n| settings['cluster']['routing']['allocation']['exclude']['_host']                  | text         |\n| settings['cluster']['routing']['allocation']['exclude']['_id']                    | text         |\n| settings['cluster']['routing']['allocation']['exclude']['_ip']                    | text         |\n| settings['cluster']['routing']['allocation']['exclude']['_name']                  | text         |\n| settings['cluster']['routing']['allocation']['include']                           | object       |\n| settings['cluster']['routing']['allocation']['include']['_host']                  | text         |\n| settings['cluster']['routing']['allocation']['include']['_id']                    | text         |\n| settings['cluster']['routing']['allocation']['include']['_ip']                    | text         |\n| settings['cluster']['routing']['allocation']['include']['_name']                  | text         |\n| settings['cluster']['routing']['allocation']['node_concurrent_recoveries']        | integer      |\n| settings['cluster']['routing']['allocation']['node_initial_primaries_recoveries'] | integer      |\n| settings['cluster']['routing']['allocation']['require']                           | object       |\n| settings['cluster']['routing']['allocation']['require']['_host']                  | text         |\n| settings['cluster']['routing']['allocation']['require']['_id']                    | text         |\n| settings['cluster']['routing']['allocation']['require']['_ip']                    | text         |\n| settings['cluster']['routing']['allocation']['require']['_name']                  | text         |\n| settings['cluster']['routing']['allocation']['total_shards_per_node']             | integer      |\n| settings['cluster']['routing']['rebalance']                                       | object       |\n| settings['cluster']['routing']['rebalance']['enable']                             | text         |\n| settings['gateway']                                                               | object       |\n| settings['gateway']['expected_data_nodes']                                        | integer      |\n| settings['gateway']['expected_nodes']                                             | integer      |\n| settings['gateway']['recover_after_data_nodes']                                   | integer      |\n| settings['gateway']['recover_after_nodes']                                        | integer      |\n| settings['gateway']['recover_after_time']                                         | text         |\n| settings['indices']                                                               | object       |\n| settings['indices']['breaker']                                                    | object       |\n| settings['indices']['breaker']['query']                                           | object       |\n| settings['indices']['breaker']['query']['limit']                                  | text         |\n| settings['indices']['breaker']['request']                                         | object       |\n| settings['indices']['breaker']['request']['limit']                                | text         |\n| settings['indices']['breaker']['total']                                           | object       |\n| settings['indices']['breaker']['total']['limit']                                  | text         |\n| settings['indices']['recovery']                                                   | object       |\n| settings['indices']['recovery']['internal_action_long_timeout']                   | text         |\n| settings['indices']['recovery']['internal_action_timeout']                        | text         |\n| settings['indices']['recovery']['max_bytes_per_sec']                              | text         |\n| settings['indices']['recovery']['recovery_activity_timeout']                      | text         |\n| settings['indices']['recovery']['retry_delay_network']                            | text         |\n| settings['indices']['recovery']['retry_delay_state_sync']                         | text         |\n| settings['indices']['replication']                                                | object       |\n| settings['indices']['replication']['retry_timeout']                               | text         |\n| settings['logger']                                                                | object_array |\n| settings['logger']['level']                                                       | text_array   |\n| settings['logger']['name']                                                        | text_array   |\n| settings['memory']                                                                | object       |\n| settings['memory']['allocation']                                                  | object       |\n| settings['memory']['allocation']['type']                                          | text         |\n| settings['memory']['operation_limit']                                             | integer      |\n| settings['overload_protection']                                                   | object       |\n| settings['overload_protection']['dml']                                            | object       |\n| settings['overload_protection']['dml']['initial_concurrency']                     | integer      |\n| settings['overload_protection']['dml']['max_concurrency']                         | integer      |\n| settings['overload_protection']['dml']['min_concurrency']                         | integer      |\n| settings['overload_protection']['dml']['queue_size']                              | integer      |\n| settings['replication']                                                           | object       |\n| settings['replication']['logical']                                                | object       |\n| settings['replication']['logical']['ops_batch_size']                              | integer      |\n| settings['replication']['logical']['reads_poll_duration']                         | text         |\n| settings['replication']['logical']['recovery']                                    | object       |\n| settings['replication']['logical']['recovery']['chunk_size']                      | text         |\n| settings['replication']['logical']['recovery']['max_concurrent_file_chunks']      | integer      |\n| settings['statement_timeout']                                                     | text         |\n| settings['stats']                                                                 | object       |\n| settings['stats']['breaker']                                                      | object       |\n| settings['stats']['breaker']['log']                                               | object       |\n| settings['stats']['breaker']['log']['jobs']                                       | object       |\n| settings['stats']['breaker']['log']['jobs']['limit']                              | text         |\n| settings['stats']['breaker']['log']['operations']                                 | object       |\n| settings['stats']['breaker']['log']['operations']['limit']                        | text         |\n| settings['stats']['enabled']                                                      | boolean      |\n| settings['stats']['jobs_log_expiration']                                          | text         |\n| settings['stats']['jobs_log_filter']                                              | text         |\n| settings['stats']['jobs_log_persistent_filter']                                   | text         |\n| settings['stats']['jobs_log_size']                                                | integer      |\n| settings['stats']['operations_log_expiration']                                    | text         |\n| settings['stats']['operations_log_size']                                          | integer      |\n| settings['stats']['service']                                                      | object       |\n| settings['stats']['service']['interval']                                          | text         |\n| settings['stats']['service']['max_bytes_per_sec']                                 | text         |\n| settings['udc']                                                                   | object       |\n| settings['udc']['enabled']                                                        | boolean      |\n| settings['udc']['initial_delay']                                                  | text         |\n| settings['udc']['interval']                                                       | text         |\n| settings['udc']['url']                                                            | text         |\n+-----------------------------------------------------------------------------------+--------------+\nSELECT ... rows in set (... sec)\n\n\nFor further details, see the Cluster Settings configuration section.\n\nNodes\n\nTo get information about the nodes query for sys.nodes.\n\nThis table can be queried for one, multiple or all nodes within a cluster.\n\nThe table schema is as follows:\n\nid\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nA unique ID within the cluster generated by the system.\n\n\t\n\nTEXT\n\nname\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nname\n\n\t\n\nThe node name within a cluster. The system will choose a random name. You can also customize the node name, see Node-specific settings.\n\n\t\n\nTEXT\n\nhostname\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nhostname\n\n\t\n\nThe specified host name of the machine the node is running on.\n\n\t\n\nTEXT\n\nrest_url\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nrest_url\n\n\t\n\nFull HTTP(s) address where the REST API of the node is exposed, including schema, hostname (or IP) and port.\n\n\t\n\nTEXT\n\nattributes\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nattributes\n\n\t\n\nThe custom attributes set for the node, e.g. if node.attr.color is blue, and node.attr.location is east`, the value of this column would be: ``{color=blue, location=east}\n\n\t\n\nOBJECT\n\nport\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nport\n\n\t\n\nThe specified ports for both HTTP and binary transport interfaces. You can also customize the ports setting, see Ports.\n\n\t\n\nOBJECT\n\n\n\n\nport['http']\n\n\t\n\nCrateDB’s HTTP port.\n\n\t\n\nINTEGER\n\n\n\n\nport['transport']\n\n\t\n\nCrateDB’s binary transport port.\n\n\t\n\nINTEGER\n\n\n\n\nport['psql']\n\n\t\n\nThe PostgreSQL wire protocol port.\n\n\t\n\nINTEGER\n\nload\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nload\n\n\t\n\nSystem load statistics\n\n\t\n\nOBJECT\n\n\n\n\nload['1']\n\n\t\n\nAverage load over the last 1 minute.\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\nload['5']\n\n\t\n\nAverage load over the last 5 minutes.\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\nload['15']\n\n\t\n\nAverage load over the last 15 minutes.\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\nload['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the load probe.\n\n\t\n\nBIGINT\n\nmem\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nmem\n\n\t\n\nMemory utilization statistics of the host.\n\n\t\n\nOBJECT\n\n\n\n\nmem['used']\n\n\t\n\nCurrently used memory in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nmem['used_percent']\n\n\t\n\nCurrently used memory in percent of total.\n\n\t\n\nSMALLINT\n\n\n\n\nmem['free']\n\n\t\n\nCurrently available memory in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nmem['free_percent']\n\n\t\n\nCurrently available memory in percent of total.\n\n\t\n\nSMALLINT\n\n\n\n\nmem['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the memory probe.\n\n\t\n\nBIGINT\n\nheap\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nheap\n\n\t\n\nHeap memory utilization statistics.\n\n\t\n\nOBJECT\n\n\n\n\nheap['used']\n\n\t\n\nCurrently used heap memory in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nheap['max']\n\n\t\n\nMaximum available heap memory. You can specify the max heap memory CrateDB should use in the Configuration.\n\n\t\n\nBIGINT\n\n\n\n\nheap['free']\n\n\t\n\nCurrently available heap memory in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nheap['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the heap probe.\n\n\t\n\nBIGINT\n\nversion\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nversion\n\n\t\n\nCrateDB version information.\n\n\t\n\nOBJECT\n\n\n\n\nversion['number']\n\n\t\n\nVersion string in format \"major.minor.hotfix\"\n\n\t\n\nTEXT\n\n\n\n\nversion['build_hash']\n\n\t\n\nSHA hash of the GitHub commit which this build was built from.\n\n\t\n\nTEXT\n\n\n\n\nversion['build_snapshot']\n\n\t\n\nIndicates whether this build is a snapshot build.\n\n\t\n\nBOOLEAN\n\n\n\n\nversion['minimum_index_compatibility_version']\n\n\t\n\nIndicates the minimum compatible index version which is supported.\n\n\t\n\nTEXT\n\n\n\n\nversion['minimum_wire_compatibility_version']\n\n\t\n\nIndicates the minimum compatible wire protocol version which is supported.\n\n\t\n\nTEXT\n\ncluster_state_version\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\ncluster_state_version\n\n\t\n\nThe current version of the cluster state. The cluster state is an immutable structure and that is recreated when a change is published.\n\n\t\n\nBIGINT\n\nfs\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nfs\n\n\t\n\nUtilization statistics about the file system.\n\n\t\n\nOBJECT\n\n\n\n\nfs['total']\n\n\t\n\nAggregated usage statistic of all disks on the host.\n\n\t\n\nOBJECT\n\n\n\n\nfs['total']['size']\n\n\t\n\nTotal size of all disks in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['used']\n\n\t\n\nTotal used space of all disks in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['available']\n\n\t\n\nTotal available space of all disks in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['reads']\n\n\t\n\nTotal number of reads on all disks.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['bytes_read']\n\n\t\n\nTotal size of reads on all disks in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['writes']\n\n\t\n\nTotal number of writes on all disks.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['bytes_written']\n\n\t\n\nTotal size of writes on all disks in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['disks']\n\n\t\n\nUsage statistics of individual disks on the host.\n\n\t\n\nARRAY\n\n\n\n\nfs['disks']['dev']\n\n\t\n\nDevice name\n\n\t\n\nTEXT\n\n\n\n\nfs['disks']['size']\n\n\t\n\nTotal size of the disk in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['disks']['used']\n\n\t\n\nUsed space of the disk in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['disks']['available']\n\n\t\n\nAvailable space of the disk in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['data']\n\n\t\n\nInformation about data paths used by the node.\n\n\t\n\nARRAY\n\n\n\n\nfs['data']['dev']\n\n\t\n\nDevice name\n\n\t\n\nTEXT\n\n\n\n\nfs['data']['path']\n\n\t\n\nFile path where the data of the node resides.\n\n\t\n\nTEXT\n\nthread_pools\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nthread_pools\n\n\t\n\nUsage statistics of Java thread pools.\n\n\t\n\nARRAY\n\n\n\n\nthread_pools['name']\n\n\t\n\nName of the pool.\n\n\t\n\nTEXT\n\n\n\n\nthread_pools['active']\n\n\t\n\nNumber of currently running thread in the thread pool.\n\n\t\n\nINTEGER\n\n\n\n\nthread_pools['rejected']\n\n\t\n\nTotal number of rejected threads in the thread pool.\n\n\t\n\nBIGINT\n\n\n\n\nthread_pools['largest']\n\n\t\n\nLargest number of threads that have ever simultaneously been in the pool.\n\n\t\n\nINTEGER\n\n\n\n\nthread_pools['completed']\n\n\t\n\nTotal number of completed thread in the thread pool.\n\n\t\n\nBIGINT\n\n\n\n\nthread_pools['threads']\n\n\t\n\nSize of the thread pool.\n\n\t\n\nINTEGER\n\n\n\n\nthread_pools['queue']\n\n\t\n\nNumber of thread currently in the queue.\n\n\t\n\nINTEGER\n\nos\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nos\n\n\t\n\nOperating system stats\n\n\t\n\nOBJECT\n\n\n\n\nos['uptime']\n\n\t\n\nSystem uptime in milliseconds\n\nRequires allowing system calls on Windows and macOS. See notes in Uptime limitations.\n\n\t\n\nBIGINT\n\n\n\n\nos['timestamp']\n\n\t\n\nUNIX timestamp in millisecond resolution\n\n\t\n\nBIGINT\n\n\n\n\nos['cpu']\n\n\t\n\nInformation about CPU utilization\n\n\t\n\nOBJECT\n\n\n\n\nos['cpu']['used']\n\n\t\n\nSystem CPU usage as percentage\n\n\t\n\nSMALLINT\n\n\n\n\nos['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the OS probe.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']\n\n\t\n\nInformation about cgroups (Linux only)\n\n\t\n\nOBJECT\n\n\n\n\nos['cgroup']['cpuacct']\n\n\t\n\nInformation about CPU accounting\n\n\t\n\nOBJECT\n\n\n\n\nos['cgroup']['cpuacct']['control_group']\n\n\t\n\nThe path to the CPU accounting cgroup\n\n\t\n\nTEXT\n\n\n\n\nos['cgroup']['cpuacct']['usage_nanos']\n\n\t\n\nThe total CPU time (in nanoseconds) consumed by all tasks in this cgroup.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['cpu']\n\n\t\n\nInformation about the CPU subsystem\n\n\t\n\nOBJECT\n\n\n\n\nos['cgroup']['cpu']['control_group']\n\n\t\n\nThe path to the CPU cgroup\n\n\t\n\nTEXT\n\n\n\n\nos['cgroup']['cpu']['cfs_period_micros']\n\n\t\n\nThe period of time (in microseconds) the cgroup access to the CPU gets reallocated.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['cpu']['cfs_quota_micros']\n\n\t\n\nThe total amount of time (in microseconds) for which all tasks in the cgroup can run during one period (cfs_period_micros).\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['cpu']['num_elapsed_periods']\n\n\t\n\nThe nr. of period intervals (cfs_period_micros) that have elapsed.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['cpu']['num_times_throttled']\n\n\t\n\nThe nr. of times tasks in the cgroup have been throttled.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['cpu']['time_throttled_nanos']\n\n\t\n\nThe total time (in nanoseconds) for which tasks in the cgroup have been throttled.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['mem']\n\n\t\n\nInformation about memory resources used by tasks in a cgroup.\n\n\t\n\nOBJECT\n\n\n\n\nos['cgroup']['mem']['control_group']\n\n\t\n\nThe path to the memory cgroup\n\n\t\n\nTEXT\n\n\n\n\nos['cgroup']['mem']['usage_bytes']\n\n\t\n\nThe total current memory usage by processes in the cgroup.\n\n\t\n\nTEXT\n\n\n\n\nos['cgroup']['mem']['limit_bytes']\n\n\t\n\nThe max. amount of user memory in the cgroup.\n\n\t\n\nTEXT\n\nThe CPU information values are cached for 1s. They might differ from the actual values at query time. Use the probe timestamp to get the time of collection. When analyzing the CPU usage over time, always use os['probe_timestamp'] to calculate the time difference between 2 probes.\n\ncgroup limitations\n\nNote\n\ncgroup metrics only work if the stats are available from /sys/fs/cgroup/cpu and /sys/fs/cgroup/cpuacct.\n\nUptime limitations\n\nNote\n\nos[‘uptime’] required a system call when running CrateDB on Windows or macOS, however, system calls are not permitted by default. If you require this metric you need to allow system calls by setting bootstrap.seccomp to false. This setting must be set in the crate.yml or via command line argument and cannot be changed at runtime.\n\nos_info\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nos_info\n\n\t\n\nOperating system information\n\n\t\n\nOBJECT\n\n\n\n\nos_info['available_processors']\n\n\t\n\nNumber of processors that are available in the JVM. This is usually equal to the number of cores of the CPU.\n\n\t\n\nINTEGER\n\n\n\n\nos_info['name']\n\n\t\n\nName of the operating system (ex: Linux, Windows, macOS)\n\n\t\n\nTEXT\n\n\n\n\nos_info['arch']\n\n\t\n\nName of the JVM architecture (ex: amd64, x86)\n\n\t\n\nTEXT\n\n\n\n\nos_info['version']\n\n\t\n\nVersion of the operating system\n\n\t\n\nTEXT\n\n\n\n\nos_info['jvm']\n\n\t\n\nInformation about the JVM (Java Virtual Machine)\n\n\t\n\nOBJECT\n\n\n\n\nos_info['jvm']['version']\n\n\t\n\nThe JVM version\n\n\t\n\nTEXT\n\n\n\n\nos_info['jvm']['vm_name']\n\n\t\n\nThe name of the JVM (e.g. OpenJDK, Java HotSpot(TM) )\n\n\t\n\nTEXT\n\n\n\n\nos_info['jvm']['vm_vendor']\n\n\t\n\nThe vendor name of the JVM\n\n\t\n\nTEXT\n\n\n\n\nos_info['jvm']['vm_version']\n\n\t\n\nThe version of the JVM\n\n\t\n\nTEXT\n\nnetwork\n\nNetwork statistics are deprecated in CrateDB 2.3 and may completely be removed in subsequent versions. All BIGINT columns always return 0.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nnetwork\n\n\t\n\nStatistics about network activity on the host.\n\n\t\n\nOBJECT\n\n\n\n\nnetwork['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the network probe.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']\n\n\t\n\nTCP network activity on the host.\n\n\t\n\nOBJECT\n\n\n\n\nnetwork['tcp']['connections']\n\n\t\n\nInformation about TCP network connections.\n\n\t\n\nOBJECT\n\n\n\n\nnetwork['tpc']['connections']['initiated']\n\n\t\n\nTotal number of initiated TCP connections.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tpc']['connections']['accepted']\n\n\t\n\nTotal number of accepted TCP connections.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tpc']['connections']['curr_established']\n\n\t\n\nTotal number of currently established TCP connections.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']['connections']['dropped']\n\n\t\n\nTotal number of dropped TCP connections.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']['connections']['embryonic_dropped']\n\n\t\n\nTotal number of TCP connections that have been dropped before they were accepted.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']['packets']\n\n\t\n\nInformation about TCP packets.\n\n\t\n\nOBJECT\n\n\n\n\nnetwork['tpc']['packets']['sent']\n\n\t\n\nTotal number of TCP packets sent.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']['packets']['received']\n\n\t\n\nTotal number of TCP packets received.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tpc']['packets']['retransmitted']\n\n\t\n\nTotal number of TCP packets retransmitted due to an error.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']['packets']['errors_received']\n\n\t\n\nTotal number of TCP packets that contained checksum errors, had a bad offset, were dropped because of a lack of memory or were too short.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']]['packets']['rst_sent']\n\n\t\n\nTotal number of RST packets sent due to left unread data in queue when socket is closed. See tools.ietf.org.\n\n\t\n\nBIGINT\n\nconnections\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nhttp\n\n\t\n\nNumber of connections established via HTTP\n\n\t\n\nOBJECT\n\n\n\n\nhttp['open']\n\n\t\n\nThe currently open connections established via HTTP\n\n\t\n\nBIGINT\n\n\n\n\nhttp['total']\n\n\t\n\nThe total number of connections that have been established via HTTP over the life time of a CrateDB node\n\n\t\n\nBIGINT\n\n\n\n\npsql\n\n\t\n\nNumber of connections established via PostgreSQL protocol\n\n\t\n\nOBJECT\n\n\n\n\npsql['open']\n\n\t\n\nThe currently open connections established via PostgreSQL protocol\n\n\t\n\nBIGINT\n\n\n\n\npsql['total']\n\n\t\n\nThe total number of connections that have been established via PostgreSQL protocol over the life time of a CrateDB node\n\n\t\n\nBIGINT\n\n\n\n\ntransport\n\n\t\n\nNumber of connections established via Transport protocol\n\n\t\n\nOBJECT\n\n\n\n\ntransport['open']\n\n\t\n\nThe currently open connections established via Transport protocol\n\n\t\n\nBIGINT\n\nprocess\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nprocess\n\n\t\n\nStatistics about the CrateDB process.\n\n\t\n\nOBJECT\n\n\n\n\nprocess['open_file_descriptors']\n\n\t\n\nNumber of currently open file descriptors used by the CrateDB process.\n\n\t\n\nBIGINT\n\n\n\n\nprocess['max_open_file_descriptors']\n\n\t\n\nThe maximum number of open file descriptors CrateDB can use.\n\n\t\n\nBIGINT\n\n\n\n\nprocess['probe_timestamp']\n\n\t\n\nThe system UNIX timestamp at the moment of the probe collection.\n\n\t\n\nBIGINT\n\n\n\n\nprocess['cpu']\n\n\t\n\nInformation about the CPU usage of the CrateDB process.\n\n\t\n\nOBJECT\n\n\n\n\nprocess['cpu']['percent']\n\n\t\n\nThe CPU usage of the CrateDB JVM process given in percent.\n\n\t\n\nSMALLINT\n\nThe CPU information values are cached for 1s. They might differ from the actual values at query time. Use the probe timestamp to get the time of the collect. When analyzing the CPU usage over time, always use process['probe_timestamp'] to calculate the time difference between 2 probes.\n\nNote\n\nIf one of the queried nodes is not responding within three seconds it returns null every column except id and name. This behaviour could be used to detect hanging nodes.\n\nNode checks\n\nThe table sys.node_checks exposes a list of internal node checks and results of their validation.\n\nThe table schema is the following:\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe unique check ID.\n\n\t\n\nINTEGER\n\n\n\n\nnode_id\n\n\t\n\nThe unique node ID.\n\n\t\n\nTEXT\n\n\n\n\nseverity\n\n\t\n\nThe level of severity. The higher the value of the field the higher severity.\n\n\t\n\nINTEGER\n\n\n\n\ndescription\n\n\t\n\nThe description message for the setting check.\n\n\t\n\nTEXT\n\n\n\n\npassed\n\n\t\n\nThe flag determines whether the check for the setting has passed.\n\n\t\n\nBOOLEAN\n\n\n\n\nacknowledged\n\n\t\n\nThe flag determines whether the check for this setting has been acknowledged by the user in order to ignored the value of passed column. This column can be updated.\n\n\t\n\nBOOLEAN\n\nExample query:\n\ncr> select id, node_id, description from sys.node_checks order by id, node_id;\n+----+---------...-+--------------------------------------------------------------...-+\n| id | node_id     | description                                                      |\n+----+---------...-+--------------------------------------------------------------...-+\n|  1 | ...         | It has been detected that the 'gateway.expected_data_nodes' s... |\n|  2 | ...         | The cluster setting 'gateway.recover_after_data_nodes' (or th... |\n|  3 | ...         | If any of the \"expected data nodes\" recovery settings are set... |\n|  5 | ...         | The high disk watermark is exceeded on the node. The cluster ... |\n|  6 | ...         | The low disk watermark is exceeded on the node. The cluster w... |\n|  7 | ...         | The flood stage disk watermark is exceeded on the node. Table... |\n|  8 | ...         | The amount of shards on the node reached 90 % of the limit of... |\n+----+---------...-+--------------------------------------------------------------...-+\nSELECT 7 rows in set (... sec)\n\nAcknowledge failed checks\n\nIt is possible to acknowledge every check by updating the acknowledged column. By doing this, specially CrateDB’s built-in Admin UI won’t complain anymore about failing checks.\n\nImagine we’ve added a new node to our cluster, but as the gateway.expected_data_nodes column can only be set via config-file or command-line argument, the check for this setting will not pass on the already running nodes until the config-file or command-line argument on these nodes is updated and the nodes are restarted (which is not what we want on a healthy well running cluster).\n\nIn order to make the Admin UI accept a failing check (so the checks label goes green again), we must acknowledge this check by updating it’s acknowledged flag:\n\ncr> update sys.node_checks set acknowledged = true where id = 1;\nUPDATE OK, 1 row affected (... sec)\n\n\nCaution\n\nUpdates on this column are transient, so changed values are lost after the affected node is restarted.\n\nDescription of checked node settings\nRecovery expected data nodes\n\nThis check looks at the gateway.expected_data_nodes setting and checks if its value matches the actual number of data nodes present in the cluster. If the actual number of nodes is below the expected number, the warning is raised to indicate some nodes are down. If the actual number is greater, this is flagged to indicate the setting should be updated.\n\nNote\n\nFor backward compatibility, setting the deprecated gateway.expected_nodes instead is still supported. It counts all nodes, not only data-carrying nodes.\n\nRecovery after data nodes\n\nThis check looks at the gateway.recover_after_data_nodes setting and checks if its value is greater than half the configured expected number, but not greater than the configured expected number.\n\nNote\n\nFor backward compatibility, setting the deprecated gateway.recover_after_nodes instead is still supported.\n\n(E / 2) < R <= E\n\n\nHere, R is the number of recovery nodes and E is the number of expected (data) nodes.\n\nIf recovery is started when some nodes are down, CrateDB proceeds on the basis the nodes that are down may not be coming back, and it will create new replicas and rebalance shards as necessary. This is throttled, and it can be controlled with routing allocation settings, but depending on the context, you may prefer to delay recovery if the nodes are only down for a short period of time, so it is advisable to review the documentation around the settings involved and configure them carefully.\n\nRecovery after time\n\nIf gateway.recover_after_data_nodes is set, then gateway.recover_after_time must not be set to 0s, otherwise the gateway.recover_after_data_nodes setting wouldn’t have any effect.\n\nNote\n\nFor backward compatibility, setting the deprecated gateway.recover_after_nodes instead is still supported.\n\nRouting allocation disk watermark high\n\nThe check for the cluster.routing.allocation.disk.watermark.high setting verifies that the high watermark is not exceeded on the current node. The usage of each disk for configured CrateDB data paths is verified against the threshold setting. If one or more verification fails the check is marked as not passed.\n\nRouting allocation disk watermark low\n\nThe check for the cluster.routing.allocation.disk.watermark.low which controls the low watermark for the node disk usage. The check verifies that the low watermark is not exceeded on the current node. The verification is done against each disk for configured CrateDB data paths. The check is not passed if the verification for one or more disk fails.\n\nMaximum shards per node\n\nThe check verifies that the amount of shards on the current node is less than 90 percent of cluster.max_shards_per_node. Creating new tables or partitions which would push the number of shards beyond 100 % of the limit will be rejected.\n\nShards\n\nThe table sys.shards contains real-time statistics for all shards of all (non-system) tables.\n\nTable schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nnode\n\n\t\n\nInformation about the node the shard is located at.\n\n\t\n\nOBJECT\n\n\n\n\nnode['name']\n\n\t\n\nThe name of the node the shard is located at.\n\n\t\n\nTEXT\n\n\n\n\nnode['id']\n\n\t\n\nThe id of the node the shard is located at.\n\n\t\n\nTEXT\n\n\n\n\nblob_path\n\n\t\n\nPath to the directory which contains the blob files of the shard, or null if the shard is not a blob shard.\n\n\t\n\nTEXT\n\n\n\n\nid\n\n\t\n\nThe shard id. This shard id is managed by the system, ranging from 0 up to the number of configured shards of the table.\n\n\t\n\nINTEGER\n\n\n\n\nmin_lucene_version\n\n\t\n\nShows the oldest Lucene segment version used in this shard.\n\n\t\n\nTEXT\n\n\n\n\nnum_docs\n\n\t\n\nThe total amount of documents within a shard.\n\n\t\n\nBIGINT\n\n\n\n\noprhan_partition\n\n\t\n\nTrue if this shard belongs to an orphaned partition which doesn’t belong to any table anymore.\n\n\t\n\nBOOLEAN\n\n\n\n\npartition_ident\n\n\t\n\nThe partition ident of a partitioned table. Empty for non-partitioned tables.\n\n\t\n\nTEXT\n\n\n\n\npath\n\n\t\n\nPath to the shard directory on the filesystem. This directory contains state and index files.\n\n\t\n\nTEXT\n\n\n\n\nprimary\n\n\t\n\nIndicates if this shard is the primary shard.\n\n\t\n\nBOOLEAN\n\n\n\n\nrecovery\n\n\t\n\nRecovery statistics for a shard.\n\n\t\n\nOBJECT\n\n\n\n\nrecovery['files']\n\n\t\n\nFile recovery statistics\n\n\t\n\nOBJECT\n\n\n\n\nrecovery['files']['percent']\n\n\t\n\nPercentage of files already recovered.\n\n\t\n\nREAL\n\n\n\n\nrecovery['files']['recovered']\n\n\t\n\nNumber of files recovered in the shard. Includes both existing and reused files.\n\n\t\n\nINTEGER\n\n\n\n\nrecovery['files']['reused']\n\n\t\n\nTotal number of files reused from a local copy while recovering the shard.\n\n\t\n\nINTEGER\n\n\n\n\nrecovery['files']['used']\n\n\t\n\nTotal number of files in the shard.\n\n\t\n\nINTEGER\n\n\n\n\nrecovery['size']\n\n\t\n\nRecovery statistics for the shard in bytes\n\n\t\n\nOBJECT\n\n\n\n\nrecovery['size']['percent']\n\n\t\n\nPercentage of bytes already recovered\n\n\t\n\nREAL\n\n\n\n\nrecovery['size']['recovered']\n\n\t\n\nNumber of bytes recovered. Includes both existing and re-used bytes.\n\n\t\n\nBIGINT\n\n\n\n\nrecovery['size']['reused']\n\n\t\n\nNumber of bytes re-used from a local copy while recovering the shard.\n\n\t\n\nBIGINT\n\n\n\n\nrecovery['size']['used']\n\n\t\n\nTotal number of bytes in the shard.\n\n\t\n\nBIGINT\n\n\n\n\nrecovery['stage']\n\n\t\n\nRecovery stage:\n\ninit: Recovery has not started\n\nindex: Reading the Lucene index meta-data and copying bytes from source to destination\n\nstart: Starting the engine, opening the index for use\n\ntranslog: Replaying transaction log\n\nfinalize: Cleanup\n\ndone: Complete\n\n\t\n\nTEXT\n\n\n\n\nrecovery['total_time']\n\n\t\n\nReturns elapsed time from the start of the shard recovery.\n\n\t\n\nBIGINT\n\n\n\n\nrecovery['type']\n\n\t\n\nRecovery type:\n\ngateway\n\nsnapshot\n\nreplica\n\nrelocating\n\n\t\n\nTEXT\n\n\n\n\nrelocating_node\n\n\t\n\nThe id of the node to which the shard is getting relocated to.\n\n\t\n\nTEXT\n\n\n\n\nrouting_state\n\n\t\n\nThe current state of the shard in the routing table. Possible states are:\n\nUNASSIGNED\n\nINITIALIZING\n\nSTARTED\n\nRELOCATING\n\n\t\n\nTEXT\n\n\n\n\nschema_name\n\n\t\n\nThe schema name of the table the shard belongs to\n\n\t\n\nTEXT\n\n\n\n\nsize\n\n\t\n\nThe current size in bytes. This value is cached for a short period and may return slightly outdated values.\n\n\t\n\nBIGINT\n\n\n\n\nstate\n\n\t\n\nThe current state of the shard. Possible states are:\n\nCREATED\n\nRECOVERING\n\nPOST_RECOVERY\n\nSTARTED\n\nRELOCATED\n\nCLOSED\n\nINITIALIZING\n\nUNASSIGNED\n\n\t\n\nTEXT\n\n\n\n\nclosed\n\n\t\n\nThe state of the table associated with the shard.\n\n\t\n\nBOOLEAN\n\n\n\n\ntable_name\n\n\t\n\nThe name of the table this shard belongs to\n\n\t\n\nTEXT\n\n\n\n\nseq_no_stats\n\n\t\n\nContains information about internal sequence numbering and checkpoints for these sequence numbers.\n\n\t\n\nOBJECT\n\n\n\n\nseq_no_stats['max_seq_no']\n\n\t\n\nThe highest sequence number that has been issued so far on the shard.\n\n\t\n\nBIGINT\n\n\n\n\nseq_no_stats['local_checkpoint']\n\n\t\n\nThe highest sequence number for which all lower sequence number of been processed on this shard. Due to concurrent indexing this can be lower than max_seq_no.\n\n\t\n\nBIGINT\n\n\n\n\nseq_no_stats['global_checkpoint']\n\n\t\n\nThe highest sequence number for which the local shard can guarantee that all lower sequence numbers have been processed on all active shard copies.\n\n\t\n\nBIGINT\n\n\n\n\ntranslog_stats\n\n\t\n\nContains information for the translog of the shard.\n\n\t\n\nOBJECT\n\n\n\n\ntranslog_stats['size']\n\n\t\n\nThe current size of the translog file in bytes.\n\n\t\n\nBIGINT\n\n\n\n\ntranslog_stats['uncommitted_size']\n\n\t\n\nThe size in bytes of the translog that has not been committed to Lucene yet.\n\n\t\n\nBIGINT\n\n\n\n\ntranslog_stats['number_of_operations']\n\n\t\n\nThe number of operations recorded in the translog.\n\n\t\n\nINTEGER\n\n\n\n\ntranslog_stats['uncommitted_operations']\n\n\t\n\nThe number of operations in the translog which have not been committed to Lucene yet.\n\n\t\n\nINTEGER\n\n\n\n\nretention_leases\n\n\t\n\nVersioned collection of retention leases.\n\n\t\n\nOBJECT\n\n\n\n\nflush_stats\n\n\t\n\nFlush information. Shard relocation resets this information.\n\n\t\n\nOBJECT\n\n\n\n\nflush_stats['count']\n\n\t\n\nThe total amount of flush operations that happened on the shard.\n\n\t\n\nBIGINT\n\n\n\n\nflush_stats['periodic_count']\n\n\t\n\nThe number of periodic flushes. Each periodic flush also counts as a regular flush. A periodic flush can happen after writes depending on settings like the translog flush threshold.\n\n\t\n\nBIGINT\n\n\n\n\nflush_stats['total_time_ns']\n\n\t\n\nThe total time spent on flush operations on the shard.\n\n\t\n\nBIGINT\n\nNote\n\nThe sys.shards table is subject to Shard table permissions.\n\nExample\n\nFor example, you can query shards like this:\n\ncr> select schema_name as schema,\n...   table_name as t,\n...   id,\n...   partition_ident as p_i,\n...   num_docs as docs,\n...   primary,\n...   relocating_node as r_n,\n...   routing_state as r_state,\n...   state,\n...   orphan_partition as o_p\n... from sys.shards where table_name = 'locations' and id = 1;\n+--------+-----------+----+-----+------+---------+------+---------+---------+-------+\n| schema | t         | id | p_i | docs | primary | r_n  | r_state |  state  | o_p   |\n+--------+-----------+----+-----+------+---------+------+---------+---------+-------+\n| doc    | locations |  1 |     |    8 | TRUE    | NULL | STARTED | STARTED | FALSE |\n+--------+-----------+----+-----+------+---------+------+---------+---------+-------+\nSELECT 1 row in set (... sec)\n\nSegments\n\nThe sys.segments table contains information about the Lucene segments of the shards.\n\nThe segment information is useful to understand the behaviour of the underlying Lucene file structures for troubleshooting and performance optimization of shards.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nsegment_name\n\n\t\n\nName of the segment, derived from the segment generation and used internally to create file names in the directory of the shard.\n\n\t\n\nTEXT\n\n\n\n\nshard_id\n\n\t\n\nID of the effected shard.\n\n\t\n\nINTEGER\n\n\n\n\ntable_schema\n\n\t\n\nSchema name of the table of the shard.\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nTable name of the shard.\n\n\t\n\nTEXT\n\n\n\n\npartition_ident\n\n\t\n\nThe partition ident of a partitioned table. Empty for non-partitioned tables.\n\n\t\n\nTEXT\n\n\n\n\nnode\n\n\t\n\nInformation about the node the shard is located at.\n\n\t\n\nOBJECT\n\n\n\n\nnode['name']\n\n\t\n\nThe name of the node the shard is located at.\n\n\t\n\nTEXT\n\n\n\n\nnode['id']\n\n\t\n\nThe id of the node the shard is located at.\n\n\t\n\nTEXT\n\n\n\n\ngeneration\n\n\t\n\nGeneration number of the segment, increments for each segment written.\n\n\t\n\nLONG\n\n\n\n\nnum_docs\n\n\t\n\nNumber of non-deleted Lucene documents in this segment.\n\n\t\n\nINTEGER\n\n\n\n\ndeleted_docs\n\n\t\n\nNumber of deleted Lucene documents in this segment.\n\n\t\n\nINTEGER\n\n\n\n\nsize\n\n\t\n\nDisk space used by the segment in bytes.\n\n\t\n\nLONG\n\n\n\n\nmemory\n\n\t\n\nUnavailable starting from CrateDB 5.0. Always returns -1.\n\n\t\n\nLONG\n\n\n\n\ncommitted\n\n\t\n\nIndicates if the segments are synced to disk. Segments that are synced can survive a hard reboot.\n\n\t\n\nBOOLEAN\n\n\n\n\nprimary\n\n\t\n\nDescribes if this segment is part of a primary shard.\n\n\t\n\nBOOLEAN\n\n\n\n\nsearch\n\n\t\n\nIndicates if the segment is searchable. If false, the segment has most likely been written to disk but needs a refresh to be searchable.\n\n\t\n\nBOOLEAN\n\n\n\n\nversion\n\n\t\n\nVersion of Lucene used to write the segment.\n\n\t\n\nTEXT\n\n\n\n\ncompound\n\n\t\n\nIf true, Lucene merges all files from the segment into a single file to save file descriptors.\n\n\t\n\nBOOLEAN\n\n\n\n\nattributes\n\n\t\n\nContains information about whether high compression was enabled.\n\n\t\n\nOBJECT\n\nNote\n\nThe information in the sys.segments table is expensive to calculate and therefore this information should be retrieved with awareness that it can have performance implications on the cluster.\n\nNote\n\nThe sys.shards table is subject to Shard table permissions.\n\nJobs, operations, and logs\n\nTo let you inspect the activities currently taking place in a cluster, CrateDB provides system tables that let you track current cluster jobs and operations. See Jobs Table and Operations Table.\n\nJobs and operations that finished executing are additionally recorded in memory. There are two retention policies available to control how many records should be kept.\n\nOne option is to configure the maximum number of records which should be kept. Once the configured table size is reached, the older log records are deleted as newer records are added. This is configurable using stats.jobs_log_size and stats.operations_log_size.\n\nAnother option is to configure an expiration time for the records. In this case, the records in the logs tables are periodically cleared if they are older than the expiry time. This behaviour is configurable using stats.jobs_log_expiration and stats.operations_log_expiration.\n\nIn addition to these retention policies, there is a memory limit in place preventing these tables from taking up too much memory. The amount of memory that can be used to store the jobs can be configured using stats.breaker.log.jobs.limit and stats.breaker.log.operations.limit. If the memory limit is reached, an error message will be logged and the log table will be cleared completely.\n\nIt is also possible to define a filter which must match for jobs to be recorded after they finished executing. This can be useful to only record slow queries or queries that failed due to an error. This filter can be configured using the stats.jobs_log_filer setting.\n\nFurthermore, there is a second filter setting which also results in a log entry in the regular CrateDB log file for all finished jobs that match this filter. This can be configured using stats.jobs_log_persistent_filter. This could be used to create a persistent slow query log.\n\nJobs\n\nThe sys.jobs table is a constantly updated view of all jobs that are currently being executed in the cluster.\n\nTable schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe job UUID.\n\nThis job ID is generated by the system.\n\n\t\n\nTEXT\n\n\n\n\nnode\n\n\t\n\nInformation about the node that created the job.\n\n\t\n\nOBJECT\n\n\n\n\nnode['id']\n\n\t\n\nThe id of the node.\n\n\t\n\nTEXT\n\n\n\n\nnode['name']\n\n\t\n\nThe name of the node.\n\n\t\n\nTEXT\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the job started.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nstmt\n\n\t\n\nShows the data query or manipulation statement represented by this job.\n\n\t\n\nTEXT\n\n\n\n\nusername\n\n\t\n\nThe user who is executing the statement.\n\n\t\n\nTEXT\n\nThe field username corresponds to the SESSION_USER that is performing the query:\n\ncr> select stmt, username, started from sys.jobs where stmt like 'sel% from %jobs%';\n+---------------------------------------------------------------------------------+----------+-...-----+\n| stmt                                                                            | username | started |\n+---------------------------------------------------------------------------------+----------+-...-----+\n| select stmt, username, started from sys.jobs where stmt like 'sel% from %jobs%' | crate    | ...     |\n+---------------------------------------------------------------------------------+----------+-...-----+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nIf the user management module is not available, the username is given as crate.\n\nEvery request that queries data or manipulates data is considered a “job” if it is a valid query. Requests that are not valid queries (for example, a request that tries to query a non-existent table) will not show up as jobs.\n\nNote\n\nThe sys.jobs table is subject to sys jobs tables permissions.\n\nJobs metrics\n\nThe sys.jobs_metrics table provides an overview of the query latency in the cluster. Jobs metrics are not persisted across node restarts.\n\nThe metrics are aggregated for each node and each unique classification of the statements.\n\nNote\n\nIn order to reduce the memory requirements for these metrics, the times are statistically sampled and therefore may have slight inaccuracies. In addition, durations are only tracked up to 10 minutes. Statements taking longer than that are capped to 10 minutes.\n\nsys.jobs_metrics Table schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nnode\n\n\t\n\nAn object containing the id and name of the node on which the metrics have been sampled.\n\n\t\n\nOBJECT\n\n\n\n\nclassification\n\n\t\n\nAn object containing the statement classification.\n\n\t\n\nOBJECT\n\n\n\n\nclassification['type']\n\n\t\n\nThe general type of the statement. Types are: INSERT, SELECT, UPDATE, DELETE, COPY, DDL, and MANAGEMENT.\n\n\t\n\nTEXT\n\n\n\n\nclassification['labels']\n\n\t\n\nLabels are only available for certain statement types that can be classified more accurately than just by their type.\n\n\t\n\nTEXT_ARRAY\n\n\n\n\ntotal_count\n\n\t\n\nTotal number of queries executed\n\n\t\n\nBIGINT\n\n\n\n\nfailed_count\n\n\t\n\nTotal number of queries that failed to complete successfully.\n\n\t\n\nBIGINT\n\n\n\n\nsum_of_durations\n\n\t\n\nSum of durations in ms of all executed queries per statement type.\n\n\t\n\nBIGINT\n\n\n\n\nstdev\n\n\t\n\nThe standard deviation of the query latencies\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\nmean\n\n\t\n\nThe mean query latency in ms\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\nmax\n\n\t\n\nThe maximum query latency in ms\n\n\t\n\nBIGINT\n\n\n\n\nmin\n\n\t\n\nThe minimum query latency in ms\n\n\t\n\nBIGINT\n\n\n\n\npercentiles\n\n\t\n\nAn object containing different percentiles\n\n\t\n\nOBJECT\n\nClassification\n\nCertain statement types (such as SELECT statements) have additional labels in their classification. These labels are the names of the logical plan operators that are involved in the query.\n\nFor example, the following UNION statement:\n\nSELECT name FROM t1 where id = 1\nUNION ALL\nSELECT name FROM t2 where id < 2\n\n\nwould result in the following labels:\n\nUnion` for the UNION ALL\n\nGet for the left SELECT\n\nCollect for the right SELECT\n\nNote\n\nLabels may be subject to change as they only represent internal properties of the statement!\n\nOperations\n\nThe sys.operations table is a constantly updated view of all operations that are currently being executed in the cluster:\n\ncr> select node['name'], job_id, name, used_bytes from sys.operations\n... order by name limit 1;\n+--------------+--------...-+-----...-+------------+\n| node['name'] | job_id     | name    | used_bytes |\n+--------------+--------...-+-----...-+------------+\n| crate        | ...        | ...     | ...        |\n+--------------+--------...-+-----...-+------------+\nSELECT 1 row in set (... sec)\n\n\nAn operation is a node-specific sub-component of a job (for when a job involves multi-node processing). Jobs that do not require multi-node processing will not produce any operations.\n\nTable schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe operation UUID.\n\nThis operation ID is generated by the system.\n\n\t\n\nTEXT\n\n\n\n\njob_id\n\n\t\n\nThe job id this operation belongs to.\n\n\t\n\nTEXT\n\n\n\n\nname\n\n\t\n\nThe name of the operation.\n\n\t\n\nTEXT\n\n\n\n\nnode\n\n\t\n\nInformation about the node that created the operation.\n\n\t\n\nOBJECT\n\n\n\n\nnode['id']\n\n\t\n\nThe id of the node.\n\n\t\n\nTEXT\n\n\n\n\nnode['name']\n\n\t\n\nThe name of the node.\n\n\t\n\nTEXT\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the operation started.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nused_bytes\n\n\t\n\nCurrently loaded amount of data by the operation.\n\n\t\n\nBIGINT\n\nNote\n\nIn some cases, operations are generated for internal CrateDB work that does not directly correspond to a user request. These entries do not have corresponding entries in sys.jobs.\n\nLogs\n\nThe sys.jobs and sys.operations tables have corresponding log tables: sys.jobs_log and sys.operations_log.\n\nsys.jobs_log Table schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe job ID.\n\n\t\n\nTEXT\n\n\n\n\nended\n\n\t\n\nThe point in time when the job finished.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nerror\n\n\t\n\nIf the job encountered an error, this will hold the error message.\n\n\t\n\nTEXT\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the job started.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nstmt\n\n\t\n\nShows the data query or manipulation statement executed by the job.\n\n\t\n\nTEXT\n\n\n\n\nusername\n\n\t\n\nThe user who executed the statement.\n\n\t\n\nTEXT\n\n\n\n\nclassification\n\n\t\n\nAn object containing the statement classification.\n\n\t\n\nOBJECT\n\n\n\n\nclassification['type']\n\n\t\n\nThe general type of the statement. Types are: INSERT, SELECT, UPDATE, DELETE,``COPY``, DDL, and MANAGEMENT.\n\n\t\n\nTEXT\n\n\n\n\nclassification['labels']\n\n\t\n\nLabels are only available for certain statement types that can be classified more accurately than just by their type.\n\n\t\n\nTEXT_ARRAY\n\n\n\n\nnode\n\n\t\n\nInformation about the node that created the job.\n\n\t\n\nOBJECT\n\n\n\n\nnode['id']\n\n\t\n\nThe id of the node.\n\n\t\n\nTEXT\n\n\n\n\nnode['name']\n\n\t\n\nThe name of the node.\n\n\t\n\nTEXT\n\nNote\n\nYou can control which jobs are recorded using the stats.jobs_log_filter\n\nNote\n\nThe sys.jobs_log table is subject to sys jobs tables permissions.\n\nsys.operations_log Table schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe operation ID.\n\n\t\n\nTEXT\n\n\n\n\njob_id\n\n\t\n\nThe job id.\n\n\t\n\nTEXT\n\n\n\n\nended\n\n\t\n\nThe point in time when the operation finished.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nerror\n\n\t\n\nIf the operation encountered an error, this will hold the error message.\n\n\t\n\nTEXT\n\n\n\n\nname\n\n\t\n\nThe name of the operation.\n\n\t\n\nTEXT\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the operation started.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nused_bytes\n\n\t\n\nThe amount of data loaded by the operation.\n\n\t\n\nBIGINT\n\nAfter a job or operation finishes, the corresponding entry will be moved into the corresponding log table:\n\ncr> select id, stmt, username, started, ended, error\n... from sys.jobs_log order by ended desc limit 2;\n+-...+----------------------------------------------...-+----------+-...-----+-...---+-------+\n| id | stmt                                             | username | started | ended | error |\n+-...+----------------------------------------------...-+----------+-...-----+-...---+-------+\n| ...| select node['name'], ...                         | crate    | ...     | ...   |  NULL |\n| ...| select stmt, username, started from sys.jobs ... | crate    | ...     | ...   |  NULL |\n+-...+----------------------------------------------...-+----------+-...-----+-...---+-------+\nSELECT 2 rows in set (... sec)\n\n\nInvalid queries are also logged in the sys.jobs_log table, i.e. queries that never make it to the sys.jobs table because they could not be executed.\n\nThe log tables are bound by a fixed size (stats.jobs_log_size) or by an expiration time (stats.jobs_log_expiration)\n\nSee Collecting stats for information on how to configure logs.\n\nCaution\n\nIf you deactivate statistics tracking, the logs tables will be truncated.\n\nCluster checks\n\nThe table sys.checks exposes a list of internal cluster checks and results of their validation.\n\nThe sys.checks table looks like this:\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe unique check id.\n\n\t\n\nINTEGER\n\n\n\n\nseverity\n\n\t\n\nThe level of severity. The higher the value of the field the higher severity.\n\n\t\n\nINTEGER\n\n\n\n\ndescription\n\n\t\n\nThe description message for the setting check.\n\n\t\n\nTEXT\n\n\n\n\npassed\n\n\t\n\nThe flag determines whether the check for the setting has passed.\n\n\t\n\nBOOLEAN\n\nHere’s an example query:\n\ncr> select id, description from sys.checks order by id;\n+----+--------------------------------------------------------------...-+\n| id | description                                                      |\n+----+--------------------------------------------------------------...-+\n|  2 | The total number of partitions of one or more partitioned tab... |\n|  3 | The following tables need to be recreated for compatibility w... |\n+----+--------------------------------------------------------------...-+\nSELECT 2 rows in set (... sec)\n\n\nCluster checks are also indicated in the CrateDB admin console. When all cluster checks (and all Node checks) pass, the Checks icon will be green. Here’s what it looks like when some checks are failing at the CRITICAL severity level:\n\nCurrent Checks\nNumber of partitions\n\nThis check warns if any partitioned table has more than 1000 partitions to detect the usage of a high cardinality field for partitioning.\n\nTables need to be recreated\n\nWarning\n\nDo not attempt to upgrade your cluster to a newer major version if this cluster check is failing. Follow the instructions below to get this cluster check passing.\n\nThis check warns you if your cluster contains tables that you need to reindex before you can upgrade to a future major version of CrateDB.\n\nIf you try to upgrade to a later major CrateDB version without reindexing the tables, CrateDB will refuse to start.\n\nCrateDB table version compatibility scheme\n\nCrateDB maintains backward compatibility for tables created in majorVersion - 1:\n\nTable Origin\n\n\t\n\nCurrent Version\n\n\t\n\nCurrent Version\n\n\t\n\nCurrent Version\n\n\n\t\n\n3.x\n\n\t\n\n4.x\n\n\t\n\n5.x\n\n\n\n\n3.x\n\n\t\n\n✔️\n\n\t\n\n✔️\n\n\t\n\n❌\n\n\n\n\n4.x\n\n\t\n\n❌\n\n\t\n\n✔️\n\n\t\n\n✔️\n\n\n\n\n5.x\n\n\t\n\n❌\n\n\t\n\n❌\n\n\t\n\n✔️\n\nAvoiding reindex using partitioned tables\n\nReindexing tables is an expensive operation which can take a long time. If you are storing time series data for a certain retention period and intend to delete old data, it is possible to use the partitioned tables to avoid reindex operations.\n\nYou will have to use a partition column that denotes time. For example, if you have a retention period of nine months, you could partition a table by a month column. Then, every month, the system will create a new partition. This new partition is created using the active CrateDB version and is compatible with the next major CrateDB version. Now to achieve your goal of avoiding a reindex, you must manually delete any partition older than nine months. If you do that, then after nine months you rolled through all partitions and the remaining nine are compatible with the next major CrateDB version.\n\nHow to reindex\n\nUse SHOW CREATE TABLE to get the schema required to create an empty copy of the table to recreate:\n\ncr> SHOW CREATE TABLE rx.metrics;\n+-----------------------------------------------------+\n| SHOW CREATE TABLE rx.metrics                        |\n+-----------------------------------------------------+\n| CREATE TABLE IF NOT EXISTS \"rx\".\"metrics\" (         |\n|    \"id\" TEXT NOT NULL,                                       |\n|    \"temperature\" REAL,                              |\n|    PRIMARY KEY (\"id\")                               |\n| )                                                   |\n| CLUSTERED BY (\"id\") INTO 4 SHARDS                   |\n| WITH (                                              |\n|    \"allocation.max_retries\" = 5,                    |\n|    \"blocks.metadata\" = false,                       |\n|    \"blocks.read\" = false,                           |\n|    \"blocks.read_only\" = false,                      |\n|    \"blocks.read_only_allow_delete\" = false,         |\n|    \"blocks.write\" = false,                          |\n|    codec = 'default',                               |\n|    column_policy = 'strict',                        |\n|    \"mapping.total_fields.limit\" = 1000,             |\n|    max_ngram_diff = 1,                              |\n|    max_shingle_diff = 3,                            |\n|    number_of_replicas = '0-1',                      |\n|    \"routing.allocation.enable\" = 'all',             |\n|    \"routing.allocation.total_shards_per_node\" = -1, |\n|    \"store.type\" = 'fs',                             |\n|    \"translog.durability\" = 'REQUEST',               |\n|    \"translog.flush_threshold_size\" = 536870912,     |\n|    \"translog.sync_interval\" = 5000,                 |\n|    \"unassigned.node_left.delayed_timeout\" = 60000,  |\n|    \"write.wait_for_active_shards\" = '1'             |\n| )                                                   |\n+-----------------------------------------------------+\nSHOW 1 row in set (... sec)\n\n\nCreate a new temporary table, using the schema returned from SHOW CREATE TABLE:\n\ncr> CREATE TABLE rx.tmp_metrics (id TEXT PRIMARY KEY, temperature REAL);\nCREATE OK, 1 row affected (... sec)\n\n\nCopy the data:\n\ncr> INSERT INTO rx.tmp_metrics (id, temperature) (SELECT id, temperature FROM rx.metrics);\nINSERT OK, 2 rows affected (... sec)\n\n\nSwap the tables:\n\ncr> ALTER CLUSTER SWAP TABLE rx.tmp_metrics TO rx.metrics;\nALTER OK, 1 row affected  (... sec)\n\n\nConfirm the new your_table contains all data and has the new version:\n\ncr> SELECT count(*) FROM rx.metrics;\n+----------+\n| count(*) |\n+----------+\n|        2 |\n+----------+\nSELECT 1 row in set (... sec)\n\ncr> SELECT version['created'] FROM information_schema.tables\n... WHERE table_schema = 'rx' AND table_name = 'metrics';\n+--------------------+\n| version['created'] |\n+--------------------+\n| 5.5.5              |\n+--------------------+\nSELECT 1 row in set (... sec)\n\n\nDrop the old table, as it is now obsolete:\n\ncr> DROP TABLE rx.tmp_metrics;\nDROP OK, 1 row affected  (... sec)\n\n\nAfter you reindexed all tables, this cluster check will pass.\n\nNote\n\nSnapshots of your tables created prior to them being upgraded will not work with future versions of CrateDB. For this reason, you should create a new snapshot for each of your tables. (See Snapshots.)\n\nLicense check\n\nNote\n\nThis check was removed in version 4.5 because CrateDB no longer requires an enterprise license, see also Farewell to the CrateDB Enterprise License.\n\nThis check warns you when your license is close to expiration, is already expired, or if the cluster contains more nodes than allowed by your license. It will yield a MEDIUM alert when your license is valid for less than 15 days and a HIGH alert when your license is valid for less than a day. All other cases, like already expired or max-nodes-violation, it will result in a HIGH alert. We recommend that you request a new license when this check triggers, in order to avoid the situation where operations are rejected due to an invalid license.\n\nHealth\n\nThe sys.health table lists the health of each table and table partition. The health is computed by checking the states of the shard of each table/partition.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\ntable_name\n\n\t\n\nThe table name.\n\n\t\n\nTEXT\n\n\n\n\ntable_schema\n\n\t\n\nThe schema of the table.\n\n\t\n\nTEXT\n\n\n\n\npartition_ident\n\n\t\n\nThe ident of the partition. NULL for non-partitioned tables.\n\n\t\n\nTEXT\n\n\n\n\nhealth\n\n\t\n\nThe health label. Can be RED, YELLOW or GREEN.\n\n\t\n\nTEXT\n\n\n\n\nseverity\n\n\t\n\nThe health as a smallint value. Useful when ordering on health.\n\n\t\n\nSMALLINT\n\n\n\n\nmissing_shards\n\n\t\n\nThe number of not assigned or started shards.\n\n\t\n\nINTEGER\n\n\n\n\nunderreplicated_shards\n\n\t\n\nThe number of shards which are not fully replicated.\n\n\t\n\nINTEGER\n\nBoth missing_shards and underreplicated_shards might return -1 if the cluster is in an unhealthy state that prevents the exact number from being calculated. This could be the case when the cluster can’t elect a master, because there are not enough eligible nodes available.\n\ncr> select * from sys.health order by severity desc, table_name;\n+--------+----------------+-----------------+----------+------------+--------------+------------------------+\n| health | missing_shards | partition_ident | severity | table_name | table_schema | underreplicated_shards |\n+--------+----------------+-----------------+----------+------------+--------------+------------------------+\n| GREEN  |              0 |            NULL |        1 | locations  | doc          |                      0 |\n| GREEN  |              0 |            NULL |        1 | quotes     | doc          |                      0 |\n+--------+----------------+-----------------+----------+------------+--------------+------------------------+\nSELECT 2 rows in set (... sec)\n\n\nThe health with the highest severity will always define the health of the query scope.\n\nExample of getting a cluster health (health of all tables):\n\ncr> select health from sys.health order by severity desc limit 1;\n+--------+\n| health |\n+--------+\n| GREEN  |\n+--------+\nSELECT 1 row in set (... sec)\n\nHealth definition\n\nHealth\n\n\t\n\nDescription\n\n\n\n\nRED\n\n\t\n\nAt least one primary shard is missing (primary shard not started or unassigned).\n\n\n\n\nYELLOW\n\n\t\n\nAt least one shard is underreplicated (replica shard not started or unassigned).\n\n\n\n\nGREEN\n\n\t\n\nAll primary and replica shards have been started.\n\nNote\n\nThe sys.health table is subject to Shard table permissions as it will expose a summary of table shard states.\n\nRepositories\n\nThe table sys.repositories lists all configured repositories that can be used to create, manage and restore snapshots (see Snapshots).\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nname\n\n\t\n\nThe repository name\n\n\t\n\nTEXT\n\n\n\n\ntype\n\n\t\n\nThe type of the repository determining how and where the repository stores its snapshots.\n\n\t\n\nTEXT\n\n\n\n\nsettings\n\n\t\n\nThe configuration settings the repository has been created with. The specific settings depend on the repository type, see CREATE REPOSITORY.\n\n\t\n\nOBJECT\n\ncr> SELECT name, type, settings FROM sys.repositories\n... ORDER BY name;\n+---------+------+---------------------------------------------------...--+\n| name    | type | settings                                               |\n+---------+------+---------------------------------------------------...--+\n| my_repo | fs   | {\"compress\": \"true\", \"location\": \"repo_location\", ...} |\n+---------+------+---------------------------------------------------...--+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nSensitive user account information will be masked and thus not visible to the user.\n\nSnapshots\n\nThe table sys.snapshots lists all existing snapshots in all configured repositories (see Snapshots).\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nname\n\n\t\n\nThe name of the snapshot\n\n\t\n\nTEXT\n\n\n\n\nrepository\n\n\t\n\nThe name of the repository that contains this snapshot.\n\n\t\n\nTEXT\n\n\n\n\nconcrete_indices\n\n\t\n\nContains the names of all tables and partitions that are contained in this snapshot how they are represented as ES index names.\n\n\t\n\nARRAY(TEXT)\n\n\n\n\ntables\n\n\t\n\nContains the fully qualified names of all tables within the snapshot.\n\n\t\n\nARRAY(TEXT)\n\n\n\n\ntable_partitions\n\n\t\n\nContains the table schema, table name and partition values of partitioned tables within the snapshot.\n\n\t\n\nARRAY(OBJECT)\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the creation of the snapshot started. Changes made after that are not stored in this snapshot.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nfinished\n\n\t\n\nThe point in time when the snapshot creation finished.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nstate\n\n\t\n\nThe current state of the snapshot. One of: IN_PROGRESS, SUCCESS, PARTIAL, or FAILED.\n\n\t\n\nTEXT\n\n\n\n\nversion\n\n\t\n\nAn internal version this snapshot was created with.\n\n\t\n\nTEXT\n\n\n\n\nfailures\n\n\t\n\nA list of failures that occurred while taking the snapshot. If taking the snapshot was successful this is empty.\n\n\t\n\nARRAY(TEXT)\n\nSnapshot/Restore operates on a per-shard basis. Hence, the state column indicates whether all (SUCCESS), some (PARTIAL), or no shards(FAILED) have been backed up. PARTIAL snapshots are the result of some primaries becoming unavailable while taking the snapshot when there are no replicas at hand (cluster state is RED). If there are replicas of the (now unavailable) primaries (cluster state is YELLOW) the snapshot succeeds and all shards are included (state SUCCESS). Building on a PARTIAL snapshot will include all primaries again.\n\nWarning\n\nIn case of a PARTIAL state another snapshot should be created in order to guarantee a full backup! Only SUCCESS includes all shards.\n\nThe concrete_indices column contains the names of all Elasticsearch indices that were stored in the snapshot. A normal CrateDB table maps to one Elasticsearch index, a partitioned table maps to one Elasticsearch index per partition. The mapping follows the following pattern:\n\nCrateDB table / partition name\n\n\t\n\nconcrete_indices entry\n\n\n\n\ndoc.my_table\n\n\t\n\nmy_table\n\n\n\n\nmy_schema.my_table\n\n\t\n\nmy_schema.my_table\n\n\n\n\ndoc.parted_table (value=null)\n\n\t\n\n.partitioned.my_table.0400\n\n\n\n\nmy_schema.parted_table (value=null)\n\n\t\n\nmy_schema..partitioned.my_table.0400\n\ncr> SELECT \"repository\", name, state, concrete_indices\n... FROM sys.snapshots order by \"repository\", name;\n+------------+-------------+---------+-----------------...-+\n| repository | name        | state   | concrete_indices    |\n+------------+-------------+---------+-----------------...-+\n| my_repo    | my_snapshot | SUCCESS | [...]               |\n+------------+-------------+---------+-----------------...-+\nSELECT 1 row in set (... sec)\n\nSnapshot Restore\n\nThe sys.snapshot_restore table contains information about the current state of snapshot restore operations.\n\npg_stats schema\n\nName\n\n\t\n\nDescription\n\n\t\n\nType\n\n\n\n\nid\n\n\t\n\nThe UUID of the restore snapshot operation.\n\n\t\n\nTEXT\n\n\n\n\nrepository\n\n\t\n\nThe name of the repository that contains the snapshot.\n\n\t\n\nTEXT\n\n\n\n\nsnapshot\n\n\t\n\nThe name of the snapshot.\n\n\t\n\nTEXT\n\n\n\n\nstate\n\n\t\n\nThe current state of the snapshot restore operations. Possible states are: INIT, STARTED, SUCCESS, and FAILURE.\n\n\t\n\nTEXT\n\n\n\n\nshards['table_schema']\n\n\t\n\nThe schema name of the table of the shard.\n\n\t\n\nTEXT\n\n\n\n\nshards['table_name']\n\n\t\n\nThe table name of the shard.\n\n\t\n\nTEXT\n\n\n\n\nshards['partition_ident']\n\n\t\n\nThe identifier of the partition of the shard. NULL if the is not partitioned.\n\n\t\n\nTEXT\n\n\n\n\nshards['shard_id']\n\n\t\n\nThe ID of the shard.\n\n\t\n\nINTEGER\n\n\n\n\nshards['state']\n\n\t\n\nThe restore state of the shard. Possible states are: INIT, STARTED, SUCCESS, and FAILURE.\n\n\t\n\nTEXT\n\nTo get more information about the restoring snapshots and shards one can join the sys.snapshot_restore with sys.shards or sys.snapshots table.\n\nSummits\n\nThe sys.summits table contains the information about the mountains in the Alps higher than 2000m. The mountain names from the table are also used to generate random nodes names.\n\nUsers\n\nThe sys.users table contains all existing database users in the cluster.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nname\n\n\t\n\nThe name of the database user.\n\n\t\n\nTEXT\n\n\n\n\nsuperuser\n\n\t\n\nFlag to indicate whether the user is a superuser.\n\n\t\n\nBOOLEAN\n\n\n\n\npassword\n\n\t\n\n******** if there is a password set or NULL if there is not.\n\n\t\n\nTEXT\n\nPrivileges\n\nThe sys.privileges table contains all privileges for each user and role of the database.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nclass\n\n\t\n\nThe class on which the privilege applies\n\n\t\n\nTEXT\n\n\n\n\ngrantee\n\n\t\n\nThe name of the database user or role for which the privilege is granted or denied\n\n\t\n\nTEXT\n\n\n\n\ngrantor\n\n\t\n\nThe name of the database user who granted or denied the privilege\n\n\t\n\nTEXT\n\n\n\n\nident\n\n\t\n\nThe name of the database object on which the privilege applies\n\n\t\n\nTEXT\n\n\n\n\nstate\n\n\t\n\nEither GRANT or DENY, which indicates if the user or role has been granted or denied access to the specific database object\n\n\t\n\nARRAY\n\n\n\n\ntype\n\n\t\n\nThe type of access for the specific database object\n\n\t\n\nTEXT\n\nAllocations\n\nThe sys.allocations table contains information about shards and their allocation state. The table contains:\n\nshards that are unassigned and why they are unassigned\n\nshards that are assigned but cannot be moved or rebalanced and why they remain on their current node\n\nIt can help to identify problems if shard allocations behave different than expected, e.g. when a shard stays unassigned or a shard does not move off a node.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\ntable_schema\n\n\t\n\nSchema name of the table of the shard.\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nTable name of the shard.\n\n\t\n\nTEXT\n\n\n\n\npartition_ident\n\n\t\n\nIdentifier of the partition of the shard. NULL if the table is not partitioned.\n\n\t\n\nTEXT\n\n\n\n\nshard_id\n\n\t\n\nID of the effected shard.\n\n\t\n\nINTEGER\n\n\n\n\nnode_id\n\n\t\n\nID of the node on which the shard resides. NULL if the shard is unassigned.\n\n\t\n\nTEXT\n\n\n\n\nprimary\n\n\t\n\nWhether the shard is a primary shard.\n\n\t\n\nBOOLEAN\n\n\n\n\ncurrent_state\n\n\t\n\nCurrent state of the shard. Possible states are: UNASSIGNED, INITIALIZING, STARTED, RELOCATING\n\n\t\n\nTEXT\n\n\n\n\nexplanation\n\n\t\n\nExplanation why the shard cannot be allocated, moved or rebalanced.\n\n\t\n\nTEXT\n\n\n\n\ndecisions\n\n\t\n\nA list of decisions that describe in detail why the shard in the current state.\n\n\t\n\nARRAY\n\n\n\n\ndecisions['node_id']\n\n\t\n\nID of the node of the decision.\n\n\t\n\nTEXT\n\n\n\n\ndecisions['node_name']\n\n\t\n\nName of the node of the decision.\n\n\t\n\nTEXT\n\n\n\n\ndecisions['explanations']\n\n\t\n\nDetailed list of human readable explanations why the node decided whether to allocate or rebalance the shard. Returns NULL if there is no need to rebalance the shard.\n\n\t\n\nARRAY\n\nNote\n\nThe sys.allocations table is subject to Shard table permissions.\n\nShard table permissions\n\nAccessing tables that return shards (sys.shards, sys.allocations) is subjected to the same privileges constraints as the other tables. Namely, in order to query them, the connected user needs to have the DQL privilege on that particular table, either directly or inherited from the SCHEMA or CLUSTER (for more information on privileges inheritance see Hierarchical Inheritance of Privileges).\n\nHowever, being able to query shard returning system tables will not allow the user to retrieve all the rows in the table, as they may contain information related to tables, which the connected user does not have any privileges for. The only rows that will be returned will be the ones the user is allowed to access.\n\nFor example, if the user john has any privilege on the doc.books table but no privilege at all on doc.locations, when john issues a SELECT * FROM sys.shards statement, the shards information related to the doc.locations table will not be returned.\n\nsys jobs tables permissions\n\nAccessing sys.jobs and sys.jobs_log tables is subjected to the same privileges constraints as other tables. To query them, the current user needs to have the DQL privilege on that particular table, either directly or inherited from the SCHEMA or CLUSTER.\n\nA user that doesn’t have superuser privileges is allowed to retrieve only their own job logs entries, while a user with superuser privileges has access to all.\n\npg_stats\n\nThe pg_stats table in the pg_catalog system schema contains statistical data about the contents of the CrateDB cluster.\n\nEntries are periodically created or updated in the interval configured with the stats.service.interval setting.\n\nAlternatively the statistics can also be updated using the ANALYZE command.\n\nThe table contains 1 entry per column for each table in the cluster which has been analyzed.\n\npg_stats schema\n\nName\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\nschemaname\n\n\t\n\ntext\n\n\t\n\nName of the schema containing the table.\n\n\n\n\ntablename\n\n\t\n\ntext\n\n\t\n\nName of the table.\n\n\n\n\nattname\n\n\t\n\ntext\n\n\t\n\nName of the column.\n\n\n\n\ninherited\n\n\t\n\nbool\n\n\t\n\nAlways false in CrateDB; For compatibility with PostgreSQL.\n\n\n\n\nnull_frac\n\n\t\n\nreal\n\n\t\n\nFraction of column entries that are null.\n\n\n\n\navg_width\n\n\t\n\ninteger\n\n\t\n\nAverage size in bytes of column’s entries.\n\n\n\n\nn_distinct\n\n\t\n\nreal\n\n\t\n\nAn approximation of the number of distinct values in a column.\n\n\n\n\nmost_common_vals\n\n\t\n\nstring[]\n\n\t\n\nA list of the most common values in the column. null if no values seem. more common than others.\n\n\n\n\nmost_common_freqs\n\n\t\n\nreal[]\n\n\t\n\nA list of the frequencies of the most common values. The size of the array always matches most_common_vals. If most_common_vals is null this is null as well.\n\n\n\n\nhistogram_bounds\n\n\t\n\nstring[]\n\n\t\n\nA list of values that divide the column’s values into groups of approximately equal population. The values in most_common_vals, if present, are omitted from this histogram calculation.\n\n\n\n\ncorrelation\n\n\t\n\nreal\n\n\t\n\nAlways 0.0. This column exists for PostgreSQL compatibility.\n\n\n\n\nmost_common_elems\n\n\t\n\nstring[]\n\n\t\n\nAlways null. Exists for PostgreSQL compatibility.\n\n\n\n\nmost_common_elem_freqs\n\n\t\n\nreal[]\n\n\t\n\nAlways null. Exists for PostgreSQL compatibility.\n\n\n\n\nelem_count_histogram\n\n\t\n\nreal[]\n\n\t\n\nAlways null. Exists for PostgreSQL compatibility.\n\nNote\n\nNot all data types support creating statistics. So some columns may not show up in the table.\n\npg_publication\n\nThe pg_publication table in the pg_catalog system schema contains all publications created in the cluster.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\noid\n\n\t\n\nRow identifier.\n\n\t\n\nINTEGER\n\n\n\n\npubname\n\n\t\n\nName of the publication.\n\n\t\n\nTEXT\n\n\n\n\npubowner\n\n\t\n\noid of the owner of the publication.\n\n\t\n\nINTEGER\n\n\n\n\npuballtables\n\n\t\n\nWhether this publication includes all tables in the cluster, including tables created in the future.\n\n\t\n\nBOOLEAN\n\n\n\n\npubinsert\n\n\t\n\nWhether INSERT operations are replicated for tables in the publication. Always true.\n\n\t\n\nBOOLEAN\n\n\n\n\npubupdate\n\n\t\n\nWhether UPDATE operations are replicated for tables in the publication. Always true.\n\n\t\n\nBOOLEAN\n\n\n\n\npubdelete\n\n\t\n\nWhether DELETE operations are replicated for tables in the publication. Always true.\n\n\t\n\nBOOLEAN\n\npg_publication_tables\n\nThe pg_publication_tables table in the pg_catalog system schema contains tables replicated by a publication.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\npubname\n\n\t\n\nName of the publication.\n\n\t\n\nTEXT\n\n\n\n\nschemaname\n\n\t\n\nName of the schema containing table.\n\n\t\n\nTEXT\n\n\n\n\ntablename\n\n\t\n\nName of the table.\n\n\t\n\nTEXT\n\npg_subscription\n\nThe pg_subscription table in the pg_catalog system schema contains all subscriptions created in the cluster.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\noid\n\n\t\n\nRow identifier.\n\n\t\n\nINTEGER\n\n\n\n\nsubdbid\n\n\t\n\nnoop value, always 0.\n\n\t\n\nINTEGER\n\n\n\n\nsubname\n\n\t\n\nName of the subscription.\n\n\t\n\nTEXT\n\n\n\n\nsubowner\n\n\t\n\noid of the owner of the subscription.\n\n\t\n\nINTEGER\n\n\n\n\nsubenabled\n\n\t\n\nWhether the subscription is enabled, always true.\n\n\t\n\nBOOLEAN\n\n\n\n\nsubbinary\n\n\t\n\nNoop value, always true.\n\n\t\n\nBOOLEAN\n\n\n\n\nsubstream\n\n\t\n\nNoop value, always true.\n\n\t\n\nBOOLEAN\n\n\n\n\nsubconninfo\n\n\t\n\nConnection string to the publishing cluster.\n\n\t\n\nTEXT\n\n\n\n\nsubslotname\n\n\t\n\nNoop value, always NULL.\n\n\t\n\nTEXT\n\n\n\n\nsubsynccommit\n\n\t\n\nNoop value, always NULL.\n\n\t\n\nTEXT\n\n\n\n\nsubpublications\n\n\t\n\nArray of subscribed publication names. These publications are defined in the publishing cluster.\n\n\t\n\nARRAY\n\npg_subscription_rel\n\nThe pg_subscription_rel table in the pg_catalog system schema contains the state for each replicated relation in each subscription.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nsrsubid\n\n\t\n\nReference to subscription.\n\n\t\n\nINTEGER\n\n\n\n\nsrrelid\n\n\t\n\nReference to relation.\n\n\t\n\nREGCLASS\n\n\n\n\nsrsubstate\n\n\t\n\nReplication state of the relation. State code: i - initializing; d - restoring; r - monitoring, i.e. waiting for new changes; e - error.\n\n\t\n\nTEXT\n\n\n\n\nsrsubstate_reason\n\n\t\n\nError message if there was a replication error for the relation or NULL.\n\n\t\n\nTEXT\n\n\n\n\nsrsublsn\n\n\t\n\nNoop value, always NULL.\n\n\t\n\nLONG"
  },
  {
    "title": "Optimistic Concurrency Control — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/general/occ.html",
    "html": "5.5\nOptimistic Concurrency Control\n\nTable of contents\n\nIntroduction\n\nOptimistic update\n\nOptimistic delete\n\nKnown limitations\n\nIntroduction\n\nEven though CrateDB does not support transactions, Optimistic Concurrency Control can be achieved by using the internal system columns _seq_no and _primary_term.\n\nEvery new primary shard row has an initial sequence number of 0. This value is increased by 1 on every insert, delete or update operation the primary shard executes. The primary term will be incremented when a shard is promoted to primary so the user can know if they are executing an update against the most up to date cluster configuration.\n\nIt’s possible to fetch the _seq_no and _primary_term by selecting them:\n\ncr> SELECT id, type, _seq_no, _primary_term FROM sensors ORDER BY 1;\n+-----+-------+---------+---------------+\n| id  | type  | _seq_no | _primary_term |\n+-----+-------+---------+---------------+\n| ID1 | DHT11 |       0 |             1 |\n| ID2 | DHT21 |       0 |             1 |\n+-----+-------+---------+---------------+\nSELECT 2 rows in set (... sec)\n\n\nThese _seq_no and _primary_term values can now be used on updates and deletes.\n\nNote\n\nOptimistic concurrency control only works using the = operator, checking for the exact _seq_no and _primary_term your update or delete is based on.\n\nOptimistic update\n\nQuerying for the correct _seq_no and _primary_term ensures that no concurrent update and cluster configuration change has taken place:\n\ncr> UPDATE sensors SET last_verification = '2020-01-10 09:40'\n... WHERE\n...   id = 'ID1'\n...   AND \"_seq_no\" = 0\n...   AND \"_primary_term\" = 1;\nUPDATE OK, 1 row affected (... sec)\n\n\nUpdating a row with a wrong or outdated sequence number or primary term will not execute the update and results in 0 affected rows:\n\ncr> UPDATE sensors SET last_verification = '2020-01-10 09:40'\n... WHERE\n...   id = 'ID1'\n...   AND \"_seq_no\" = 42\n...   AND \"_primary_term\" = 5;\nUPDATE OK, 0 rows affected (... sec)\n\nOptimistic delete\n\nThe same can be done when deleting a row:\n\ncr> DELETE FROM sensors WHERE id = 'ID2'\n...   AND \"_seq_no\" = 0\n...   AND \"_primary_term\" = 1;\nDELETE OK, 1 row affected (... sec)\n\nKnown limitations\n\nThe _seq_no and _primary_term columns can only be used when specifying the whole primary key in a query. For example, the query below is not possible with the database schema used for testing, because type is not declared as a primary key:\n\ncr> DELETE FROM sensors WHERE type = 'DHT11'\n...   AND \"_seq_no\" = 3\n...   AND \"_primary_term\" = 1;\nUnsupportedFeatureException[\"_seq_no\" and \"_primary_term\" columns can only be used\ntogether in the WHERE clause with equals comparisons and if there are also equals\ncomparisons on primary key columns]\n\n\nIn order to use the optimistic concurrency control mechanism, both the _seq_no and _primary_term columns need to be specified. It is not possible to only specify one of them. For example, the query below will result in an error:\n\ncr> DELETE FROM sensors WHERE id = 'ID1' AND \"_seq_no\" = 3;\nVersioningValidationException[\"_seq_no\" and \"_primary_term\" columns can only be used\ntogether in the WHERE clause with equals comparisons and if there are also equals\ncomparisons on primary key columns]\n\n\nNote\n\nBoth DELETE and UPDATE commands will return a row count of 0, if the given required version does not match the actual version of the relevant row."
  },
  {
    "title": "Blobs — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/general/blobs.html",
    "html": "5.5\nBlobs\n\nCrateDB includes support to store binary large objects. By utilizing CrateDB’s cluster features the files can be replicated and sharded just like regular data.\n\nTable of contents\n\nCreating a table for blobs\n\nCustom location for storing blob data\n\nGlobal by configuration\n\nPer blob table setting\n\nList\n\nAltering a blob table\n\nDeleting a blob table\n\nUsing blob tables\n\nUploading\n\nDownloading\n\nDeleting\n\nCreating a table for blobs\n\nBefore adding blobs a blob table must be created. Blob tables can be sharded. This makes it possible to distribute binaries over multiple nodes. Lets use the CrateDB shell crash to issue the SQL statement:\n\nsh$ crash -c \"create blob table myblobs clustered into 3 shards with (number_of_replicas=0)\"\nCREATE OK, 1 row affected (... sec)\n\n\nNow CrateDB is configured to allow blobs to be management under the /_blobs/myblobs endpoint.\n\nCustom location for storing blob data\n\nIt is possible to define a custom directory path for storing blob data which can be completely different than the normal data path. Best use case for this is storing normal data on a fast SSD and blob data on a large cheap spinning disk.\n\nThe custom blob data path can be set either globally by configuration or while creating a blob table. The path can be either absolute or relative and must be creatable/writable by the user CrateDB is running as. A relative path value is relative to CRATE_HOME.\n\nBlob data will be stored under this path with the following layout:\n\n/<blobs.path>/nodes/<NODE_NO>/indices/<INDEX_UUID>/<SHARD_ID>/blobs\n\nGlobal by configuration\n\nJust uncomment or add following entry at the CrateDB configuration in order to define a custom path globally for all blob tables:\n\nblobs.path: /path/to/blob/data\n\n\nAlso see Configuration.\n\nPer blob table setting\n\nIt is also possible to define a custom blob data path per table instead of global by configuration. Also per table setting take precedence over the configuration setting.\n\nSee CREATE BLOB TABLE for details.\n\nCreating a blob table with a custom blob data path:\n\nsh$ crash -c \"create blob table myblobs clustered into 3 shards with (blobs_path='/tmp/crate_blob_data')\" # doctest: +SKIP\nCREATE OK, 1 row affected (... sec)\n\nList\n\nTo list all blobs inside a blob table a SELECT statement can be used:\n\nsh$ crash -c \"select digest, last_modified from blob.myblobs\"\n+------------------------------------------+---------------+\n| digest                                   | last_modified |\n+------------------------------------------+---------------+\n| 4a756ca07e9487f482465a99e8286abc86ba4dc7 | ...           |\n+------------------------------------------+---------------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nTo query blob tables it is necessary to always specify the schema name blob.\n\nAltering a blob table\n\nThe number of replicas a blob table has can be changed using the ALTER BLOB TABLE clause:\n\nsh$ crash -c \"alter blob table myblobs set (number_of_replicas=0)\"\nALTER OK, -1 rows affected (... sec)\n\nDeleting a blob table\n\nBlob tables can be deleted similar to normal tables:\n\nsh$ crash -c \"drop blob table myblobs\"\nDROP OK, 1 row affected (... sec)\n\nUsing blob tables\n\nThe usage of Blob Tables is only supported using the HTTP/HTTPS protocol. This section describes how binaries can be stored, fetched and deleted.\n\nNote\n\nFor the reason of internal optimization any successful request could lead to a 307 Temporary Redirect response.\n\nUploading\n\nTo upload a blob the SHA1 hash of the blob has to be known upfront since this has to be used as the ID of the new blob. For this example we use a fancy Python one-liner to compute the SHA hash:\n\nsh$ python3 -c 'import hashlib;print(hashlib.sha1(\"contents\".encode(\"utf-8\")).hexdigest())'\n4a756ca07e9487f482465a99e8286abc86ba4dc7\n\n\nThe blob can now be uploaded by issuing a PUT request:\n\nsh$ curl -isSX PUT '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7' -d 'contents'\nHTTP/1.1 201 Created\ncontent-length: 0\n\n\nIf a blob already exists with the given hash a 409 Conflict is returned:\n\nsh$ curl -isSX PUT '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7' -d 'contents'\nHTTP/1.1 409 Conflict\ncontent-length: 0\n\nDownloading\n\nTo download a blob simply use a GET request:\n\nsh$ curl -sS '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7'\ncontents\n\n\nIf the blob doesn’t exist a 404 Not Found error is returned:\n\nsh$ curl -isS '127.0.0.1:4200/_blobs/myblobs/e5fa44f2b31c1fb553b6021e7360d07d5d91ff5e'\nHTTP/1.1 404 Not Found\ncontent-length: 0\n\n\nTo determine if a blob exists without downloading it, a HEAD request can be used:\n\nsh$ curl -sS -I '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7'\nHTTP/1.1 200 OK\ncontent-length: 8\naccept-ranges: bytes\nexpires: Thu, 31 Dec 2037 23:59:59 GMT\ncache-control: max-age=315360000\n\n\nNote\n\nThe cache headers for blobs are static and basically allows clients to cache the response forever since the blob is immutable.\n\nDeleting\n\nTo delete a blob simply use a DELETE request:\n\nsh$ curl -isS -XDELETE '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7'\nHTTP/1.1 204 No Content\n\n\nIf the blob doesn’t exist a 404 Not Found error is returned:\n\nsh$ curl -isS -XDELETE '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7'\nHTTP/1.1 404 Not Found\ncontent-length: 0\n"
  },
  {
    "title": "User-defined functions — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/general/user-defined-functions.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n5.5\nUser-defined functions\n\nTable of contents\n\nCREATE OR REPLACE\n\nSupported types\n\nOverloading\n\nDeterminism\n\nDROP FUNCTION\n\nSupported languages\n\nJavaScript\n\nJavaScript supported types\n\nWorking with NUMBERS\n\nCREATE OR REPLACE\n\nCrateDB supports user-defined functions. See CREATE FUNCTION for a full syntax description.\n\nCREATE FUNCTION defines a new function:\n\ncr> CREATE FUNCTION my_subtract_function(integer, integer)\n... RETURNS integer\n... LANGUAGE JAVASCRIPT\n... AS 'function my_subtract_function(a, b) { return a - b; }';\nCREATE OK, 1 row affected  (... sec)\n\ncr> SELECT doc.my_subtract_function(3, 1) AS col;\n+-----+\n| col |\n+-----+\n|   2 |\n+-----+\nSELECT 1 row in set (... sec)\n\n\nCREATE OR REPLACE FUNCTION will either create a new function or replace an existing function definition:\n\ncr> CREATE OR REPLACE FUNCTION log10(bigint)\n... RETURNS double precision\n... LANGUAGE JAVASCRIPT\n... AS 'function log10(a) {return Math.log(a)/Math.log(10); }';\nCREATE OK, 1 row affected  (... sec)\n\ncr> SELECT doc.log10(10) AS col;\n+-----+\n| col |\n+-----+\n| 1.0 |\n+-----+\nSELECT 1 row in set (... sec)\n\n\nIt is possible to use named function arguments in the function signature. For example, the calculate_distance function signature has two geo_point arguments named start and end:\n\ncr> CREATE OR REPLACE FUNCTION calculate_distance(\"start\" geo_point, \"end\" geo_point)\n... RETURNS real\n... LANGUAGE JAVASCRIPT\n... AS 'function calculate_distance(start, end) {\n...       return Math.sqrt(\n...            Math.pow(end[0] - start[0], 2),\n...            Math.pow(end[1] - start[1], 2));\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\n\nNote\n\nArgument names are used for query documentation purposes only. You cannot reference arguments by name in the function body.\n\nOptionally, a schema-qualified function name can be defined. If you omit the schema, the current session schema is used:\n\ncr> CREATE OR REPLACE FUNCTION my_schema.log10(bigint)\n... RETURNS double precision\n... LANGUAGE JAVASCRIPT\n... AS 'function log10(a) { return Math.log(a)/Math.log(10); }';\nCREATE OK, 1 row affected  (... sec)\n\n\nNote\n\nIn order to improve the PostgreSQL server compatibility CrateDB allows the creation of user defined functions against the pg_catalog schema. However, the creation of user defined functions against the read-only System information and Information schema schemas is prohibited.\n\nSupported types\n\nFunction arguments and return values can be any of the supported data types. The values passed into a function must strictly correspond to the specified argument data types.\n\nNote\n\nThe value returned by the function will be casted to the return type provided in the definition if required. An exception will be thrown if the cast is not successful.\n\nOverloading\n\nWithin a specific schema, you can overload functions by defining functions with the same name but a different set of arguments:\n\ncr> CREATE FUNCTION my_schema.my_multiply(integer, integer)\n... RETURNS integer\n... LANGUAGE JAVASCRIPT\n... AS 'function my_multiply(a, b) { return a * b; }';\nCREATE OK, 1 row affected  (... sec)\n\n\nThis would overload the my_multiply function with different argument types:\n\ncr> CREATE FUNCTION my_schema.my_multiply(bigint, bigint)\n... RETURNS bigint\n... LANGUAGE JAVASCRIPT\n... AS 'function my_multiply(a, b) { return a * b; }';\nCREATE OK, 1 row affected  (... sec)\n\n\nThis would overload the my_multiply function with more arguments:\n\ncr> CREATE FUNCTION my_schema.my_multiply(bigint, bigint, bigint)\n... RETURNS bigint\n... LANGUAGE JAVASCRIPT\n... AS 'function my_multiply(a, b, c) { return a * b * c; }';\nCREATE OK, 1 row affected  (... sec)\n\n\nCaution\n\nIt is considered bad practice to create functions that have the same name as the CrateDB built-in functions.\n\nNote\n\nIf you call a function without a schema name, CrateDB will look it up in the built-in functions first and only then in the user-defined functions available in the search_path.\n\nTherefore a built-in function with the same name as a user-defined function will hide the latter, even if it contains a different set of arguments. However, such functions can still be called if the schema name is explicitly provided.\n\nDeterminism\n\nCaution\n\nUser-defined functions need to be deterministic, meaning that they must always return the same result value when called with the same argument values, because CrateDB might cache the returned values and reuse the value if the function is called multiple times with the same arguments.\n\nDROP FUNCTION\n\nFunctions can be dropped like this:\n\ncr> DROP FUNCTION doc.log10(bigint);\nDROP OK, 1 row affected  (... sec)\n\n\nAdding IF EXISTS prevents from raising an error if the function doesn’t exist:\n\ncr> DROP FUNCTION IF EXISTS doc.log10(integer);\nDROP OK, 1 row affected  (... sec)\n\n\nOptionally, argument names can be specified within the drop statement:\n\ncr> DROP FUNCTION IF EXISTS doc.calculate_distance(start_point geo_point, end_point geo_point);\nDROP OK, 1 row affected  (... sec)\n\n\nOptionally, you can provide a schema:\n\ncr> DROP FUNCTION my_schema.log10(bigint);\nDROP OK, 1 row affected  (... sec)\n\nSupported languages\n\nCurrently, CrateDB only supports JavaScript for user-defined functions.\n\nJavaScript\n\nThe user defined function JavaScript is compatible with the ECMAScript 2019 specification.\n\nCrateDB uses the GraalVM JavaScript engine as a JavaScript (ECMAScript) language execution runtime. The GraalVM JavaScript engine is a Java application that works on the stock Java Virtual Machines (VMs). The interoperability between Java code (host language) and JavaScript user-defined functions (guest language) is guaranteed by the GraalVM Polyglot API.\n\nPlease note: CrateDB does not use the GraalVM JIT compiler as optimizing compiler. However, the stock host Java VM JIT compilers can JIT-compile, optimize, and execute the GraalVM JavaScript codebase to a certain extent.\n\nThe execution context for guest JavaScript is created with restricted privileges to allow for the safe execution of less trusted guest language code. The guest language application context for each user-defined function is created with default access modifiers, so any access to managed resources is denied. The only exception is the host language interoperability configuration which explicitly allows access to Java lists and arrays. Please refer to GraalVM Security Guide for more detailed information.\n\nAlso, even though user-defined functions implemented with ECMA-compliant JavaScript, objects that are normally accessible with a web browser (e.g. window, console, and so on) are not available.\n\nNote\n\nGraalVM treats objects provided to JavaScript user-defined functions as close as possible to their respective counterparts and therefore by default only a subset of prototype functions are available in user-defined functions. For CrateDB 4.6 and earlier the object prototype was disabled.\n\nPlease refer to the GraalVM JavaScript Compatibility FAQ to learn more about the compatibility.\n\nJavaScript supported types\n\nJavaScript functions can handle all CrateDB data types. However, for some return types the function output must correspond to the certain format.\n\nIf a function requires geo_point as a return type, then the JavaScript function must return a double precision array of size 2, WKT string or GeoJson object.\n\nHere is an example of a JavaScript function returning a double array:\n\ncr> CREATE FUNCTION rotate_point(point geo_point, angle real)\n... RETURNS geo_point\n... LANGUAGE JAVASCRIPT\n... AS 'function rotate_point(point, angle) {\n...       var cos = Math.cos(angle);\n...       var sin = Math.sin(angle);\n...       var x = cos * point[0] - sin * point[1];\n...       var y = sin * point[0] + cos * point[1];\n...       return [x, y];\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\n\nBelow is an example of a JavaScript function returning a WKT string, which will be cast to geo_point:\n\ncr> CREATE FUNCTION symmetric_point(point geo_point)\n... RETURNS geo_point\n... LANGUAGE JAVASCRIPT\n... AS 'function symmetric_point (point, angle) {\n...       var x = - point[0],\n...           y = - point[1];\n...       return \"POINT (\\\" + x + \\\", \\\" + y +\\\")\";\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\n\nSimilarly, if the function specifies the geo_shape return data type, then the JavaScript function should return a GeoJson object or WKT string:\n\ncr> CREATE FUNCTION line(\"start\" array(double precision), \"end\" array(double precision))\n... RETURNS object\n... LANGUAGE JAVASCRIPT\n... AS 'function line(start, end) {\n...        return { \"type\": \"LineString\", \"coordinates\" : [start_point, end_point] };\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\n\nNote\n\nIf the return value of the JavaScript function is undefined, it is converted to NULL.\n\nWorking with NUMBERS\n\nThe JavaScript engine interprets numbers as java.lang.Double, java.lang.Long, or java.lang.Integer, depending on the computation performed. In most cases, this is not an issue, since the return type of the JavaScript function will be cast to the return type specified in the CREATE FUNCTION statement, although cast might result in a loss of precision.\n\nHowever, when you try to cast DOUBLE PRECISION to TIMESTAMP WITH TIME ZONE, it will be interpreted as UTC seconds and will result in a wrong value:\n\ncr> CREATE FUNCTION utc(bigint, bigint, bigint)\n... RETURNS TIMESTAMP WITH TIME ZONE\n... LANGUAGE JAVASCRIPT\n... AS 'function utc(year, month, day) {\n...       return Date.UTC(year, month, day, 0, 0, 0);\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\ncr> SELECT date_format(utc(2016,04,6)) as epoque;\n+------------------------------+\n| epoque                       |\n+------------------------------+\n| 48314-07-22T00:00:00.000000Z |\n+------------------------------+\nSELECT 1 row in set (... sec)\n\n\nTo avoid this behavior, the numeric value should be divided by 1000 before it is returned:\n\ncr> CREATE FUNCTION utc(bigint, bigint, bigint)\n... RETURNS TIMESTAMP WITH TIME ZONE\n... LANGUAGE JAVASCRIPT\n... AS 'function utc(year, month, day) {\n...       return Date.UTC(year, month, day, 0, 0, 0)/1000;\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\ncr> SELECT date_format(utc(2016,04,6)) as epoque;\n+-----------------------------+\n| epoque                      |\n+-----------------------------+\n| 2016-05-06T00:00:00.000000Z |\n+-----------------------------+\nSELECT 1 row in set (... sec)\n"
  },
  {
    "title": "Built-in functions and operators — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/general/builtins/index.html",
    "html": "5.5\nBuilt-in functions and operators\n\nThis chapter provides an overview of built-in functions and operators.\n\nTable of contents\n\nScalar functions\nString functions\nDate and time functions\nGeo functions\nMathematical functions\nRegular expression functions\nArray functions\nObject functions\nConditional functions and expressions\nSystem information functions\nSpecial functions\nAggregation\nAggregate expressions\nAggregate functions\nLimitations\nArithmetic operators\nBit operators\nTable functions\nScalar functions\nempty_row( )\nunnest( array [ array , ] )\npg_catalog.generate_series(start, stop, [step])\npg_catalog.generate_subscripts(array, dim, [reverse])\nregexp_matches(source, pattern [, flags])\npg_catalog.pg_get_keywords()\ninformation_schema._pg_expandarray(array)\nComparison operators\nBasic operators\nWHERE clause operators\nArray comparisons\nIN (value [, ...])\nANY/SOME (array expression)\nALL (array_expression)\nSubquery expressions\nIN (subquery)\nANY/SOME (subquery)\nALL (subquery)\nWindow functions\nWindow function call\nWindow definition\nGeneral-purpose window functions\nAggregate window functions"
  },
  {
    "title": "Querying — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/general/dql/index.html",
    "html": "5.5\nQuerying\n\nThis section provides an overview of how to query CrateDB.\n\nSee Also\n\nGeneral use: Data definition\n\nGeneral use: Data manipulation\n\nTable of contents\n\nSelecting data\nIntroduction\nFROM clause\nJoins\nDISTINCT clause\nWHERE clause\nComparison operators\nArray comparisons\nEXISTS\nContainer data types\nAggregation\nWindow functions\nGROUP BY\nWITH Queries (Common Table Expressions)\nJoins\nCross joins\nInner joins\nOuter joins\nJoin conditions\nAvailable join algorithms\nLimitations\nUnion\nUnion All\nUnion Distinct\nUnion of object types\nUnion of different types\nRefresh\nIntroduction\nMultiple Table Refresh\nPartition Refresh\nFulltext search\nMATCH Predicate\nUsage\nSearching On Multiple Columns\nNegative Search\nFilter By _score\nGeo search\nIntroduction\nMATCH predicate\nExact queries"
  },
  {
    "title": "Data manipulation — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/general/dml.html",
    "html": "5.5\nData manipulation\n\nThis section provides an overview of how to manipulate data (e.g., inserting rows) with CrateDB.\n\nSee Also\n\nGeneral use: Data definition\n\nGeneral use: Querying\n\nTable of contents\n\nInserting data\n\nInserting data by query\n\nUpserts (ON CONFLICT DO UPDATE SET)\n\nUpdating data\n\nDeleting data\n\nImport and export\n\nImporting data\n\nExample\n\nDetailed error reporting\n\nExporting data\n\nInserting data\n\nInserting data to CrateDB is done by using the SQL INSERT statement.\n\nNote\n\nThe column list is always ordered based on the column position in the CREATE TABLE statement of the table. If the insert columns are omitted, the values in the VALUES clauses must correspond to the table columns in that order.\n\nInserting a row:\n\ncr> insert into locations (id, date, description, kind, name, position)\n... values (\n...   '14',\n...   '2013-09-12T21:43:59.000Z',\n...   'Blagulon Kappa is the planet to which the police are native.',\n...   'Planet',\n...   'Blagulon Kappa',\n...   7\n... );\nINSERT OK, 1 row affected (... sec)\n\n\nWhen inserting rows with the VALUES clause all data is validated in terms of data types compatibility and compliance with defined constraints, and if there are any issues an error message is returned and no rows are inserted.\n\nInserting multiple rows at once (aka. bulk insert) can be done by defining multiple values for the INSERT statement:\n\ncr> insert into locations (id, date, description, kind, name, position) values\n... (\n...   '16',\n...   '2013-09-14T21:43:59.000Z',\n...   'Blagulon Kappa II is the planet to which the police are native.',\n...   'Planet',\n...   'Blagulon Kappa II',\n...   19\n... ),\n... (\n...   '17',\n...   '2013-09-13T16:43:59.000Z',\n...   'Brontitall is a planet with a warm, rich atmosphere and no mountains.',\n...   'Planet',\n...   'Brontitall',\n...   10\n... );\nINSERT OK, 2 rows affected (... sec)\n\n\nWhen inserting into tables containing Generated columns or Base Columns having the Default clause specified, their values can be safely omitted. They are generated upon insert:\n\ncr> CREATE TABLE debit_card (\n...   owner text,\n...   num_part1 integer,\n...   num_part2 integer,\n...   check_sum integer GENERATED ALWAYS AS ((num_part1 + num_part2) * 42),\n...   \"user\" text DEFAULT 'crate'\n... );\nCREATE OK, 1 row affected (... sec)\n\ncr> insert into debit_card (owner, num_part1, num_part2) values\n... ('Zaphod Beeblebrox', 1234, 5678);\nINSERT OK, 1 row affected (... sec)\n\ncr> select * from debit_card;\n+-------------------+-----------+-----------+-----------+-------+\n| owner             | num_part1 | num_part2 | check_sum | user  |\n+-------------------+-----------+-----------+-----------+-------+\n| Zaphod Beeblebrox |      1234 |      5678 |    290304 | crate |\n+-------------------+-----------+-----------+-----------+-------+\nSELECT 1 row in set (... sec)\n\n\nFor Generated columns, if the value is given, it is validated against the generation clause of the column and the currently inserted row:\n\ncr> insert into debit_card (owner, num_part1, num_part2, check_sum) values\n... ('Arthur Dent', 9876, 5432, 642935);\nSQLParseException[Given value 642935 for generated column check_sum does not match calculation ((num_part1 + num_part2) * 42) = 642936]\n\nInserting data by query\n\nIt is possible to insert data using a query instead of values. Column data types of source and target table can differ as long as the values are castable. This gives the opportunity to restructure the tables data, renaming a field, changing a field’s data type or convert a normal table into a partitioned one.\n\nCaution\n\nWhen inserting data from a query, there is no error message returned when rows fail to be inserted, they are instead skipped, and the number of rows affected is decreased to reflect the actual number of rows for which the operation succeeded.\n\nExample of changing a field’s data type, in this case, changing the position data type from integer to smallint:\n\ncr> create table locations2 (\n...     id text primary key,\n...     name text,\n...     date timestamp with time zone,\n...     kind text,\n...     position smallint,\n...     description text\n... ) clustered by (id) into 2 shards with (number_of_replicas = 0);\nCREATE OK, 1 row affected (... sec)\n\ncr> insert into locations2 (id, name, date, kind, position, description)\n... (\n...     select id, name, date, kind, position, description\n...     from locations\n...     where position < 10\n... );\nINSERT OK, 14 rows affected (... sec)\n\n\nExample of creating a new partitioned table out of the locations table with data partitioned by year:\n\ncr> create table locations_parted (\n...     id text primary key,\n...     name text,\n...     year text primary key,\n...     date timestamp with time zone,\n...     kind text,\n...     position integer\n... ) clustered by (id) into 2 shards\n... partitioned by (year) with (number_of_replicas = 0);\nCREATE OK, 1 row affected (... sec)\n\ncr> insert into locations_parted (id, name, year, date, kind, position)\n... (\n...     select\n...         id,\n...         name,\n...         date_format('%Y', date),\n...         date,\n...         kind,\n...         position\n...     from locations\n... );\nINSERT OK, 16 rows affected (... sec)\n\n\nResulting partitions of the last insert by query:\n\ncr> select table_name, partition_ident, values, number_of_shards, number_of_replicas\n... from information_schema.table_partitions\n... where table_name = 'locations_parted'\n... order by partition_ident;\n+------------------+-----------------+------------------+------------------+--------------------+\n| table_name       | partition_ident | values           | number_of_shards | number_of_replicas |\n+------------------+-----------------+------------------+------------------+--------------------+\n| locations_parted | 042j2e9n74      | {\"year\": \"1979\"} |                2 |                  0 |\n| locations_parted | 042j4c1h6c      | {\"year\": \"2013\"} |                2 |                  0 |\n+------------------+-----------------+------------------+------------------+--------------------+\nSELECT 2 rows in set (... sec)\n\n\nNote\n\nlimit, offset and order by are not supported inside the query statement.\n\nUpserts (ON CONFLICT DO UPDATE SET)\n\nThe ON CONFLICT DO UPDATE SET clause is used to update the existing row if inserting is not possible because of a duplicate-key conflict if a document with the same PRIMARY KEY already exists. This is type of operation is commonly referred to as an upsert, being a combination of “update” and “insert”.\n\ncr> SELECT\n...     name,\n...     visits,\n...     extract(year from last_visit) AS last_visit\n... FROM uservisits ORDER BY NAME;\n+----------+--------+------------+\n| name     | visits | last_visit |\n+----------+--------+------------+\n| Ford     |      1 | 2013       |\n| Trillian |      3 | 2013       |\n+----------+--------+------------+\nSELECT 2 rows in set (... sec)\n\ncr> INSERT INTO uservisits (id, name, visits, last_visit) VALUES\n... (\n...     0,\n...     'Ford',\n...     1,\n...     '2015-01-12'\n... ) ON CONFLICT (id) DO UPDATE SET\n...     visits = visits + 1;\nINSERT OK, 1 row affected (... sec)\n\ncr> SELECT\n...     name,\n...     visits,\n...     extract(year from last_visit) AS last_visit\n... FROM uservisits WHERE id = 0;\n+------+--------+------------+\n| name | visits | last_visit |\n+------+--------+------------+\n| Ford |      2 | 2013       |\n+------+--------+------------+\nSELECT 1 row in set (... sec)\n\n\nIt’s possible to refer to values which would be inserted if no duplicate-key conflict occurred, by using the special excluded table. This table is especially useful in multiple-row inserts, to refer to the current rows values:\n\ncr> INSERT INTO uservisits (id, name, visits, last_visit) VALUES\n... (\n...     0,\n...     'Ford',\n...     2,\n...     '2016-01-13'\n... ),\n... (\n...     1,\n...     'Trillian',\n...     5,\n...     '2016-01-15'\n... ) ON CONFLICT (id) DO UPDATE SET\n...     visits = visits + excluded.visits,\n...     last_visit = excluded.last_visit;\nINSERT OK, 2 rows affected (... sec)\n\ncr> SELECT\n...     name,\n...     visits,\n...     extract(year from last_visit) AS last_visit\n... FROM uservisits ORDER BY name;\n+----------+--------+------------+\n| name     | visits | last_visit |\n+----------+--------+------------+\n| Ford     |      4 | 2016       |\n| Trillian |      8 | 2016       |\n+----------+--------+------------+\nSELECT 2 rows in set (... sec)\n\n\nThis can also be done when using a query instead of values:\n\ncr> CREATE TABLE uservisits2 (\n...   id integer primary key,\n...   name text,\n...   visits integer,\n...   last_visit timestamp with time zone\n... ) CLUSTERED BY (id) INTO 2 SHARDS WITH (number_of_replicas = 0);\nCREATE OK, 1 row affected (... sec)\n\ncr> INSERT INTO uservisits2 (id, name, visits, last_visit)\n... (\n...     SELECT id, name, visits, last_visit\n...     FROM uservisits\n... );\nINSERT OK, 2 rows affected (... sec)\n\ncr> INSERT INTO uservisits2 (id, name, visits, last_visit)\n... (\n...     SELECT id, name, visits, last_visit\n...     FROM uservisits\n... ) ON CONFLICT (id) DO UPDATE SET\n...     visits = visits + excluded.visits,\n...     last_visit = excluded.last_visit;\nINSERT OK, 2 rows affected (... sec)\n\ncr> SELECT\n...     name,\n...     visits,\n...     extract(year from last_visit) AS last_visit\n... FROM uservisits ORDER BY name;\n+----------+--------+------------+\n| name     | visits | last_visit |\n+----------+--------+------------+\n| Ford     |      4 | 2016       |\n| Trillian |      8 | 2016       |\n+----------+--------+------------+\nSELECT 2 rows in set (... sec)\n\n\nSee Also\n\nSQL syntax: ON CONFLICT DO UPDATE SET\n\nUpdating data\n\nIn order to update documents in CrateDB the SQL UPDATE statement can be used:\n\ncr> update locations set description = 'Updated description'\n... where name = 'Bartledan';\nUPDATE OK, 1 row affected (... sec)\n\n\nUpdating nested objects is also supported:\n\ncr> update locations set inhabitants['name'] = 'Human' where name = 'Bartledan';\nUPDATE OK, 1 row affected (... sec)\n\n\nIt’s also possible to reference a column within the expression, for example to increment a number like this:\n\ncr> update locations set position = position + 1 where position < 3;\nUPDATE OK, 6 rows affected (... sec)\n\n\nNote\n\nIf the same documents are updated concurrently an VersionConflictException might occur. CrateDB contains a retry logic that tries to resolve the conflict automatically.\n\nDeleting data\n\nDeleting rows in CrateDB is done using the SQL DELETE statement:\n\ncr> delete from locations where position > 3;\nDELETE OK, ... rows affected (... sec)\n\nImport and export\nImporting data\n\nUsing the COPY FROM statement, CrateDB nodes can import data from local files or files that are available over the network.\n\nThe supported data formats are JSON and CSV. The format is inferred from the file extension, if possible. Alternatively the format can also be provided as an option (see WITH). If the format is not provided and cannot be inferred from the file extension, it will be processed as JSON.\n\nJSON files must contain a single JSON object per line.\n\nExample JSON data:\n\n{\"id\": 1, \"quote\": \"Don't panic\"}\n{\"id\": 2, \"quote\": \"Ford, you're turning into a penguin. Stop it.\"}\n\n\nCSV files must contain a header with comma-separated values, which will be added as columns.\n\nExample CSV data:\n\nid,quote\n1,\"Don't panic\"\n2,\"Ford, you're turning into a penguin. Stop it.\"\n\n\nNote\n\nThe COPY FROM statement will convert and validate your data.\n\nValues for generated columns will be computed if the data does not contain them, otherwise they will be imported and validated\n\nFurthermore, column names in your data are considered case sensitive (as if they were quoted in a SQL statement).\n\nFor further information, including how to import data to Partitioned tables, take a look at the COPY FROM reference.\n\nExample\n\nHere’s an example statement:\n\ncr> COPY quotes FROM 'file:///tmp/import_data/quotes.json';\nCOPY OK, 3 rows affected (... sec)\n\n\nThis statement imports data from the /tmp/import_data/quotes.json file into a table named quotes.\n\nNote\n\nThe file you specify must be available on one of the CrateDB nodes. This statement will not work with files that are local to your client.\n\nFor the above statement, every node in the cluster will attempt to import data from a file located at /tmp/import_data/quotes.json relative to the crate process (i.e., if you are running CrateDB inside a container, the file must also be inside the container).\n\nIf you want to import data from a file that on your local computer using COPY FROM, you must first transfer the file to one of the CrateDB nodes.\n\nConsult the COPY FROM reference for additional information.\n\nIf you want to import all files inside the /tmp/import_data directory on every CrateDB node, you can use a wildcard, like so:\n\ncr> COPY quotes FROM '/tmp/import_data/*' WITH (bulk_size = 4);\nCOPY OK, 3 rows affected (... sec)\n\n\nThis wildcard can also be used to only match certain files in a directory:\n\ncr> COPY quotes FROM '/tmp/import_data/qu*.json';\nCOPY OK, 3 rows affected (... sec)\n\nDetailed error reporting\n\nIf the RETURN_SUMMARY clause is specified, a result set containing information about failures and successfully imported records is returned.\n\ncr> COPY locations FROM '/tmp/import_data/locations_with_failure/locations*.json' RETURN SUMMARY;\n +--...--+----------...--------+---------------+-------------+--------------------...-------------------------------------+\n | node  | uri                 | success_count | error_count | errors                                                     |\n +--...--+----------...--------+---------------+-------------+--------------------...-------------------------------------+\n | {...} | .../locations1.json |             6 |           0 | {}                                                         |\n | {...} | .../locations2.json |             5 |           2 | {\"Cannot cast value...{\"count\": ..., \"line_numbers\": ...}} |\n +--...--+----------...--------+---------------+-------------+--------------------...-------------------------------------+\n COPY 2 rows in set (... sec)\n\n\nIf an error happens while processing the URI in general, the error_count and success_count columns will contains NULL values to indicate that no records were processed.\n\ncr> COPY locations FROM '/tmp/import_data/not-existing.json' RETURN SUMMARY;\n +--...--+-----------...---------+---------------+-------------+------------------------...------------------------+\n | node  | uri                   | success_count | error_count | errors                                            |\n +--...--+-----------...---------+---------------+-------------+------------------------...------------------------+\n | {...} | .../not-existing.json |          NULL |        NULL | {\"...not-existing.json (...)\": {\"count\": 1, ...}} |\n +--...--+-----------...---------+---------------+-------------+------------------------...------------------------+\nCOPY 1 row in set (... sec)\n\n\nSee COPY FROM for more information.\n\nExporting data\n\nData can be exported using the COPY TO statement. Data is exported in a distributed way, meaning each node will export its own data.\n\nReplicated data is not exported. So every row of an exported table is stored only once.\n\nThis example shows how to export a given table into files named after the table and shard ID with gzip compression:\n\ncr> REFRESH TABLE quotes;\nREFRESH OK...\n\ncr> COPY quotes TO DIRECTORY '/tmp/' with (compression='gzip');\nCOPY OK, 3 rows affected ...\n\n\nInstead of exporting a whole table, rows can be filtered by an optional WHERE clause condition. This is useful if only a subset of the data needs to be exported:\n\ncr> COPY quotes WHERE match(quote_ft, 'time') TO DIRECTORY '/tmp/' WITH (compression='gzip');\nCOPY OK, 2 rows affected ...\n\n\nFor further details see COPY TO."
  },
  {
    "title": "Data definition — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/general/ddl/index.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n5.5\nData definition\n\nThis section provides an overview of how to create tables and perform other data-definition related operations with CrateDB.\n\nSee Also\n\nGeneral use: Data manipulation\n\nGeneral use: Querying\n\nTable of contents\n\nCreating tables\nTable definition\nTable configuration\nData types\nOverview\nPrimitive types\nContainer types\nFLOAT_VECTOR\nGeographic types\nType casting\nPostgreSQL compatibility\nSystem columns\nGenerated columns\nGeneration expressions\nLast modified dates\nPartitioning\nConstraints\nPrimary key\nNot null\nCheck\nStorage\nColumn store\nPartitioned tables\nIntroduction\nCreation\nInformation schema\nInsert\nUpdate\nDelete\nQuerying\nAlter\nLimitations\nConsistency notes related to concurrent DML statement\nSharding\nIntroduction\nNumber of shards\nRouting\nReplication\nTable configuration\nShard recovery\nUnderreplication\nShard allocation filtering\nSettings\nSpecial attributes\nColumn policy\nstrict\ndynamic\nFulltext indices\nIndex definition\nDisable indexing\nPlain index (default)\nCreating a custom analyzer\nExtending a built-in analyzer\nFulltext analyzers\nOverview\nBuilt-in analyzers\nBuilt-in tokenizers\nBuilt-in token filters\nBuilt-in char filter\nShow Create Table\nViews\nCreating views\nQuerying views\nDropping views\nAltering tables\nUpdating parameters\nAdding columns\nClosing and opening tables\nRenaming tables\nReroute shards"
  },
  {
    "title": "Environment variables — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/config/environment.html",
    "html": "5.5\nEnvironment variables\n\nCrateDB can be configured with some environment variables.\n\nThere are many different ways to set environment variables, depending on how CrateDB is being deployed.\n\nHere is a trivial example:\n\nsh$ export CRATE_HOME=/tmp/crate\nsh$ ./bin/crate\n\n\nHere, we set CRATE_HOME to /tmp/crate, export it so that sub-processes of the shell have access, and then start CrateDB.\n\nCrateDB supports two kinds of environment variables:\n\nApplication variables\n\nJava Virtual Machine (JVM) variables\n\nWe use “application” here to distinguish between CrateDB running as a Java application and the JVM itself, which runs CrateDB.\n\nTable of contents\n\nApplication variables\n\nJVM variables\n\nGeneral\n\nApplication variables\nCRATE_HOME: directory path\n\nThe home directory of the CrateDB installation.\n\nThis directory is used as the root for the configuration directory, data directory, log directory, and so on.\n\nIf you have installed CrateDB from a package, this variable should be set for you.\n\nIf you are installing manually, in most cases, this should be set to the directory from which you would normally execute bin/crate, i.e. the root directory of the basic installation.\n\nJVM variables\nGeneral\nCRATE_JAVA_OPTS: Java options\n\nThe Java options to use when running CrateDB.\n\nFor example, you could change the stack size like this:\n\nCRATE_JAVA_OPTS=-Xss500k\n\n\nSee Also\n\nFor more information about Java options, consult the documentation for Microsoft Windows or Unix-like operating systems.\n\nCRATE_HEAP_SIZE: size\n\nThe Java heap size, i.e. the amount of memory that can be used.\n\nYou can set the heap size to four gigabytes like this:\n\nCRATE_HEAP_SIZE=4g\n\n\nUse g for gigabytes or m for megabytes.\n\nSee Also\n\nAppropriate memory configuration is important for optimal performance.\n\nCRATE_HEAP_DUMP_PATH: file or directory path (default: varies)\n\nThe directory to be used for heap dumps in the case of a crash.\n\nIf a directory path is configured, new heap dumps will be written to that directory every time CrateDB crashes.\n\nIf a file path is configured (i.e. the last node of the path is non-existent or exists and is a file) CrateDB will overwrite that file with a heap dump every time it crashes.\n\nDefault values are as follows:\n\nFor a basic installation, the process working directory\n\nIf you have installed a CrateDB Linux package, /var/lib/crate\n\nWhen running CrateDB on Docker, /data/data\n\nWarning\n\nMake sure there is enough disk space available for heap dumps."
  },
  {
    "title": "Logging — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/config/logging.html",
    "html": "5.5\nLogging\n\nCrateDB supports two kinds of logging:\n\nApplication logging with Log4j\n\nJava Virtual Machine (JVM) garbage collection logging\n\nWe use “application” here to distinguish between CrateDB running as a Java application and the JVM itself, which runs CrateDB.\n\nBecause garbage collection logging is a native feature of the JVM it behaves differently and is configured differently.\n\nTable of contents\n\nApplication logging\n\nLog4j\n\nConfiguration file\n\nLog levels\n\nRun-time configuration\n\nJVM logging\n\nGarbage collection\n\nEnvironment variables\n\nApplication logging\nLog4j\n\nCrateDB uses Log4j.\n\nConfiguration file\n\nYou can configure Log4j with the log4j2.properties file in the CrateDB configuration directory.\n\nThe log4j2.properties file is formatted using YAML and simplifies Log4j configuration by allowing you to use the PropertyConfigurator but without having to tediously repeat the log4j prefix.\n\nHere’s one example:\n\nrootLogger.level = info\nrootLogger.appenderRef.console.ref = console\n\n# log query execution errors for easier debugging\nlogger.action.name = org.crate.action.sql\nlogger.action.level = debug\n\nappender.console.type = Console\nappender.console.name = console\nappender.console.layout.type = PatternLayout\nappender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c{1.}] %marker%m%n\n\n\nAnd here is a snippet of the generated properties ready for use with log4j. You get the point.\n\nSee Also\n\nConsult the PropertyConfigurator documentation or the configuration section of the Log4j documentation for more information.\n\nLog levels\n\nPossible log levels are the same as for Log4j, in order of increasing importance:\n\nTRACE\n\nDEBUG\n\nINFO\n\nWARN\n\nERROR\n\nLog levels must be provided as string literals in the SET statement.\n\nNote\n\nBe careful using the TRACE log level because it’s extremely verbose, can obscure other important log messages and even fill up entire data disks in some cases.\n\nRun-time configuration\n\nIt’s possible to set the log level of loggers at runtime using SET, like so:\n\nSET GLOBAL TRANSIENT \"logger.action\" = 'INFO';\n\n\nIn this example, the log level INFO is applied to the action logger.\n\nIn addition to being able to configure any of the standard loggers, you can configure the root (i.e. default) logger using logger._root.\n\nAs with any setting, you can inspect the current configuration by querying the sys.cluster table.\n\nTip\n\nRun-time logging configuration is particularly useful if you are debugging a problem and you want to increase the log level without restarting nodes.\n\nRun-time logging configuration is applied across the whole cluster, and overrides the start-up configuration defined in each respective log4j2.properties file.\n\nCaution\n\nThe RESET statement is supported but logging configuration is only reset when the whole cluster is restarted.\n\nJVM logging\n\nCrateDB exposes some native JVM logging functionality.\n\nGarbage collection\n\nCrateDB logs JVM garbage collection times using the built-in garbage collection logging of the JVM.\n\nEnvironment variables\n\nThe following environment variables can be used to configure garbage collection logging.\n\nCRATE_DISABLE_GC_LOGGING: boolean integer (default: 0)\n\nWhether to disable garbage collection logging.\n\nSet to 1 to disable.\n\nNote\n\nSince CrateDB 3.0, Garbage collection logging is enabled by default.\n\nCRATE_GC_LOG_DIR: path to logs directory (default: varies)\n\nThe log file directory.\n\nFor a basic installation, the logs directory in the CRATE_HOME directory is the default.\n\nIf you have installed a CrateDB Linux package, the default directory is /var/log/crate instead.\n\nCRATE_GC_LOG_SIZE: file size (default: 64m)\n\nMaximum file size of log files before they are rotated.\n\nCRATE_GC_LOG_FILES: number (default: 16)\n\nThe amount of files kept in rotation.\n\nCaution\n\nWith the default configuration of 16 rotated 64 megabyte log files, garbage collection logs will grow to occupy one gigabyte on disk.\n\nMake sure you have enough available disk space for configuration."
  },
  {
    "title": "Session settings — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/config/session.html",
    "html": "5.5\nSession settings\n\nTable of contents\n\nUsage\n\nSupported session settings\n\nSession settings only apply to the currently connected client session.\n\nUsage\n\nTo configure a modifiable session setting, use SET, for example:\n\nSET search_path TO myschema, doc;\n\n\nTo retrieve the current value of a session setting, use SHOW e.g:\n\nSHOW search_path;\n\n\nBesides using SHOW, it is also possible to use the current_setting scalar function.\n\nSupported session settings\nsearch_path\nDefault: pg_catalog, doc\nModifiable: yes\n\nThe list of schemas to be searched when a relation is referenced without a schema.\n\nCrateDB will try to resolve an unqualified relation name against the configured search_path by iterating over the configured schemas in the order they were declared. The first matching relation in the search_path is used. CrateDB will report an error if there is no match.\n\nNote\n\nThis setting mirrors the PostgreSQL search_path setting.\n\nSome PostgreSQL clients require access to various tables in the pg_catalog schema. Usually, this is to extract information about built-in data types or functions.\n\nCrateDB implements the system pg_catalog schema and it automatically includes it in the search_path before the configured schemas, unless it is already explicitly in the schema configuration.\n\napplication_name\nDefault: null\nModifiable: yes\n\nAn arbitrary application name that can be set to identify an application that connects to a CrateDB node.\n\nSome clients set this implicitly to their client name.\n\nstatement_timeout\nDefault: '0'\nModifiable: yes\n\nThe maximum duration of any statement before it gets cancelled. If 0 (the default), queries are allowed to run infinitely and don’t get cancelled automatically.\n\nThe value is an INTERVAL with a maximum of 2147483647 milliseconds. That’s roughly 24 days.\n\nmemory.operation_limit\nDefault: 0\nModifiable: yes\n\nThis is an experimental expert setting defining the maximal amount of memory in bytes that an individual operation can consume before triggering an error.\n\n0 means unlimited. In that case only the global circuit breaker limits apply.\n\nThere is no 1:1 mapping from SQL statement to operation. Some SQL statements have no corresponding operation. Other SQL statements can have more than one operation. You can use the sys.operations view to get some insights, but keep in mind that both, operations which are used to execute a query, and their name could change with any release, including hotfix releases.\n\nenable_hashjoin\nDefault: true\nModifiable: yes\n\nAn experimental setting which enables CrateDB to consider whether a JOIN operation should be evaluated using the HashJoin implementation instead of the Nested-Loops implementation.\n\nNote\n\nIt is not always possible or efficient to use the HashJoin implementation. Having this setting enabled, will only add the option of considering it, it will not guarantee it. See also the available join algorithms for more insights on this topic.\n\nerror_on_unknown_object_key\nDefault: true\nModifiable: yes\n\nThis setting controls the behaviour of querying unknown object keys to dynamic objects. CrateDB will throw an error by default if any of the queried object keys are unknown or will return a null if the setting is set to false.\n\ndatestyle\nDefault: ISO\nModifiable: yes\n\nShows the display format for date and time values. Only the ISO style is supported. Optionally provided pattern conventions for the order of date parts (Day, Month, Year) are ignored.\n\nNote\n\nThe session setting currently has no effect in CrateDB and exists for compatibility with PostgreSQL. Trying to set this to a date format style other than ISO will raise an exception.\n\nmax_index_keys\nDefault: 32\nModifiable: no\n\nShows the maximum number of index keys.\n\nNote\n\nThe session setting has no effect in CrateDB and exists for compatibility with PostgreSQL.\n\nmax_identifier_length\nDefault: 255\nModifiable: no\n\nShows the maximum length of identifiers in bytes.\n\nserver_version_num\nDefault: 100500\nModifiable: no\n\nShows the emulated PostgreSQL server version.\n\nserver_version\nDefault: 10.5\nModifiable: no\n\nShows the emulated PostgreSQL server version.\n\nstandard_conforming_strings\nDefault: on\nModifiable: no\n\nCauses '...' strings to treat backslashes literally.\n\noptimizer\nDefault: true\nModifiable: yes\n\nThis setting indicates whether a query optimizer rule is activated. The name of the query optimizer rule has to be provided as a suffix as part of the setting e.g. SET optimizer_rewrite_collect_to_get = false.\n\nNote\n\nThe optimizer setting is for advanced use only and can significantly impact the performance behavior of the queries.\n\noptimizer_eliminate_cross_join\nDefault: true\nModifiable: yes\n\nThis setting indicates if the cross join elimination rule of the optimizer rule is activated.\n\nWarning\n\nExperimental session settings might be removed in the future even in minor feature releases."
  },
  {
    "title": "Cluster-wide settings — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/config/cluster.html",
    "html": "5.5\nCluster-wide settings\n\nAll current applied cluster settings can be read by querying the sys.cluster.settings column. Most cluster settings can be changed at runtime. This is documented at each setting.\n\nTable of contents\n\nNon-runtime cluster-wide settings\n\nCollecting stats\n\nShard limits\n\nUsage data collector\n\nGraceful stop\n\nBulk operations\n\nDiscovery\n\nUnicast host discovery\n\nDiscovery via DNS\n\nDiscovery on Amazon EC2\n\nRouting allocation\n\nShard balancing\n\nAttribute-based shard allocation\n\nCluster-wide attribute awareness\n\nCluster-wide attribute filtering\n\nDisk-based shard allocation\n\nRecovery\n\nMemory management\n\nQuery circuit breaker\n\nRequest circuit breaker\n\nAccounting circuit breaker\n\nStats circuit breakers\n\nTotal circuit breaker\n\nThread pools\n\nSettings for fixed thread pools\n\nOverload Protection\n\nMetadata\n\nMetadata gateway\n\nLogical Replication\n\nNon-runtime cluster-wide settings\n\nCluster wide settings which cannot be changed at runtime need to be specified in the configuration of each node in the cluster.\n\nCaution\n\nCluster settings specified via node configurations are required to be exactly the same on every node in the cluster for proper operation of the cluster.\n\nCollecting stats\nstats.enabled\nDefault: true\nRuntime: yes\n\nA boolean indicating whether or not to collect statistical information about the cluster.\n\nCaution\n\nThe collection of statistical information incurs a slight performance penalty, as details about every job and operation across the cluster will cause data to be inserted into the corresponding system tables.\n\nstats.jobs_log_size\nDefault: 10000\nRuntime: yes\n\nThe maximum number of job records kept to be kept in the sys.jobs_log table on each node.\n\nA job record corresponds to a single SQL statement to be executed on the cluster. These records are used for performance analytics. A larger job log produces more comprehensive stats, but uses more RAM.\n\nOlder job records are deleted as newer records are added, once the limit is reached.\n\nSetting this value to 0 disables collecting job information.\n\nstats.jobs_log_expiration\nDefault: 0s (disabled)\nRuntime: yes\n\nThe job record expiry time in seconds.\n\nJob records in the sys.jobs_log table are periodically cleared if they are older than the expiry time. This setting overrides stats.jobs_log_size.\n\nIf the value is set to 0, time based log entry eviction is disabled.\n\nNote\n\nIf both the stats.operations_log_size and stats.operations_log_expiration settings are disabled, jobs will not be recorded.\n\nstats.jobs_log_filter\nDefault: true (Include everything)\nRuntime: yes\n\nAn expression to determine if a job should be recorded into sys.jobs_log. The expression must evaluate to a boolean. If it evaluates to true the statement will show up in sys.jobs_log until it’s evicted due to one of the other rules. (expiration or size limit reached).\n\nThe expression may reference all columns contained in sys.jobs_log. A common use case is to include only jobs that took a certain amount of time to execute:\n\ncr> SET GLOBAL \"stats.jobs_log_filter\" = 'ended - started > 100';\n\nstats.jobs_log_persistent_filter\nDefault: false (Include nothing)\nRuntime: yes\n\nAn expression to determine if a job should also be recorded to the regular CrateDB log. Entries that match this filter will be logged under the StatementLog logger with the INFO level.\n\nThis is similar to stats.jobs_log_filter except that these entries are persisted to the log file. This should be used with caution and shouldn’t be set to an expression that matches many queries as the logging operation will block on IO and can therefore affect performance.\n\nA common use case is to use this for slow query logging.\n\nstats.operations_log_size\nDefault: 10000\nRuntime: yes\n\nThe maximum number of operations records to be kept in the sys.operations_log table on each node.\n\nA job consists of one or more individual operations. Operations records are used for performance analytics. A larger operations log produces more comprehensive stats, but uses more RAM.\n\nOlder operations records are deleted as newer records are added, once the limit is reached.\n\nSetting this value to 0 disables collecting operations information.\n\nstats.operations_log_expiration\nDefault: 0s (disabled)\nRuntime: yes\n\nEntries of sys.operations_log are cleared by a periodically job when they are older than the specified expire time. This setting overrides stats.operations_log_size. If the value is set to 0 the time based log entry eviction is disabled.\n\nNote\n\nIf both settings stats.operations_log_size and stats.operations_log_expiration are disabled, no job information will be collected.\n\nstats.service.interval\nDefault: 24h\nRuntime: yes\n\nDefines the refresh interval to refresh tables statistics used to produce optimal query execution plans.\n\nThis field expects a time value either as a bigint or double precision or alternatively as a string literal with a time suffix (ms, s, m, h, d, w).\n\nIf the value provided is 0 then the refresh is disabled.\n\nCaution\n\nUsing a very small value can cause a high load on the cluster.\n\nstats.service.max_bytes_per_sec\nDefault: 40mb\nRuntime: yes\n\nSpecifies the maximum number of bytes per second that can be read on data nodes to collect statistics. If this is set to a positive number, the underlying I/O operations of the ANALYZE statement are throttled.\n\nIf the value provided is 0 then the throttling is disabled.\n\nShard limits\ncluster.max_shards_per_node\nDefault: 1000\nRuntime: yes\n\nThe maximum amount of shards per node.\n\nAny operations that would result in the creation of additional shard copies that would exceed this limit are rejected.\n\nFor example. If you have 999 shards in the current cluster and you try to create a new table, the create table operation will fail.\n\nSimilarly, if a write operation would lead to the creation of a new partition, the statement will fail.\n\nEach shard on a node requires some memory and increases the size of the cluster state. Having too many shards per node will impact the clusters stability and it is therefore discouraged to raise the limit above 1000.\n\nNote\n\nThe maximum amount of shards per node setting is also used for the Maximum shards per node check.\n\nUsage data collector\n\nThe settings of the Usage Data Collector are read-only and cannot be set during runtime. Please refer to Usage Data Collector to get further information about its usage.\n\nudc.enabled\nDefault: true\nRuntime: no\n\ntrue: Enables the Usage Data Collector.\n\nfalse: Disables the Usage Data Collector.\n\nudc.initial_delay\nDefault: 10m\nRuntime: no\n\nThe delay for first ping after start-up.\n\nThis field expects a time value either as a bigint or double precision or alternatively as a string literal with a time suffix (ms, s, m, h, d, w).\n\nudc.interval\nDefault: 24h\nRuntime: no\n\nThe interval a UDC ping is sent.\n\nThis field expects a time value either as a bigint or\n\ndouble precision or alternatively as a string literal with a time suffix (ms, s, m, h, d, w).\n\nudc.url\nDefault: https://udc.crate.io\nRuntime: no\n\nThe URL the ping is sent to.\n\nGraceful stop\n\nBy default, when the CrateDB process stops it simply shuts down, possibly making some shards unavailable which leads to a red cluster state and lets some queries fail that required the now unavailable shards. In order to safely shutdown a CrateDB node, the graceful stop procedure can be used.\n\nThe following cluster settings can be used to change the shutdown behaviour of nodes of the cluster:\n\ncluster.graceful_stop.min_availability\nDefault: primaries\nRuntime: yes\nAllowed values: none | primaries | full\n\nnone: No minimum data availability is required. The node may shut down even if records are missing after shutdown.\n\nprimaries: At least all primary shards need to be available after the node has shut down. Replicas may be missing.\n\nfull: All records and all replicas need to be available after the node has shut down. Data availability is full.\n\nNote\n\nThis option is ignored if there is only 1 node in a cluster!\n\ncluster.graceful_stop.timeout\nDefault: 2h\nRuntime: yes\n\nDefines the maximum waiting time in milliseconds for the reallocation process to finish. The force setting will define the behaviour when the shutdown process runs into this timeout.\n\nThe timeout expects a time value either as a bigint or double precision or alternatively as a string literal with a time suffix (ms, s, m, h, d, w).\n\ncluster.graceful_stop.force\nDefault: false\nRuntime: yes\n\nDefines whether graceful stop should force stopping of the node if it runs into the timeout which is specified with the cluster.graceful_stop.timeout setting.\n\nBulk operations\n\nSQL DML Statements involving a huge amount of rows like COPY FROM, INSERT or UPDATE can take an enormous amount of time and resources. The following settings change the behaviour of those queries.\n\nbulk.request_timeout\nDefault: 1m\nRuntime: yes\n\nDefines the timeout of internal shard-based requests involved in the execution of SQL DML Statements over a huge amount of rows.\n\nDiscovery\n\nData sharding and work splitting are at the core of CrateDB. This is how we manage to execute very fast queries over incredibly large datasets. In order for multiple CrateDB nodes to work together a cluster needs to be formed. The process of finding other nodes with which to form a cluster is called discovery. Discovery runs when a CrateDB node starts and when a node is not able to reach the master node and continues until a master node is found or a new master node is elected.\n\ndiscovery.seed_hosts\nDefault: 127.0.0.1\nRuntime: no\n\nIn order to form a cluster with CrateDB instances running on other nodes a list of seed master-eligible nodes needs to be provided. This setting should normally contain the addresses of all the master-eligible nodes in the cluster. In order to seed the discovery process the nodes listed here must be live and contactable. This setting contains either an array of hosts or a comma-delimited string. By default a node will bind to the available loopback and scan for local ports between 4300 and 4400 to try to connect to other nodes running on the same server. This default behaviour provides local auto clustering without any configuration. Each value should be in the form of host:port or host (where port defaults to the setting transport.tcp.port).\n\nNote\n\nIPv6 hosts must be bracketed.\n\ncluster.initial_master_nodes\nDefault: not set\nRuntime: no\n\nContains a list of node names, full-qualified hostnames or IP addresses of the master-eligible nodes which will vote in the very first election of a cluster that’s bootstrapping for the first time. By default this is not set, meaning it expects this node to join an already formed cluster. In development mode, with no discovery settings configured, this step is performed by the nodes themselves, but this auto-bootstrapping is designed to aim development and is not safe for production. In production you must explicitly list the names or IP addresses of the master-eligible nodes whose votes should be counted in the very first election.\n\ndiscovery.type\nDefault: zen\nRuntime: no\nAllowed values: zen | single-node\n\nSpecifies whether CrateDB should form a multiple-node cluster. By default, CrateDB discovers other nodes when forming a cluster and allows other nodes to join the cluster later. If discovery.type is set to single-node, CrateDB forms a single-node cluster and the node won’t join any other clusters. This can be useful for testing. It is not recommend to use this for production setups. The single-node mode also skips bootstrap checks.\n\nCaution\n\nIf a node is started without any initial_master_nodes or a discovery_type set to single-node (e.g., the default configuration), it will never join a cluster even if the configuration is subsequently changed.\n\nIt is possible to force the node to forget its current cluster state by using the crate-node CLI tool. However, be aware that this may result in data loss.\n\nUnicast host discovery\n\nAs described above, CrateDB has built-in support for statically specifying a list of addresses that will act as the seed nodes in the discovery process using the discovery.seed_hosts setting.\n\nCrateDB also has support for several different mechanisms of seed nodes discovery. Currently there are two other discovery types: via DNS and via EC2 API.\n\nWhen a node starts up with one of these discovery types enabled, it performs a lookup using the settings for the specified mechanism listed below. The hosts and ports retrieved from the mechanism will be used to generate a list of unicast hosts for node discovery.\n\nThe same lookup is also performed by all nodes in a cluster whenever the master is re-elected (see Cluster Meta Data).\n\ndiscovery.seed_providers\nDefault: not set\nRuntime: no\nAllowed values: srv, ec2\n\nSee also: Discovery.\n\nDiscovery via DNS\n\nCrate has built-in support for discovery via DNS. To enable DNS discovery the discovery.seed_providers setting needs to be set to srv.\n\nThe order of the unicast hosts is defined by the priority, weight and name of each host defined in the SRV record. For example:\n\n_crate._srv.example.com. 3600 IN SRV 2 20 4300 crate1.example.com.\n_crate._srv.example.com. 3600 IN SRV 1 10 4300 crate2.example.com.\n_crate._srv.example.com. 3600 IN SRV 2 10 4300 crate3.example.com.\n\n\nwould result in a list of discovery nodes ordered like:\n\ncrate2.example.com:4300, crate3.example.com:4300, crate1.example.com:4300\n\ndiscovery.srv.query\nRuntime: no\n\nThe DNS query that is used to look up SRV records, usually in the format _service._protocol.fqdn If not set, the service discovery will not be able to look up any SRV records.\n\ndiscovery.srv.resolver\nRuntime: no\n\nThe hostname or IP of the DNS server used to resolve DNS records. If this is not set, or the specified hostname/IP is not resolvable, the default (system) resolver is used.\n\nOptionally a custom port can be specified using the format hostname:port.\n\nDiscovery on Amazon EC2\n\nCrateDB has built-in support for discovery via the EC2 API. To enable EC2 discovery the discovery.seed_providers settings needs to be set to ec2.\n\ndiscovery.ec2.access_key\nRuntime: no\n\nThe access key ID to identify the API calls.\n\ndiscovery.ec2.secret_key\nRuntime: no\n\nThe secret key to identify the API calls.\n\nFollowing settings control the discovery:\n\ndiscovery.ec2.groups\nRuntime: no\n\nA list of security groups; either by ID or name. Only instances with the given group will be used for unicast host discovery.\n\ndiscovery.ec2.any_group\nDefault: true\nRuntime: no\n\nDefines whether all (false) or just any (true) security group must be present for the instance to be used for discovery.\n\ndiscovery.ec2.host_type\nDefault: private_ip\nRuntime: no\nAllowed values: private_ip, public_ip, private_dns, public_dns\n\nDefines via which host type to communicate with other instances.\n\ndiscovery.ec2.availability_zones\nRuntime: no\n\nA list of availability zones. Only instances within the given availability zone will be used for unicast host discovery.\n\ndiscovery.ec2.tag.<name>\nRuntime: no\n\nEC2 instances for discovery can also be filtered by tags using the discovery.ec2.tag. prefix plus the tag name.\n\nE.g. to filter instances that have the environment tags with the value dev your setting will look like: discovery.ec2.tag.environment: dev.\n\ndiscovery.ec2.endpoint\nRuntime: no\n\nIf you have your own compatible implementation of the EC2 API service you can set the endpoint that should be used.\n\nRouting allocation\ncluster.routing.allocation.enable\nDefault: all\nRuntime: yes\nAllowed values: all | none | primaries | new_primaries\n\nall allows all shard allocations, the cluster can allocate all kinds of shards.\n\nnone allows no shard allocations at all. No shard will be moved or created.\n\nprimaries only primaries can be moved or created. This includes existing primary shards.\n\nnew_primaries allows allocations for new primary shards only. This means that for example a newly added node will not allocate any replicas. However it is still possible to allocate new primary shards for new indices. Whenever you want to perform a zero downtime upgrade of your cluster you need to set this value before gracefully stopping the first node and reset it to all after starting the last updated node.\n\nNote\n\nThis allocation setting has no effect on the recovery of primary shards! Even when cluster.routing.allocation.enable is set to none, nodes will recover their unassigned local primary shards immediately after restart.\n\ncluster.routing.rebalance.enable\nDefault: all\nRuntime: yes\nAllowed values: all | none | primaries | replicas\n\nEnables or disables rebalancing for different types of shards:\n\nall allows shard rebalancing for all types of shards.\n\nnone disables shard rebalancing for any types.\n\nprimaries allows shard rebalancing only for primary shards.\n\nreplicas allows shard rebalancing only for replica shards.\n\ncluster.routing.allocation.allow_rebalance\nDefault: indices_all_active\nRuntime: yes\nAllowed values: always | indices_primary_active | indices_all_active\n\nDefines when rebalancing will happen based on the total state of all the indices shards in the cluster.\n\nDefaults to indices_all_active to reduce chatter during initial recovery.\n\ncluster.routing.allocation.cluster_concurrent_rebalance\nDefault: 2\nRuntime: yes\n\nDefines how many concurrent rebalancing tasks are allowed across all nodes.\n\ncluster.routing.allocation.node_initial_primaries_recoveries\nDefault: 4\nRuntime: yes\n\nDefines how many concurrent primary shard recoveries are allowed on a node.\n\nSince primary recoveries use data that is already on disk (as opposed to inter-node recoveries), recovery should be fast and so this setting can be higher than node_concurrent_recoveries.\n\ncluster.routing.allocation.node_concurrent_recoveries\nDefault: 2\nRuntime: yes\n\nDefines how many concurrent recoveries are allowed on a node.\n\nShard balancing\n\nYou can configure how CrateDB attempts to balance shards across a cluster by specifying one or more property weights. CrateDB will consider a cluster to be balanced when no further allowed action can bring the weighted properties of each node closer together.\n\nNote\n\nBalancing may be restricted by other settings (e.g., attribute-based and disk-based shard allocation).\n\ncluster.routing.allocation.balance.shard\nDefault: 0.45f\nRuntime: yes\n\nDefines the weight factor for shards allocated on a node (float). Raising this raises the tendency to equalize the number of shards across all nodes in the cluster.\n\ncluster.routing.allocation.balance.index\nDefault: 0.55f\nRuntime: yes\n\nDefines a factor to the number of shards per index allocated on a specific node (float). Increasing this value raises the tendency to equalize the number of shards per index across all nodes in the cluster.\n\ncluster.routing.allocation.balance.threshold\nDefault: 1.0f\nRuntime: yes\n\nMinimal optimization value of operations that should be performed (non negative float). Increasing this value will cause the cluster to be less aggressive about optimising the shard balance.\n\nAttribute-based shard allocation\n\nYou can control how shards are allocated to specific nodes by setting custom attributes on each node (e.g., server rack ID or node availability zone). After doing this, you can define cluster-wide attribute awareness and then configure cluster-wide attribute filtering.\n\nSee Also\n\nFor an in-depth example of using custom node attributes, check out the multi-zone setup how-to guide.\n\nCluster-wide attribute awareness\n\nTo make use of custom attributes for attribute-based shard allocation, you must configure cluster-wide attribute awareness.\n\ncluster.routing.allocation.awareness.attributes\nRuntime: no\n\nYou may define custom node attributes which can then be used to do awareness based on the allocation of a shard and its replicas.\n\nFor example, let’s say we want to use an attribute named rack_id. We start two nodes with node.attr.rack_id set to rack_one. Then we create a single table with five shards and one replica. The table will be fully deployed on the current nodes (five shards and one replica each, making a total of 10 shards).\n\nNow, if we start two more nodes with node.attr.rack_id set to rack_two, CrateDB will relocate shards to even out the number of shards across the nodes. However, a shard and its replica will not be allocated to nodes sharing the same rack_id value.\n\nThe awareness.attributes setting supports using several values.\n\ncluster.routing.allocation.awareness.force.*.values\nRuntime: no\n\nAttributes on which shard allocation will be forced. Here, * is a placeholder for the awareness attribute, which can be configured using the cluster.routing.allocation.awareness.attributes setting.\n\nFor example, let’s say we configured forced shard allocation for an awareness attribute named zone with values set to zone1, zone2. Start two nodes with node.attr.zone set to zone1. Then, create a table with five shards and one replica. The table will be created, but only five shards will be allocated (with no replicas). The replicas will only be allocated when we start one or more nodes with node.attr.zone set to zone2.\n\nCluster-wide attribute filtering\n\nTo control how CrateDB uses custom attributes for attribute-based shard allocation, you must configure cluster-wide attribute filtering.\n\nNote\n\nCrateDB will retroactively enforce filter definitions. If a new filter would prevent newly created matching shards from being allocated to a node, CrateDB would also move any existing matching shards away from that node.\n\ncluster.routing.allocation.include.*\nRuntime: yes\n\nOnly allocate shards on nodes where at least one of the specified values matches the attribute.\n\nFor example:\n\ncluster.routing.allocation.include.zone: \"zone1,zone2\"`\n\ncluster.routing.allocation.exclude.*\nRuntime: yes\n\nOnly allocate shards on nodes where none of the specified values matches the attribute.\n\nFor example:\n\ncluster.routing.allocation.exclude.zone: \"zone1\"\n\ncluster.routing.allocation.require.*\nRuntime: yes\n\nUsed to specify a number of rules, which all of them must match for a node in order to allocate a shard on it.\n\nDisk-based shard allocation\ncluster.routing.allocation.disk.threshold_enabled\nDefault: true\nRuntime: yes\n\nPrevent shard allocation on nodes depending of the disk usage.\n\ncluster.routing.allocation.disk.watermark.low\nDefault: 85%\nRuntime: yes\n\nDefines the lower disk threshold limit for shard allocations. New shards will not be allocated on nodes with disk usage greater than this value. It can also be set to an absolute bytes value (like e.g. 500mb) to prevent the cluster from allocating new shards on node with less free disk space than this value.\n\ncluster.routing.allocation.disk.watermark.high\nDefault: 90%\nRuntime: yes\n\nDefines the higher disk threshold limit for shard allocations. The cluster will attempt to relocate existing shards to another node if the disk usage on a node rises above this value. It can also be set to an absolute bytes value (like e.g. 500mb) to relocate shards from nodes with less free disk space than this value.\n\ncluster.routing.allocation.disk.watermark.flood_stage\nDefault: 95%\nRuntime: yes\n\nDefines the threshold on which CrateDB enforces a read-only block on every index that has at least one shard allocated on a node with at least one disk exceeding the flood stage.\n\nNote\n\nblocks.read_only_allow_delete setting is automatically reset to FALSE for the tables if the disk space is freed and the threshold is undershot.\n\ncluster.routing.allocation.disk.watermark settings may be defined as percentages or bytes values. However, it is not possible to mix the value types.\n\nBy default, the cluster will retrieve information about the disk usage of the nodes every 30 seconds. This can also be changed by setting the cluster.info.update.interval setting.\n\nNote\n\nThe watermark settings are also used for the Routing allocation disk watermark low and Routing allocation disk watermark high node check.\n\nSetting cluster.routing.allocation.disk.threshold_enabled to false will disable the allocation decider, but the node checks will still be active and warn users about running low on disk space.\n\ncluster.routing.allocation.total_shards_per_node\nDefault: -1\nRuntime: yes\n\nLimits the number of shards that can be allocated per node. A value of -1 means unlimited.\n\nSetting this to 1000, for example, will prevent CrateDB from assigning more than 1000 shards per node. A node with 1000 shards would be excluded from allocation decisions and CrateDB would attempt to allocate shards to other nodes, or leave shards unassigned if no suitable node can be found.\n\nRecovery\nindices.recovery.max_bytes_per_sec\nDefault: 40mb\nRuntime: yes\n\nSpecifies the maximum number of bytes that can be transferred during shard recovery per seconds. Limiting can be disabled by setting it to 0. This setting allows to control the network usage of the recovery process. Higher values may result in higher network utilization, but also faster recovery process.\n\nindices.recovery.retry_delay_state_sync\nDefault: 500ms\nRuntime: yes\n\nDefines the time to wait after an issue caused by cluster state syncing before retrying to recover.\n\nindices.recovery.retry_delay_network\nDefault: 5s\nRuntime: yes\n\nDefines the time to wait after an issue caused by the network before retrying to recover.\n\nindices.recovery.internal_action_timeout\nDefault: 15m\nRuntime: yes\n\nDefines the timeout for internal requests made as part of the recovery.\n\nindices.recovery.internal_action_long_timeout\nDefault: 30m\nRuntime: yes\n\nDefines the timeout for internal requests made as part of the recovery that are expected to take a long time. Defaults to twice internal_action_timeout.\n\nindices.recovery.recovery_activity_timeout\nDefault: 30m\nRuntime: yes\n\nRecoveries that don’t show any activity for more then this interval will fail. Defaults to internal_action_long_timeout.\n\nindices.recovery.max_concurrent_file_chunks\nDefault: 2\nRuntime: yes\n\nControls the number of file chunk requests that can be sent in parallel per recovery. As multiple recoveries are already running in parallel, controlled by cluster.routing.allocation.node_concurrent_recoveries, increasing this expert-level setting might only help in situations where peer recovery of a single shard is not reaching the total inbound and outbound peer recovery traffic as configured by indices.recovery.max_bytes_per_sec, but is CPU-bound instead, typically when using transport-level security or compression.\n\nMemory management\nmemory.allocation.type\nDefault: on-heap\nRuntime: yes\n\nSupported values are on-heap and off-heap. This influences if memory is preferably allocated in the heap space or in the off-heap/direct memory region.\n\nSetting this to off-heap doesn’t imply that the heap won’t be used anymore. Most allocations will still happen in the heap space but some operations will be allowed to utilize off heap buffers.\n\nWarning\n\nUsing off-heap is considered experimental.\n\nmemory.operation_limit\nDefault: 0\nRuntime: yes\n\nDefault value for the memory.operation_limit session setting. Changing the cluster setting will only affect new sessions, not existing sessions.\n\nQuery circuit breaker\n\nThe Query circuit breaker will keep track of the used memory during the execution of a query. If a query consumes too much memory or if the cluster is already near its memory limit it will terminate the query to ensure the cluster keeps working.\n\nindices.breaker.query.limit\nDefault: 60%\nRuntime: yes\n\nSpecifies the limit for the query breaker. Provided values can either be absolute values (interpreted as a number of bytes), byte sizes (like 1mb) or percentage of the heap size (like 12%). A value of -1 disables breaking the circuit while still accounting memory usage.\n\nRequest circuit breaker\n\nThe request circuit breaker allows an estimation of required heap memory per request. If a single request exceeds the specified amount of memory, an exception is raised.\n\nindices.breaker.request.limit\nDefault: 60%\nRuntime: yes\n\nSpecifies the JVM heap limit for the request circuit breaker.\n\nAccounting circuit breaker\n\nTracks things that are held in memory independent of queries. For example the memory used by Lucene for segments.\n\nindices.breaker.accounting.limit\nDefault: 100%\nRuntime: yes\n\nSpecifies the JVM heap limit for the accounting circuit breaker\n\nCaution\n\nThis setting is deprecated and will be removed in a future release.\n\nStats circuit breakers\n\nSettings that control the behaviour of the stats circuit breaker. There are two breakers in place, one for the jobs log and one for the operations log. For each of them, the breaker limit can be set.\n\nstats.breaker.log.jobs.limit\nDefault: 5%\nRuntime: yes\n\nThe maximum memory that can be used from CRATE_HEAP_SIZE for the sys.jobs_log table on each node.\n\nWhen this memory limit is reached, the job log circuit breaker logs an error message and clears the sys.jobs_log table completely.\n\nstats.breaker.log.operations.limit\nDefault: 5%\nRuntime: yes\n\nThe maximum memory that can be used from CRATE_HEAP_SIZE for the sys.operations_log table on each node.\n\nWhen this memory limit is reached, the operations log circuit breaker logs an error message and clears the sys.operations_log table completely.\n\nTotal circuit breaker\nindices.breaker.total.limit\nDefault: 95%\nRuntime: yes\n\nThe maximum memory that can be used by all aforementioned circuit breakers together.\n\nEven if an individual circuit breaker doesn’t hit its individual limit, queries might still get aborted if several circuit breakers together would hit the memory limit configured in indices.breaker.total.limit.\n\nThread pools\n\nEvery node holds several thread pools to improve how threads are managed within a node. There are several pools, but the important ones include:\n\nwrite: For index, update and delete operations, defaults to fixed\n\nsearch: For count/search operations, defaults to fixed\n\nget: For queries on sys.shards and sys.nodes, defaults to fixed.\n\nrefresh: For refresh operations, defaults to cache\n\nlogical_replication: For operations used by the logical replication, defaults to fixed.\n\nthread_pool.<name>.type\nRuntime: no\nAllowed values: fixed | scaling\n\nfixed holds a fixed size of threads to handle the requests. It also has a queue for pending requests if no threads are available.\n\nscaling ensures that a thread pool holds a dynamic number of threads that are proportional to the workload.\n\nSettings for fixed thread pools\n\nIf the type of a thread pool is set to fixed there are a few optional settings.\n\nthread_pool.<name>.size\nRuntime: no\n\nNumber of threads. The default size of the different thread pools depend on the number of available CPU cores.\n\nthread_pool.<name>.queue_size\nDefault write: 200\nDefault search: 1000\nDefault get: 100\nRuntime: no\n\nSize of the queue for pending requests. A value of -1 sets it to unbounded.\n\nOverload Protection\n\nOverload protection settings control how many resources operations like INSERT INTO FROM QUERY or COPY can use.\n\nThe values here serve as a starting point for an algorithm that dynamically adapts the effective concurrency limit based on the round-trip time of requests. Whenever one of these settings is updated, the previously calculated effective concurrency is reset.\n\nChanging settings will only effect new operations, already running operations will continue with the previous settings.\n\noverload_protection.dml.initial_concurrency\nDefault: 5\nRuntime: yes\n\nThe initial number of concurrent operations allowed per target node.\n\noverload_protection.dml.min_concurrency\nDefault: 1\nRuntime: yes\n\nThe minimum number of concurrent operations allowed per target node.\n\noverload_protection.dml.max_concurrency\nDefault: 2000\nRuntime: yes\n\nThe maximum number of concurrent operations allowed per target node.\n\noverload_protection.dml.queue_size\nDefault: 200\nRuntime: yes\n\nHow many operations are allowed to queue up.\n\nMetadata\ncluster.info.update.interval\nDefault: 30s\nRuntime: yes\n\nDefines how often the cluster collect metadata information (e.g. disk usages etc.) if no concrete event is triggered.\n\nMetadata gateway\n\nThe following settings can be used to configure the behavior of the metadata gateway.\n\ngateway.expected_nodes\nDefault: -1\nRuntime: no\n\nThe setting gateway.expected_nodes defines the total number of nodes expected in the cluster. It is evaluated together with gateway.recover_after_nodes to decide if the cluster can start with recovery.\n\nCaution\n\nThis setting is deprecated and will be removed in a future version. Use gateway.expected_data_nodes instead.\n\ngateway.expected_data_nodes\nDefault: -1\nRuntime: no\n\nThe setting gateway.expected_data_nodes defines the total number of data nodes expected in the cluster. It is evaluated together with gateway.recover_after_data_nodes to decide if the cluster can start with recovery.\n\ngateway.recover_after_time\nDefault: 5m\nRuntime: no\n\nThe gateway.recover_after_time setting defines the time to wait for the number of nodes set in gateway.expected_data_nodes (or gateway.expected_nodes) to become available, before starting the recovery, once the number of nodes defined in gateway.recover_after_data_nodes (or gateway.recover_after_nodes) has already been reached. This setting is ignored if gateway.expected_data_nodes or gateway.expected_nodes are set to 0 or 1. It also has no effect if gateway.recover_after_data_nodes is set equal to gateway.expected_data_nodes (or gateway.recover_after_nodes is set equal to gateway.expected_nodes). The cluster also proceeds to immediate recovery, and the default 5 minutes waiting time does not apply, if neither this setting nor expected_nodes and expected_data_nodes are explicitly set.\n\ngateway.recover_after_nodes\nDefault: -1\nRuntime: no\n\nThe gateway.recover_after_nodes setting defines the number of nodes that need to join the cluster before the cluster state recovery can start. If this setting is -1 and gateway.expected_nodes is set, all nodes will need to be started before the cluster state recovery can start. Please note that proceeding with recovery when not all nodes are available could trigger the promotion of shards and the creation of new replicas, generating disk and network load, which may be unnecessary. You can use a combination of this setting with gateway.recovery_after_time to mitigate this risk.\n\nCaution\n\nThis setting is deprecated and will be removed in CrateDB 5.0. Use gateway.recover_after_data_nodes instead.\n\ngateway.recover_after_data_nodes\nDefault: -1\nRuntime: no\n\nThe gateway.recover_after_data_nodes setting defines the number of data nodes that need to be started before the cluster state recovery can start. If this setting is -1 and gateway.expected_data_nodes is set, all data nodes will need to be started before the cluster state recovery can start. Please note that proceeding with recovery when not all data nodes are available could trigger the promotion of shards and the creation of new replicas, generating disk and network load, which may be unnecessary. You can use a combination of this setting with gateway.recovery_after_time to mitigate this risk.\n\nLogical Replication\n\nReplication process can be configured by the following settings. Settings are dynamic and can be changed in runtime.\n\nreplication.logical.ops_batch_size\nDefault: 50000\nMin value: 16\nRuntime: yes\n\nMaximum number of operations to replicate from the publisher cluster per poll. Represents a number to advance a sequence.\n\nreplication.logical.reads_poll_duration\nDefault: 50\nRuntime: yes\n\nThe maximum time (in milliseconds) to wait for changes per poll operation. When a subscriber makes another one request to a publisher, it has reads_poll_duration milliseconds to harvest changes from the publisher.\n\nreplication.logical.recovery.chunk_size\nDefault: 1MB\nMin value: 1KB\nMax value: 1GB\nRuntime: yes\n\nChunk size to transfer files during the initial recovery of a replicating table.\n\nreplication.logical.recovery.max_concurrent_file_chunks\nDefault: 2\nMin value: 1\nMax value: 5\nRuntime: yes\n\nControls the number of file chunk requests that can be sent in parallel between clusters during the recovery."
  },
  {
    "title": "Node-specific settings — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/config/node.html",
    "html": "5.5\nNode-specific settings\n\nTable of contents\n\nBasics\n\nNode types\n\nGeneral\n\nNetworking\n\nHosts\n\nPorts\n\nAdvanced TCP settings\n\nTransport settings\n\nPaths\n\nPlug-ins\n\nCPU\n\nMemory\n\nGarbage collection\n\nAuthentication\n\nTrust authentication\n\nHost-based authentication\n\nHBA entries\n\nSecured communications (SSL/TLS)\n\nCross-origin resource sharing (CORS)\n\nBlobs\n\nRepositories\n\nQueries\n\nLegacy\n\nJavaScript language\n\nCustom attributes\n\nBasics\ncluster.name\nDefault: crate\nRuntime: no\n\nThe name of the CrateDB cluster the node should join to.\n\nnode.name\nRuntime: no\n\nThe name of the node. If no name is configured a random one will be generated.\n\nNote\n\nNode names must be unique in a CrateDB cluster.\n\nnode.store.allow_mmap\nDefault: true\nRuntime: no\n\nThe setting indicates whether or not memory-mapping is allowed.\n\nNode types\n\nCrateDB supports different types of nodes.\n\nThe following settings can be used to differentiate nodes upon startup:\n\nnode.master\nDefault: true\nRuntime: no\n\nWhether or not this node is able to get elected as master node in the cluster.\n\nnode.data\nDefault: true\nRuntime: no\n\nWhether or not this node will store data.\n\nUsing different combinations of these two settings, you can create four different types of node. Each type of node is differentiated by what types of load it will handle.\n\nTabulating the truth values for node.master and node.data produces a truth table outlining the four different types of node:\n\n\t\n\nMaster\n\n\t\n\nNo master\n\n\n\n\nData\n\n\t\n\nHandle all loads.\n\n\t\n\nHandles client requests and query execution.\n\n\n\n\nNo data\n\n\t\n\nHandles cluster management.\n\n\t\n\nHandles client requests.\n\nNodes marked as node.master will only handle cluster management if they are elected as the cluster master. All other loads are shared equally.\n\nGeneral\nnode.sql.read_only\nDefault: false\nRuntime: no\n\nIf set to true, the node will only allow SQL statements which are resulting in read operations.\n\nstatement_timeout\nDefault: 0\nRuntime: yes\n\nThe maximum duration of any statement before it gets cancelled.\n\nThis value is used as default value for the statement_timeout session setting\n\nIf 0 queries are allowed to run infinitely and don’t get cancelled automatically.\n\nNote\n\nUpdating this setting won’t affect existing sessions, it will only take effect for new sessions.\n\nNetworking\nHosts\nnetwork.host\nDefault: _local_\nRuntime: no\n\nThe IP address CrateDB will bind itself to. This setting sets both the network.bind_host and network.publish_host values.\n\nnetwork.bind_host\nDefault: _local_\nRuntime: no\n\nThis setting determines to which address CrateDB should bind itself to.\n\nnetwork.publish_host\nDefault: _local_\nRuntime: no\n\nThis setting is used by a CrateDB node to publish its own address to the rest of the cluster.\n\nTip\n\nApart from IPv4 and IPv6 addresses there are some special values that can be used for all above settings:\n\n_local_\n\n\t\n\nAny loopback addresses on the system, for example 127.0.0.1.\n\n\n\n\n_site_\n\n\t\n\nAny site-local addresses on the system, for example 192.168.0.1.\n\n\n\n\n_global_\n\n\t\n\nAny globally-scoped addresses on the system, for example 8.8.8.8.\n\n\n\n\n_[INTERFACE]_\n\n\t\n\nAddresses of a network interface, for example _en0_.\n\nPorts\nhttp.port\nRuntime: no\n\nThis defines the TCP port range to which the CrateDB HTTP service will be bound to. It defaults to 4200-4300. Always the first free port in this range is used. If this is set to an integer value it is considered as an explicit single port.\n\nThe HTTP protocol is used for the REST endpoint which is used by all clients except the Java client.\n\nhttp.publish_port\nRuntime: no\n\nThe port HTTP clients should use to communicate with the node. It is necessary to define this setting if the bound HTTP port (http.port) of the node is not directly reachable from outside, e.g. running it behind a firewall or inside a Docker container.\n\ntransport.tcp.port\nRuntime: no\n\nThis defines the TCP port range to which the CrateDB transport service will be bound to. It defaults to 4300-4400. Always the first free port in this range is used. If this is set to an integer value it is considered as an explicit single port.\n\nThe transport protocol is used for internal node-to-node communication.\n\ntransport.publish_port\nRuntime: no\n\nThe port that the node publishes to the cluster for its own discovery. It is necessary to define this setting when the bound tranport port (transport.tcp.port) of the node is not directly reachable from outside, e.g. running it behind a firewall or inside a Docker container.\n\npsql.port\nRuntime: no\n\nThis defines the TCP port range to which the CrateDB Postgres service will be bound to. It defaults to 5432-5532. Always the first free port in this range is used. If this is set to an integer value it is considered as an explicit single port.\n\nAdvanced TCP settings\n\nAny interface that uses TCP (Postgres wire, HTTP & Transport protocols) shares the following settings:\n\nnetwork.tcp.no_delay\nDefault: true\nRuntime: no\n\nEnable or disable the Nagle’s algorithm for buffering TCP packets. Buffering is disabled by default.\n\nnetwork.tcp.keep_alive\nDefault: true\nRuntime: no\n\nConfigures the SO_KEEPALIVE option for sockets, which determines whether they send TCP keepalive probes.\n\nnetwork.tcp.reuse_address\nDefault: true on non-windows machines and false otherwise\nRuntime: no\n\nConfigures the SO_REUSEADDRS option for sockets, which determines whether they should reuse the address.\n\nnetwork.tcp.send_buffer_size\nDefault: -1\nRuntime: no\n\nThe size of the TCP send buffer (SO_SNDBUF socket option). By default not explicitly set.\n\nnetwork.tcp.receive_buffer_size\nDefault: -1\nRuntime: no\n\nThe size of the TCP receive buffer (SO_RCVBUF socket option). By default not explicitly set.\n\nNote\n\nEach setting in this section has its counterpart for HTTP and transport. To provide a protocol specific setting, remove network prefix and use either http or transport instead. For example, no_delay can be configured as http.tcp.no_delay and transport.tcp.no_delay. Please note, that PG interface takes its settings from transport.\n\nTransport settings\ntransport.connect_timeout\nDefault: 30s\nRuntime: no\n\nThe connect timeout for initiating a new connection.\n\ntransport.compress\nDefault: false\nRuntime: no\n\nSet to true to enable compression (DEFLATE) between all nodes.\n\ntransport.ping_schedule\nDefault: -1\nRuntime: no\n\nSchedule a regular application-level ping message to ensure that transport connections between nodes are kept alive. Defaults to -1 (disabled). It is preferable to correctly configure TCP keep-alives instead of using this feature, because TCP keep-alives apply to all kinds of long-lived connections and not just to transport connections.\n\nPaths\n\nNote\n\nRelative paths are relative to CRATE_HOME. Absolute paths override this behavior.\n\npath.conf\nDefault: config\nRuntime: no\n\nFilesystem path to the directory containing the configuration files crate.yml and log4j2.properties.\n\npath.data\nDefault: data\nRuntime: no\n\nFilesystem path to the directory where this CrateDB node stores its data (table data and cluster metadata).\n\nMultiple paths can be set by using a comma separated list and each of these paths will hold full shards (instead of striping data across them). For example:\n\npath.data: /path/to/data1,/path/to/data2\n\n\nWhen CrateDB finds striped shards at the provided locations (from CrateDB <0.55.0), these shards will be migrated automatically on startup.\n\npath.logs\nDefault: logs\nRuntime: no\n\nFilesystem path to a directory where log files should be stored.\n\nCan be used as a variable inside log4j2.properties.\n\nFor example:\n\nappender:\n  file:\n    file: ${path.logs}/${cluster.name}.log\n\npath.repo\nRuntime: no\n\nA list of filesystem or UNC paths where repositories of type fs may be stored.\n\nWithout this setting a CrateDB user could write snapshot files to any directory that is writable by the CrateDB process. To safeguard against this security issue, the possible paths have to be whitelisted here.\n\nSee also location setting of repository type fs.\n\nSee Also\n\nblobs.path\n\nPlug-ins\nplugin.mandatory\nRuntime: no\n\nA list of plug-ins that are required for a node to startup.\n\nIf any plug-in listed here is missing, the CrateDB node will fail to start.\n\nCPU\nprocessors\nRuntime: no\n\nThe number of processors is used to set the size of the thread pools CrateDB is using appropriately. If not set explicitly, CrateDB will infer the number from the available processors on the system.\n\nIn environments where the CPU amount can be restricted (like Docker) or when multiple CrateDB instances are running on the same hardware, the inferred number might be too high. In such a case, it is recommended to set the value explicitly.\n\nMemory\nbootstrap.memory_lock\nDefault: false\nRuntime: no\n\nCrateDB performs poorly when the JVM starts swapping: you should ensure that it never swaps. If set to true, CrateDB will use the mlockall system call on startup to ensure that the memory pages of the CrateDB process are locked into RAM.\n\nGarbage collection\n\nCrateDB logs if JVM garbage collection on different memory pools takes too long. The following settings can be used to adjust these timeouts:\n\nmonitor.jvm.gc.collector.young.warn\nDefault: 1000ms\nRuntime: no\n\nCrateDB will log a warning message if it takes more than the configured timespan to collect the Eden Space (heap).\n\nmonitor.jvm.gc.collector.young.info\nDefault: 700ms\nRuntime: no\n\nCrateDB will log an info message if it takes more than the configured timespan to collect the Eden Space (heap).\n\nmonitor.jvm.gc.collector.young.debug\nDefault: 400ms\nRuntime: no\n\nCrateDB will log a debug message if it takes more than the configured timespan to collect the Eden Space (heap).\n\nmonitor.jvm.gc.collector.old.warn\nDefault: 10000ms\nRuntime: no\n\nCrateDB will log a warning message if it takes more than the configured timespan to collect the Old Gen / Tenured Gen (heap).\n\nmonitor.jvm.gc.collector.old.info\nDefault: 5000ms\nRuntime: no\n\nCrateDB will log an info message if it takes more than the configured timespan to collect the Old Gen / Tenured Gen (heap).\n\nmonitor.jvm.gc.collector.old.debug\nDefault: 2000ms\nRuntime: no\n\nCrateDB will log a debug message if it takes more than the configured timespan to collect the Old Gen / Tenured Gen (heap).\n\nAuthentication\nTrust authentication\nauth.trust.http_default_user\nDefault: crate\nRuntime: no\n\nThe default user that should be used for authentication when clients connect to CrateDB via HTTP protocol and they do not specify a user via the Authorization request header.\n\nauth.trust.http_support_x_real_ip\nDefault: false\nRuntime: no\n\nIf enabled, the HTTP transport will trust the X-Real-IP header sent by the client to determine the client’s IP address. This is useful when CrateDB is running behind a reverse proxy or load-balancer. For improved security, any _local_ IP address (127.0.0.1 and ::1) defined in this header will be ignored.\n\nWarning\n\nEnabling this setting can be a security risk, as it allows clients to impersonate other clients by sending a fake X-Real-IP header.\n\nHost-based authentication\n\nAuthentication settings (auth.host_based.*) are node settings, which means that their values apply only to the node where they are applied and different nodes may have different authentication settings.\n\nauth.host_based.enabled\nDefault: false\nRuntime: no\n\nSetting to enable or disable Host Based Authentication (HBA). It is disabled by default.\n\nHBA entries\n\nThe auth.host_based.config. setting is a group setting that can have zero, one or multiple groups that are defined by their group key (${order}) and their fields (user, address, method, protocol, ssl).\n\n${order}:\nAn identifier that is used as a natural order key when looking up the host\nbased configuration entries. For example, an order key of a will be\nlooked up before an order key of b. This key guarantees that the entry\nlookup order will remain independent from the insertion order of the\nentries.\n\nThe Host-Based Authentication (HBA) setting is a list of predicates that users can specify to restrict or allow access to CrateDB.\n\nThe meaning of the fields of the are as follows:\n\nauth.host_based.config.${order}.user\nRuntime: no\nSpecifies an existing CrateDB username, only crate user (superuser) is\navailable. If no user is specified in the entry, then all existing users\ncan have access.\nauth.host_based.config.${order}.address\nRuntime: no\nThe client machine addresses that the client matches, and which are allowed\nto authenticate. This field may contain an IPv4 address, an IPv6 address or\nan IPv4 CIDR mask. For example: 127.0.0.1 or 127.0.0.1/32. It also\nmay contain a hostname or the special _local_ notation which will match\nboth IPv4 and IPv6 connections from localhost. A hostname specification\nthat starts with a dot (.) matches a suffix of the actual hostname.\nSo .crate.io would match foo.crate.io but not just crate.io. If no address\nis specified in the entry, then access to CrateDB is open for all hosts.\nauth.host_based.config.${order}.method\nRuntime: no\nThe authentication method to use when a connection matches this entry.\nValid values are trust, cert, and password. If no method is\nspecified, the trust method is used by default.\nSee Trust method, Client certificate authentication method and Password authentication method for more\ninformation about these methods.\nauth.host_based.config.${order}.protocol\nRuntime: no\nSpecifies the protocol for which the authentication entry should be used.\nIf no protocol is specified, then this entry will be valid for all\nprotocols that rely on host based authentication see Trust method).\nauth.host_based.config.${order}.ssl\nDefault: optional\nRuntime: no\nSpecifies whether the client must use SSL/TLS to connect to the cluster.\nIf set to on then the client must be connected through SSL/TLS\notherwise is not authenticated. If set to off then the client must\nnot be connected via SSL/TLS otherwise is not authenticated. Finally\noptional, which is the value when the option is completely skipped,\nmeans that the client can be authenticated regardless of SSL/TLS is used\nor not.\n\nExample of config groups:\n\nauth.host_based.config:\n  entry_a:\n    user: crate\n    address: 127.16.0.0/16\n  entry_b:\n    method: trust\n  entry_3:\n    user: crate\n    address: 172.16.0.0/16\n    method: trust\n    protocol: pg\n    ssl: on\n\nSecured communications (SSL/TLS)\n\nSecured communications via SSL allows you to encrypt traffic between CrateDB nodes and clients connecting to them. Connections are secured using Transport Layer Security (TLS).\n\nssl.http.enabled\nDefault: false\nRuntime: no\n\nSet this to true to enable secure communication between the CrateDB node and the client through SSL via the HTTPS protocol.\n\nssl.psql.enabled\nDefault: false\nRuntime: no\n\nSet this to true to enable secure communication between the CrateDB node and the client through SSL via the PostgreSQL wire protocol.\n\nssl.transport.mode\nDefault: legacy\nRuntime: no\n\nFor communication between nodes, choose:\n\noff\n\nSSL cannot be used\n\nlegacy\n\nSSL is not used. If HBA is enabled, transport connections won’t be verified Any reachable host can establish a connection.\n\non\n\nSSL must be used\n\nssl.keystore_filepath\nRuntime: no\n\nThe full path to the node keystore file.\n\nssl.keystore_password\nRuntime: no\n\nThe password used to decrypt the keystore file defined with ssl.keystore_filepath.\n\nssl.keystore_key_password\nRuntime: no\n\nThe password entered at the end of the keytool -genkey command.\n\nNote\n\nOptionally trusted CA certificates can be stored separately from the node’s keystore into a truststore for CA certificates.\n\nssl.truststore_filepath\nRuntime: no\n\nThe full path to the node truststore file. If not defined, then only a keystore will be used.\n\nssl.truststore_password\nRuntime: no\n\nThe password used to decrypt the truststore file defined with ssl.truststore_filepath.\n\nssl.resource_poll_interval\nDefault: 5m\nRuntime: no\n\nThe frequency at which SSL files such as keystore and truststore are polled for changes.\n\nCross-origin resource sharing (CORS)\n\nMany browsers support the same-origin policy which requires web applications to explicitly allow requests across origins. The cross-origin resource sharing settings in CrateDB allow for configuring these.\n\nhttp.cors.enabled\nDefault: false\nRuntime: no\n\nEnable or disable cross-origin resource sharing.\n\nhttp.cors.allow-origin\nDefault: <empty>\nRuntime: no\n\nDefine allowed origins of a request. * allows any origin (which can be a substantial security risk) and by prepending a / the string will be treated as a regular expression. For example /https?:\\/\\/crate.io/ will allow requests from https://crate.io and https://crate.io. This setting disallows any origin by default.\n\nhttp.cors.max-age\nDefault: 1728000 (20 days)\nRuntime: no\n\nMax cache age of a preflight request in seconds.\n\nhttp.cors.allow-methods\nDefault: OPTIONS, HEAD, GET, POST, PUT, DELETE\nRuntime: no\n\nAllowed HTTP methods.\n\nhttp.cors.allow-headers\nDefault: X-Requested-With, Content-Type, Content-Length\nRuntime: no\n\nAllowed HTTP headers.\n\nhttp.cors.allow-credentials\nDefault: false\nRuntime: no\n\nAdd the Access-Control-Allow-Credentials header to responses.\n\nBlobs\nblobs.path\nRuntime: no\n\nPath to a filesystem directory where to store blob data allocated for this node.\n\nBy default blobs will be stored under the same path as normal data. A relative path value is interpreted as relative to CRATE_HOME.\n\nRepositories\n\nRepositories are used to backup a CrateDB cluster.\n\nrepositories.url.allowed_urls\nRuntime: no\n\nThis setting only applies to repositories of type url.\n\nWith this setting a list of urls can be specified which are allowed to be used if a repository of type url is created.\n\nWildcards are supported in the host, path, query and fragment parts.\n\nThis setting is a security measure to prevent access to arbitrary resources.\n\nIn addition, the supported protocols can be restricted using the repositories.url.supported_protocols setting.\n\nrepositories.url.supported_protocols\nDefault: http, https, ftp, file and jar\nRuntime: no\n\nA list of protocols that are supported by repositories of type url.\n\nThe jar protocol is used to access the contents of jar files. For more info, see the java JarURLConnection documentation.\n\nSee also the path.repo Setting.\n\nQueries\nindices.query.bool.max_clause_count\nDefault: 8192\nRuntime: no\n\nThis setting defines the maximum number of elements an array can have so that the != ANY(), LIKE ANY(), ILIKE ANY(), NOT LIKE ANY() and the NOT ILIKE ANY() operators can be applied on it.\n\nNote\n\nIncreasing this value to a large number (e.g. 10M) and applying those ANY operators on arrays of that length can lead to heavy memory, consumption which could cause nodes to crash with OutOfMemory exceptions.\n\nLegacy\nlegacy.table_function_column_naming\nDefault: false\nRuntime: no\n\nSince CrateDB 5.0.0, if the table function is not aliased and is returning a single base data typed column, the table function name is used as the column name. This setting can be set in order to use the naming convention prior to 5.0.0.\n\nThe following table functions are affected by this setting:\n\nunnest\n\nregexp_matches\n\ngenerate_series\n\nWhen the setting is set and a single column is expected to be returned, the returned column will be named col1, groups, or col1 respectively.\n\nNote\n\nBeware that if not all nodes in the cluster are consistently set or unset, the behaviour will depend on the node handling the query.\n\nJavaScript language\nlang.js.enabled\nDefault: true\nRuntime: no\n\nSetting to enable or disable JavaScript UDF support.\n\nCustom attributes\n\nThe node.attr namespace is a bag of custom attributes. Custom attributes can be used to control shard allocation.\n\nYou can create any attribute you want under this namespace, like node.attr.key: value. These attributes use the node.attr namespace to distinguish them from core node attribute like node.name.\n\nCustom attributes are not validated by CrateDB, unlike core node attributes."
  },
  {
    "title": "Resiliency — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/concepts/resiliency.html",
    "html": "5.5\nResiliency\n\nDistributed systems are tricky. All sorts of things can go wrong that are beyond your control. The network can go away, disks can fail, hosts can be terminated unexpectedly. CrateDB tries very hard to cope with these sorts of issues while maintaining availability, consistency, and durability.\n\nHowever, as with any distributed system, sometimes, rarely, things can go wrong.\n\nThankfully, for most use-cases, if you follow best practices, you are extremely unlikely to experience resiliency issues with CrateDB.\n\nSee Also\n\nAppendix: Resiliency Issues\n\nTable of contents\n\nMonitoring cluster status\n\nStorage and consistency\n\nDeployment strategies\n\nMonitoring cluster status\n\nThe Admin UI in CrateDB has a status indicator which can be used to determine the stability and health of a cluster.\n\nA green status indicates that all shards have been replicated, are available, and are not being relocated. This is the lowest risk status for a cluster. The status will turn yellow when there is an elevated risk of encountering issues, due to a network failure or the failure of a node in the cluster.\n\nThe status is updated every few seconds (variable on your cluster ping configuration).\n\nStorage and consistency\n\nCode that expects the behavior of an ACID compliant database like MySQL may not always work as expected with CrateDB.\n\nCrateDB does not support ACID transactions, but instead has atomic operations and eventual consistency at the row level. See also Clustering.\n\nEventual consistency is the trade-off that CrateDB makes in exchange for high-availability that can tolerate most hardware and network failures. So you may observe data from different cluster nodes temporarily falling very briefly out-of-sync with each other, although over time they will become consistent.\n\nFor example, you know a row has been written as soon as you get the INSERT OK message. But that row might not be read back by a subsequent SELECT on a different node until after a table refresh (which typically occurs within one second).\n\nYour applications should be designed to work this storage and consistency model.\n\nDeployment strategies\n\nWhen deploying CrateDB you should carefully weigh your need for high-availability and disaster recovery against operational complexity and expense.\n\nWhich strategy you pick is going to depend on the specifics of your situation.\n\nHere are some considerations:\n\nCrateDB is designed to scale horizontally. Make sure that your machines are fit for purpose, i.e. use SSDs, increase RAM up to 64 GB, and use multiple CPU cores when you can. But if you want to dynamically increase (or decrease) the capacity of your cluster, add (or remove) nodes.\n\nIf availability is a concern, you can add nodes across multiple zones (e.g. different data centers or geographical regions). The more available your CrateDB cluster is, the more likely it is to withstand external failures like a zone going down.\n\nIf data durability or read performance is a concern, you can increase the number of table replicas. More table replicas means a smaller chance of permanent data loss due to hardware failures, in exchange for the use of more disk space and more intra-cluster network traffic.\n\nIf disaster recovery is important, you can take regular snapshots and store those snapshots in cold storage. This safeguards data that has already been successfully written and replicated across the cluster.\n\nCrateDB works well as part of a data pipeline, especially if you’re working with high-volume data. If you have a message queue in front of CrateDB, you can configure it with backups and replay the data flow for a specific timeframe. This can be used to recover from issues that affect your data before it has been successfully written and replicated across the cluster.\n\nIndeed, this is the generally recommended way to recover from any of the rare consistency or data-loss issues you might encounter when CrateDB experiences network or hardware failures (see next section)."
  },
  {
    "title": "Clustering — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/concepts/clustering.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n5.5\nClustering\n\nThe aim of this document is to describe, on a high level, how the distributed SQL database CrateDB uses a shared nothing architecture to form high- availability, resilient database clusters with minimal effort of configuration.\n\nIt will lay out the core concepts of the shared nothing architecture at the heart of CrateDB. The main difference to a primary-secondary architecture is that every node in the CrateDB cluster can perform every operation - hence all nodes are equal in terms of functionality (see Components of a CrateDB Node) and are configured the same.\n\nTable of contents\n\nComponents of a CrateDB Node\n\nSQL Handler\n\nJob Execution Service\n\nCluster State Service\n\nData storage\n\nMulti-node setup: Clusters\n\nCluster state management\n\nSettings, metadata, and routing\n\nMaster Node Election\n\nDiscovery\n\nNetworking\n\nCluster behavior\n\nApplication use case\n\nComponents of a CrateDB Node\n\nTo understand how a CrateDB cluster works it makes sense to first take a look at the components of an individual node of the cluster.\n\nFigure 1\n\nMultiple interconnected instances of CrateDB form a single database cluster. The components of each node are equal.\n\nFigure 1 shows that in CrateDB each node of a cluster contains the same components that (a) interface with each other, (b) with the same component from a different node and/or (c) with the outside world. These four major components are: SQL Handler, Job Execution Service, Cluster State Service, and Data Storage.\n\nSQL Handler\n\nThe SQL Handler part of a node is responsible for three aspects:\n\nhandling incoming client requests,\n\nparsing and analyzing the SQL statement from the request and\n\ncreating an execution plan based on the analyzed statement (abstract syntax tree)\n\nThe SQL Handler is the only of the four components that interfaces with the “outside world”. CrateDB supports three protocols to handle client requests:\n\nHTTP\n\na Binary Transport Protocol\n\nthe PostgreSQL Wire Protocol\n\nA typical request contains a SQL statement and its corresponding arguments.\n\nJob Execution Service\n\nThe Job Execution Service is responsible for the execution of a plan (“job”). The phases of the job and the resulting operations are already defined in the execution plan. A job usually consists of multiple operations that are distributed via the Transport Protocol to the involved nodes, be it the local node and/or one or multiple remote nodes. Jobs maintain IDs of their individual operations. This allows CrateDB to “track” (or for example “kill”) distributed queries.\n\nCluster State Service\n\nThe three main functions of the Cluster State Service are:\n\ncluster state management,\n\nelection of the master node and\n\nnode discovery, thus being the main component for cluster building (as described in section Multi-node setup: Clusters).\n\nIt communicates using the Binary Transport Protocol.\n\nData storage\n\nThe data storage component handles operations to store and retrieve data from disk based on the execution plan.\n\nIn CrateDB, the data stored in the tables is sharded, meaning that tables are divided and (usually) stored across multiple nodes. Each shard is a separate Lucene index that is stored physically on the filesystem. Reads and writes are operating on a shard level.\n\nMulti-node setup: Clusters\n\nA CrateDB cluster is a set of two or more CrateDB instances (referred to as nodes) running on different hosts which form a single, distributed database.\n\nFor inter-node communication, CrateDB uses a software specific transport protocol that utilizes byte-serialized Plain Old Java Objects (POJOs) and operates on a separate port. That so-called “transport port” must be open and reachable from all nodes in the cluster.\n\nCluster state management\n\nThe cluster state is versioned and all nodes in a cluster keep a copy of the latest cluster state. However, only a single node in the cluster – the master node – is allowed to change the state at runtime.\n\nSettings, metadata, and routing\n\nThe cluster state contains all necessary meta information to maintain the cluster and coordinate operations:\n\nGlobal cluster settings\n\nDiscovered nodes and their status\n\nSchemas of tables\n\nThe status and location of primary and replica shards\n\nWhen the master node updates the cluster state it will publish the new state to all nodes in the cluster and wait for all nodes to respond before processing the next update.\n\nMaster Node Election\n\nIn a CrateDB cluster there can only be one master node at any single time. The cluster only becomes available to serve requests once a master has been elected, and a new election takes place if the current master node becomes unavailable.\n\nBy default, all nodes are master-eligible, but a node setting is available to indicate, if desired, that a node must not take on the role of master.\n\nTo elect a master among the eligible nodes, a majority (floor(half)+1), also known as quorum, is required among a subset of all master-eligible nodes, this subset of nodes is known as the voting configuration. The voting configuration is a list which is persisted as part of the cluster state. It is maintained automatically in a way that makes so that split-brain scenarios are never possible.\n\nEvery time a node joins the cluster, or leaves the cluster, even if it is for a few seconds, CrateDB re-evaluates the voting configuration. If the new number of master-eligible nodes in the cluster is odd, CrateDB will put them all in the voting configuration. If the number is even, CrateDB will exclude one of the master-eligible nodes from the voting configuration.\n\nThe voting configuration is not shrunk below 3 nodes, meaning that if there were 3 nodes in the voting configuration and one of them becomes unavailable, they all stay in the voting configuration and a quorum of 2 nodes is still required. A master node rescinds its role if it cannot contact a quorum of nodes from the latest voting configuration.\n\nWarning\n\nIf you do infrastructure maintenance, please note that as nodes are shutdown or rebooted, they will temporarily leave the voting configuration, and for the cluster to elect a master a quorum is required among the nodes that were last in the voting configuration.\n\nFor instance, if you have a 5-nodes cluster, with all nodes master-eligible, and node 1 is currently the master, and you shutdown node 5, then node 4, then node 3, the cluster will stay available as the voting configuration will have adapted to only have nodes 1, 2, and 3 on it.\n\nIf you then shutdown one more node the cluster will become unavailable as a quorum of 2 nodes is now required and not available. To bring the cluster back online at this point you will require two nodes among 1, 2, and 3. Bringing back nodes 3, 4, and 5, will not be sufficient.\n\nNote\n\nSpecial settings and considerations applied prior to CrateDB version 4.0.0.\n\nDiscovery\n\nThe process of finding, adding and removing nodes is done in the discovery module.\n\nFigure 2\n\nPhases of the node discovery process. n1 and n2 already form a cluster where n1 is the elected master node, n3 joins the cluster. The cluster state update happens in parallel!\n\nNode discovery happens in multiple steps:\n\nCrateDB requires a list of potential host addresses for other CrateDB nodes when it is starting up. That list can either be provided by a static configuration or can be dynamically generated, for example by fetching DNS SRV records, querying the Amazon EC2 API, and so on.\n\nAll potential host addresses are pinged. Nodes which receive the request respond to it with information about the cluster it belongs to, the current master node, and its own node name.\n\nNow that the node knows the master node, it sends a join request. The Primary verifies the incoming request and adds the new node to the cluster state that now contains the complete list of all nodes in the cluster.\n\nThe cluster state is then published across the cluster. This guarantees the common knowledge of the node addition.\n\nCaution\n\nIf a node is started without any initial_master_nodes or a discovery_type set to single-node (e.g., the default configuration), it will never join a cluster even if the configuration is subsequently changed.\n\nIt is possible to force the node to forget its current cluster state by using the crate-node CLI tool. However, be aware that this may result in data loss.\n\nNetworking\n\nIn a CrateDB cluster all nodes have a direct link to all other nodes; this is known as full mesh topology. Due to simplicity reasons every node maintains a one-way connections to every other node in the network. The network topology of a 5 node cluster looks like this:\n\nFigure 3\n\nNetwork topology of a 5 node CrateDB cluster. Each line represents a one-way connection.\n\nThe advantages of a fully connected network are that it provides a high degree of reliability and the paths between nodes are the shortest possible. However, there are limitations in the size of such networked applications because the number of connections (c) grows quadratically with the number of nodes (n):\n\nc = n * (n - 1)\n\nCluster behavior\n\nThe fact that each CrateDB node in a cluster is equal allows applications and users to connect to any node and get the same response for the same operations. As already described in section Components of a CrateDB Node, the SQL handler is responsible for handling incoming client SQL requests, either using the HTTP transport protocol, or the PostgreSQL wire protocol.\n\nThe “handler node” that accepts the client request also returns the response to the client. It does neither redirect nor delegate the request to a different nodes. The handler node parses the incoming request into a syntax tree, analyzes it and creates an execution plan locally. Then the operations of the plan are executed in a distributed manner. The upstream of the final phase of the execution is always the handler which then returns the response to the client.\n\nApplication use case\n\nIn a conventional setup of an application using a primary-secondary database the deployed stack looks similar to this:\n\nFigure 4\n\nConventional deployment of an application-database stack.\n\nHowever, this given setup does not scale because all application servers use the same, single entry point to the database for writes (the application can still read from secondaries) and if that entry point is unavailable the complete stack is broken.\n\nChoosing a shared nothing architecture allows DevOps to deploy their applications in an “elastic” manner without SPoF. The idea is to extend the shared nothing architecture from the database to the application which in most cases is stateless already.\n\nFigure 5\n\nElastic deployment making use of the shared nothing architecture.\n\nIf you deploy an instance of CrateDB together with every application server you will be able to dynamically scale up and down your database backend depending on your needs. The application only needs to communicate to its “bound” CrateDB instance on localhost. The load balancer tracks the health of the hosts and if either the application or the database on a single host fails the complete host will taken out of the load balancing."
  },
  {
    "title": "Joins — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/concepts/joins.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n5.5\nJoins\n\nJoins are essential operations in relational databases. They create a link between rows based on common values and allow the meaningful combination of these rows. CrateDB supports joins and due to its distributed nature allows you to work with large amounts of data.\n\nIn this document we will present the following topics. First, an overview of the existing types of joins and algorithms provided. Then a description of how CrateDB implements them along with the necessary optimizations, which allows us to work with huge datasets.\n\nTable of contents\n\nJoin types\n\nCross join\n\nInner join\n\nEqui Join\n\nOuter join\n\nJoin algorithms\n\nNested loop join\n\nPrimitive nested loop\n\nDistributed nested loop\n\nHash join\n\nBasic algorithm\n\nBlock hash join\n\nSwitch tables optimization\n\nDistributed block hash join\n\nJoin optimizations\n\nQuery then fetch\n\nPush-down query optimization\n\nCross join elimination\n\nJoin types\n\nA join is a relational operation that merges two data sets based on certain properties. Join Types (Inspired by this article) shows which elements appear in which join.\n\nJoin Types\n\nFrom left to right, top to bottom: left join, right join, inner join, outer join, and cross join of a set L and R.\n\nCross join\n\nA cross join returns the Cartesian product of two or more relations. The result of the Cartesian product on the relation L and R consists of all possible permutations of each tuple of the relation L with every tuple of the relation R.\n\nInner join\n\nAn inner join is a join of two or more relations that returns only tuples that satisfy the join condition.\n\nEqui Join\n\nAn equi join is a subset of an inner join and a comparison-based join, that uses equality comparisons in the join condition. The equi join of the relation L and R combines tuple l of relation L with a tuple r of the relation R if the join attributes of both tuples are identical.\n\nOuter join\n\nAn outer join returns a relation consisting of tuples that satisfy the join condition and dangling tuples from both or one of the relations, respectively to the outer join type.\n\nAn outer join can be one of the following types:\n\nLeft outer join returns tuples of the relation L matching tuples of the relation R and dangling tuples of the relation R padded with null values.\n\nRight outer join returns tuples of the relation R matching tuples of the relation L and dangling tuples from the relation L padded with null values.\n\nFull outer join returns matching tuples of both relations and dangling tuples produced by left and right outer joins.\n\nJoin algorithms\n\nCrateDB supports (a) CROSS JOIN, (b) INNER JOIN, (c) EQUI JOIN, (d) LEFT JOIN, (e) RIGHT JOIN and (f) FULL JOIN. All of these join types are executed using the nested loop join algorithm except for the Equi Joins which are executed using the hash join algorithm. Special optimizations, according to the specific use cases, are applied to improve execution performance.\n\nNested loop join\n\nThe nested loop join is the simplest join algorithm. One of the relations is nominated as the inner relation and the other as the outer relation. Each tuple of the outer relation is compared with each tuple of the inner relation and if the join condition is satisfied, the tuples of the relation L and R are concatenated and added into the returned virtual relation:\n\nfor each tuple l ∈ L do\n    for each tuple r ∈ R do\n        if l.a Θ r.b\n            put tuple(l, r) in Q\n\n\nListing 1. Nested loop join algorithm.\n\nPrimitive nested loop\n\nFor joins on some relations, the nested loop operation can be executed directly on the handler node. Specifically for queries involving a CROSS JOIN or joins on system tables /information_schema each shard sends the data to the handler node. Afterwards, this node runs the nested loop, applies limits, etc. and ultimately returns the results. Similarly, joins can be nested, so instead of collecting data from shards the rows can be the result of a previous join or table function.\n\nDistributed nested loop\n\nRelations are usually distributed to different nodes which require the nested loop to acquire the data before being able to join. After finding the locations of the required shards (which is done in the planning stage), the smaller data set (based on the row count) is broadcast amongst all the nodes holding the shards they are joined with.\n\nAfter that, each of the receiving nodes can start running a nested loop on the subset it has just received. Finally, these intermediate results are pushed to the original (handler) node to merge and return the results to the requesting client (see Nodes that are holding the smaller shards broadcast the data to the processing nodes which then return the results to the requesting node.).\n\nNodes that are holding the smaller shards broadcast the data to the processing nodes which then return the results to the requesting node.\n\nQueries can be optimized if they contain (a) ORDER BY, (b) LIMIT, or (c) if INNER/EQUI JOIN. In any of these cases, the nested loop can be terminated earlier:\n\nOrdering allows determining whether there are records left\n\nLimit states the maximum number of rows that are returned\n\nConsequently, the number of rows is significantly reduced allowing the operation to complete much faster.\n\nHash join\n\nThe Hash Join algorithm is used to execute certain types of joins in a more efficient way than Nested Loop.\n\nBasic algorithm\n\nThe operation takes place in one node (the handler node to which the client is connected). The rows of the left relation of the join are read and a hashing algorithm is applied on the fields of the relation which participate in the join condition. The hashing algorithm generates a hash value which is used to store every row of the left relation in the proper position in a hash table.\n\nThen the rows of the right relation are read one-by-one and the same hashing algorithm is applied on the fields that participate in the join condition. The generated hash value is used to make a lookup in the hash table. If no entry is found, the row is skipped and the processing continues with the next row from the right relation. If an entry is found, the join condition is validated (handling hash collisions) and on successful validation the combined tuple of left and right relation is returned.\n\nBasic hash join algorithm\n\nBlock hash join\n\nThe Hash Join algorithm requires a hash table containing all the rows of the left relation to be stored in memory. Therefore, depending on the size of the relation (number of rows) and the size of each row, the size of this hash table might exceed the available memory of the node executing the hash join. To resolve this limitation the rows of the left relation are loaded into the hash table in blocks.\n\nOn every iteration the maximum available size of the hash table is calculated, based on the number of rows and size of each row of the table but also taking into account the available memory for query execution on the node. Once this block-size is calculated the rows of the left relation are processed and inserted into the hash table until the block-size is reached.\n\nThe operation then starts reading the rows of the right relation, process them one-by-one and performs the lookup and the join condition validation. Once all rows from the right relation are processed the hash table is re-initialized based on a new calculation of the block size and a new iteration starts until all rows of the left relation are processed.\n\nWith this algorithm the memory limitation is handled in expense of having to iterate over the rows of the right table multiple times, and it is the default algorithm used for Hash Join execution by CrateDB.\n\nSwitch tables optimization\n\nSince the right table can be processed multiple times (number of rows from left / block-size) the right table should be the smaller (in number of rows) of the two relations participating in the join. Therefore, if originally the right relation is larger than the left the query planner performs a switch to take advantage of this detail and execute the hash join with better performance.\n\nDistributed block hash join\n\nSince CrateDB is a distributed database and a standard deployment consists of at least three nodes and in most case of much more, the Hash Join algorithm execution can be further optimized (performance-wise) by executing it in a distributed manner across the CrateDB cluster.\n\nThe idea is to have the hash join operation executing in multiple nodes of the cluster in parallel and then merge the intermediate results before returning them to the client.\n\nA hashing algorithm is applied on every row of both the left and right relations. On the integer value generated by this hash, a modulo, by the number of nodes in the cluster, is applied and the resulting number defines the node to which this row should be sent. As a result each node of the cluster receives a subset of the whole data set which is ensured (by the hashing and modulo) to contain all candidate matching rows.\n\nEach node in turn performs a block hash join on this subset and sends its result tuples to the handler node (where the client issued the query). Finally, the handler node receives those intermediate results, merges them and applies any pending ORDER BY, LIMIT and OFFSET and sends the final result to the client.\n\nThis algorithm is used by CrateDB for most cases of hash join execution except for joins on complex subqueries that contain LIMIT and/or OFFSET.\n\nDistributed hash join algorithm\n\nJoin optimizations\nQuery then fetch\n\nJoin operations on large relation can be extremely slow especially if the join is executed with a Nested Loop. - which means that the runtime complexity grows quadratically (O(n*m)). Specifically for cross joins this results in large amounts of data sent over the network and loaded into memory at the handler node. CrateDB reduces the volume of data transferred by employing “Query Then Fetch”: First, filtering and ordering are applied (if possible where the data is located) to obtain the required document IDs. Next, as soon as the final data set is ready, CrateDB fetches the selected fields and returns the data to the client.\n\nPush-down query optimization\n\nComplex queries such as Listing 2 require the planner to decide when to filter, sort, and merge in order to efficiently execute the plan. In this case, the query would be split internally into subqueries before running the join. As shown in Figure 5, first filtering (and ordering) is applied to relations L and R on their shards, then the result is directly broadcast to the nodes running the join. Not only will this behavior reduce the number of rows to work with, it also distributes the workload among the nodes so that the (expensive) join operation can run faster.\n\nSELECT L.a, R.x\nFROM L, R\nWHERE L.id = R.id\n  AND L.b > 100\n  AND R.y < 10\nORDER BY L.a\n\n\nListing 2. An INNER JOIN on ids (effectively an EQUI JOIN) which can be optimized.\n\nFigure 5\n\nComplex queries are broken down into subqueries that are run on their shards before joining.\n\nCross join elimination\n\nThe optimizer will try to eliminate cross joins in the query plan by changing the join-order. Cross join elimination replaces a CROSS JOIN with an INNER JOIN if query conditions used in the WHERE clause or other join conditions allow for it. An example:\n\nSELECT *\nFROM t1 CROSS JOIN t2\nINNER JOIN t3\nON t3.z = t1.x AND t3.z = t2.y\n\n\nThe cross join elimination will change the order of the query from t1, t2, t3 to t2, t1, t3 so that each join has a join condition and the CROSS JOIN can be replaced by an INNER JOIN. When reordering, it will try to preserve the original join order as much as possible. If a CROSS JOIN cannot be eliminated, the original join order will be maintained. This optimizer rule can be disabled with the optimizer eliminate cross join session setting:\n\nSET optimizer_eliminate_cross_join = false\n\n\nNote that this setting is experimental, and may change in the future."
  },
  {
    "title": "Glossary — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/appendices/glossary.html",
    "html": "master\nGlossary\n\nThis glossary defines key terms used in the CrateDB reference manual.\n\nTable of contents\n\nTerms\n\nB\n\nC\n\nE\n\nF\n\nM\n\nN\n\nO\n\nP\n\nR\n\nS\n\nU\n\nV\n\nTerms\nB\nBinary operator\n\nSee operation.\n\nC\nCLUSTERED BY column\n\nSee routing column.\n\nE\nEvaluation\n\nSee expression.\n\nExpression\n\nAny valid SQL that produces a value (e.g., column references, comparison operators, and functions) through a process known as evaluation.\n\nContrary to a statement.\n\nSee Also\n\nSQL: Value expressions\n\nBuilt-ins: Subquery expressions\n\nData definition: Generation expressions\n\nScalar functions: Conditional functions and expressions\n\nAggregation: Aggregation expressions\n\nF\nFunction\n\nA token (e.g., replace) that takes zero or more arguments (e.g., three strings), performs a specific task, and may return one or more values (e.g., a modified string). Functions that return more than one value are called multi-valued functions.\n\nFunctions may be called in an SQL statement, like so:\n\ncr> SELECT replace('Hello world!', 'world', 'friend') as result;\n+---------------+\n| result        |\n+---------------+\n| Hello friend! |\n+---------------+\nSELECT 1 row in set (... sec)\n\n\nSee Also\n\nScalar functions\n\nAggregate functions\n\nTable functions\n\nWindow functions\n\nUser-defined functions\n\nM\nMetadata gateway\n\nPersists cluster metadata on disk every time the metadata changes. This data is stored persistently across full cluster restarts and recovered after nodes are started again.\n\nSee Also\n\nCluster configuration: Metadata gateway\n\nMulti-valued function\n\nA function that returns two or more values.\n\nSee Also\n\nTable functions\n\nWindow functions\n\nN\nNonscalar\n\nA data type that can have more than one value (e.g., arrays and objects).\n\nContrary to a scalar.\n\nSee Also\n\nGeographic types\n\nContainer types\n\nO\nOperand\n\nSee operator.\n\nOperation\n\nSee operator.\n\nOperator\n\nA reserved keyword (e.g., IN) or sequence of symbols (e.g., >=) that can be used in an SQL statement to manipulate one or more expressions and return a result (e.g., true or false). This process is known as an operation and the expressions can be called operands or arguments.\n\nAn operator that takes one operand is known as a unary operator and an operator that takes two is known as a binary operator.\n\nSee Also\n\nArithmetic operators\n\nComparison operators\n\nArray comparisons\n\nP\nPartition column\n\nA column used to partition a table. Specified by the PARTITIONED BY clause.\n\nAlso known as a PARTITIONED BY column or partitioned column.\n\nA table may be partitioned by one or more columns:\n\nIf a table is partitioned by one column, a new partition is created for every unique value in that partition column\n\nIf a table is partitioned by multiple columns, a new partition is created for every unique combination of row values in those partition columns\n\nSee Also\n\nData definition: Partitioned tables\n\nGenerated columns: Partitioning\n\nCREATE TABLE: PARTITIONED BY clause\n\nALTER TABLE: PARTITION clause\n\nREFRESH: PARTITION clause\n\nOPTIMIZE: PARTITION clause\n\nCOPY TO: PARTITION clause\n\nCOPY FROM: PARTITION clause\n\nCREATE SNAPSHOT: PARTITION clause\n\nRESTORE SNAPSHOT: PARTITION clause\n\nPARTITIONED BY column\n\nSee partition column.\n\nPartitioned column\n\nSee partition column.\n\nR\nRegular expression\n\nAn expression used to search for patterns in a string.\n\nSee Also\n\nWikipedia: Regular expression\n\nData definition: Fulltext analyzers\n\nQuerying: Regular expressions\n\nScalar functions: Regular expressions\n\nTable functions: regexp_matches\n\nRouting column\n\nValues in this column are used to compute a hash which is then used to route the corresponding row to a specific shard.\n\nAlso known as the CLUSTERED BY column.\n\nAll rows that have the same routing column row value are stored in the same shard.\n\nNote\n\nThe routing of rows to a specific shard is not the same as the routing of shards to a specific node (also known as shard allocation).\n\nSee Also\n\nStorage and consistency: Addressing documents\n\nSharding: Routing\n\nCREATE TABLE: CLUSTERED clause\n\nS\nScalar\n\nA data type with a single value (e.g., numbers and strings).\n\nContrary to a nonscalar.\n\nSee Also\n\nPrimitive types\n\nShard allocation\n\nThe process by which CrateDB allocates shards to a specific nodes.\n\nNote\n\nShard allocation is sometimes referred to as shard routing, which is not to be confused with row routing.\n\nSee Also\n\nShard allocation filtering\n\nCluster configuration: Routing allocation\n\nSharding: Number of shards\n\nAltering tables: Changing the number of shards\n\nAltering tables: Reroute shards\n\nShard recovery\n\nThe process by which CrateDB synchronizes a replica shard from a primary shard.\n\nShard recovery can happen during node startup, after node failure, when replicating a primary shard, when moving a shard to another node (i.e., when rebalancing the cluster), or during snapshot restoration.\n\nA shard that is being recovered cannot be queried until the recovery process is complete.\n\nSee Also\n\nCluster settings: Recovery\n\nSystem information: Checked node settings\n\nShard routing\n\nSee shard allocation.\n\nStatement\n\nAny valid SQL that serves as a database instruction (e.g., CREATE TABLE, INSERT, and SELECT) instead of producing a value.\n\nContrary to an expression.\n\nSee Also\n\nData definition\n\nData manipulation\n\nQuerying\n\nSQL Statements\n\nSubquery\n\nA SELECT statement used as a relation in the FROM clause of a parent SELECT statement.\n\nAlso known as a subselect.\n\nSubselect\n\nSee subquery.\n\nU\nUnary operator\n\nSee operation.\n\nUncorrelated subquery\n\nA scalar subquery that does not reference any relations (e.g., tables) in the parent SELECT statement.\n\nSee Also\n\nBuilt-ins: Subquery expressions\n\nV\nValue expression\n\nSee expression."
  },
  {
    "title": "Resiliency Issues — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/appendices/resiliency.html",
    "html": "master\nResiliency Issues\n\nCrateDB uses Elasticsearch for data distribution and replication. Most of the resiliency issues exist in the Elasticsearch layer and can be tested by Jepsen.\n\nTable of contents\n\nKnown issues\n\nRetry of updates causes double execution\n\nFixed issues\n\nRepeated cluster partitions can cause lost cluster updates\n\nVersion number representing ambiguous row versions\n\nReplicas can fall out of sync when a primary shard fails\n\nLoss of rows due to network partition\n\nDirty reads caused by bad primary handover\n\nChanges are overwritten by old data in danger of lost data\n\nMake table creation resilient to closing and full cluster crashes\n\nUnaware master accepts cluster updates\n\nKnown issues\nRetry of updates causes double execution\nStatus\tWork ongoing (More info)\nSeverity\tModerate\nLikelihood\tVery rare\nCause\tNetwork issues, unresponsive nodes\nWorkloads\tNon-Idempotent writes\n\nScenario\n\nA node with a primary shard receives an update, writes it to disk, but goes offline before having sent a confirmation back to the executing node. When the node comes back online, it receives an update retry and executes the update again.\n\nConsequence\n\nIncorrect data for non-idempotent writes.\n\nFor example:\n\nAn double insert on a table without an explicit primary key would be executed twice and would result in duplicate data.\n\nA double update would incorrectly increment the row version number twice.\n\nFixed issues\nRepeated cluster partitions can cause lost cluster updates\nStatus\tFixed in CrateDB v4.0 (#32006, #32171)\nSeverity\tModerate\nLikelihood\tVery rare\nCause\tNetwork issues, unresponsive nodes\nWorkloads\tAll\n\nScenario\n\nA cluster is partitioned and a new master is elected on the side that has quorum. The cluster is repaired and simultaneously a change is made to the cluster state. The cluster is partitioned again before the new master node has a chance to publish the new cluster state and the partition the master lands on does not have quorum.\n\nConsequence\n\nThe node steps down as master and the uncommunicated state changes are lost.\n\nCluster state is very important and contains information like shard location, schemas, and so on. Lost cluster state updates can cause data loss, reset settings, and problems with table structures.\n\nPartially fixed\n\nThis problem is mostly fixed by #20384 (CrateDB v2.0.x), which uses committed cluster state updates during master election process. This does not fully solve this rare problem but considerably reduces the chance of occurrence. The reason is that if the second partition happens concurrently with a cluster state update and blocks the cluster state commit message from reaching a majority of nodes, it may be that the in flight update is lost. If the now-isolated master can still acknowledge the cluster state update to the client this will result to the loss of an acknowledged change.\n\nVersion number representing ambiguous row versions\nStatus\tFixed in CrateDB v4.0 (#19269, #10708)\nSeverity\tSignificant\nLikelihood\tVery rare\nCause\tNetwork issues, unresponsive nodes\nWorkloads\tVersioned reads with replicated tables while writing.\n\nScenario\n\nA client is writing to a primary shard. The node holding the primary shard is partitioned from the cluster. It usually takes between 30 and 60 seconds (depending on ping configuration) before the master node notices the partition. During this time, the same row is updated on both the primary shard (partitioned) and a replica shard (not partitioned).\n\nConsequence\n\nThere are two different versions of the same row using the same version number. When the primary shard rejoins the cluster and its data is replicated, the update that was made on the replicated shard is lost but the new version number matches the lost update. This will break Optimistic Concurrency Control.\n\nReplicas can fall out of sync when a primary shard fails\nStatus\tFixed in CrateDB v4.0 (#10708)\nSeverity\tModest\nLikelihood\tRare\nCause\tPrimary fails and in-flight writes are only written to a subset of its replicas\nWorkloads\tWrites on replicated table\n\nScenario\n\nWhen a primary shard fails, a replica shard will be promoted to be the primary shard. If there is more than one replica shard, it is possible for the remaining replicas to be out of sync with the new primary shard. This is caused by operations that were in-flight when the primary shard failed and may not have been processed on all replica shards. Currently, the discrepancies are not repaired on primary promotion but instead would be repaired if replica shards are relocated (e.g., from hot to cold nodes); this does mean that the length of time which replicas can be out of sync with the primary shard is unbounded.\n\nConsequence\n\nStale data may be read from replicas.\n\nLoss of rows due to network partition\nStatus\tFixed in Crate v2.0.x (#7572, #14252)\nSeverity\tSignificant\nLikelihood\tVery rare\nCause\tSingle node isolation\nWorkloads\tWrites on replicated table\n\nScenario\n\nA node with a primary shard is partitioned from the cluster. The node continues to accept writes until it notices the network partition. In the meantime, another shard has been elected as the primary. Eventually, the partitioned node rejoins the cluster.\n\nConsequence\n\nData that was written to the original primary shard on the partitioned node is lost as data from the newly elected primary shard replaces it when it rejoins the cluster.\n\nThe risk window depends on your ping configuration. The default configuration of a 30 second ping timeout with three retries corresponds to a 90 second risk window. However, it is very rare for a node to lose connectivity within the cluster but maintain connectivity with clients.\n\nDirty reads caused by bad primary handover\nStatus\tFixed in CrateDB v2.0.x (#15900, #12573)\nSeverity\tModerate\nLikelihood\tRare\nCause\tRace Condition\nWorkloads\tReads\n\nScenario\n\nDuring a primary handover, there is a small risk window when a shard can find out it has been elected as the new primary before the old primary shard notices that it is no longer the primary.\n\nA primary handover can happen in the following scenarios:\n\nA shard is relocated and then elected as the new primary, as two separate but sequential actions. Relocating a shard means creating a new shard and then deleting the old shard.\n\nAn existing replica shard gets promoted to primary because the primary shard was partitioned from the cluster.\n\nConsequence\n\nWrites that occur on the new primary during the risk window will not be replicated to the old shard (which still believes it is the primary) so any subsequent reads on the old shard may return incorrect data.\n\nChanges are overwritten by old data in danger of lost data\nStatus\tFixed in CrateDB v2.0.x (#14671)\nSeverity\tSignificant\nLikelihood\tVery rare\nCause\tNetwork problems\nWorkloads\tWrites\n\nScenario\n\nA node with a primary that contains new data is partitioned from the cluster.\n\nConsequence\n\nCrateDB prefers old data over no data, and so promotes an a shard with stale data as a new primary. The data on the original primary shard is lost. Even if the node with the original primary shard rejoins the cluster, CrateDB has no way of distinguishing correct and incorrect data, so that data replaced with data from the new primary shard.\n\nMake table creation resilient to closing and full cluster crashes\nStatus\tThe issue has been fixed with the following issues. Table recovery: #9126 Reopening tables: #14739 Allocation IDs: #15281\nSeverity\tModest\nLikelihood\tVery Rare\nCause\tEither the cluster fails while recovering a table or the table is closed during shard creation.\nWorkloads\tTable creation\n\nScenario\n\nRecovering a table requires a quorum of shard copies to be available to allocate a primary. This means that a primary cannot be assigned if the cluster dies before enough shards have been allocated. The same happens if a table is closed before enough shard copies were started, making it impossible to reopen the table. Allocation IDs solve this issue by tracking allocated shard copies in the cluster. This makes it possible to safely recover a table in the presence of a single shard copy. Allocation IDs can also distinguish the situation where a table has been created but none of the shards have been started. If such an table was inadvertently closed before at least one shard could be started, a fresh shard will be allocated upon reopening the table.\n\nConsequence\n\nThe primary shard of the table cannot be assigned or a closed table cannot be re-opened.\n\nUnaware master accepts cluster updates\nStatus\tFixed in CrateDB v2.0.x (#13062)\nSeverity\tModerate\nLikelihood\tVery rare\nCause\tNetwork problems\nWorkloads\tDDL statements\n\nScenario\n\nIf a master has lost quorum (i.e. the number of nodes it is in communication with has fallen below the configured minimum) it should step down as master and stop answering requests to perform cluster updates. There is a small risk window between losing quorum and noticing that quorum has been lost, depending on your ping configuration.\n\nConsequence\n\nIf a cluster update request is made to the node between losing quorum and noticing the loss of quorum, that request will be confirmed. However, those updates will be lost because the node will not be able to perform a successful cluster update.\n\nCluster state is very important and contains information like shard location, schemas, and so on. Lost cluster state updates can cause data loss, reset settings, and problems with table structures."
  },
  {
    "title": "SQL standard compliance — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/appendices/compliance.html",
    "html": "master\nSQL standard compliance\n\nThis page documents the standard SQL (ISO/IEC 9075) features that CrateDB supports, along with implementation notes and any associated caveats.\n\nCaution\n\nThis list is approximate and features that are listed as supported might be nonconforming in their implementation. However, the main reference documentation always contains the most accurate information about the features CrateDB supports and how to use them.\n\nSee Also\n\nSQL compatibility\n\nID\n\n\t\n\nPackage\n\n\t\n\n#\n\n\t\n\nDescription\n\n\t\n\nComments\n\n\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n1\n\n\t\n\nINTEGER and SMALLINT data types\n\n\t\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n2\n\n\t\n\nREAL, DOUBLE PRECISION, and FLOAT data types\n\n\t\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n3\n\n\t\n\nDECIMAL and NUMERIC data types\n\n\t\n\nNot supported in DDL\n\n\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n4\n\n\t\n\nArithmetic operators\n\n\t\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n5\n\n\t\n\nNumeric comparison\n\n\t\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n6\n\n\t\n\nImplicit casting among the numeric data types\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n1\n\n\t\n\nCHARACTER data type\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n2\n\n\t\n\nCHARACTER VARYING data type\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n3\n\n\t\n\nCharacter literals\n\n\t\n\nOnly simple ‘ quoting\n\n\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n4\n\n\t\n\nCHARACTER_LENGTH function\n\n\t\n\nchar_length only\n\n\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n5\n\n\t\n\nOCTET_LENGTH function\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n6\n\n\t\n\nSUBSTRING function\n\n\t\n\nsubstr scalar\n\n\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n7\n\n\t\n\nCharacter concatenation\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n8\n\n\t\n\nUPPER and LOWER functions\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n9\n\n\t\n\nTRIM function\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n10\n\n\t\n\nImplicit casting among the character string types\n\n\t\n\njust one type\n\n\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n12\n\n\t\n\nCharacter comparison\n\n\t\n\n\nE031\n\n\t\n\nIdentifiers\n\n\t\t\t\n\n\nE031\n\n\t\n\nIdentifiers\n\n\t\n\n1\n\n\t\n\nDelimited identifiers\n\n\t\n\n\nE031\n\n\t\n\nIdentifiers\n\n\t\n\n2\n\n\t\n\nLower case identifiers\n\n\t\n\n\nE031\n\n\t\n\nIdentifiers\n\n\t\n\n3\n\n\t\n\nTrailing underscore\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\t\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n1\n\n\t\n\nSELECT DISTINCT\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n2\n\n\t\n\nGROUP BY clause\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n4\n\n\t\n\nGROUP BY can contain columns not in <select list>\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n5\n\n\t\n\nSelect list items can be renamed\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n6\n\n\t\n\nHAVING clause\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n7\n\n\t\n\nQualified * in select list\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n8\n\n\t\n\nCorrelation names in the FROM clause\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n9\n\n\t\n\nRename columns in the FROM clause\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n1\n\n\t\n\nComparison predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n2\n\n\t\n\nBETWEEN predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n3\n\n\t\n\nIN predicate with list of values\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n4\n\n\t\n\nLIKE predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n6\n\n\t\n\nNULL predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n8\n\n\t\n\nEXISTS predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n9\n\n\t\n\nSubqueries in comparison predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n11\n\n\t\n\nSubqueries in IN predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n12\n\n\t\n\nSubqueries in quantified comparison predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n13\n\n\t\n\nCorrelated subqueries\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n14\n\n\t\n\nSearch condition\n\n\t\n\n\nE071\n\n\t\n\nBasic query expressions\n\n\t\n\n1\n\n\t\n\nUNION DISTINCT table operator\n\n\t\n\n\nE071\n\n\t\n\nBasic query expressions\n\n\t\n\n2\n\n\t\n\nUNION ALL table operator\n\n\t\n\n\nE081\n\n\t\n\nBasic Privileges\n\n\t\t\t\n\n\nE081\n\n\t\n\nBasic Privileges\n\n\t\n\n1\n\n\t\n\nSELECT privilege\n\n\t\n\n\nE081\n\n\t\n\nBasic Privileges\n\n\t\n\n2\n\n\t\n\nDELETE privilege\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\t\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n1\n\n\t\n\nAVG\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n2\n\n\t\n\nCOUNT\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n3\n\n\t\n\nMAX\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n4\n\n\t\n\nMIN\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n5\n\n\t\n\nSUM\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n6\n\n\t\n\nALL quantifier\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n7\n\n\t\n\nDISTINCT quantifier\n\n\t\n\n\nE101\n\n\t\n\nBasic data manipulation\n\n\t\t\t\n\n\nE101\n\n\t\n\nBasic data manipulation\n\n\t\n\n1\n\n\t\n\nINSERT statement\n\n\t\n\n\nE101\n\n\t\n\nBasic data manipulation\n\n\t\n\n3\n\n\t\n\nSearched UPDATE statement\n\n\t\n\n\nE101\n\n\t\n\nBasic data manipulation\n\n\t\n\n4\n\n\t\n\nSearched DELETE statement\n\n\t\n\n\nE121\n\n\t\n\nBasic cursor support\n\n\t\n\n1\n\n\t\n\nDECLARE CURSOR\n\n\t\n\n\nE121\n\n\t\n\nBasic cursor support\n\n\t\n\n8\n\n\t\n\nCLOSE statement\n\n\t\n\n\nE121\n\n\t\n\nBasic cursor support\n\n\t\n\n10\n\n\t\n\nFETCH statement implicit NEXT\n\n\t\n\n\nE121\n\n\t\n\nBasic cursor support\n\n\t\n\n17\n\n\t\n\nWITH HOLD cursors\n\n\t\n\n\nE131\n\n\t\n\nNull value support (nulls in lieu of values)\n\n\t\t\t\n\n\nE141\n\n\t\n\nBasic integrity constraints\n\n\t\n\n1\n\n\t\n\nNOT NULL constraints\n\n\t\n\n\nE141\n\n\t\n\nBasic integrity constraints\n\n\t\n\n3\n\n\t\n\nPRIMARY KEY constraints\n\n\t\n\n\nE141\n\n\t\n\nBasic integrity constraints\n\n\t\n\n6\n\n\t\n\nCHECK constraints\n\n\t\n\n\nE141\n\n\t\n\nBasic integrity constraints\n\n\t\n\n7\n\n\t\n\nColumn defaults\n\n\t\n\n\nE141\n\n\t\n\nBasic integrity constraints\n\n\t\n\n8\n\n\t\n\nNOT NULL inferred on PRIMARY KEY\n\n\t\n\n\nE151\n\n\t\n\nTransaction support\n\n\t\n\n1\n\n\t\n\nCOMMIT statement\n\n\t\n\n\nE152\n\n\t\n\nBasic SET TRANSACTION statement\n\n\t\t\t\n\n\nE152\n\n\t\n\nBasic SET TRANSACTION statement\n\n\t\n\n1\n\n\t\n\nSET TRANSACTION statement: ISOLATION LEVEL SERIALIZABLE clause\n\n\t\n\nIs ignored\n\n\n\n\nE152\n\n\t\n\nBasic SET TRANSACTION statement\n\n\t\n\n2\n\n\t\n\nSET TRANSACTION statement: READ ONLY and READ WRITE clauses\n\n\t\n\nIs ignored\n\n\n\n\nE161\n\n\t\n\nSQL comments using leading double minus\n\n\t\t\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n1\n\n\t\n\nCOLUMNS view\n\n\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n2\n\n\t\n\nTABLES view\n\n\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n3\n\n\t\n\nVIEWS view\n\n\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n4\n\n\t\n\nTABLE_CONSTRAINTS view\n\n\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n5\n\n\t\n\nREFERENTIAL_CONSTRAINTS view\n\n\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n6\n\n\t\n\nCHECK_CONSTRAINTS view\n\n\t\n\n\nF031\n\n\t\n\nBasic schema manipulation\n\n\t\n\n1\n\n\t\n\nCREATE TABLE statement to create persistent base tables\n\n\t\n\n\nF031\n\n\t\n\nBasic schema manipulation\n\n\t\n\n2\n\n\t\n\nCREATE VIEW statement\n\n\t\n\n\nF031\n\n\t\n\nBasic schema manipulation\n\n\t\n\n3\n\n\t\n\nGRANT statement\n\n\t\n\n\nF031\n\n\t\n\nBasic schema manipulation\n\n\t\n\n4\n\n\t\n\nALTER TABLE statement: ADD COLUMN clause\n\n\t\n\n\nF033\n\n\t\n\nALTER TABLE statement: DROP COLUMN clause\n\n\t\t\t\n\n\nF034\n\n\t\n\nExtended REVOKE statement\n\n\t\t\t\n\n\nF034\n\n\t\n\nExtended REVOKE statement\n\n\t\n\n1\n\n\t\n\nREVOKE statement performed by other than the owner of a schema object\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\t\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n1\n\n\t\n\nInner join (but not necessarily the INNER keyword)\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n2\n\n\t\n\nINNER keyword\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n3\n\n\t\n\nLEFT OUTER JOIN\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n4\n\n\t\n\nRIGHT OUTER JOIN\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n5\n\n\t\n\nOuter joins can be nested\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n7\n\n\t\n\nThe inner table in a left or right outer join can also be used in an inner join\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n8\n\n\t\n\nAll comparison operators are supported (rather than just =)\n\n\t\n\n\nF051\n\n\t\n\nBasic date and time\n\n\t\n\n1\n\n\t\n\nDATE data type (including support of DATE literal)\n\n\t\n\n\nF051\n\n\t\n\nBasic date and time\n\n\t\n\n3\n\n\t\n\nTIMESTAMP data type (including support of TIMESTAMP literal) with fractional seconds precision of at least 0 and 6\n\n\t\n\n\nF051\n\n\t\n\nBasic date and time\n\n\t\n\n4\n\n\t\n\nComparison predicate on DATE, TIME, and TIMESTAMP data types\n\n\t\n\n\nF051\n\n\t\n\nBasic date and time\n\n\t\n\n5\n\n\t\n\nExplicit CAST between datetime types and character string types\n\n\t\n\n\nF051\n\n\t\n\nBasic date and time\n\n\t\n\n6\n\n\t\n\nCURRENT_DATE\n\n\t\n\n\nF052\n\n\t\n\nIntervals and datetime arithmetic\n\n\t\t\t\n\n\nF111\n\n\t\n\nIsolation levels other than SERIALIZABLE\n\n\t\t\t\n\n\nF111\n\n\t\n\nIsolation levels other than SERIALIZABLE\n\n\t\n\n1\n\n\t\n\nREAD UNCOMMITTED isolation level\n\n\t\n\nIs ignored\n\n\n\n\nF111\n\n\t\n\nIsolation levels other than SERIALIZABLE\n\n\t\n\n2\n\n\t\n\nREAD COMMITTED isolation level\n\n\t\n\nIs ignored\n\n\n\n\nF111\n\n\t\n\nIsolation levels other than SERIALIZABLE\n\n\t\n\n3\n\n\t\n\nREPEATABLE READ isolation level\n\n\t\n\nIs ignored\n\n\n\n\nF131\n\n\t\n\nGrouped operations\n\n\t\n\n1\n\n\t\n\nWHERE, GROUP BY, and HAVING clauses supported in queries with grouped views\n\n\t\n\n\nF131\n\n\t\n\nGrouped operations\n\n\t\n\n3\n\n\t\n\nSet functions supported in queries with grouped views\n\n\t\n\n\nF171\n\n\t\n\nMultiple schemas per user\n\n\t\t\t\n\n\nF201\n\n\t\n\nCAST function\n\n\t\t\t\n\n\nF221\n\n\t\n\nExplicit defaults\n\n\t\t\t\n\n\nF222\n\n\t\n\nINSERT statement: DEFAULT VALUES clause\n\n\t\t\t\n\n\nF261\n\n\t\n\nCASE expression\n\n\t\t\t\n\n\nF261\n\n\t\n\nCASE expression\n\n\t\n\n1\n\n\t\n\nSimple CASE\n\n\t\n\n\nF261\n\n\t\n\nCASE expression\n\n\t\n\n2\n\n\t\n\nSearched CASE\n\n\t\n\n\nF261\n\n\t\n\nCASE expression\n\n\t\n\n3\n\n\t\n\nNULLIF\n\n\t\n\n\nF261\n\n\t\n\nCASE expression\n\n\t\n\n4\n\n\t\n\nCOALESCE\n\n\t\n\n\nF262\n\n\t\n\nExtended CASE expression\n\n\t\t\t\n\n\nF311\n\n\t\n\nSchema definition statement\n\n\t\n\n2\n\n\t\n\nCREATE TABLE for persistent base tables\n\n\t\n\n\nF311\n\n\t\n\nSchema definition statement\n\n\t\n\n3\n\n\t\n\nCREATE VIEW\n\n\t\n\n\nF381\n\n\t\n\nExtended schema manipulation\n\n\t\t\t\n\n\nF381\n\n\t\n\nExtended schema manipulation\n\n\t\n\n1\n\n\t\n\nALTER TABLE statement: ALTER COLUMN clause\n\n\t\n\n\nF381\n\n\t\n\nExtended schema manipulation\n\n\t\n\n2\n\n\t\n\nALTER TABLE statement: ADD CONSTRAINT clause\n\n\t\n\n\nF381\n\n\t\n\nExtended schema manipulation\n\n\t\n\n3\n\n\t\n\nALTER TABLE statement: DROP CONSTRAINT clause\n\n\t\n\n\nF391\n\n\t\n\nLong identifiers\n\n\t\t\t\n\n\nF392\n\n\t\n\nUnicode escapes in identifiers\n\n\t\t\t\n\n\nF401\n\n\t\n\nExtended joined table\n\n\t\n\n2\n\n\t\n\nFULL OUTER JOIN\n\n\t\n\n\nF401\n\n\t\n\nExtended joined table\n\n\t\n\n4\n\n\t\n\nCROSS JOIN\n\n\t\n\n\nF431\n\n\t\n\nRead-only scrollable cursors\n\n\t\t\t\n\n\nF431\n\n\t\n\nRead-only scrollable cursors\n\n\t\n\n1\n\n\t\n\nFETCH with explicit NEXT\n\n\t\n\n\nF431\n\n\t\n\nRead-only scrollable cursors\n\n\t\n\n2\n\n\t\n\nFETCH FIRST\n\n\t\n\n\nF431\n\n\t\n\nRead-only scrollable cursors\n\n\t\n\n3\n\n\t\n\nFETCH LAST\n\n\t\n\n\nF431\n\n\t\n\nRead-only scrollable cursors\n\n\t\n\n4\n\n\t\n\nFETCH PRIOR\n\n\t\n\n\nF431\n\n\t\n\nRead-only scrollable cursors\n\n\t\n\n5\n\n\t\n\nFETCH ABSOLUTE\n\n\t\n\n\nF431\n\n\t\n\nRead-only scrollable cursors\n\n\t\n\n6\n\n\t\n\nFETCH RELATIVE\n\n\t\n\n\nF471\n\n\t\n\nScalar subquery values\n\n\t\t\t\n\n\nF481\n\n\t\n\nExpanded NULL predicate\n\n\t\t\t\n\n\nF501\n\n\t\n\nFeatures and conformance views\n\n\t\n\n1\n\n\t\n\nSQL_FEATURES view\n\n\t\n\n\nF571\n\n\t\n\nTruth value tests\n\n\t\t\t\n\n\nF651\n\n\t\n\nCatalog name qualifiers\n\n\t\t\t\n\n\nF763\n\n\t\n\nCURRENT_SCHEMA\n\n\t\t\t\n\n\nF791\n\n\t\n\nInsensitive cursors\n\n\t\t\t\n\n\nF850\n\n\t\n\nTop-level <order by clause> in <query expression>\n\n\t\t\t\n\n\nF851\n\n\t\n\n<order by clause> in subqueries\n\n\t\t\t\n\n\nF852\n\n\t\n\nTop-level <order by clause> in views\n\n\t\t\t\n\n\nF855\n\n\t\n\nNested <order by clause> in <query expression>\n\n\t\t\t\n\n\nF856\n\n\t\n\nNested <fetch first clause> in <query expression>\n\n\t\t\t\n\n\nF857\n\n\t\n\nTop-level <fetch first clause> in <query expression>\n\n\t\t\t\n\n\nF858\n\n\t\n\n<fetch first clause> in subqueries\n\n\t\t\t\n\n\nF859\n\n\t\n\nTop-level <fetch first clause> in views\n\n\t\t\t\n\n\nF860\n\n\t\n\n<fetch first row count> in <fetch first clause>\n\n\t\t\t\n\n\nS091\n\n\t\n\nBasic array support\n\n\t\t\t\n\nspecial syntax\n\n\n\n\nS091\n\n\t\n\nBasic array support\n\n\t\n\n1\n\n\t\n\nArrays of built-in data types\n\n\t\n\nspecial syntax\n\n\n\n\nS098\n\n\t\n\nARRAY_AGG\n\n\t\t\t\n\n\nT031\n\n\t\n\nBOOLEAN data type\n\n\t\t\t\n\n\nT051\n\n\t\n\nRow types\n\n\t\t\t\n\nLimited to built-in table functions\n\n\n\n\nT054\n\n\t\n\nGREATEST and LEAST\n\n\t\t\t\n\n\nT055\n\n\t\n\nString padding functions\n\n\t\t\t\n\n\nT056\n\n\t\n\nMulti-character trim functions\n\n\t\t\t\n\n\nT071\n\n\t\n\nBIGINT data type\n\n\t\t\t\n\n\nT081\n\n\t\n\nOptional string types maximum length\n\n\t\t\t\n\n\nT121\n\n\t\n\nWITH (excluding RECURSIVE) in query expression\n\n\t\t\t\n\n\nT122\n\n\t\n\nWITH (excluding RECURSIVE) in subquery\n\n\t\t\t\n\n\nT175\n\n\t\n\nGenerated columns\n\n\t\t\t\n\n\nT241\n\n\t\n\nSTART TRANSACTION statement\n\n\t\t\t\n\nIs ignored\n\n\n\n\nT321\n\n\t\n\nBasic SQL-invoked routines\n\n\t\n\n1\n\n\t\n\nUser-defined functions with no overloading\n\n\t\n\n\nT321\n\n\t\n\nBasic SQL-invoked routines\n\n\t\n\n3\n\n\t\n\nFunction invocation\n\n\t\n\n\nT321\n\n\t\n\nBasic SQL-invoked routines\n\n\t\n\n6\n\n\t\n\nROUTINES view\n\n\t\n\n\nT331\n\n\t\n\nBasic roles\n\n\t\t\t\n\n\nT351\n\n\t\n\nBracketed SQL comments (/…/ comments)\n\n\t\t\t\n\n\nT441\n\n\t\n\nABS and MOD functions\n\n\t\t\t\n\n\nT461\n\n\t\n\nSymmetric BETWEEN predicate\n\n\t\t\t\n\n\nT471\n\n\t\n\nResult sets return value\n\n\t\t\t\n\n\nT615\n\n\t\n\nLEAD and LAG functions\n\n\t\t\t\n\n\nT617\n\n\t\n\nFIRST_VALUE and LAST_VALUE function\n\n\t\t\t\n\n\nT618\n\n\t\n\nNTH_VALUE function\n\n\t\t\t\n\n\nT621\n\n\t\n\nEnhanced numeric functions\n\n\t\t\t\n\n\nT626\n\n\t\n\nANY_VALUE aggregation\n\n\t\t\t\n\n\nT631\n\n\t\n\nIN predicate with one list element\n\n\t\t\t\n\n\nT662\n\n\t\n\nUnderscores in numeric literals\n\n\t\t\t"
  },
  {
    "title": "SQL compatibility — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/appendices/compatibility.html",
    "html": "master\nSQL compatibility\n\nCrateDB provides a standards-based SQL implementation similar to many other SQL databases. In particular, CrateDB aims for compatibility with PostgreSQL. However, CrateDB’s SQL dialect does have some unique characteristics, documented on this page.\n\nSee Also\n\nSQL: Syntax reference\n\nTable of contents\n\nImplementation notes\n\nData types\n\nCreate table\n\nAlter table\n\nSystem information tables\n\nBLOB support\n\nTransactions (BEGIN, START, COMMIT, and ROLLBACK)\n\nUnsupported features and functions\n\nImplementation notes\nData types\n\nCrateDB supports a set of primitive data types. The following table defines how data types of standard SQL map to CrateDB Data types.\n\nStandard SQL\n\n\t\n\nCrateDB\n\n\n\n\ninteger\n\n\t\n\ninteger, int, int4\n\n\n\n\nbit[8]\n\n\t\n\nbyte, char\n\n\n\n\nboolean, bool\n\n\t\n\nboolean\n\n\n\n\nchar [(n)], varchar [(n)]\n\n\t\n\nstring, text, varchar, character varying\n\n\n\n\ntimestamp with time zone\n\n\t\n\ntimestamp with time zone, timestamptz\n\n\n\n\ntimestamp\n\n\t\n\ntimestamp without time zone\n\n\n\n\nsmallint\n\n\t\n\nshort, int2, smallint\n\n\n\n\nbigint\n\n\t\n\nlong, bigint, int8\n\n\n\n\nreal\n\n\t\n\nfloat, real\n\n\n\n\ndouble precision\n\n\t\n\ndouble, double precision\n\nCreate table\n\nCREATE TABLE supports additional storage and table parameters for sharding, replication and routing of data, and does not support inheritance.\n\nAlter table\n\nALTER COLUMN action is not currently supported (see ALTER TABLE).\n\nSystem information tables\n\nThe read-only System information and Information schema tables have a slightly different schema than specified in standard SQL. They provide schema information and can be queried to get real-time statistical data about the cluster, its nodes, and their shards.\n\nBLOB support\n\nStandard SQL defines a binary string type, called BLOB or BINARY LARGE OBJECT. With CrateDB, Binary Data is instead stored in separate BLOB Tables (see Blobs) which can be sharded and replicated.\n\nTransactions (BEGIN, START, COMMIT, and ROLLBACK)\n\nCrateDB is focused on providing analytical capabilities over supporting traditional transactional use cases, and thus it does not provide transaction control. Every statement commits immediately and is replicated within the cluster.\n\nHowever, every row in CrateDB has a version number that is incremented whenever the record is modified. This version number can be used to implement patterns like Optimistic Concurrency Control, which can be used to solve many of the use cases that would otherwise require traditional transactions.\n\nUnsupported features and functions\n\nThese features of standard SQL are not supported:\n\nStored procedures\n\nTriggers\n\nWITH Queries (Common Table Expressions)\n\nSequences\n\nInheritance\n\nConstraints\n\nUnique\n\nForeign key\n\nExclusion constraints\n\nThese functions of standard SQL are either not supported or only partly supported:\n\nAggregate functions\n\nVarious functions available (see Aggregation)\n\nWindow functions\n\nVarious functions available (see Window functions)\n\nENUM support functions\n\nIS DISTINCT FROM\n\nNetwork address functions and operators\n\nMathematical functions\n\nCertain functions supported (see Mathematical functions)\n\nSet returning functions\n\nTrigger functions\n\nXML functions\n\nNote\n\nThe currently supported and unsupported features in CrateDB are exposed in the Information schema table (see sql_features for usage).\n\nCrateDB also supports the PostgreSQL wire protocol.\n\nIf you have use cases for any missing features, functions, or dialect improvements, let us know on GitHub! We are always improving and extending CrateDB and would love to hear your feedback."
  },
  {
    "title": "Release Notes — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/appendices/release-notes/index.html",
    "html": "master\nRelease Notes\n\nInformation about individual CrateDB releases, typically including upgrade information and changelog.\n\nNote\n\nThe latest stable, testing, and nightly version is always available from the CrateDB download page.\n\nOlder version of CrateDB are available in the release archives.\n\nVersions\n5.x\n5.7.x\nVersion 5.7.0 - Unreleased\n5.6.x\nVersion 5.6.4 - Unreleased\nVersion 5.6.3\nVersion 5.6.2\nVersion 5.6.1\nVersion 5.6.0\n5.5.x\nVersion 5.5.5 - Unreleased\nVersion 5.5.4\nVersion 5.5.3\nVersion 5.5.2\nVersion 5.5.1\nVersion 5.5.0\n5.4.x\nVersion 5.4.8\nVersion 5.4.7\nVersion 5.4.6\nVersion 5.4.5\nVersion 5.4.4\nVersion 5.4.3\nVersion 5.4.2\nVersion 5.4.1\nVersion 5.4.0\n5.3.x\nVersion 5.3.9\nVersion 5.3.8\nVersion 5.3.7\nVersion 5.3.6\nVersion 5.3.5\nVersion 5.3.4\nVersion 5.3.3\nVersion 5.3.2\nVersion 5.3.1\nVersion 5.3.0\n5.2.x\nVersion 5.2.11\nVersion 5.2.10\nVersion 5.2.9\nVersion 5.2.8\nVersion 5.2.7\nVersion 5.2.6\nVersion 5.2.5\nVersion 5.2.4\nVersion 5.2.3\nVersion 5.2.2\nVersion 5.2.1\nVersion 5.2.0\n5.1.x\nVersion 5.1.4\nVersion 5.1.3\nVersion 5.1.2\nVersion 5.1.1\nVersion 5.1.0\n5.0.x\nVersion 5.0.3\nVersion 5.0.2\nVersion 5.0.1\nVersion 5.0.0\n4.x\n4.8.x\nVersion 4.8.4\nVersion 4.8.3\nVersion 4.8.2\nVersion 4.8.1\nVersion 4.8.0\n4.7.x\nVersion 4.7.3\nVersion 4.7.2\nVersion 4.7.1\nVersion 4.7.0\n4.6.x\nVersion 4.6.8\nVersion 4.6.7\nVersion 4.6.6\nVersion 4.6.5\nVersion 4.6.4\nVersion 4.6.3\nVersion 4.6.2\nVersion 4.6.1\nVersion 4.6.0\n4.5.x\nVersion 4.5.5\nVersion 4.5.4\nVersion 4.5.3\nVersion 4.5.2\nVersion 4.5.1\nVersion 4.5.0\n4.4.x\nVersion 4.4.3\nVersion 4.4.2\nVersion 4.4.1\nVersion 4.4.0\n4.3.x\nVersion 4.3.4\nVersion 4.3.3\nVersion 4.3.2\nVersion 4.3.1\nVersion 4.3.0\n4.2.x\nVersion 4.2.7\nVersion 4.2.6\nVersion 4.2.5\nVersion 4.2.4\nVersion 4.2.3\nVersion 4.2.2\nVersion 4.2.1\nVersion 4.2.0\n4.1.x\nVersion 4.1.8\nVersion 4.1.7\nVersion 4.1.6\nVersion 4.1.5\nVersion 4.1.4\nVersion 4.1.3\nVersion 4.1.2\nVersion 4.1.1\nVersion 4.1.0\n4.0.x\nVersion 4.0.12\nVersion 4.0.11\nVersion 4.0.10\nVersion 4.0.9\nVersion 4.0.8\nVersion 4.0.7\nVersion 4.0.6\nVersion 4.0.5\nVersion 4.0.4\nVersion 4.0.3\nVersion 4.0.2\nVersion 4.0.1\nVersion 4.0.0\n3.x\n3.3.x\nVersion 3.3.6\nVersion 3.3.5\nVersion 3.3.4\nVersion 3.3.3\nVersion 3.3.2\nVersion 3.3.1\nVersion 3.3.0\n3.2.x\nVersion 3.2.8\nVersion 3.2.7\nVersion 3.2.6\nVersion 3.2.5\nVersion 3.2.4\nVersion 3.2.3\nVersion 3.2.2\nVersion 3.2.1\nVersion 3.2.0\n3.1.x\nVersion 3.1.6\nVersion 3.1.5\nVersion 3.1.4\nVersion 3.1.3\nVersion 3.1.2\nVersion 3.1.1\nVersion 3.1.0\n3.0.x\nVersion 3.0.7\nVersion 3.0.6\nVersion 3.0.5\nVersion 3.0.4\nVersion 3.0.3\nVersion 3.0.2\nVersion 3.0.1\nVersion 3.0.0\n2.x\n2.3.x\nVersion 2.3.11\nVersion 2.3.10\nVersion 2.3.9\nVersion 2.3.8\nVersion 2.3.7\nVersion 2.3.6\nVersion 2.3.5\nVersion 2.3.4\nVersion 2.3.3\nVersion 2.3.2\nVersion 2.3.1\nVersion 2.3.0\n2.2.x\nVersion 2.2.7\nVersion 2.2.6\nVersion 2.2.5\nVersion 2.2.4\nVersion 2.2.3\nVersion 2.2.2\nVersion 2.2.1\nVersion 2.2.0\n2.1.x\nVersion 2.1.10\nVersion 2.1.9\nVersion 2.1.8\nVersion 2.1.7\nVersion 2.1.6\nVersion 2.1.5\nVersion 2.1.4\nVersion 2.1.3\nVersion 2.1.2\nVersion 2.1.1\nVersion 2.1.0\n2.0.x\nVersion 2.0.7\nVersion 2.0.6\nVersion 2.0.5\nVersion 2.0.4\nVersion 2.0.3\nVersion 2.0.2\nVersion 2.0.1\nVersion 2.0.0\n1.x\n1.2.x\nVersion 1.2.0\n1.1.x\nVersion 1.1.6\nVersion 1.1.5\nVersion 1.1.4\nVersion 1.1.3\nVersion 1.1.2\nVersion 1.1.1\nVersion 1.1.0\n1.0.x\nVersion 1.0.6\nVersion 1.0.5\nVersion 1.0.4\nVersion 1.0.3\nVersion 1.0.2\nVersion 1.0.1\nVersion 1.0.0\nOlder Versions\n\nFor older versions, see the 0.57.8 CHANGES.txt file."
  },
  {
    "title": "PostgreSQL wire protocol — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/interfaces/postgres.html",
    "html": "master\nPostgreSQL wire protocol\n\nCrateDB supports the PostgreSQL wire protocol v3.\n\nIf a node is started with PostgreSQL wire protocol support enabled it will bind to port 5432 by default. To use a custom port, set the corresponding Ports in the Configuration.\n\nHowever, even though connecting PostgreSQL tools and client libraries is supported, the actual SQL statements have to be supported by CrateDB’s SQL dialect. A notable difference is that CrateDB doesn’t support transactions, which is why clients should generally enable autocommit.\n\nNote\n\nIn order to use setFetchSize in JDBC it is possible to set auto commit to false.\n\nThe client will utilize the fetchSize on SELECT statements and only load up to fetchSize rows into memory.\n\nSee the PostgreSQL JDBC Query docs for more information.\n\nWrite operations will still behave as if auto commit was enabled and commit or rollback calls are ignored.\n\nTable of contents\n\nServer compatibility\n\nStart-up\n\nSSL Support\n\nAuthentication\n\nParameterStatus\n\nDatabase selection\n\nQuery modes\n\nSimple query\n\nExtended query\n\nCopy operations\n\nFunction call\n\nCanceling requests\n\npg_catalog\n\npg_type\n\nOID types\n\nShow transaction isolation\n\nBEGIN, START, and COMMIT statements\n\nClient compatibility\n\nJDBC\n\nLimitations\n\nConnection failover and load balancing\n\nImplementation differences\n\nCopy operations\n\nData types\n\nDates and times\n\nObjects\n\nArrays\n\nDeclaration of arrays\n\nType casts\n\nText search functions and operators\n\nServer compatibility\n\nCrateDB emulates PostgreSQL server version 14.\n\nStart-up\nSSL Support\n\nSSL can be configured using Secured communications (SSL/TLS).\n\nAuthentication\n\nAuthentication methods can be configured using Host-Based Authentication (HBA).\n\nParameterStatus\n\nAfter the authentication succeeded, the server has the possibility to send multiple ParameterStatus messages to the client. These are used to communicate information like server_version (emulates PostgreSQL 9.5) or server_encoding.\n\nCrateDB also sends a message containing the crate_version parameter. This contains the current CrateDB version number.\n\nThis information is useful for clients to detect that they’re connecting to CrateDB instead of a PostgreSQL instance.\n\nDatabase selection\n\nSince CrateDB uses schemas instead of databases, the database parameter sets the default schema name for future queries. If no schema is specified, the schema doc will be used as default. Additionally, the only supported charset is UTF8.\n\nQuery modes\nSimple query\n\nThe PostgreSQL simple query protocol mode is fully implemented.\n\nExtended query\n\nThe PostgreSQL extended query protocol mode is implemented with the following limitations:\n\nThe ParameterDescription message works for the most common use cases except for DDL statements.\n\nTo optimize the execution of bulk operations the execution of statements is delayed until the Sync message is received\n\nCopy operations\n\nCrateDB does not support the COPY sub-protocol, see also Copy operations.\n\nFunction call\n\nThe function call sub-protocol is not supported since it’s a legacy feature.\n\nCanceling requests\n\nPostgreSQL cancelling requests is fully implemented.\n\npg_catalog\n\nFor improved compatibility, the pg_catalog schema is implemented containing following tables:\n\npg_am\n\npg_attrdef\n\npg_attribute\n\npg_class\n\npg_constraint\n\npg_cursors\n\npg_database\n\npg_depend\n\npg_description\n\npg_enum\n\npg_event_trigger\n\npg_index\n\npg_indexes\n\npg_locks\n\npg_namespace\n\npg_proc\n\npg_publication\n\npg_publication_tables\n\npg_range\n\npg_roles\n\npg_settings\n\npg_shdescription\n\npg_stats\n\npg_subscription\n\npg_subscription_rel\n\npg_tables\n\npg_tablespace\n\npg_type\n\npg_views\n\npg_type\n\nSome clients require the pg_catalog.pg_type in order to be able to stream arrays or other non-primitive types.\n\nFor compatibility reasons, there is a trimmed down pg_type table available in CrateDB:\n\ncr> SELECT oid, typname, typarray, typelem, typlen, typtype, typcategory\n... FROM pg_catalog.pg_type\n... ORDER BY oid;\n+------+--------------+----------+---------+--------+---------+-------------+\n|  oid | typname      | typarray | typelem | typlen | typtype | typcategory |\n+------+--------------+----------+---------+--------+---------+-------------+\n|   16 | bool         |     1000 |       0 |      1 | b       | N           |\n|   18 | char         |     1002 |       0 |      1 | b       | S           |\n|   19 | name         |       -1 |       0 |     64 | b       | S           |\n|   20 | int8         |     1016 |       0 |      8 | b       | N           |\n|   21 | int2         |     1005 |       0 |      2 | b       | N           |\n|   23 | int4         |     1007 |       0 |      4 | b       | N           |\n|   24 | regproc      |     1008 |       0 |      4 | b       | N           |\n|   25 | text         |     1009 |       0 |     -1 | b       | S           |\n|   26 | oid          |     1028 |       0 |      4 | b       | N           |\n|   30 | oidvector    |     1013 |      26 |     -1 | b       | A           |\n|  114 | json         |      199 |       0 |     -1 | b       | U           |\n|  199 | _json        |        0 |     114 |     -1 | b       | A           |\n|  600 | point        |     1017 |       0 |     16 | b       | G           |\n|  700 | float4       |     1021 |       0 |      4 | b       | N           |\n|  701 | float8       |     1022 |       0 |      8 | b       | N           |\n|  705 | unknown      |        0 |       0 |     -2 | p       | X           |\n| 1000 | _bool        |        0 |      16 |     -1 | b       | A           |\n| 1002 | _char        |        0 |      18 |     -1 | b       | A           |\n| 1005 | _int2        |        0 |      21 |     -1 | b       | A           |\n| 1007 | _int4        |        0 |      23 |     -1 | b       | A           |\n| 1008 | _regproc     |        0 |      24 |     -1 | b       | A           |\n| 1009 | _text        |        0 |      25 |     -1 | b       | A           |\n| 1014 | _bpchar      |        0 |    1042 |     -1 | b       | A           |\n| 1015 | _varchar     |        0 |    1043 |     -1 | b       | A           |\n| 1016 | _int8        |        0 |      20 |     -1 | b       | A           |\n| 1017 | _point       |        0 |     600 |     -1 | b       | A           |\n| 1021 | _float4      |        0 |     700 |     -1 | b       | A           |\n| 1022 | _float8      |        0 |     701 |     -1 | b       | A           |\n| 1042 | bpchar       |     1014 |       0 |     -1 | b       | S           |\n| 1043 | varchar      |     1015 |       0 |     -1 | b       | S           |\n| 1082 | date         |     1182 |       0 |      8 | b       | D           |\n| 1114 | timestamp    |     1115 |       0 |      8 | b       | D           |\n| 1115 | _timestamp   |        0 |    1114 |     -1 | b       | A           |\n| 1182 | _date        |        0 |    1082 |     -1 | b       | A           |\n| 1184 | timestamptz  |     1185 |       0 |      8 | b       | D           |\n| 1185 | _timestamptz |        0 |    1184 |     -1 | b       | A           |\n| 1186 | interval     |     1187 |       0 |     16 | b       | T           |\n| 1187 | _interval    |        0 |    1186 |     -1 | b       | A           |\n| 1231 | _numeric     |        0 |    1700 |     -1 | b       | A           |\n| 1266 | timetz       |     1270 |       0 |     12 | b       | D           |\n| 1270 | _timetz      |        0 |    1266 |     -1 | b       | A           |\n| 1560 | bit          |     1561 |       0 |     -1 | b       | V           |\n| 1561 | _bit         |        0 |    1560 |     -1 | b       | A           |\n| 1700 | numeric      |     1231 |       0 |     -1 | b       | N           |\n| 2205 | regclass     |     2210 |       0 |      4 | b       | N           |\n| 2210 | _regclass    |        0 |    2205 |     -1 | b       | A           |\n| 2249 | record       |     2287 |       0 |     -1 | p       | P           |\n| 2276 | any          |        0 |       0 |      4 | p       | P           |\n| 2277 | anyarray     |        0 |    2276 |     -1 | p       | P           |\n| 2287 | _record      |        0 |    2249 |     -1 | p       | A           |\n+------+--------------+----------+---------+--------+---------+-------------+\nSELECT 50 rows in set (... sec)\n\n\nNote\n\nThis is just a snapshot of the table.\n\nCheck table information_schema.columns to get information for all supported columns.\n\nOID types\n\nObject Identifiers (OIDs) are used internally by PostgreSQL as primary keys for various system tables.\n\nCrateDB supports the oid type and the following aliases:\n\nName\n\n\t\n\nReference\n\n\t\n\nDescription\n\n\t\n\nExample\n\n\n\n\nregproc\n\n\t\n\npg_proc\n\n\t\n\nA function name\n\n\t\n\nsum\n\n\n\n\nregclass\n\n\t\n\npg_class\n\n\t\n\nA relation name\n\n\t\n\npg_type\n\nCrateDB also supports the oidvector type.\n\nNote\n\nCasting a string or an integer to the regproc type does not result in a function lookup (as it does with PostgreSQL).\n\nInstead:\n\nCasting a string to the regproc type results in an object of the regproc type with a name equal to the string value and an oid equal to an integer hash of the string.\n\nCasting an integer to the regproc type results in an object of the regproc type with a name equal to the string representation of the integer and an oid equal to the integer value.\n\nConsult the CrateDB data types reference for more information about each OID type (including additional type casting behaviour).\n\nShow transaction isolation\n\nFor compatibility with JDBC the SHOW TRANSACTION ISOLATION LEVEL statement is implemented:\n\ncr> show transaction isolation level;\n+-----------------------+\n| transaction_isolation |\n+-----------------------+\n| read uncommitted      |\n+-----------------------+\nSHOW 1 row in set (... sec)\n\nBEGIN, START, and COMMIT statements\n\nFor compatibility with clients that use the PostgresSQL wire protocol (e.g., the Golang lib/pq and pgx drivers), CrateDB will accept the BEGIN, COMMIT, and START TRANSACTION statements. For example:\n\ncr> BEGIN TRANSACTION ISOLATION LEVEL READ UNCOMMITTED,\n...                   READ ONLY,\n...                   NOT DEFERRABLE;\nBEGIN OK, 0 rows affected  (... sec)\n\ncr> COMMIT\nCOMMIT OK, 0 rows affected  (... sec)\n\n\nCrateDB will silently ignore the COMMIT, BEGIN, and START TRANSACTION statements and all respective parameters.\n\nClient compatibility\nJDBC\n\npgjdbc JDBC drivers version 9.4.1209 and above are compatible.\n\nLimitations\n\nReflection methods like conn.getMetaData().getTables(...) won’t work since the required tables are unavailable in CrateDB.\n\nAs a workaround it’s possible to use SHOW TABLES or query the information_schema tables manually using SELECT statements.\n\nOBJECT and GEO_SHAPE columns can be streamed as JSON but require pgjdbc version 9.4.1210 or newer.\n\nMultidimensional arrays will be streamed as JSON encoded string to avoid a protocol limitation where all sub-arrays are required to have the same length.\n\nThe behavior of PreparedStatement.executeBatch in error cases depends on in which stage an error occurs: A BatchUpdateException is thrown if no processing has been done yet, whereas single operations failing after the processing started are indicated by an EXECUTE_FAILED (-3) return value.\n\nTransaction limitations as described above.\n\nHaving escape processing enabled could prevent the usage of Object Literals in case an object key’s starting character clashes with a JDBC escape keyword (see also JDBC escape syntax). Disabling escape processing will remedy this appropriately for pgjdbc version >= 9.4.1212.\n\nConnection failover and load balancing\n\nConnection failover and load balancing is supported as described here: PostgreSQL JDBC connection failover.\n\nNote\n\nIt is not recommended to use the targetServerType parameter since CrateDB has no concept of master-replica nodes.\n\nImplementation differences\n\nThe PostgreSQL Wire Protocol makes it easy to use many PostgreSQL compatible tools and libraries directly with CrateDB. However, many of these tools assume that they are talking to PostgreSQL specifically, and thus rely on SQL extensions and idioms that are unique to PostgreSQL. Because of this, some tools or libraries may not work with other SQL databases such as CrateDB.\n\nCrateDB’s SQL query engine enables real-time search & aggregations for online analytic processing (OLAP) and business intelligence (BI) with the benefit of the ability to scale horizontally. The use-cases of CrateDB are different than those of PostgreSQL, as CrateDB’s specialized storage schema and query execution engine addresses different needs (see Clustering).\n\nThe features listed below cover the main differences in implementation and dialect between CrateDB and PostgreSQL. A detailed comparison between CrateDB’s SQL dialect and standard SQL is outlined in SQL compatibility.\n\nCopy operations\n\nCrateDB does not support the distinct sub-protocol that is used to serve COPY operations and provides another implementation for transferring bulk data using the COPY FROM and COPY TO statements.\n\nData types\nDates and times\n\nAt the moment, CrateDB does not support TIME without a time zone.\n\nAdditionally, CrateDB does not support the INTERVAL input units MILLENNIUM, CENTURY, DECADE, MILLISECOND, or MICROSECOND.\n\nObjects\n\nThe definition of structured values by using JSON types, composite types or HSTORE are not supported. CrateDB alternatively allows the definition of nested documents (of type OBJECT) that store fields containing any CrateDB supported data type, including nested object types.\n\nArrays\nDeclaration of arrays\n\nWhile multidimensional arrays in PostgreSQL must have matching extends for each dimension, CrateDB allows different length nested arrays as this example shows:\n\ncr> select [[1,2,3],[1,2]] from sys.cluster;\n+---------------------+\n| [[1, 2, 3], [1, 2]] |\n+---------------------+\n| [[1, 2, 3], [1, 2]] |\n+---------------------+\nSELECT 1 row in set (... sec)\n\nType casts\n\nCrateDB accepts the Type casting syntax for conversion of one data type to another.\n\nSee Also\n\nPostgreSQL value expressions\n\nCrateDB value expressions\n\nText search functions and operators\n\nThe functions and operators provided by PostgreSQL for full-text search (see PostgreSQL fulltext Search) are not compatible with those provided by CrateDB.\n\nIf you are missing features, functions or dialect improvements and have a great use case for it, let us know on GitHub. We’re always improving and extending CrateDB and we love to hear feedback."
  },
  {
    "title": "HTTP endpoint — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/interfaces/http.html",
    "html": "master\nHTTP endpoint\n\nCrateDB provides a HTTP Endpoint that can be used to submit SQL queries. The endpoint is accessible under <servername:port>/_sql.\n\nSQL statements are sent to the _sql endpoint in json format, whereby the statement is sent as value associated to the key stmt.\n\nSee Also\n\nData manipulation\n\nA simple SELECT statement can be submitted like this:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' \\\n... -d '{\"stmt\":\"select name, position from locations order by id limit 2\"}'\n{\n  \"cols\": [\n    \"name\",\n    \"position\"\n  ],\n  \"rows\": [\n    [\n      \"North West Ripple\",\n      1\n    ],\n    [\n      \"Outer Eastern Rim\",\n      2\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nNote\n\nWe’re using a simple command line invocation of curl here so you can see how to run this by hand in the terminal. For the rest of the examples in this document, we use here documents (i.e. EOF) for multi line readability.\n\nTable of contents\n\nParameter substitution\n\nDefault schema\n\nColumn types\n\nAvailable data types\n\nBulk operations\n\nError handling\n\nError codes\n\nBulk errors\n\nParameter substitution\n\nIn addition to the stmt key the request body may also contain an args key which can be used for SQL parameter substitution.\n\nThe SQL statement has to be changed to use placeholders where the values should be inserted. Placeholders can either be numbered (in the form of $1, $2, etc.) or unnumbered using a question mark ?.\n\nThe placeholders will then be substituted with values from an array that is expected under the args key:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\":\n...     \"select date,position from locations\n...     where date <= \\$1 and position < \\$2 order by position\",\n...   \"args\": [\"1979-10-12\", 3]\n... }\n... EOF\n{\n  \"cols\": [\n    \"date\",\n    \"position\"\n  ],\n  \"rows\": [\n    [\n      308534400000,\n      1\n    ],\n    [\n      308534400000,\n      2\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nNote\n\nIn this example the placeholders start with an backslash due to shell escaping.\n\nWarning\n\nParameter substitution must not be used within subscript notation.\n\nFor example, column[?] is not allowed.\n\nThe same query using question marks as placeholders looks like this:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\":\n...     \"select date,position from locations\n...     where date <= ? and position < ? order by position\",\n...   \"args\": [\"1979-10-12\", 3]\n... }\n... EOF\n{\n  \"cols\": [\n    \"date\",\n    \"position\"\n  ],\n  \"rows\": [\n    [\n      308534400000,\n      1\n    ],\n    [\n      308534400000,\n      2\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nNote\n\nWith some queries the row count is not ascertainable. In this cases rowcount is -1.\n\nDefault schema\n\nIt is possible to set a default schema while querying the CrateDB cluster via _sql end point. In such case the HTTP request should contain the Default-Schema header with the specified schema name:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' \\\n... -H 'Default-Schema: doc' -d@- <<- EOF\n... {\n...   \"stmt\":\"select name, position from locations order by id limit 2\"\n... }\n... EOF\n{\n  \"cols\": [\n    \"name\",\n    \"position\"\n  ],\n  \"rows\": [\n    [\n      \"North West Ripple\",\n      1\n    ],\n    [\n      \"Outer Eastern Rim\",\n      2\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nIf the schema name is not specified in the header, the default doc schema will be used instead.\n\nColumn types\n\nCrateDB can respond a list col_types with the data type ID of every responded column. This way one can know what exact data type a column is holding.\n\nIn order to get the list of column data types, a types query parameter must be passed to the request:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql?types' -d@- <<- EOF\n... {\n...   \"stmt\":\n...     \"select date, position from locations\n...      where date <= \\$1 and position < \\$2 order by position\",\n...   \"args\": [\"1979-10-12\", 3]\n... }\n... EOF\n{\n  \"cols\": [\n    \"date\",\n    \"position\"\n  ],\n  \"col_types\": [\n    11,\n    9\n  ],\n  \"rows\": [\n    [\n      308534400000,\n      1\n    ],\n    [\n      308534400000,\n      2\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nThe Array collection data type is displayed as a list where the first value is the collection type and the second is the inner type. The inner type could also be a collection.\n\nExample of JSON representation of a column list of (String, Integer[]):\n\n\"column_types\": [ 4, [ 100, 9 ] ]\n\nAvailable data types\n\nIDs of all currently available data types:\n\nID\n\n\t\n\nData type\n\n\n\n\n0\n\n\t\n\nNULL\n\n\n\n\n1\n\n\t\n\nNot supported\n\n\n\n\n2\n\n\t\n\nCHAR\n\n\n\n\n3\n\n\t\n\nBOOLEAN\n\n\n\n\n4\n\n\t\n\nTEXT\n\n\n\n\n5\n\n\t\n\nIP\n\n\n\n\n6\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\n7\n\n\t\n\nREAL\n\n\n\n\n8\n\n\t\n\nSMALLINT\n\n\n\n\n9\n\n\t\n\nINTEGER\n\n\n\n\n10\n\n\t\n\nBIGINT\n\n\n\n\n11\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\n12\n\n\t\n\nOBJECT\n\n\n\n\n13\n\n\t\n\nGEO_POINT\n\n\n\n\n14\n\n\t\n\nGEO_SHAPE\n\n\n\n\n15\n\n\t\n\nTIMESTAMP WITHOUT TIME ZONE\n\n\n\n\n16\n\n\t\n\nUnchecked object\n\n\n\n\n17\n\n\t\n\nINTERVAL\n\n\n\n\n19\n\n\t\n\nREGPROC\n\n\n\n\n20\n\n\t\n\nTIME\n\n\n\n\n21\n\n\t\n\nOIDVECTOR\n\n\n\n\n22\n\n\t\n\nNUMERIC\n\n\n\n\n23\n\n\t\n\nREGCLASS\n\n\n\n\n24\n\n\t\n\nDATE\n\n\n\n\n25\n\n\t\n\nBIT\n\n\n\n\n26\n\n\t\n\nJSON\n\n\n\n\n27\n\n\t\n\nCHARACTER\n\n\n\n\n28\n\n\t\n\nFLOAT VECTOR\n\n\n\n\n100\n\n\t\n\nARRAY\n\nBulk operations\n\nThe REST endpoint allows to issue bulk operations which are executed as single calls on the back-end site. It can be compared to prepared statement.\n\nA bulk operation can be expressed simply as an SQL statement.\n\nSupported bulk SQL statements are:\n\nInsert\n\nUpdate\n\nDelete\n\nInstead of the args (Parameter substitution) key, use the key bulk_args. This allows to specify a list of lists, containing all the records which shall be processed. The inner lists need to match the specified columns.\n\nThe bulk response contains a results array, with a row count for each bulk operation. Those results are in the same order as the issued operations of the bulk operation.\n\nThe following example describes how to issue an insert bulk operation and insert three records at once:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\": \"INSERT INTO locations (id, name, kind, description)\n...           VALUES (?, ?, ?, ?)\",\n...   \"bulk_args\": [\n...     [1337, \"Earth\", \"Planet\", \"An awesome place to spend some time on.\"],\n...     [1338, \"Sun\", \"Star\", \"An extraordinarily hot place.\"],\n...     [1339, \"Titan\", \"Moon\", \"Titan, where it rains fossil fuels.\"]\n...   ]\n... }\n... EOF\n{\n  \"cols\": [],\n  \"duration\": ...,\n  \"results\": [\n    {\n      \"rowcount\": 1\n    },\n    {\n      \"rowcount\": 1\n    },\n    {\n      \"rowcount\": 1\n    }\n  ]\n}\n\nError handling\n\nQueries that are invalid or cannot be satisfied will result in an error response. The response will contain an error code, an error message and in some cases additional arguments that are specific to the error code.\n\nClient libraries should use the error code to translate the error into an appropriate exception:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\":\"select name, position from foo.locations\"\n... }\n... EOF\n{\n  \"error\": {\n    \"message\": \"SchemaUnknownException[Schema 'foo' unknown]\",\n    \"code\": 4045\n  }\n}\n\n\nTo get more insight into what exactly went wrong an additional error_trace GET parameter can be specified to return the stack trace:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql?error_trace=true' -d@- <<- EOF\n... {\n...   \"stmt\":\"select name, position from foo.locations\"\n... }\n... EOF\n{\n  \"error\": {\n    \"message\": \"SchemaUnknownException[Schema 'foo' unknown]\",\n    \"code\": 4045\n  },\n  \"error_trace\": \"...\"\n}\n\n\nNote\n\nThis parameter is intended for CrateDB developers or for users requesting support for CrateDB. Client libraries shouldn’t make use of this option and not include the stack trace.\n\nError codes\n\nCode\n\n\t\n\nError\n\n\n\n\n4000\n\n\t\n\nThe statement contains an invalid syntax or unsupported SQL statement\n\n\n\n\n4001\n\n\t\n\nThe statement contains an invalid analyzer definition.\n\n\n\n\n4002\n\n\t\n\nThe name of the relation is invalid.\n\n\n\n\n4003\n\n\t\n\nField type validation failed\n\n\n\n\n4004\n\n\t\n\nPossible feature not supported (yet)\n\n\n\n\n4005\n\n\t\n\nAlter table using a table alias is not supported.\n\n\n\n\n4006\n\n\t\n\nThe used column alias is ambiguous.\n\n\n\n\n4007\n\n\t\n\nThe operation is not supported on this relation, as it is not accessible.\n\n\n\n\n4008\n\n\t\n\nThe name of the column is invalid.\n\n\n\n\n4009\n\n\t\n\nCrateDB License is expired. (Deprecated.)\n\n\n\n\n4010\n\n\t\n\nUser is not authorized to perform the SQL statement.\n\n\n\n\n4011\n\n\t\n\nMissing privilege for user.\n\n\n\n\n4031\n\n\t\n\nOnly read operations are allowed on this node.\n\n\n\n\n4041\n\n\t\n\nUnknown relation.\n\n\n\n\n4042\n\n\t\n\nUnknown analyzer.\n\n\n\n\n4043\n\n\t\n\nUnknown column.\n\n\n\n\n4044\n\n\t\n\nUnknown type.\n\n\n\n\n4045\n\n\t\n\nUnknown schema.\n\n\n\n\n4046\n\n\t\n\nUnknown Partition.\n\n\n\n\n4047\n\n\t\n\nUnknown Repository.\n\n\n\n\n4048\n\n\t\n\nUnknown Snapshot.\n\n\n\n\n4049\n\n\t\n\nUnknown user-defined function.\n\n\n\n\n40410\n\n\t\n\nUnknown user.\n\n\n\n\n4091\n\n\t\n\nA document with the same primary key exists already.\n\n\n\n\n4092\n\n\t\n\nA VersionConflict. Might be thrown if an attempt was made to update the same document concurrently.\n\n\n\n\n4093\n\n\t\n\nA relation with the same name exists already.\n\n\n\n\n4094\n\n\t\n\nThe used table alias contains tables with different schema.\n\n\n\n\n4095\n\n\t\n\nA repository with the same name exists already.\n\n\n\n\n4096\n\n\t\n\nA snapshot with the same name already exists in the repository.\n\n\n\n\n4097\n\n\t\n\nA partition for the same values already exists in this table.\n\n\n\n\n4098\n\n\t\n\nA user-defined function with the same signature already exists.\n\n\n\n\n4099\n\n\t\n\nA user with the same name already exists.\n\n\n\n\n4100\n\n\t\n\nAn object with the same name already exists.\n\n\n\n\n5000\n\n\t\n\nUnhandled server error.\n\n\n\n\n5001\n\n\t\n\nThe execution of one or more tasks failed.\n\n\n\n\n5002\n\n\t\n\nOne or more shards are not available.\n\n\n\n\n5003\n\n\t\n\nThe query failed on one or more shards\n\n\n\n\n5004\n\n\t\n\nCreating a snapshot failed\n\n\n\n\n5030\n\n\t\n\nThe query was killed by a kill statement\n\nBulk errors\n\nIf a bulk operation fails, the resulting row count will be -2 and the resulting object may contain an error_message depending on the resulting error:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\": \"INSERT into locations (name, id) values (?,?)\",\n...   \"bulk_args\": [\n...     [\"Mars\", 1341],\n...     [\"Sun\", 1341]\n...   ]\n... }\n... EOF\n{\n  \"cols\": [],\n  \"duration\": ...,\n  \"results\": [\n    {\n      \"rowcount\": 1\n    },\n    {\n      \"rowcount\": -2\n    }\n  ]\n}\n\n\nNote\n\nEvery bulk operation will be executed, independent if one of the operation fails."
  },
  {
    "title": "Storage and consistency — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/concepts/storage-consistency.html",
    "html": "5.5\nStorage and consistency\n\nThis document provides an overview on how CrateDB stores and distributes state across the cluster and what consistency and durability guarantees are provided.\n\nNote\n\nSince CrateDB heavily relies on Elasticsearch and Lucene for storage and cluster consensus, concepts shown here might look familiar to Elasticsearch users, since the implementation is actually reused from the Elasticsearch code.\n\nTable of contents\n\nData storage\n\nAtomicity at document level\n\nDurability\n\nAddressing documents\n\nConsistency\n\nCluster meta data\n\nData storage\n\nEvery table in CrateDB is sharded, which means that tables are divided and distributed across the nodes of a cluster. Each shard in CrateDB is a Lucene index broken down into segments getting stored on the filesystem. Physically the files reside under one of the configured data directories of a node.\n\nLucene only appends data to segment files, which means that data written to the disc will never be mutated. This makes it easy for replication and recovery, since syncing a shard is simply a matter of fetching data from a specific marker.\n\nAn arbitrary number of replica shards can be configured per table. Every operational replica holds a full synchronized copy of the primary shard.\n\nWith read operations, there is no difference between executing the operation on the primary shard or on any of the replicas. CrateDB randomly assigns a shard when routing an operation. It is possible to configure this behavior if required, see our best practice guide on multi zone setups for more details.\n\nWrite operations are handled differently than reads. Such operations are synchronous over all active replicas with the following flow:\n\nThe primary shard and the active replicas are looked up in the cluster state for the given operation. The primary shard and a quorum of the configured replicas need to be available for this step to succeed.\n\nThe operation is routed to the according primary shard for execution.\n\nThe operation gets executed on the primary shard\n\nIf the operation succeeds on the primary, the operation gets executed on all replicas in parallel.\n\nAfter all replica operations finish the operation result gets returned to the caller.\n\nShould any replica shard fail to write the data or times out in step 5, it’s immediately considered as unavailable.\n\nAtomicity at document level\n\nEach row of a table in CrateDB is a semi structured document which can be nested arbitrarily deep through the use of object and array types.\n\nOperations on documents are atomic. Meaning that a write operation on a document either succeeds as a whole or has no effect at all. This is always the case, regardless of the nesting depth or size of the document.\n\nCrateDB does not provide transactions. Since every document in CrateDB has a version number assigned, which gets increased every time a change occurs, patterns like Optimistic Concurrency Control can help to work around that limitation.\n\nDurability\n\nEach shard has a WAL also known as translog. It guarantees that operations on documents are persisted to disk without having to issue a Lucene-Commit for every write operation. When the translog gets flushed all data is written to the persistent index storage of Lucene and the translog gets cleared.\n\nIn case of an unclean shutdown of a shard, the transactions in the translog are getting replayed upon startup to ensure that all executed operations are permanent.\n\nThe translog is also directly transferred when a newly allocated replica initializes itself from the primary shard. There is no need to flush segments to disc just for replica recovery purposes.\n\nAddressing documents\n\nEvery document has an internal identifier. By default this identifier is derived from the primary key. Documents living in tables without a primary key are assigned a unique auto-generated ID automatically when created.\n\nEach document is routed to one specific shard according to the routing column. All rows that have the same routing column row value are stored in the same shard. The routing column can be specified with the CLUSTERED clause when creating the table. If a primary key has been defined, it will be used as the default routing column, otherwise the internal document ID is used.\n\nWhile transparent to the user, internally there are two ways how CrateDB accesses documents:\n\nget\n\nDirect access by identifier. Only applicable if the routing key and the identifier can be computed from the given query specification. (e.g: the full primary key is defined in the where clause).\n\nThis is the most efficient way to access a document, since only a single shard gets accessed and only a simple index lookup on the _id field has to be done.\n\nsearch\n\nQuery by matching against fields of documents across all candidate shards of the table.\n\nConsistency\n\nCrateDB is eventual consistent for search operations. Search operations are performed on shared IndexReaders which besides other functionality, provide caching and reverse lookup capabilities for shards. An IndexReader is always bound to the Lucene segment it was started from, which means it has to be refreshed in order to see new changes, this is done on a time based manner, but can also be done manually (see refresh). Therefore a search only sees a change if the according IndexReader was refreshed after that change occurred.\n\nIf a query specification results in a get operation, changes are visible immediately. This is achieved by looking up the document in the translog first, which will always have the most recent version of the document. The common update and fetch use-case is therefore possible. If a client updates a row and that row is looked up by its primary key after that update the changes will always be visible, since the information will be retrieved directly from the translog.\n\nNote\n\nDirty reads can occur if the primary shard becomes isolated. The primary will only realize it is isolated once it tries to communicate with its replicas or the master. At that point, a write operation is already committed into the primary and can be read by a concurrent read operation. In order to minimise the window of opportunity for this phenomena, the CrateDB nodes communicate with the master every second (by default) and once they realise no master is known, they will start rejecting write operations.\n\nEvery replica shard is updated synchronously with its primary and always carries the same information. Therefore it does not matter if the primary or a replica shard is accessed in terms of consistency. Only the refresh of the IndexReader affects consistency.\n\nCaution\n\nSome outage conditions can affect these consistency claims. See the resiliency documentation for details.\n\nCluster meta data\n\nCluster meta data is held in the so called “Cluster State”, which contains the following information:\n\nTables schemas.\n\nPrimary and replica shard locations. Basically just a mapping from shard number to the storage node.\n\nStatus of each shard, which tells if a shard is currently ready for use or has any other state like “initializing”, “recovering” or cannot be assigned at all.\n\nInformation about discovered nodes and their status.\n\nConfiguration information.\n\nEvery node has its own copy of the cluster state. However there is only one node allowed to change the cluster state at runtime. This node is called the “master” node and gets auto-elected. The “master” node has no special configuration at all, all nodes are master-eligible by default, and any master-eligible node can be elected as the master. There is also an automatic re-election if the current master node goes down for some reason.\n\nNote\n\nTo avoid a scenario where two masters could be elected due to network partitioning, CrateDB automatically defines a quorum of nodes with which it is possible to elect a master. For details on how this works and further information see Master Node Election.\n\nTo explain the flow of events for any cluster state change, here is an example flow for an ALTER TABLE statement which changes the schema of a table:\n\nA node in the cluster receives the ALTER TABLE request.\n\nThe node sends out a request to the current master node to change the table definition.\n\nThe master node applies the changes locally to the cluster state and sends out a notification to all affected nodes about the change.\n\nThe nodes apply the change, so that they are now in sync with the master.\n\nEvery node might take some local action depending on the type of cluster state change."
  },
  {
    "title": "SQL Statements — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/sql/statements/index.html",
    "html": "master\nSQL Statements\n\nTable of contents\n\nALTER CLUSTER\nALTER PUBLICATION\nALTER TABLE\nALTER ROLE\nALTER USER\nANALYZE\nBEGIN\nCLOSE\nCOMMIT\nCOPY FROM\nCOPY TO\nCREATE ANALYZER\nCREATE BLOB TABLE\nCREATE FOREIGN TABLE\nCREATE FUNCTION\nCREATE PUBLICATION\nCREATE REPOSITORY\nCREATE ROLE\nCREATE SERVER\nCREATE SNAPSHOT\nCREATE SUBSCRIPTION\nCREATE TABLE\nCREATE TABLE AS\nCREATE USER\nCREATE USER MAPPING\nCREATE VIEW\nDEALLOCATE\nDECLARE\nDELETE\nDENY\nDISCARD\nDROP ANALYZER\nDROP FOREIGN TABLE\nDROP FUNCTION\nDROP PUBLICATION\nDROP REPOSITORY\nDROP ROLE\nDROP SERVER\nDROP SNAPSHOT\nDROP SUBSCRIPTION\nDROP TABLE\nDROP USER\nDROP USER MAPPING\nDROP VIEW\nEND\nEXPLAIN\nFETCH\nGRANT\nINSERT\nKILL\nOPTIMIZE\nREFRESH\nRESTORE SNAPSHOT\nREVOKE\nSELECT\nSET and RESET\nSET LICENSE\nSET AND RESET SESSION AUTHORIZATION\nSET TRANSACTION\nSHOW (session settings)\nSHOW COLUMNS\nSHOW CREATE TABLE\nSHOW SCHEMAS\nSHOW TABLES\nSTART TRANSACTION\nUPDATE\nVALUES\nWITH"
  },
  {
    "title": "General SQL — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/sql/general/index.html",
    "html": "master\nGeneral SQL\n\nTable of contents\n\nConstraints\nValue expressions\nLexical structure"
  },
  {
    "title": "Usage Data Collector — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/admin/udc.html",
    "html": "master\nUsage Data Collector\n\nThe CrateDB Usage Data Collector (UDC) is a sub-system that gathers usage data, reporting it to the UDC server at https://udc.crate.io. It is easy to disable, and does not collect any data that is confidential. For more information about what is being sent, see below.\n\nCrateDB uses this information as a form of automatic, effortless feedback from the community. We want to verify that we are doing the right thing by matching download statistics with usage statistics. After each release, we can see if there is a larger retention span of the server software.\n\nThe data collected is clearly stated here. If any future versions of this system collect additional data, we will clearly announce those changes.\n\nCrateDB is concerned about your privacy. We do not disclose any personally identifiable information.\n\nTable of contents\n\nTechnical information\n\nAdmin UI tracking\n\nConfiguration\n\nHow to disable UDC\n\nBy configuration\n\nBy system property\n\nTechnical information\n\nTo gather good statistics about CrateDB usage, UDC collects this information:\n\nName\n\n\t\n\nDescription\n\n\n\n\nKernel Version\n\n\t\n\nThe build number, and if there are any modifications to the kernel.\n\n\n\n\nCluster Id\n\n\t\n\nA randomized globally unique ID created every time the whole cluster is restarted.\n\n\n\n\nMaster\n\n\t\n\nBoolean whether the current node is master.\n\n\n\n\nPing Count\n\n\t\n\nUDC holds an internal counter per node which is incremented for every ping, and reset on every restart of the a node.\n\n\n\n\nCrateDB Version\n\n\t\n\nThe CrateDB version.\n\n\n\n\nJava Version\n\n\t\n\nThe Java version CrateDB is currently running with.\n\n\n\n\nHardware Address\n\n\t\n\nMAC address to uniquely identify instances behind firewalls.\n\n\n\n\nProcessor count\n\n\t\n\nNumber of available CPUs as reported by Runtime.availableProcessors\n\n\n\n\nEnterprise\n\n\t\n\nIdentifies whether the Enterprise Edition is used. 1\n\nAfter startup, UDC waits for 10 minutes before sending the first ping. It does this for two reasons; first, we don’t want the startup to be slower because of UDC, and secondly, we want to keep pings from automatic tests to a minimum. By default, UDC is sending pings every 24 hours. The ping to the UDC servers is done with a HTTP GET.\n\nAdmin UI tracking\n\nSince Admin UI v0.16.0 we are tracking the user ID along with the cluster ID to know how many active users are currently using CrateDB.\n\nConfiguration\n\nThe Usage Data Collector can be configured by adapting the crate.yml configuration file or adding a system property setting. Refer to Usage data collector to see how these settings can be accessed and how they are configured.\n\nHow to disable UDC\n\nBelow are two ways you can disable UDC. However we hope you support us offering the open source edition, and leave UDC on, so we learn how many people use CrateDB.\n\nBy configuration\n\nJust add following to your crate.yml configuration file:\n\nudc.enabled:  false\n\nBy system property\n\nIf you do not want to make any change to the jars or to the configuration, a system property setting like this will also make sure that UDC is never activated:\n\n-Cudc.enabled=false\n\n1\n\nThe “CrateDB Enterprise Edition” has been dissolved starting with CrateDB 4.5.0, see also Farewell to the CrateDB Enterprise License."
  },
  {
    "title": "Foreign Data Wrappers — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/admin/fdw.html",
    "html": "master\nForeign Data Wrappers\n\nTable of contents\n\njdbc\n\nCREATE SERVER OPTIONS\n\nCREATE FOREIGN TABLE OPTIONS\n\nCREATE USER MAPPING OPTIONS\n\nForeign data wrappers allow you to make data in foreign systems available as tables within CrateDB. You can then query these foreign tables like regular user tables.\n\nFor this to work, you’ll need several parts:\n\nA foreign data wrapper implementation, which takes care of managing the connection to the remote system and implements the actual data retrieval. CrateDB contains a jdbc data wrapper implementation.\n\nA CREATE SERVER definition. This gives your foreign system a name and provides information like the host name or port. The concrete options depend on the used foreign data wrapper. For example, for jdbc there’s a url option that defines the JDBC connection URL.\n\nA CREATE FOREIGN TABLE definition. This statement is used to create the table and define the schema of the foreign data. The statement does not validate the used schema. You must make sure to use compatible data types, otherwise queries on the table will fail at runtime.\n\nOptionally one or more user mappings, created with CREATE USER MAPPING. A user mapping allows you to map from a CrateDB user to a foreign system user. If no user mapping exists, CrateDB will try to connect with the current user.\n\nQuery clauses like GROUP BY, HAVING, LIMIT or ORDER BY are executed within CrateDB, not within the foreign system. WHERE clauses can in some circumstances be pushed to the foreign system, but that depends on the concrete foreign data wrapper implementation. You can check if this is the case by using the EXPLAIN statement.\n\nFor example, in the following explain output there is a dedicated Filter node, indicating that the filter is executed within CrateDB:\n\ncr> explain select * from summits where mountain like 'H%';\n+--------------------------------------------------------------------+\n| QUERY PLAN                                                         |\n+--------------------------------------------------------------------+\n| Filter[(mountain LIKE 'H%')] (rows=0)                              |\n|   └ ForeignCollect[doc.summits | [mountain] | true] (rows=unknown) |\n+--------------------------------------------------------------------+\n\n\nCompare this to the following output, where the query became part of the ForeignCollect node, indicating that it evaluates within the foreign system:\n\ncr> explain select * from summits where mountain = 'Monte Verena';\n+---------------------------------------------------------------------------------------+\n| QUERY PLAN                                                                            |\n+---------------------------------------------------------------------------------------+\n| ForeignCollect[doc.summits | [mountain] | (mountain = 'Monte Verena')] (rows=unknown) |\n+---------------------------------------------------------------------------------------+\n\njdbc\n\nThe jdbc foreign data wrapper allows to connect to a foreign database via JDBC. Bundled JDBC drivers include:\n\nPostgreSQL\n\nCREATE SERVER OPTIONS\n\nThe JDBC foreign data wrapper supports the following OPTIONS for use with CREATE SERVER:\n\nurl\n\nA JDBC connection string. This option is required.\n\nYou should avoid specifying user and password information in the URL, and instead make use of the CREATE USER MAPPING feature.\n\nExample:\n\nCREATE SERVER my_postgresql FOREIGN DATA WRAPPER jdbc\nOPTIONS (url 'jdbc:postgresql://example.com:5432/');\n\n\nNote\n\nBy default only the crate user can use server definitions that connect to localhost. Other users are not allowed to connect to instances running on the same host as CrateDB. This is a security measure to prevent users from by-passing Host-Based Authentication (HBA) restrictions. See fdw.allow_local.\n\nCREATE FOREIGN TABLE OPTIONS\n\nThe JDBC foreign data wrapper supports the following OPTIONS for use with CREATE FOREIGN TABLE:\n\nschema_name\n\nThe schema name used when accessing a table in the foreign system. If not specified this defaults to the schema name of the table created within CrateDB.\n\nUse this if the names between CrateDB and the foreign system are different.\n\ntable_name\n\nThe table name used when accessing a table in the foreign system. If not specified this defaults to the table name of the table within CrateDB.\n\nUse this if the names between CrateDB and the foreign system are different.\n\nExample:\n\nCREATE FOREIGN TABLE doc.remote_documents (name text) SERVER my_postgresql\nOPTIONS (schema_name 'public', table_name 'documents');\n\nCREATE USER MAPPING OPTIONS\n\nThe JDBC foreign data wrapper supports the following OPTIONS for use with CREATE USER MAPPING:\n\nuser\n\nThe name of the user in the foreign system.\n\npassword\n\nThe password for the user in the foreign system.\n\nExample:\n\nCREATE USER MAPPING FOR USER SERVER my_postgresql OPTIONS (\"user\" 'trillian', password 'secret');\n\n\nSee Also\n\nCREATE SERVER\n\nCREATE FOREIGN TABLE\n\nCREATE USER MAPPING"
  },
  {
    "title": "Cloud discovery — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/admin/discovery.html",
    "html": "master\nCloud discovery\n\nTable of contents\n\nAmazon EC2 discovery\n\nAmazon EC2 discovery\n\nCrateDB has native discovery support when running a cluster with Amazon Web Services (AWS). The discovery mechanism uses the Amazon EC2 API to generate the list of hosts for the unicast host discovery (see Unicast host discovery).\n\nThere is a best practice how to configure and run a CrateDB cluster on Amazon EC2."
  },
  {
    "title": "Logical replication — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/admin/logical-replication.html",
    "html": "master\nLogical replication\n\nTable of contents\n\nPublication\n\nSubscription\n\nSecurity\n\nMonitoring\n\nLogical replication is a method of data replication across multiple clusters. CrateDB uses a publish and subscribe model where subscribers pull data from the publications of the publisher they subscribed to.\n\nReplicated tables on a subscriber can again be published further to other clusters and thus chaining subscriptions is possible.\n\nNote\n\nA replicated index on a subscriber is read-only.\n\nLogical replication is useful for the following use cases:\n\nConsolidating data from multiple clusters into a single one for aggregated reports.\n\nEnsure high availability if one cluster becomes unavailable.\n\nReplicating between different compatible versions of CrateDB. Replicating tables created on a cluster with higher major/minor version to a cluster with lower major/minor version is not supported.\n\nSee Also\n\nreplication.logical.ops_batch_size replication.logical.reads_poll_duration replication.logical.recovery.chunk_size replication.logical.recovery.max_concurrent_file_chunks\n\nPublication\n\nA publication is the upstream side of logical replication and it’s created on the cluster which acts as a data source.\n\nEach table can be added to multiple publications if needed. Publications can only contain tables. All operation types (INSERT, UPDATE, DELETE and schema changes) are replicated.\n\nEvery publication can have multiple subscribers.\n\nA publication is created using the CREATE PUBLICATION command. The individual tables can be added or removed dynamically using ALTER PUBLICATION. Publications can be removed using the DROP PUBLICATION command.\n\nCaution\n\nThe publishing cluster must have soft_deletes.enabled set to true so that a subscribing cluster can catch up with all changes made during replication pauses caused by network issues or explicitly done by a user.\n\nAlso, soft_deletes.retention_lease.period should be greater than or equal to replication.logical.reads_poll_duration.\n\nSubscription\n\nA subscription is the downstream side of logical replication. A subscription defines the connection to another database and set of publications to which it wants to subscribe. By default, the subscription creation triggers the replication process on the subscriber cluster. The subscriber cluster behaves in the same way as any other CrateDB cluster and can be used as a publisher for other clusters by defining its own publications.\n\nA cluster can have multiple subscriptions. It is also possible for a cluster to have both subscriptions and publications. A cluster cannot subscribe to locally already existing tables, therefore it is not possible to setup a bi-directional replication (both sides subscribing to ALL TABLES leads to a cluster trying to replicate its own tables from another cluster). However, two clusters still can cross-subscribe to each other if one cluster subscribes to locally non-existing tables of another cluster and vice versa.\n\nA subscription is added using the CREATE SUBSCRIPTION command and can be removed using the DROP SUBSCRIPTION command. A subscription starts replicating on its creation and stops on its removal (if no failure happen in-between).\n\nPublished tables must not exist on the subscriber. A cluster cannot subscribe to a table on another cluster if it exists already on its side, therefore it’s not possible to drop and re-create a subscription without starting from scratch i.e removing all replicated tables.\n\nOnly regular tables (including partitions) may be the target of a replication. For example, you can not replicate system tables or views.\n\nThe tables are matched between the publisher and the subscriber using the fully qualified table name. Replication to differently-named tables on the subscriber is not supported.\n\nSecurity\n\nTo create, alter or drop a publication, a user must have the AL privilege on the cluster. Only the owner (the user who created the publication) or a superuser is allowed to ALTER or DROP a publication. To add tables to a publication, the user must have DQL, DML, and DDL privileges on the table. When a user creates a publication that publishes all tables automatically, only those tables where the user has DQL, DML, and DDL privileges will be published. The user a subscriber uses to connect to the publisher must have DQL privileges on the published tables. Tables, included into a publication but not available for a subscriber due to lack of DQL privilege, will not be replicated.\n\nTo create or drop a subscription, a user must have the AL privilege on the cluster. Only the owner (the user who created the subscription) or a superuser is allowed to DROP a subscription.\n\nCaution\n\nA network setup that allows the two clusters to communicate is a pre-requisite for a working publication/subscription setup. See HBA.\n\nMonitoring\n\nAll publications are listed in the pg_publication table. More details for a publication are available in the pg_publication_tables table. It lists the replicated tables for a specific publication.\n\nAll subscriptions are listed in the pg_subscription table. More details for a subscription are available in the pg_subscription_rel table. The table contains detailed information about the replication state per table, including error messages if there was an error."
  },
  {
    "title": "Snapshots — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/admin/snapshots.html",
    "html": "master\nSnapshots\n\nTable of contents\n\nSnapshot\n\nCreating a repository\n\nCreating a snapshot\n\nRestore\n\nRestore data granularity\n\nCleanup\n\nDropping snapshots\n\nDropping repositories\n\nSnapshot\n\nIn CrateDB, backups are called Snapshots. They represent the state of the tables in a CrateDB cluster at the time the Snapshot was created. A Snapshot is always stored in a Repository which has to be created first.\n\nCaution\n\nYou cannot snapshot BLOB tables.\n\nCreating a repository\n\nRepositories are used to store, manage and restore snapshots.\n\nThey are created using the CREATE REPOSITORY statement:\n\ncr> CREATE REPOSITORY where_my_snapshots_go TYPE fs\n... WITH (location='repo_path', compress=true);\nCREATE OK, 1 row affected (... sec)\n\n\nRepositories are uniquely identified by their name. Every repository has a specific type which determines how snapshots are stored.\n\nCrateDB supports different repository types, see Types.\n\nThe creation of a repository configures it inside the CrateDB cluster. In general no data is written, no snapshots inside repositories changed or deleted. This way you can tell the CrateDB cluster about existing repositories which already contain snapshots.\n\nCreating a repository with the same name will result in an error:\n\ncr> CREATE REPOSITORY where_my_snapshots_go TYPE fs\n... WITH (location='another_repo_path', compress=false);\nRepositoryAlreadyExistsException[Repository 'where_my_snapshots_go' already exists]\n\nCreating a snapshot\n\nSnapshots are created inside a repository and can contain any number of tables. The CREATE SNAPSHOT statement is used to create a snapshots:\n\ncr> CREATE SNAPSHOT where_my_snapshots_go.snapshot1 ALL\n... WITH (wait_for_completion=true, ignore_unavailable=true);\nCREATE OK, 1 row affected (... sec)\n\n\nA snapshot is referenced by the name of the repository and the snapshot name, separated by a dot. If ALL is used, all user created tables of the cluster (except blob tables) are stored inside the snapshot.\n\nIt’s possible to only save a specific subset of tables in the snapshot by listing them explicitly:\n\ncr> CREATE SNAPSHOT where_my_snapshots_go.snapshot2 TABLE quotes, doc.locations\n... WITH (wait_for_completion=true);\nCREATE OK, 1 row affected (... sec)\n\n\nEven single partition of Partitioned tables can be selected for backup. This is especially useful if old partitions need to be deleted but it should be possible to restore them if needed:\n\ncr> CREATE SNAPSHOT where_my_snapshots_go.snapshot3 TABLE\n...    locations,\n...    parted_table PARTITION (date='1970-01-01')\n... WITH (wait_for_completion=true);\nCREATE OK, 1 row affected (... sec)\n\n\nSnapshots are incremental. Snapshots of the same cluster created later only store data not already contained in the repository.\n\nAll examples above are used with the argument wait_for_completion set to true. As described in the CREATE REPOSITORY reference documentation, by doing this, the statement will only respond (successfully or not) when the snapshot is fully created. Otherwise the snapshot will be created in the background and the statement will immediately respond as successful. The status of a created snapshot can be retrieved by querying the sys.snapshots system table.\n\nRestore\n\nCaution\n\nIf you are restoring a snapshot into a newer version of CrateDB, be sure to check the Release Notes for upgrade instructions.\n\nCaution\n\nIf you try to restore a table that already exists, CrateDB will return an error. However, if you try to restore metadata or cluster settings that already exist, they will be overwritten.\n\nOnce a snapshot is created, it can be used to restore its tables to the state when the snapshot was created.\n\nTo get basic information about snapshots the sys.snapshots table can be queried:\n\ncr> SELECT repository, name, state, concrete_indices\n... FROM sys.snapshots\n... ORDER BY repository, name;\n+-----------------------+-----------+---------+--------------------...-+\n| repository            | name      | state   | concrete_indices       |\n+-----------------------+-----------+---------+--------------------...-+\n| where_my_snapshots_go | snapshot1 | SUCCESS | [...]                  |\n| where_my_snapshots_go | snapshot2 | SUCCESS | [...]                  |\n| where_my_snapshots_go | snapshot3 | SUCCESS | [...]                  |\n+-----------------------+-----------+---------+--------------------...-+\nSELECT 3 rows in set (... sec)\n\n\nTo restore a table from a snapshot we have to drop it beforehand:\n\ncr> DROP TABLE quotes;\nDROP OK, 1 row affected (... sec)\n\n\nRestoring a snapshot using the RESTORE SNAPSHOT statement.:\n\ncr> RESTORE SNAPSHOT where_my_snapshots_go.snapshot2\n... TABLE quotes\n... WITH (wait_for_completion=true);\nRESTORE OK, 1 row affected (... sec)\n\n\nIn this case only the quotes table from snapshot where_my_snapshots_go.snapshot2 is restored.\n\nIt’s not possible to restore tables that exist in the current cluster:\n\ncr> RESTORE SNAPSHOT where_my_snapshots_go.snapshot2 TABLE quotes;\nRelationAlreadyExists[Relation 'doc.quotes' already exists.]\n\n\nSingle partitions can be either imported into an existing partitioned table the partition belongs to.\n\nTo monitor the progress of RESTORE SNAPSHOT operations please query the sys.snapshot_restore table.\n\ncr> RESTORE SNAPSHOT where_my_snapshots_go.snapshot3 TABLE\n...    parted_table PARTITION (date='1970-01-01')\n... WITH (wait_for_completion=true);\nRESTORE OK, 1 row affected (... sec)\n\n\nOr if no matching partition table exists, it will be implicitly created during restore.\n\nCaution\n\nThis is only possible with CrateDB version 0.55.5 or greater!\n\nSnapshots of single partitions that have been created with earlier versions of CrateDB may be restored, but lead to orphaned partitions!\n\nWhen using CrateDB prior to 0.55.5 you will have to create the table schema first before restoring.\n\ncr> RESTORE SNAPSHOT where_my_snapshots_go.snapshot3 TABLE\n...    parted_table PARTITION (date=0)\n... WITH (wait_for_completion=true);\nRESTORE OK, 1 row affected (... sec)\n\nRestore data granularity\n\nYou are not limited to only being able to restore individual tables (or table partitions). For example:\n\nYou can use ALL instead of listing all tables to restore the whole snapshot, including all metadata and settings.\n\nYou can use TABLES to restore all tables but no metadata or settings. On the other hand, you can use METADATA to restore everything but tables.\n\nYou can use USERMANAGEMENT to restore database users, roles and their privileges.\n\nSee the RESTORE SNAPSHOT documentation for all possible options.\n\nCleanup\nDropping snapshots\n\nDropping a snapshot deletes all files inside the repository that are only referenced by this snapshot. Due to its incremental nature this might be very few files (e.g. for intermediate snapshots). Snapshots are dropped using the DROP SNAPSHOT command:\n\ncr> DROP SNAPSHOT where_my_snapshots_go.snapshot3;\nDROP OK, 1 row affected (... sec)\n\nDropping repositories\n\nIf a repository is not needed anymore, it can be dropped using the DROP REPOSITORY statement:\n\ncr> DROP REPOSITORY \"OldRepository\";\nDROP OK, 1 row affected (... sec)\n\n\nThis statement, like CREATE REPOSITORY, does not manipulate repository contents but only deletes stored configuration for this repository in the cluster state, so it’s not accessible any more."
  },
  {
    "title": "Jobs management — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/admin/jobs-management.html",
    "html": "master\nJobs management\n\nEach executed SQL statement results in a corresponding job. Jobs that are currently executing are logged in the system table sys.jobs (see Jobs, operations, and logs).\n\nTo obtain the UUID of a job, stats needs to be enabled (see Collecting stats). Job logging can be disabled by setting the queue size to zero.\n\nKilling an active job forces CrateDB to stop its execution on the cluster immediately. There are two different SQL commands available for killing jobs.\n\nThe KILL ALL statement stops every single job on each node that is running. It returns the total number of contexts of all jobs that have been killed. A job can have contexts on multiple nodes.\n\ncr> kill all;\nKILL OK, ... rows affected (... sec)\n\n\nKILL job_id kills one single job with the specified job_id. Like KILL ALL it returns the total number of contexts of that job killed on all nodes.\n\ncr> kill '175011ce-9bbc-45f2-a86a-5b7f993a93a6';\nKILL OK, ... rows affected (... sec)\n\n\nSee KILL for detailed syntax information on KILL statements."
  },
  {
    "title": "JMX monitoring — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/admin/monitoring.html",
    "html": "master\nJMX monitoring\n\nThe JMX monitoring feature exposes query metrics via the JMX API.\n\nTable of contents\n\nSetup\n\nEnable collecting stats\n\nEnable the JMX API\n\nUsing Docker\n\nJMX Beans\n\nQueryStats MBean\n\nNodeStatus MBean\n\nNodeInfo MXBean\n\nConnections MBean\n\nThreadPools MXBean\n\nCircuitBreakers MXBean\n\nExposing JMX via HTTP\n\nSetup\nEnable collecting stats\n\nBy default, Collecting stats is enabled. You can disable collecting stats via the CrateDB configuration file or by running this statement:\n\ncr> SET GLOBAL \"stats.enabled\" = FALSE;\n\nEnable the JMX API\n\nTo monitor CrateDB using the JMX API, you must set the following system properties before you start CrateDB:\n\ncom.sun.management.jmxremote\ncom.sun.management.jmxremote.port=<JMX_PORT>\ncom.sun.management.jmxremote.ssl=false\ncom.sun.management.jmxremote.authenticate=false\n\n\nHere, <JMX_PORT> sets the port number of your JMX server. JMX SSL and authentication are currently not supported.\n\nMore information about the JMX monitoring properties can be found in the JMX documentation.\n\nYou can set the Java system properties with the -D option:\n\nsh$ ./bin/crate -Dcom.sun.management.jmxremote \\\n...             -Dcom.sun.management.jmxremote.port=7979 \\\n...             -Dcom.sun.management.jmxremote.ssl=false \\\n...             -Dcom.sun.management.jmxremote.authenticate=false\n\n\nHowever, the recommended way to set system properties is via the CRATE_JAVA_OPTS environment variable, like so:\n\nsh$ export CRATE_JAVA_OPTS=\"$CRATE_JAVA_OPTS \\\n      -Dcom.sun.management.jmxremote \\\n      -Dcom.sun.management.jmxremote.port=7979 \\\n      -Dcom.sun.management.jmxremote.ssl=false \\\n      -Dcom.sun.management.jmxremote.authenticate=false\"\nsh$ ./bin/crate\n\n\nIf you’re using the CrateDB Debian or RPM packages, you can set this environment variable via the /etc/default/crate configuration file.\n\nUsing Docker\n\nTo enable JMX monitoring when running CrateDB in a Docker container you have to set the following additional Java system properties:\n\n-Djava.rmi.server.hostname=<RMI_HOSTNAME>\n-Dcom.sun.management.jmxremote.rmi.port=<RMI_PORT>\n\n\nHere, <RMI_HOSTNAME> is the IP address or hostname of the Docker host and <RMI_PORT> is the statically assigned port of the RMI server. For convenience, <RMI_PORT> can be set to the same port the JMX server listens on.\n\nThe <RMI_HOSTNAME> and <RMI_PORT> can be used by JMX clients (e.g. JConsole or VisualVM) to connect to the JMX server.\n\nHere’s an example Docker command:\n\nsh> docker run -d --env CRATE_HEAP_SIZE=1g -e CRATE_JAVA_OPTS=\"\\\n      -Dcom.sun.management.jmxremote\n      -Dcom.sun.management.jmxremote.port=7979 \\\n      -Dcom.sun.management.jmxremote.ssl=false \\\n      -Dcom.sun.management.jmxremote.authenticate=false \\\n      -Dcom.sun.management.jmxremote.rmi.port=7979 \\\n      -Djava.rmi.server.hostname=<RMI_HOSTNAME>\" \\\n      -p 7979:7979 crate -Cnetwork.host=_site_\n\n\nHere, again, <RMI_HOSTNAME> is the IP address or hostname of the Docker host.\n\nJMX Beans\nQueryStats MBean\n\nThe QueryStats MBean exposes the sum of durations, in milliseconds, total and failed count of all statements executed since the node was started, grouped by type, for SELECT, UPDATE, DELETE, INSERT, MANAGEMENT, DDL, COPY and UNDEFINED queries.\n\nMetrics can be accessed using the JMX MBean object name io.crate.monitoring:type=QueryStats and the following attributes:\n\nStatements total count since the node was started:\n\nSelectQueryTotalCount\n\nInsertQueryTotalCount\n\nUpdateQueryTotalCount\n\nDeleteQueryTotalCount\n\nManagementQueryTotalCount\n\nDDLQueryTotalCount\n\nCopyQueryTotalCount\n\nUndefinedQueryTotalCount\n\nStatements failed count since the node was started:\n\nSelectQueryFailedCount\n\nInsertQueryFailedCount\n\nUpdateQueryFailedCount\n\nDeleteQueryFailedCount\n\nManagementQueryFailedCount\n\nDDLQueryFailedCount\n\nCopyQueryFailedCount\n\nUndefinedQueryFailedCount\n\nThe sum of the durations, in milliseconds, since the node was started, of all statement executions grouped by type:\n\nSelectQuerySumOfDurations\n\nInsertQuerySumOfDurations\n\nUpdateQuerySumOfDurations\n\nDeleteQuerySumOfDurations\n\nManagementQuerySumOfDurations\n\nDDLQuerySumOfDurations\n\nCopyQuerySumOfDurations\n\nUndefinedQuerySumOfDurations\n\nNodeStatus MBean\n\nThe NodeStatus JMX MBean exposes the status of the current node as boolean values.\n\nNodeStatus can be accessed using the JMX MBean object name io.crate.monitoring:type=NodeStatus and the following attributes:\n\nReady\n\nDefines if the node is able to process SQL statements.\n\nNodeInfo MXBean\n\nThe NodeInfo JMX MXBean exposes information about the current node.\n\nNodeInfo can be accessed using the JMX MXBean object name io.crate.monitoring:type=NodeInfo and the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nNodeId\n\n\t\n\nProvides the unique identifier of the node in the cluster.\n\n\n\n\nNodeName\n\n\t\n\nProvides the human friendly name of the node.\n\n\n\n\nClusterStateVersion\n\n\t\n\nProvides the version of the current applied cluster state.\n\n\n\n\nShardStats\n\n\t\n\nStatistics about the number of shards located on the node.\n\n\n\n\nShardInfo\n\n\t\n\nDetailed information about the shards located on the node.\n\nShardStats returns a CompositeData object containing statistics about the number of shards located on the node with the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nTotal\n\n\t\n\nThe number of shards located on the node.\n\n\n\n\nPrimaries\n\n\t\n\nThe number of primary shards located on the node.\n\n\n\n\nReplicas\n\n\t\n\nThe number of replica shards located on the node.\n\n\n\n\nUnassigned\n\n\t\n\nThe number of unassigned shards in the cluster. If the node is the elected master node in the cluster, this will show the total number of unassigned shards in the cluster, otherwise 0.\n\nShardInfo returns an Array of CompositeData objects containing detailed information about the shards located on the node with the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nId\n\n\t\n\nThe shard id. This shard id is managed by the system, ranging from 0 up to the number of configured shards of the table.\n\n\n\n\nTable\n\n\t\n\nThe name of the table this shard belongs to.\n\n\n\n\nPartitionIdent\n\n\t\n\nThe partition ident of a partitioned table. Empty for non-partitioned tables.\n\n\n\n\nRoutingState\n\n\t\n\nThe current state of the shard in the routing table. Possible states are:\n\nUNASSIGNED\n\nINITIALIZING\n\nSTARTED\n\nRELOCATING\n\n\n\n\nState\n\n\t\n\nThe current state of the shard. Possible states are:\n\nCREATED\n\nRECOVERING\n\nPOST_RECOVERY\n\nSTARTED\n\nRELOCATED\n\nCLOSED\n\nINITIALIZING\n\nUNASSIGNED\n\n\n\n\nSize\n\n\t\n\nThe estimated cumulated size in bytes of all files of this shard.\n\nConnections MBean\n\nThe Connections MBean exposes information about any open connections to a CrateDB node.\n\nIt can be accessed using the io.crate.monitoring:type=Connections object name and has the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nHttpOpen\n\n\t\n\nThe number of currently established connections via HTTP\n\n\n\n\nHttpTotal\n\n\t\n\nThe number of total connections established via HTTP over the life time of a node\n\n\n\n\nPsqlOpen\n\n\t\n\nThe number of currently established connections via the PostgreSQL protocol\n\n\n\n\nPsqlTotal\n\n\t\n\nThe number of total connections established via the PostgreSQL protocol over the life time of a node\n\n\n\n\nTransportOpen\n\n\t\n\nThe number of currently established connections via the transport protocol\n\nThreadPools MXBean\n\nThe ThreadPools MXBean exposes statistical information about the used thread pools of a CrateDB node.\n\nIt can be accessed using the io.crate.monitoring:type=ThreadPools object name and has following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nGeneric\n\n\t\n\nThread pool statistics of the generic thread pool.\n\n\n\n\nSearch\n\n\t\n\nThread pool statistics of the search thread pool used by read statements on user generated tables.\n\n\n\n\nWrite\n\n\t\n\nThread pool statistics of the write thread pool used for writing and deleting data.\n\n\n\n\nManagement\n\n\t\n\nThread pool statistics of the management thread pool used by management tasks like stats collecting, repository information, shard allocations, etc.\n\n\n\n\nFlush\n\n\t\n\nThread pool statistics of the flush thread pool used for fsyncing to disk and merging segments in the storage engine.\n\n\n\n\nRefresh\n\n\t\n\nThread pool statistics of the refresh thread pool used for automatic and on-demand refreshing of tables\n\n\n\n\nSnapshot\n\n\t\n\nThread pool statistics of the snapshot thread pool used for creating and restoring snapshots.\n\n\n\n\nForceMerge\n\n\t\n\nThread pool statistics of the force_merge thread pool used when running an optimize statement.\n\n\n\n\nListener\n\n\t\n\nThread pool statistics of the listener thread pool used on client nodes for asynchronous result listeners.\n\n\n\n\nGet\n\n\t\n\nThread pool statistics of the get thread pool used when querying sys.nodes or sys.shards.\n\n\n\n\nFetchShardStarted\n\n\t\n\nThread pool statistics of the fetch_shard_started thread pool used on shard allocation .\n\n\n\n\nFetchShardStore\n\n\t\n\nThread pool statistics of the fetch_shard_store used on shard replication.\n\nEach of them returns a CompositeData object containing detailed statistics of each thread pool with the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\npoolSize\n\n\t\n\nThe current number of threads in the pool.\n\n\n\n\nlargestPoolSize\n\n\t\n\nThe largest number of threads that have ever simultaneously been in the pool.\n\n\n\n\nqueueSize\n\n\t\n\nThe current number of tasks in the queue.\n\n\n\n\nactive\n\n\t\n\nThe approximate number of threads that are actively executing tasks.\n\n\n\n\ncompleted\n\n\t\n\nThe approximate total number of tasks that have completed execution.\n\n\n\n\nrejected\n\n\t\n\nThe number of rejected executions.\n\nCircuitBreakers MXBean\n\nThe CircuitBreaker MXBean exposes statistical information about all available circuit breakers of a CrateDB node.\n\nIt can be accessed using the io.crate.monitoring:type=CircuitBreakers object name and has following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nParent\n\n\t\n\nStatistics of the parent circuit breaker containing summarized counters across all circuit breakers.\n\n\n\n\nQuery\n\n\t\n\nStatistics of the query circuit breaker used to account memory usage of SQL execution including intermediate states e.g. on aggregation and resulting rows.\n\n\n\n\nJobsLog\n\n\t\n\nStatistics of the jobs_log circuit breaker used to account memory usage of the sys.jobs_log table.\n\n\n\n\nOperationsLog\n\n\t\n\nStatistics of the operations_log circuit breaker used to account memory usage of the sys.operations_log table.\n\n\n\n\nFieldData\n\n\t\n\nStatistics of the field_data circuit breaker used for estimating the amount of memory a field will require to be loaded into memory.\n\n\n\n\nInFlightRequests\n\n\t\n\nStatistics of the in_flight_requests circuit breaker used to account memory usage of all incoming requests on transport or HTTP level.\n\n\n\n\nRequest\n\n\t\n\nStatistics of the request circuit breaker used to account memory usage of per-request data structure.\n\nEach of them returns a CompositeData object containing detailed statistics of each circuit breaker with the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nname\n\n\t\n\nThe circuit breaker name this statistic belongs to.\n\n\n\n\nused\n\n\t\n\nThe currently accounted used memory estimations.\n\n\n\n\nlimit\n\n\t\n\nThe configured limit when to trip.\n\n\n\n\ntrippedCount\n\n\t\n\nThe total number of occurred trips.\n\nExposing JMX via HTTP\n\nThe JMX metrics and a readiness endpoint can be exposed via HTTP (e.g. to be used by Prometheus) by using the Crate JMX HTTP Exporter Java agent. See the README in the Crate JMX HTTP Exporter repository for more information."
  },
  {
    "title": "Optimization — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/admin/optimization.html",
    "html": "master\nOptimization\n\nTable of contents\n\nIntroduction\n\nMultiple table optimization\n\nPartition optimization\n\nIntroduction\n\nIn CrateDB every table (or if partitioned every partition) consists of segments. When inserting/deleting/updating data new segments are created following as an append-only strategy, which gives the advantage of fast writes but on the other hand can result into a big number of segments. As the number of segments increases the read operations become slower since more segments need to be visited. Moreover each segment consumes file handles, memory and CPU. CrateDB solves this problem by merging segments automatically in the background. Small segments are merged into bigger segments, which, in turn, are merged into even bigger segments. Furthermore any deleted rows and documents are not copied to the new bigger segment during this process.\n\nIf required one or more tables or table partitions can be optimized explicitly in order to improve performance. A few parameters can also be configured for the optimization process, like the max number of segments you wish to have when optimization is completed, or if you only wish to merge segments with deleted data, etc. See OPTIMIZE for detailed description of parameters.\n\ncr> OPTIMIZE table locations;\nOPTIMIZE OK, 1 row affected (... sec)\n\n\nNote\n\nSystem tables cannot be optimized.\n\nMultiple table optimization\n\nIf needed, multiple tables can be defined comma-separated in a single SQL request. The result message is printed if the request on every given table is completed.\n\ncr> OPTIMIZE TABLE locations, parted_table;\nOPTIMIZE OK, 2 rows affected (... sec)\n\n\nNote\n\nIf one or more tables or partitions do not exist, none of the given tables/partitions are optimized and an error is returned. The error returns only the first non-existent table/partition.\n\nPartition optimization\n\nAdditionally it is possible to define a specific PARTITION of a partitioned table which should be optimized (see Partitioned tables).\n\nBy using the PARTITION clause in the optimize statement a separate request for a given partition can be performed. That means that only specific partitions of a partitioned table are optimized. For further details on how to create an optimize request on partitioned tables see the SQL syntax and its synopsis (see OPTIMIZE).\n\ncr> OPTIMIZE TABLE parted_table PARTITION (day='2014-04-08');\nOPTIMIZE OK, 1 row affected (... sec)\n\n\nIn case the PARTITION clause is omitted all partitions will be optimized. If a table has many partitions this should be avoided due to performance reasons."
  },
  {
    "title": "Secured communications (SSL/TLS) — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/admin/ssl.html",
    "html": "master\nSecured communications (SSL/TLS)\n\nYou can encrypt the internal communication between CrateDB nodes and the external communication with HTTP and PostgreSQL clients. When you configure encryption, CrateDB secures connections using Transport Layer Security (TLS).\n\nYou can enable SSL on a per-protocol basis:\n\nIf you enable SSL for HTTP, all connections will require HTTPS.\n\nBy default, if you enable SSL for the PostgreSQL wire protocol, clients can negotiate on a per-connection basis whether to use SSL. However, you can enforce SSL via Host-Based Authentication.\n\nIf you enable SSL for the CrateDB transport protocol (used for intra-node communication), nodes only accept SSL connections (ssl.transport.mode set to on).\n\nTip\n\nYou can use on SSL mode to configure a multi-zone cluster to ensure encryption for nodes communicating between zones. Please note, that SSL has to be on in all nodes as communication is point-2-point, and intra-zone communication will also be encrypted.\n\nTable of contents\n\nSSL/TLS configuration\n\nConfiguring the Keystore\n\nConfiguring a separate Truststore\n\nConnecting to a CrateDB node using HTTPS\n\nConnect to a CrateDB node using the Admin UI\n\nConnect to a CrateDB node using Crash\n\nConnect to a CrateDB node using REST\n\nConnecting to a CrateDB node using the PostgreSQL wire protocol with SSL/TLS\n\nConnect to a CrateDB node using JDBC\n\nConnect to a CrateDB node using psql\n\nSetting up a Keystore/Truststore with a certificate chain\n\nGenerate Keystore with a private key\n\nGenerate a certificate signing request\n\nOptional: Use a self-signed certificate to act as a Certificate Authority (CA)\n\nGenerate a self-signed certificate\n\nGenerate a signed cert\n\nImport the CA certificate into the Keystore\n\nImport CA into Truststore\n\nImport the signed certificate\n\nConfiguring CrateDB\n\nSSL/TLS configuration\n\nTo enable SSL a keystore and a few configuration changes are necessary. These changes need to be made in the crate.yml file on each node that should have secure communications enabled.\n\nSkip to Generate Keystore with a private key for a step-by-step instruction on how to create a keystore.\n\nOnce the keystore (and optional truststore) is created, continue with the following steps:\n\nSet ssl.psql.enabled or ssl.http.enabled to true.\n\nSet ssl.transport.mode to on.\n\nConfiguring the Keystore\n\n(Optional) Configuring a separate Truststore\n\nNote\n\nCrateDB monitors SSL files such as keystore and truststore that are configured as values of the node settings. If any of these files are updated CrateDB dynamically reloads them. The polling frequency of the files is set via the ssl.resource_poll_interval setting.\n\nConfiguring the Keystore\n\nSSL/TLS needs a keystore. The keystore holds the node certificate(s) which should be signed by a certificate authority (CA). A third-party CA or your organization’s existing CA can be used.\n\nWhen a client connects to a node using SSL/TLS, the client receives the certificate provided by the node and will determine if the node’s certificate is valid, trusted, and matches the hostname or IP address it is trying to connect to.\n\nCaution\n\nTechnically, it’s possible to disable CA checks for certificates on the client. It is strongly recommended however to use certificates signed by an official CA or by a private CA (company PKI) that is also known to the client. This will help to ensure that establishing trust is as painless as possible.\n\nSee Generate Keystore with a private key for information about how to create a keystore.\n\nOnce the keystore is prepared, define the absolute file path to the keystore .jks file on the node using ssl.keystore_filepath setting.\n\nNote\n\nMake sure that the keystore file has the right permissions and is accessible by the system user crate.\n\nAlso, define the password needed to decrypt the keystore by using the ssl.keystore_password setting.\n\nUse ssl.keystore_key_password setting to define the key password used when creating the Keystore.\n\nFor a full list of the settings needed to configure SSL/TLS, refer to SSL configuration reference.\n\nConfiguring a separate Truststore\n\nTrusted CA certificates can be stored in a node’s keystore or a separate truststore can be used to store them.\n\nIf you want to use a separate truststore, create a node truststore and import the CA certificate(s) you want to trust. Once the truststore is prepared, define the absolute file path of the truststore .jks file on the node using the ssl.truststore_filepath setting.\n\nNote\n\nMake sure that the truststore file has the right permissions and is accessible by the system user crate.\n\nAlso define the password needed to decrypt the keystore by using the ssl.truststore_password setting.\n\nFor a full list of the settings needed to configure SSL/TLS, refer to SSL configuration reference.\n\nConnecting to a CrateDB node using HTTPS\nConnect to a CrateDB node using the Admin UI\n\nCrate’s HTTP endpoint remains unchanged. When you have turned on secure communication, it will use HTTPS instead of plain HTTP. Simply point your browser to the same URL you used before but changing the protocol to HTTPS:\n\nFor example, https://localhost:4200 becomes https://localhost:4200. If you have not configured the CrateDB node’s keystore with a signed certificate from a Certificate Authority (CA), then you will get something like the following: NET::ERR_CERT_AUTHORITY_INVALID. You either need to get your certificate signed from one of the CAs included in your browser or import your owned certificates into the browser. A third option is storing an exception for the CrateDB node certification in your browser after verifying that this is indeed a certificate you trust.\n\nConnect to a CrateDB node using Crash\n\nYou can connect to a CrateDB node using a secure communication:\n\ncrash --hosts https://localhost:4200\n\n\nTo validate the provided certificates, please see the options --verify-ssl and --key-file.\n\nConnect to a CrateDB node using REST\n\nIssue your REST requests to the node using the https:// protocol. You may have to configure your client to validate the received certificate accordingly.\n\nConnecting to a CrateDB node using the PostgreSQL wire protocol with SSL/TLS\nConnect to a CrateDB node using JDBC\n\nJDBC needs to validate the CrateDB node’s identity by checking that the node certificate is signed by a trusted authority. If the certificate is signed by a certificate authority (CA) that is known to the Java runtime, there is nothing further to do (as Java comes with copies of the most common CA’s certificates).\n\nIf you have a certificate that is signed by a CA not known to the Java runtime, you need to configure a truststore which contains the node’s certificate and provide the path to the truststore file along with the password when starting your Java application:\n\njava -Djavax.net.ssl.trustStore=mystore -Djavax.net.ssl.trustStorePassword=mypassword com.mycompany.MyApp\n\n\nIn case you face any issues extra debugging information is available by adding -Djavax.net.debug=ssl to your command line.\n\nLast but not least, the connection parameter ssl=true must be added to the connection URL so that the JDBC driver will try and establish an SSL connection.\n\nFor further information, visit JDBC SSL documentation.\n\nConnect to a CrateDB node using psql\n\nBy default, psql attempts to use SSL if available on the node. For further information including the different SSL modes please visit the PSQL documentation.\n\nSetting up a Keystore/Truststore with a certificate chain\n\nIn case you need to setup a Keystore or a Truststore, here are the commands to get you started. All the commands use a validity of 36500 days (about 100 years). You might want to use less.\n\nGenerate Keystore with a private key\n\nThe first step is to create a Keystore with a private key using the RSA algorithm. The “first and last name” is the common name (CN) which should overlap with the URL the service it is used with.\n\nCommand:\n\nkeytool -keystore keystore -genkey -keyalg RSA -alias server -validity 36500\n\n\nOutput:\n\nEnter keystore password:\nRe-enter new password:\nWhat is your first and last name?\n  [Unknown]:  ssl.crate.io\nWhat is the name of your organizational unit?\n  [Unknown]:  Cryptography Department\nWhat is the name of your organization?\n  [Unknown]:  Crate.io GmbH\nWhat is the name of your City or Locality?\n  [Unknown]:  Berlin\nWhat is the name of your State or Province?\n  [Unknown]:  Berlin\nWhat is the two-letter country code for this unit?\n  [Unknown]:  DE\nIs CN=ssl.crate.io, OU=Cryptography Department, O=Crate.io GmbH, L=Berlin, ST=Berlin, C=DE correct?\n  [no]:  yes\n\nEnter key password for <server>\n    (RETURN if same as keystore password):\nRe-enter new password:\n\nGenerate a certificate signing request\n\nTo establish trust for this key, we need to sign it. This is done by generating a certificate signing request.\n\nIf you have access to a certificate authority (CA), you can skip the next steps and get the signed certificate from the CA using the signing request which we will generate with the command below. If you don’t have access to a CA, then follow the optional steps after this step to establish your own CA.\n\nCommand:\n\nkeytool -keystore keystore -certreq -alias server -keyalg RSA -file server.csr\n\n\nOutput:\n\nEnter keystore password:\nEnter key password for <server>\n\nOptional: Use a self-signed certificate to act as a Certificate Authority (CA)\n\nNote\n\nOnly follow these optional steps if you want to create your own Certificate Authority (CA). Otherwise, please request a signed certificate from one of the CAs bundled with Java.\n\nGenerate a self-signed certificate\n\nIf you don’t get your certificate signed from one of the official CAs, you might want to create your own CA with a self-signed certificate. The common name (CN) should overlap with the CN of the server key generated in the first step. For example, ssl.crate.io overlaps with *.crate.io.\n\nNote\n\nIn this step by step guide it is shown how to create a server certificate. If you want to create a client certificate the steps are almost the same with the exception of providing a common name that is equivalent to the crate username as described in client certificate authentication method.\n\nCommand:\n\nopenssl req -x509 -sha256 -nodes -days 36500 -newkey rsa:2048 \\\n    -keyout rootCA.key -out rootCA.crt\n\n\nOutput:\n\nGenerating a 2048 bit RSA private key\n.......................................................................+++\n.............................................................+++\nwriting new private key to 'rootCA.key'\n-----\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n-----\nCountry Name (2 letter code) [AU]:AT\nState or Province Name (full name) [Some-State]:Vorarlberg\nLocality Name (eg, city) []:Dornbirn\nOrganization Name (eg, company) [Internet Widgits Pty Ltd]:Crate.io\nOrganizational Unit Name (eg, section) []:Cryptography Department\nCommon Name (e.g. server FQDN or YOUR name) []:*.crate.io\nEmail Address []:info@crate.io\n\nGenerate a signed cert\n\nIn order that the server can prove itself to have a valid and trusted domain it is required that the server certificate contains subjectAltName.\n\nCreate a file called ssl.ext with the following content. In section [alt_names] list valid domain names of the server:\n\nauthorityKeyIdentifier=keyid,issuer\nbasicConstraints=CA:FALSE\nkeyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment\nsubjectAltName = @alt_names\n\n[alt_names]\nDNS.1 = www.example.com\n\n\nNow you can generate a signed cert from our certificate signing request.\n\nCommand:\n\nopenssl x509 -req -in server.csr -CA rootCA.crt -CAkey rootCA.key \\\n    -CAcreateserial -out server.crt -sha256 -days 36500 -extfile ssl.ext\n\n\nOutput:\n\nSignature ok\nsubject=/C=DE/ST=Berlin/L=Berlin/O=Crate.io GmbH/OU=Cryptography Department/CN=ssl.crate.io\nGetting CA Private Key\n\nImport the CA certificate into the Keystore\n\nThe CA needs to be imported to the Keystore for the certificate chain to be available when we import our signed certificate.\n\nCommand:\n\nkeytool -import -keystore keystore -file rootCA.crt -alias theCARoot\n\n\nOutput:\n\nEnter keystore password:\nOwner: EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT\nIssuer: EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT\nSerial number: f13562ec6184401e\nValid from: Mon Jun 12 13:09:17 CEST 2017 until: Wed May 19 13:09:17 CEST 2117\nCertificate fingerprints:\n     MD5:  BB:A1:79:53:FE:71:EC:61:2A:19:81:E8:0E:E8:C9:81\n     SHA1: 96:66:C1:01:49:17:D1:19:FB:DB:83:86:50:3D:3D:AD:DA:F7:C6:A9\n     SHA256: 69:82:C5:24:9A:A1:AE:DF:80:29:7A:26:92:C1:A5:9F:AF:7D:03:56:CC:C3:E9:73:3B:FD:85:66:35:D6:8A:9B\n     Signature algorithm name: SHA256withRSA\n     Version: 3\n\nExtensions:\n\n#1: ObjectId: 2.5.29.35 Criticality=false\nAuthorityKeyIdentifier [\nKeyIdentifier [\n0000: CD 29 4E 07 3D C3 7C D0   16 45 FB 0A CE 8D B4 98  .)N.=....E......\n0010: B7 A8 4C 79                                        ..Ly\n]\n[EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT]\nSerialNumber: [    f13562ec 6184401e]\n]\n\n#2: ObjectId: 2.5.29.19 Criticality=false\nBasicConstraints:[\n  CA:true\n  PathLen:2147483647\n]\n\n#3: ObjectId: 2.5.29.14 Criticality=false\nSubjectKeyIdentifier [\nKeyIdentifier [\n0000: CD 29 4E 07 3D C3 7C D0   16 45 FB 0A CE 8D B4 98  .)N.=....E......\n0010: B7 A8 4C 79                                        ..Ly\n]\n]\n\nTrust this certificate? [no]:  yes\nCertificate was added to keystore\n\nImport CA into Truststore\n\nIf we are using our own CA, we should also import the certificate to the Truststore, such that it is available for clients which want to verify signatures.\n\nCommand:\n\nkeytool -import -keystore truststore -file rootCA.crt -alias theCARoot\n\n\nOutput:\n\nEnter keystore password:\nRe-enter new password:\nOwner: EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT\nIssuer: EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT\nSerial number: f13562ec6184401e\nValid from: Mon Jun 12 13:09:17 CEST 2017 until: Wed May 19 13:09:17 CEST 2117\nCertificate fingerprints:\n     MD5:  BB:A1:79:53:FE:71:EC:61:2A:19:81:E8:0E:E8:C9:81\n     SHA1: 96:66:C1:01:49:17:D1:19:FB:DB:83:86:50:3D:3D:AD:DA:F7:C6:A9\n     SHA256: 69:82:C5:24:9A:A1:AE:DF:80:29:7A:26:92:C1:A5:9F:AF:7D:03:56:CC:C3:E9:73:3B:FD:85:66:35:D6:8A:9B\n     Signature algorithm name: SHA256withRSA\n     Version: 3\n\nExtensions:\n\n#1: ObjectId: 2.5.29.35 Criticality=false\nAuthorityKeyIdentifier [\nKeyIdentifier [\n0000: CD 29 4E 07 3D C3 7C D0   16 45 FB 0A CE 8D B4 98  .)N.=....E......\n0010: B7 A8 4C 79                                        ..Ly\n]\n[EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT]\nSerialNumber: [    f13562ec 6184401e]\n]\n\n#2: ObjectId: 2.5.29.19 Criticality=false\nBasicConstraints:[\n  CA:true\n  PathLen:2147483647\n]\n\n#3: ObjectId: 2.5.29.14 Criticality=false\nSubjectKeyIdentifier [\nKeyIdentifier [\n0000: CD 29 4E 07 3D C3 7C D0   16 45 FB 0A CE 8D B4 98  .)N.=....E......\n0010: B7 A8 4C 79                                        ..Ly\n]\n]\n\nTrust this certificate? [no]:  yes\nCertificate was added to keystore\n\nImport the signed certificate\n\nNow we have a signed certificate, signed by either from a official CA or from our own CA. Let’s import it to the Keystore.\n\nCommand:\n\nkeytool -import -keystore keystore -file server.crt -alias server\n\n\nOutput:\n\nEnter keystore password:\nEnter key password for <server>\nCertificate reply was installed in keystore\n\nConfiguring CrateDB\n\nFinally, you want to supply the Keystore/Truststore in the CrateDB configuration, see Secured communications (SSL/TLS)."
  },
  {
    "title": "Authentication — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/admin/auth/index.html",
    "html": "master\nAuthentication\n\nTable of contents\n\nAuthentication Methods\nTrust method\nPassword authentication method\nClient certificate authentication method\nJWT authentication method\nHost-Based Authentication (HBA)\nAuthentication against CrateDB\nAuthenticating as a superuser\nAuthenticating to Admin UI\nNode-to-node communication"
  },
  {
    "title": "Privileges — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/admin/privileges.html",
    "html": "master\nPrivileges\n\nTo execute statements, a user needs to have the required privileges.\n\nTable of contents\n\nIntroduction\n\nPrivilege Classes\n\nPrivilege types\n\nDQL\n\nDML\n\nDDL\n\nAL\n\nHierarchical inheritance of privileges\n\nBehavior of GRANT, DENY and REVOKE\n\nGRANT\n\nDENY\n\nREVOKE\n\nList privileges\n\nRoles inheritance\n\nIntroduction\n\nInheritance\n\nPrivileges resolution\n\nGRANT\n\nREVOKE\n\nIntroduction\n\nCrateDB has a superuser (crate) which has the privilege to do anything. The privileges of other users and roles have to be managed using the GRANT, DENY or REVOKE statements.\n\nThe privileges that can be granted, denied or revoked are:\n\nDQL\n\nDML\n\nDDL\n\nAL\n\nSkip to Privilege types for details.\n\nPrivilege Classes\n\nThe privileges can be granted on different classes:\n\nCLUSTER\n\nSCHEMA\n\nTABLE and VIEW\n\nSkip to Hierarchical inheritance of privileges for details.\n\nA user with AL on level CLUSTER can grant privileges they have themselves to other users or roles as well.\n\nPrivilege types\nDQL\n\nGranting Data Query Language (DQL) privilege to a user or role, indicates that this user/role is allowed to execute SELECT, SHOW, REFRESH and COPY TO statements, as well as using the available user-defined functions, on the object for which the privilege applies.\n\nDML\n\nGranting Data Manipulation Language (DML) privilege to a user or role, indicates that this user/role is allowed to execute INSERT, COPY FROM, UPDATE and DELETE statements, on the object for which the privilege applies.\n\nDDL\n\nGranting Data Definition Language (DDL) privilege to a user or role, indicates that this user/role is allowed to execute the following statements on objects for which the privilege applies:\n\nCREATE TABLE\n\nDROP TABLE\n\nCREATE VIEW\n\nDROP VIEW\n\nCREATE FUNCTION\n\nDROP FUNCTION\n\nCREATE REPOSITORY\n\nDROP REPOSITORY\n\nCREATE SNAPSHOT\n\nDROP SNAPSHOT\n\nRESTORE SNAPSHOT\n\nALTER TABLE\n\nAL\n\nGranting Administration Language (AL) privilege to a user or role, enables the user/role to execute the following statements:\n\nCREATE USER/ROLE\n\nDROP USER/ROLE\n\nSET GLOBAL\n\nAll statements enabled via the AL privilege operate on a cluster level. So granting this on a schema or table level will have no effect.\n\nHierarchical inheritance of privileges\n\nPrivileges can be managed on three different levels, namely: CLUSTER, SCHEMA, and TABLE/VIEW.\n\nWhen a privilege is assigned on a certain level, the privilege will propagate down the hierarchy. Privileges defined on a lower level will always override those from a higher level:\n\n  cluster\n    ||\n  schema\n   /  \\\ntable view\n\n\nThis statement will grant DQL privilege to user riley on all the tables and functions of the doc schema:\n\ncr> GRANT DQL ON SCHEMA doc TO riley;\nGRANT OK, 1 row affected (... sec)\n\n\nThis statement will deny DQL privilege to user riley on the doc schema table doc.accounting. However, riley will still have DQL privilege on all the other tables of the doc schema:\n\ncr> DENY DQL ON TABLE doc.accounting TO riley;\nDENY OK, 1 row affected (... sec)\n\n\nNote\n\nIn CrateDB, schemas are just namespaces that are created and dropped implicitly. Therefore, when GRANT, DENY or REVOKE are invoked on a schema level, CrateDB takes the schema name provided without further validation.\n\nPrivileges can be managed on all schemas and tables of the cluster, except the information_schema.\n\nViews are on the same hierarchy with tables, i.e. a privilege on a view is gained through a GRANT on either the view itself, the schema the view belongs to, or a cluster-wide privilege. Privileges on relations which are referenced in the view do not grant any privileges on the view itself. On the contrary, even if the user/role does not have any privileges on a view’s referenced relations but on the view itself, the user/role can still access the relations through the view. For example:\n\ncr> CREATE VIEW first_customer as SELECT * from doc.accounting ORDER BY id LIMIT 1\nCREATE OK, 1 row affected (... sec)\n\n\nPreviously we had issued a DENY for user riley on doc.accounting but we can still access it through the view because we have access to it through the doc schema:\n\ncr> SELECT id from first_customer;\n+----+\n| id |\n+----+\n|  1 |\n+----+\nSELECT 1 row in set (... sec)\n\n\nSee Also\n\nViews: Privileges\n\nBehavior of GRANT, DENY and REVOKE\n\nNote\n\nYou can only grant, deny, or revoke privileges for an existing user or role. You must first create a user/role and then configure privileges.\n\nGRANT\n\nTo grant a privilege to an existing user or role on the whole cluster, we use the GRANT SQL statement, for example:\n\ncr> GRANT DML TO wolfgang;\nGRANT OK, 1 row affected (... sec)\n\n\nDQL privilege can be granted on the sys schema to user wolfgang, like this:\n\ncr> GRANT DQL ON SCHEMA sys TO wolfgang;\nGRANT OK, 1 row affected (... sec)\n\n\nThe following statement will grant all privileges on table doc.books to user wolfgang:\n\ncr> GRANT ALL PRIVILEGES ON TABLE doc.books TO wolfgang;\nGRANT OK, 4 rows affected (... sec)\n\n\nUsing “ALL PRIVILEGES” is a shortcut to grant all the currently grantable privileges to a user or role.\n\nNote\n\nIf no schema is specified in the table ident, the table will be looked up in the current schema.\n\nIf a user/role with the name specified in the SQL statement does not exist the statement returns an error:\n\ncr> GRANT DQL TO layla;\nRoleUnknownException[Role 'layla' does not exist]\n\n\nTo grant ALL PRIVILEGES to user will on the cluster, we can use the following syntax:\n\ncr> GRANT ALL PRIVILEGES TO will;\nGRANT OK, 4 rows affected (... sec)\n\n\nUsing ALL PRIVILEGES is a shortcut to grant all the currently grantable privileges to a user or role, namely DQL, DML and DDL.\n\nPrivileges can be granted to multiple users/roles in the same statement, like so:\n\ncr> GRANT DDL ON TABLE doc.books TO wolfgang, will;\nGRANT OK, 1 row affected (... sec)\n\nDENY\n\nTo deny a privilege to an existing user or role on the whole cluster, use the DENY SQL statement, for example:\n\ncr> DENY DDL TO will;\nDENY OK, 1 row affected (... sec)\n\n\nDQL privilege can be denied on the sys schema to user wolfgang like this:\n\ncr> DENY DQL ON SCHEMA sys TO wolfgang;\nDENY OK, 1 row affected (... sec)\n\n\nThe following statement will deny DQL privilege on table doc.books to user wolfgang:\n\ncr> DENY DQL ON TABLE doc.books TO wolfgang;\nDENY OK, 1 row affected (... sec)\n\n\nDENY ALL or DENY ALL PRIVILEGES will deny all privileges to a user or role, on the cluster it can be used like this:\n\ncr> DENY ALL TO will;\nDENY OK, 3 rows affected (... sec)\n\nREVOKE\n\nTo revoke a privilege that was previously granted or denied to a user or role use the REVOKE SQL statement, for example the DQL privilege that was previously denied to user wolfgang on the sys schema, can be revoked like this:\n\ncr> REVOKE DQL ON SCHEMA sys FROM wolfgang;\nREVOKE OK, 1 row affected (... sec)\n\n\nThe privileges that were granted and denied to user wolfgang on doc.books can be revoked like this:\n\ncr> REVOKE ALL ON TABLE doc.books FROM wolfgang;\nREVOKE OK, 4 rows affected (... sec)\n\n\nThe privileges that were granted to user will on the cluster can be revoked like this:\n\ncr> REVOKE ALL FROM will;\nREVOKE OK, 4 rows affected (... sec)\n\n\nNote\n\nThe REVOKE statement can remove only privileges that have been granted or denied through the GRANT or DENY statements. If the privilege on a specific object was not explicitly granted, the REVOKE statement has no effect. The effect of the REVOKE statement will be reflected in the row count.\n\nNote\n\nWhen a privilege is revoked from a user or role, it can still be active for that user/role, if the user/role inherits it, from another role.\n\nList privileges\n\nCrateDB exposes the privileges of users and roles of the database through the sys.privileges system table.\n\nBy querying the sys.privileges table you can get all information regarding the existing privileges. E.g.:\n\ncr> SELECT * FROM sys.privileges order by grantee, class, ident;\n+---------+----------+---------+----------------+-------+------+\n| class   | grantee  | grantor | ident          | state | type |\n+---------+----------+---------+----------------+-------+------+\n| SCHEMA  | riley    | crate   | doc            | GRANT | DQL  |\n| TABLE   | riley    | crate   | doc.accounting | DENY  | DQL  |\n| TABLE   | will     | crate   | doc.books      | GRANT | DDL  |\n| CLUSTER | wolfgang | crate   | NULL           | GRANT | DML  |\n+---------+----------+---------+----------------+-------+------+\nSELECT 4 rows in set (... sec)\n\nRoles inheritance\nIntroduction\n\nYou can grant, or revoke roles for an existing user or role. This allows to group granted or denied privileges and inherit them to other users or roles.\n\nYou must first create usesr and roles and then grant roles to other roles or users. You can configure the privileges of each role before or after granting roles to other roles or users.\n\nNote\n\nRoles can be granted to other roles or users, but users (roles which can also login to the database) cannot be granted to other roles or users.\n\nNote\n\nSuperuser crate cannot be granted to other users or roles, and roles cannot be granted to it.\n\nInheritance\n\nThe inheritance can span multiple levels, so you can have role_a which is granted to role_b, which in turn is granted to role_c, and so on. Each role can be granted to multiple other roles and each role or user can be granted multiple other roles. Cycles cannot be created, for example:\n\ncr> GRANT role_a TO role_b;\nGRANT OK, 1 row affected (... sec)\n\ncr> GRANT role_b TO role_c;\nGRANT OK, 1 row affected (... sec)\n\ncr> GRANT role_c TO role_a;\nSQLParseException[Cannot grant role role_c to role_a, role_a is a parent role of role_c and a cycle will be created]\n\nPrivileges resolution\n\nWhen a user executes a statement, the privileges mechanism will check first if the user has been granted the required privileges, if not, it will check if the roles which this user has been granted have those privileges and if not, it will continue checking the roles granted to those parent roles of the user and so on. For example:\n\ncr> GRANT role_a TO role_b;\nGRANT OK, 1 row affected (... sec)\n\ncr> GRANT role_b TO role_c;\nGRANT OK, 1 row affected (... sec)\n\ncr> GRANT DQL ON TABLE sys.users TO role_a;\nGRANT OK, 1 row affected (... sec)\n\ncr> GRANT role_c TO john;\nGRANT OK, 1 row affected (... sec)\n\n\nUser john is able to query sys.users, as even though he lacks DQL privilege on the table, he is granted role_c which in turn is granted role_b which is granted role_a, and role has the DQL privilege on sys.users.\n\nKeep in mind that DENY has precedence over GRANT. If a role has been both granted and denied a privilege (directly or through role inheritance), then DENY will take effect. For example, GRANT is inherited from a role and DENY directly set on the user:\n\ncr> GRANT DQL ON TABLE sys.users TO role_a;\nGRANT OK, 1 row affected (... sec)\n\ncr> GRANT role_a TO john\nGRANT OK, 1 row affected (... sec)\n\ncr> DENY DQL ON TABLE sys.users TO john\nDENY OK, 1 row affected (... sec)\n\n\nUser john cannot query sys.users.\n\nAnother example with DENY in effect, inherited from a role:\n\ncr> GRANT DQL ON TABLE sys.users TO role_a;\nGRANT OK, 1 row affected (... sec)\n\ncr> DENY DQL ON TABLE sys.users TO role_b;\nDENY OK, 1 row affected (... sec)\n\ncr> GRANT role_a, role_b TO john;\nGRANT OK, 2 rows affected (... sec)\n\n\nUser john cannot query sys.users.\n\nGRANT\n\nTo grant an existing role to an existing user or role on the whole cluster, we use the GRANT SQL statement, for example:\n\ncr> GRANT role_dql TO wolfgang;\nGRANT OK, 1 row affected (... sec)\n\n\nDML privilege can be granted on the sys schema to role role_dml, so, by inheritance, to user wolfgang as well, like this:\n\ncr> GRANT DQL ON SCHEMA sys TO role_dql;\nGRANT OK, 1 row affected (... sec)\n\n\nThe following statements will grant all privileges on table doc.books to role role_all_on_books, and by inheritance to user wolfgang as well:\n\ncr> GRANT role_all_on_books TO wolfgang;\nGRANT OK, 1 row affected (... sec)\n\ncr> GRANT ALL PRIVILEGES ON TABLE doc.books TO role_all_on_books;\nGRANT OK, 4 rows affected (... sec)\n\n\nIf a role with the name specified in the SQL statement does not exist the statement returns an error:\n\ncr> GRANT DDL TO role_ddl;\nRoleUnknownException[Role 'role_ddl' does not exist]\n\n\nMultiple roles can be granted to multiple users/roles in the same statement, like so:\n\ncr> GRANT role_dql, role_all_on_books TO layla, will;\nGRANT OK, 4 rows affected (... sec)\n\n\nNotice that 4 rows affected is returned, as in total there are 2 users, will and layla and each of them is granted two roles: role_dql and role_all_on_books.\n\nREVOKE\n\nTo revoke a role that was previously granted to a user or role use the REVOKE SQL statement. For example role role_dql which was previously granted to users wolfgang,``layla`` and will, can be revoked like this:\n\ncr> REVOKE role_dql FROM wolfgang, layla, will;\nREVOKE OK, 3 rows affected (... sec)\n\n\nIf a privilege is revoked from a role which is granted to other roles or users, the privilege is automatically revoked also for those roles and users, for example if we revoke privileges on table doc.books from role_all_on_books:\n\ncr> REVOKE ALL PRIVILEGES ON TABLE doc.books FROM role_all_on_books;\nREVOKE OK, 4 rows affected (... sec)\n\n\nuser wolfgang, who is granted the role role_all_on_books, also looses those privileges.\n\nIf a user is granted the same privilege by inheriting two different roles, when revoking one of the roles, the user still keeps the privilege. For example if user john gets granted `role_dql and role_dml:\n\ncr> GRANT DQL TO role_dql;\nGRANT OK, 1 row affected (... sec)\n\ncr> GRANT DQL, DML TO role_dml;\nGRANT OK, 2 rows affected (... sec)\n\ncr> GRANT role_dql, role_dml TO john;\nGRANT OK, 2 rows affected (... sec)\n\n\nand then we revoke role_dql from john:\n\ncr> REVOKE role_dql FROM john;\nREVOKE OK, 1 row affected (... sec)\n\n\njohn still has DQL privilege since it inherits it from role_dml which is still granted to him."
  },
  {
    "title": "Runtime configuration — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/admin/runtime-config.html",
    "html": "master\nRuntime configuration\n\nThe CrateDB cluster can be configured at runtime using the SET and RESET statement. See the Cluster Settings configuration section for details about the supported settings.\n\nIf SET is used with PERSISTENT the change will survive a cluster restart, if used with TRANSIENT the value will reset to the default value or to the value in the configuration file on a restart.\n\ncr> SET GLOBAL PERSISTENT stats.enabled = false;\nSET OK, 1 row affected (... sec)\n\ncr> select sys.cluster.settings['stats']['enabled'] from sys.cluster;\n+------------------------------+\n| settings['stats']['enabled'] |\n+------------------------------+\n| FALSE                        |\n+------------------------------+\nSELECT 1 row in set (... sec)\n\n\nYou can change multiple values at once:\n\ncr> SET GLOBAL TRANSIENT stats.enabled = true,\n... stats.jobs_log_size = 1024, stats.operations_log_size = 4096;\nSET OK, 1 row affected (... sec)\n\ncr> select settings['stats']['enabled'],\n...   settings['stats']['jobs_log_size'],\n...   settings['stats']['operations_log_size']\n... from sys.cluster;\n+-...------------+-...------------------+-...------------------------+\n| ...['enabled'] | ...['jobs_log_size'] | ...['operations_log_size'] |\n+-...------------+-...------------------+-...------------------------+\n| TRUE           |                 1024 |                       4096 |\n+-...------------+-...------------------+-...------------------------+\nSELECT 1 row in set (... sec)\n\n\nIts also possible to save a complete nested object of settings:\n\ncr> SET GLOBAL TRANSIENT stats = {\n...   jobs_log_size = 2048,\n...   operations_log_size = 8192\n... };\nSET OK, 1 row affected (... sec)\n\ncr> SELECT\n...   settings['stats']['jobs_log_size'] AS jobs_size,\n...   settings['stats']['operations_log_size'] AS op_size\n... FROM sys.cluster;\n+-----------+---------+\n| jobs_size | op_size |\n+-----------+---------+\n|      2048 |    8192 |\n+-----------+---------+\nSELECT 1 row in set (... sec)\n\n\nUsing the RESET statement, a setting will be reset to either on node startup defined configuration file value or to its default value:\n\ncr> RESET GLOBAL stats.enabled, stats.operations_log_size;\nRESET OK, 1 row affected (... sec)\n\ncr> SELECT\n...   settings['stats']['jobs_log_size'] AS jobs_size,\n...   settings['stats']['operations_log_size'] AS op_size\n... FROM sys.cluster;\n+-----------+---------+\n| jobs_size | op_size |\n+-----------+---------+\n|      2048 |   10000 |\n+-----------+---------+\nSELECT 1 row in set (... sec)\n\n\nRESET can also be done on objects:\n\ncr> RESET GLOBAL stats;\nRESET OK, 1 row affected (... sec)\n\ncr> SELECT\n...   settings['stats']['jobs_log_size'] AS jobs_size,\n...   settings['stats']['operations_log_size'] AS op_size\n... FROM sys.cluster;\n+-----------+---------+\n| jobs_size | op_size |\n+-----------+---------+\n|     10000 |   10000 |\n+-----------+---------+\nSELECT 1 row in set (... sec)\n"
  },
  {
    "title": "System information — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/admin/system-information.html",
    "html": "master\nSystem information\n\nCrateDB provides the sys schema which contains virtual tables. These tables are read-only and can be queried to get statistical real-time information about the cluster, its nodes and their shards:\n\nTable of contents\n\nCluster\n\nCluster license\n\nlicense\n\nCluster settings\n\nNodes\n\nid\n\nname\n\nhostname\n\nrest_url\n\nattributes\n\nport\n\nload\n\nmem\n\nheap\n\nversion\n\ncluster_state_version\n\nfs\n\nthread_pools\n\nos\n\ncgroup limitations\n\nUptime limitations\n\nos_info\n\nnetwork\n\nconnections\n\nprocess\n\nNode checks\n\nAcknowledge failed checks\n\nDescription of checked node settings\n\nRecovery expected data nodes\n\nRecovery after data nodes\n\nRecovery after time\n\nRouting allocation disk watermark high\n\nRouting allocation disk watermark low\n\nMaximum shards per node\n\nShards\n\nTable schema\n\nExample\n\nSegments\n\nJobs, operations, and logs\n\nJobs\n\nTable schema\n\nJobs metrics\n\nsys.jobs_metrics Table schema\n\nClassification\n\nOperations\n\nTable schema\n\nLogs\n\nsys.jobs_log Table schema\n\nsys.operations_log Table schema\n\nCluster checks\n\nCurrent Checks\n\nNumber of partitions\n\nTables need to be recreated\n\nCrateDB table version compatibility scheme\n\nAvoiding reindex using partitioned tables\n\nHow to reindex\n\nLicense check\n\nHealth\n\nHealth definition\n\nRepositories\n\nSnapshots\n\nSnapshot Restore\n\nSummits\n\nUsers\n\nRoles\n\nPrivileges\n\nAllocations\n\nShard table permissions\n\nsys jobs tables permissions\n\npg_stats\n\npg_publication\n\npg_publication_tables\n\npg_subscription\n\npg_subscription_rel\n\nCluster\n\nBasic information about the CrateDB cluster can be retrieved from the sys.cluster table:\n\nName\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nA unique ID generated by the system.\n\n\t\n\nTEXT\n\n\n\n\nlicense\n\n\t\n\nThe current CrateDB license information. Always NULL. This exists for backward compatibility\n\n\t\n\nOBJECT\n\n\n\n\nname\n\n\t\n\nThe cluster name.\n\n\t\n\nTEXT\n\n\n\n\nmaster_node\n\n\t\n\nNode ID of the node which currently operates as master\n\n\t\n\nTEXT\n\n\n\n\nsettings\n\n\t\n\nThe cluster settings.\n\n\t\n\nOBJECT\n\nThe result has at most 1 row:\n\ncr> select name from sys.cluster;\n+-----------------+\n| name            |\n+-----------------+\n| Testing-CrateDB |\n+-----------------+\nSELECT 1 row in set (... sec)\n\nCluster license\n\nThe sys.cluster.license expression returns information about the currently registered license.\n\nNote\n\nLicenses were removed in CrateDB 4.5. Accordingly, these values are deprecated and return NULL in CrateDB 4.5 and higher.\n\nlicense\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nlicense\n\n\t\nThe current CrateDB license information\n\nor NULL on CrateDB CE.\n\n\t\n\nOBJECT\n\n\n\n\nlicense['expiry_date']\n\n\t\n\nThe Dates and times on which the license expires.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nlicense['issued_to']\n\n\t\n\nThe organisation for which the license is issued.\n\n\t\n\nTEXT\n\n\n\n\nlicense['max_nodes']\n\n\t\n\nThe maximum number of nodes the license is valid for.\n\n\t\n\nINTEGER\n\nCluster settings\n\nThe sys.cluster.settings expression returns information about the currently applied cluster settings.\n\ncr> select settings from sys.cluster;\n+-----------------------------------------------------...-+\n| settings                                                |\n+-----------------------------------------------------...-+\n| {\"bulk\": {...}, \"cluster\": {...}, \"gateway\": {...}, ... |\n+-----------------------------------------------------...-+\nSELECT 1 row in set (... sec)\n\ncr> select column_name, data_type from information_schema.columns\n... where column_name like 'settings%'\n... and table_name = 'cluster';\n+-----------------------------------------------------------------------------------+--------------+\n| column_name                                                                       | data_type    |\n+-----------------------------------------------------------------------------------+--------------+\n| settings                                                                          | object       |\n| settings['bulk']                                                                  | object       |\n| settings['bulk']['request_timeout']                                               | text         |\n| settings['cluster']                                                               | object       |\n| settings['cluster']['graceful_stop']                                              | object       |\n| settings['cluster']['graceful_stop']['force']                                     | boolean      |\n| settings['cluster']['graceful_stop']['min_availability']                          | text         |\n| settings['cluster']['graceful_stop']['timeout']                                   | text         |\n| settings['cluster']['info']                                                       | object       |\n| settings['cluster']['info']['update']                                             | object       |\n| settings['cluster']['info']['update']['interval']                                 | text         |\n| settings['cluster']['max_shards_per_node']                                        | integer      |\n| settings['cluster']['routing']                                                    | object       |\n| settings['cluster']['routing']['allocation']                                      | object       |\n| settings['cluster']['routing']['allocation']['allow_rebalance']                   | text         |\n| settings['cluster']['routing']['allocation']['balance']                           | object       |\n| settings['cluster']['routing']['allocation']['balance']['index']                  | real         |\n| settings['cluster']['routing']['allocation']['balance']['shard']                  | real         |\n| settings['cluster']['routing']['allocation']['balance']['threshold']              | real         |\n| settings['cluster']['routing']['allocation']['cluster_concurrent_rebalance']      | integer      |\n| settings['cluster']['routing']['allocation']['disk']                              | object       |\n| settings['cluster']['routing']['allocation']['disk']['threshold_enabled']         | boolean      |\n| settings['cluster']['routing']['allocation']['disk']['watermark']                 | object       |\n| settings['cluster']['routing']['allocation']['disk']['watermark']['flood_stage']  | text         |\n| settings['cluster']['routing']['allocation']['disk']['watermark']['high']         | text         |\n| settings['cluster']['routing']['allocation']['disk']['watermark']['low']          | text         |\n| settings['cluster']['routing']['allocation']['enable']                            | text         |\n| settings['cluster']['routing']['allocation']['exclude']                           | object       |\n| settings['cluster']['routing']['allocation']['exclude']['_host']                  | text         |\n| settings['cluster']['routing']['allocation']['exclude']['_id']                    | text         |\n| settings['cluster']['routing']['allocation']['exclude']['_ip']                    | text         |\n| settings['cluster']['routing']['allocation']['exclude']['_name']                  | text         |\n| settings['cluster']['routing']['allocation']['include']                           | object       |\n| settings['cluster']['routing']['allocation']['include']['_host']                  | text         |\n| settings['cluster']['routing']['allocation']['include']['_id']                    | text         |\n| settings['cluster']['routing']['allocation']['include']['_ip']                    | text         |\n| settings['cluster']['routing']['allocation']['include']['_name']                  | text         |\n| settings['cluster']['routing']['allocation']['node_concurrent_recoveries']        | integer      |\n| settings['cluster']['routing']['allocation']['node_initial_primaries_recoveries'] | integer      |\n| settings['cluster']['routing']['allocation']['require']                           | object       |\n| settings['cluster']['routing']['allocation']['require']['_host']                  | text         |\n| settings['cluster']['routing']['allocation']['require']['_id']                    | text         |\n| settings['cluster']['routing']['allocation']['require']['_ip']                    | text         |\n| settings['cluster']['routing']['allocation']['require']['_name']                  | text         |\n| settings['cluster']['routing']['allocation']['total_shards_per_node']             | integer      |\n| settings['cluster']['routing']['rebalance']                                       | object       |\n| settings['cluster']['routing']['rebalance']['enable']                             | text         |\n| settings['fdw']                                                                   | object       |\n| settings['fdw']['allow_local']                                                    | boolean      |\n| settings['gateway']                                                               | object       |\n| settings['gateway']['expected_data_nodes']                                        | integer      |\n| settings['gateway']['expected_nodes']                                             | integer      |\n| settings['gateway']['recover_after_data_nodes']                                   | integer      |\n| settings['gateway']['recover_after_nodes']                                        | integer      |\n| settings['gateway']['recover_after_time']                                         | text         |\n| settings['indices']                                                               | object       |\n| settings['indices']['breaker']                                                    | object       |\n| settings['indices']['breaker']['query']                                           | object       |\n| settings['indices']['breaker']['query']['limit']                                  | text         |\n| settings['indices']['breaker']['request']                                         | object       |\n| settings['indices']['breaker']['request']['limit']                                | text         |\n| settings['indices']['breaker']['total']                                           | object       |\n| settings['indices']['breaker']['total']['limit']                                  | text         |\n| settings['indices']['recovery']                                                   | object       |\n| settings['indices']['recovery']['internal_action_long_timeout']                   | text         |\n| settings['indices']['recovery']['internal_action_timeout']                        | text         |\n| settings['indices']['recovery']['max_bytes_per_sec']                              | text         |\n| settings['indices']['recovery']['recovery_activity_timeout']                      | text         |\n| settings['indices']['recovery']['retry_delay_network']                            | text         |\n| settings['indices']['recovery']['retry_delay_state_sync']                         | text         |\n| settings['indices']['replication']                                                | object       |\n| settings['indices']['replication']['retry_timeout']                               | text         |\n| settings['logger']                                                                | object_array |\n| settings['logger']['level']                                                       | text_array   |\n| settings['logger']['name']                                                        | text_array   |\n| settings['memory']                                                                | object       |\n| settings['memory']['allocation']                                                  | object       |\n| settings['memory']['allocation']['type']                                          | text         |\n| settings['memory']['operation_limit']                                             | integer      |\n| settings['overload_protection']                                                   | object       |\n| settings['overload_protection']['dml']                                            | object       |\n| settings['overload_protection']['dml']['initial_concurrency']                     | integer      |\n| settings['overload_protection']['dml']['max_concurrency']                         | integer      |\n| settings['overload_protection']['dml']['min_concurrency']                         | integer      |\n| settings['overload_protection']['dml']['queue_size']                              | integer      |\n| settings['replication']                                                           | object       |\n| settings['replication']['logical']                                                | object       |\n| settings['replication']['logical']['ops_batch_size']                              | integer      |\n| settings['replication']['logical']['reads_poll_duration']                         | text         |\n| settings['replication']['logical']['recovery']                                    | object       |\n| settings['replication']['logical']['recovery']['chunk_size']                      | text         |\n| settings['replication']['logical']['recovery']['max_concurrent_file_chunks']      | integer      |\n| settings['statement_timeout']                                                     | text         |\n| settings['stats']                                                                 | object       |\n| settings['stats']['breaker']                                                      | object       |\n| settings['stats']['breaker']['log']                                               | object       |\n| settings['stats']['breaker']['log']['jobs']                                       | object       |\n| settings['stats']['breaker']['log']['jobs']['limit']                              | text         |\n| settings['stats']['breaker']['log']['operations']                                 | object       |\n| settings['stats']['breaker']['log']['operations']['limit']                        | text         |\n| settings['stats']['enabled']                                                      | boolean      |\n| settings['stats']['jobs_log_expiration']                                          | text         |\n| settings['stats']['jobs_log_filter']                                              | text         |\n| settings['stats']['jobs_log_persistent_filter']                                   | text         |\n| settings['stats']['jobs_log_size']                                                | integer      |\n| settings['stats']['operations_log_expiration']                                    | text         |\n| settings['stats']['operations_log_size']                                          | integer      |\n| settings['stats']['service']                                                      | object       |\n| settings['stats']['service']['interval']                                          | text         |\n| settings['stats']['service']['max_bytes_per_sec']                                 | text         |\n| settings['udc']                                                                   | object       |\n| settings['udc']['enabled']                                                        | boolean      |\n| settings['udc']['initial_delay']                                                  | text         |\n| settings['udc']['interval']                                                       | text         |\n| settings['udc']['url']                                                            | text         |\n+-----------------------------------------------------------------------------------+--------------+\nSELECT ... rows in set (... sec)\n\n\nFor further details, see the Cluster Settings configuration section.\n\nNodes\n\nTo get information about the nodes query for sys.nodes.\n\nThis table can be queried for one, multiple or all nodes within a cluster.\n\nThe table schema is as follows:\n\nid\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nA unique ID within the cluster generated by the system.\n\n\t\n\nTEXT\n\nname\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nname\n\n\t\n\nThe node name within a cluster. The system will choose a random name. You can also customize the node name, see Node-specific settings.\n\n\t\n\nTEXT\n\nhostname\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nhostname\n\n\t\n\nThe specified host name of the machine the node is running on.\n\n\t\n\nTEXT\n\nrest_url\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nrest_url\n\n\t\n\nFull HTTP(s) address where the REST API of the node is exposed, including schema, hostname (or IP) and port.\n\n\t\n\nTEXT\n\nattributes\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nattributes\n\n\t\n\nThe custom attributes set for the node, e.g. if node.attr.color is blue, and node.attr.location is east`, the value of this column would be: ``{color=blue, location=east}\n\n\t\n\nOBJECT\n\nport\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nport\n\n\t\n\nThe specified ports for both HTTP and binary transport interfaces. You can also customize the ports setting, see Ports.\n\n\t\n\nOBJECT\n\n\n\n\nport['http']\n\n\t\n\nCrateDB’s HTTP port.\n\n\t\n\nINTEGER\n\n\n\n\nport['transport']\n\n\t\n\nCrateDB’s binary transport port.\n\n\t\n\nINTEGER\n\n\n\n\nport['psql']\n\n\t\n\nThe PostgreSQL wire protocol port.\n\n\t\n\nINTEGER\n\nload\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nload\n\n\t\n\nSystem load statistics\n\n\t\n\nOBJECT\n\n\n\n\nload['1']\n\n\t\n\nAverage load over the last 1 minute.\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\nload['5']\n\n\t\n\nAverage load over the last 5 minutes.\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\nload['15']\n\n\t\n\nAverage load over the last 15 minutes.\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\nload['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the load probe.\n\n\t\n\nBIGINT\n\nmem\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nmem\n\n\t\n\nMemory utilization statistics of the host.\n\n\t\n\nOBJECT\n\n\n\n\nmem['used']\n\n\t\n\nCurrently used memory in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nmem['used_percent']\n\n\t\n\nCurrently used memory in percent of total.\n\n\t\n\nSMALLINT\n\n\n\n\nmem['free']\n\n\t\n\nCurrently available memory in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nmem['free_percent']\n\n\t\n\nCurrently available memory in percent of total.\n\n\t\n\nSMALLINT\n\n\n\n\nmem['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the memory probe.\n\n\t\n\nBIGINT\n\nheap\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nheap\n\n\t\n\nHeap memory utilization statistics.\n\n\t\n\nOBJECT\n\n\n\n\nheap['used']\n\n\t\n\nCurrently used heap memory in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nheap['max']\n\n\t\n\nMaximum available heap memory. You can specify the max heap memory CrateDB should use in the Configuration.\n\n\t\n\nBIGINT\n\n\n\n\nheap['free']\n\n\t\n\nCurrently available heap memory in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nheap['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the heap probe.\n\n\t\n\nBIGINT\n\nversion\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nversion\n\n\t\n\nCrateDB version information.\n\n\t\n\nOBJECT\n\n\n\n\nversion['number']\n\n\t\n\nVersion string in format \"major.minor.hotfix\"\n\n\t\n\nTEXT\n\n\n\n\nversion['build_hash']\n\n\t\n\nSHA hash of the GitHub commit which this build was built from.\n\n\t\n\nTEXT\n\n\n\n\nversion['build_snapshot']\n\n\t\n\nIndicates whether this build is a snapshot build.\n\n\t\n\nBOOLEAN\n\n\n\n\nversion['minimum_index_compatibility_version']\n\n\t\n\nIndicates the minimum compatible index version which is supported.\n\n\t\n\nTEXT\n\n\n\n\nversion['minimum_wire_compatibility_version']\n\n\t\n\nIndicates the minimum compatible wire protocol version which is supported.\n\n\t\n\nTEXT\n\ncluster_state_version\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\ncluster_state_version\n\n\t\n\nThe current version of the cluster state. The cluster state is an immutable structure and that is recreated when a change is published.\n\n\t\n\nBIGINT\n\nfs\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nfs\n\n\t\n\nUtilization statistics about the file system.\n\n\t\n\nOBJECT\n\n\n\n\nfs['total']\n\n\t\n\nAggregated usage statistic of all disks on the host.\n\n\t\n\nOBJECT\n\n\n\n\nfs['total']['size']\n\n\t\n\nTotal size of all disks in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['used']\n\n\t\n\nTotal used space of all disks in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['available']\n\n\t\n\nTotal available space of all disks in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['reads']\n\n\t\n\nTotal number of reads on all disks.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['bytes_read']\n\n\t\n\nTotal size of reads on all disks in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['writes']\n\n\t\n\nTotal number of writes on all disks.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['bytes_written']\n\n\t\n\nTotal size of writes on all disks in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['disks']\n\n\t\n\nUsage statistics of individual disks on the host.\n\n\t\n\nARRAY\n\n\n\n\nfs['disks']['dev']\n\n\t\n\nDevice name\n\n\t\n\nTEXT\n\n\n\n\nfs['disks']['size']\n\n\t\n\nTotal size of the disk in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['disks']['used']\n\n\t\n\nUsed space of the disk in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['disks']['available']\n\n\t\n\nAvailable space of the disk in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['data']\n\n\t\n\nInformation about data paths used by the node.\n\n\t\n\nARRAY\n\n\n\n\nfs['data']['dev']\n\n\t\n\nDevice name\n\n\t\n\nTEXT\n\n\n\n\nfs['data']['path']\n\n\t\n\nFile path where the data of the node resides.\n\n\t\n\nTEXT\n\nthread_pools\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nthread_pools\n\n\t\n\nUsage statistics of Java thread pools.\n\n\t\n\nARRAY\n\n\n\n\nthread_pools['name']\n\n\t\n\nName of the pool.\n\n\t\n\nTEXT\n\n\n\n\nthread_pools['active']\n\n\t\n\nNumber of currently running thread in the thread pool.\n\n\t\n\nINTEGER\n\n\n\n\nthread_pools['rejected']\n\n\t\n\nTotal number of rejected threads in the thread pool.\n\n\t\n\nBIGINT\n\n\n\n\nthread_pools['largest']\n\n\t\n\nLargest number of threads that have ever simultaneously been in the pool.\n\n\t\n\nINTEGER\n\n\n\n\nthread_pools['completed']\n\n\t\n\nTotal number of completed thread in the thread pool.\n\n\t\n\nBIGINT\n\n\n\n\nthread_pools['threads']\n\n\t\n\nSize of the thread pool.\n\n\t\n\nINTEGER\n\n\n\n\nthread_pools['queue']\n\n\t\n\nNumber of thread currently in the queue.\n\n\t\n\nINTEGER\n\nos\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nos\n\n\t\n\nOperating system stats\n\n\t\n\nOBJECT\n\n\n\n\nos['uptime']\n\n\t\n\nSystem uptime in milliseconds\n\nRequires allowing system calls on Windows and macOS. See notes in Uptime limitations.\n\n\t\n\nBIGINT\n\n\n\n\nos['timestamp']\n\n\t\n\nUNIX timestamp in millisecond resolution\n\n\t\n\nBIGINT\n\n\n\n\nos['cpu']\n\n\t\n\nInformation about CPU utilization\n\n\t\n\nOBJECT\n\n\n\n\nos['cpu']['used']\n\n\t\n\nSystem CPU usage as percentage\n\n\t\n\nSMALLINT\n\n\n\n\nos['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the OS probe.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']\n\n\t\n\nInformation about cgroups (Linux only)\n\n\t\n\nOBJECT\n\n\n\n\nos['cgroup']['cpuacct']\n\n\t\n\nInformation about CPU accounting\n\n\t\n\nOBJECT\n\n\n\n\nos['cgroup']['cpuacct']['control_group']\n\n\t\n\nThe path to the CPU accounting cgroup\n\n\t\n\nTEXT\n\n\n\n\nos['cgroup']['cpuacct']['usage_nanos']\n\n\t\n\nThe total CPU time (in nanoseconds) consumed by all tasks in this cgroup.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['cpu']\n\n\t\n\nInformation about the CPU subsystem\n\n\t\n\nOBJECT\n\n\n\n\nos['cgroup']['cpu']['control_group']\n\n\t\n\nThe path to the CPU cgroup\n\n\t\n\nTEXT\n\n\n\n\nos['cgroup']['cpu']['cfs_period_micros']\n\n\t\n\nThe period of time (in microseconds) the cgroup access to the CPU gets reallocated.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['cpu']['cfs_quota_micros']\n\n\t\n\nThe total amount of time (in microseconds) for which all tasks in the cgroup can run during one period (cfs_period_micros).\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['cpu']['num_elapsed_periods']\n\n\t\n\nThe nr. of period intervals (cfs_period_micros) that have elapsed.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['cpu']['num_times_throttled']\n\n\t\n\nThe nr. of times tasks in the cgroup have been throttled.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['cpu']['time_throttled_nanos']\n\n\t\n\nThe total time (in nanoseconds) for which tasks in the cgroup have been throttled.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['mem']\n\n\t\n\nInformation about memory resources used by tasks in a cgroup.\n\n\t\n\nOBJECT\n\n\n\n\nos['cgroup']['mem']['control_group']\n\n\t\n\nThe path to the memory cgroup\n\n\t\n\nTEXT\n\n\n\n\nos['cgroup']['mem']['usage_bytes']\n\n\t\n\nThe total current memory usage by processes in the cgroup.\n\n\t\n\nTEXT\n\n\n\n\nos['cgroup']['mem']['limit_bytes']\n\n\t\n\nThe max. amount of user memory in the cgroup.\n\n\t\n\nTEXT\n\nThe CPU information values are cached for 1s. They might differ from the actual values at query time. Use the probe timestamp to get the time of collection. When analyzing the CPU usage over time, always use os['probe_timestamp'] to calculate the time difference between 2 probes.\n\ncgroup limitations\n\nNote\n\ncgroup metrics only work if the stats are available from /sys/fs/cgroup/cpu and /sys/fs/cgroup/cpuacct.\n\nUptime limitations\n\nNote\n\nos[‘uptime’] required a system call when running CrateDB on Windows or macOS, however, system calls are not permitted by default. If you require this metric you need to allow system calls by setting bootstrap.seccomp to false. This setting must be set in the crate.yml or via command line argument and cannot be changed at runtime.\n\nos_info\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nos_info\n\n\t\n\nOperating system information\n\n\t\n\nOBJECT\n\n\n\n\nos_info['available_processors']\n\n\t\n\nNumber of processors that are available in the JVM. This is usually equal to the number of cores of the CPU.\n\n\t\n\nINTEGER\n\n\n\n\nos_info['name']\n\n\t\n\nName of the operating system (ex: Linux, Windows, macOS)\n\n\t\n\nTEXT\n\n\n\n\nos_info['arch']\n\n\t\n\nName of the JVM architecture (ex: amd64, x86)\n\n\t\n\nTEXT\n\n\n\n\nos_info['version']\n\n\t\n\nVersion of the operating system\n\n\t\n\nTEXT\n\n\n\n\nos_info['jvm']\n\n\t\n\nInformation about the JVM (Java Virtual Machine)\n\n\t\n\nOBJECT\n\n\n\n\nos_info['jvm']['version']\n\n\t\n\nThe JVM version\n\n\t\n\nTEXT\n\n\n\n\nos_info['jvm']['vm_name']\n\n\t\n\nThe name of the JVM (e.g. OpenJDK, Java HotSpot(TM) )\n\n\t\n\nTEXT\n\n\n\n\nos_info['jvm']['vm_vendor']\n\n\t\n\nThe vendor name of the JVM\n\n\t\n\nTEXT\n\n\n\n\nos_info['jvm']['vm_version']\n\n\t\n\nThe version of the JVM\n\n\t\n\nTEXT\n\nnetwork\n\nNetwork statistics are deprecated in CrateDB 2.3 and may completely be removed in subsequent versions. All BIGINT columns always return 0.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nnetwork\n\n\t\n\nStatistics about network activity on the host.\n\n\t\n\nOBJECT\n\n\n\n\nnetwork['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the network probe.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']\n\n\t\n\nTCP network activity on the host.\n\n\t\n\nOBJECT\n\n\n\n\nnetwork['tcp']['connections']\n\n\t\n\nInformation about TCP network connections.\n\n\t\n\nOBJECT\n\n\n\n\nnetwork['tpc']['connections']['initiated']\n\n\t\n\nTotal number of initiated TCP connections.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tpc']['connections']['accepted']\n\n\t\n\nTotal number of accepted TCP connections.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tpc']['connections']['curr_established']\n\n\t\n\nTotal number of currently established TCP connections.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']['connections']['dropped']\n\n\t\n\nTotal number of dropped TCP connections.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']['connections']['embryonic_dropped']\n\n\t\n\nTotal number of TCP connections that have been dropped before they were accepted.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']['packets']\n\n\t\n\nInformation about TCP packets.\n\n\t\n\nOBJECT\n\n\n\n\nnetwork['tpc']['packets']['sent']\n\n\t\n\nTotal number of TCP packets sent.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']['packets']['received']\n\n\t\n\nTotal number of TCP packets received.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tpc']['packets']['retransmitted']\n\n\t\n\nTotal number of TCP packets retransmitted due to an error.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']['packets']['errors_received']\n\n\t\n\nTotal number of TCP packets that contained checksum errors, had a bad offset, were dropped because of a lack of memory or were too short.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']]['packets']['rst_sent']\n\n\t\n\nTotal number of RST packets sent due to left unread data in queue when socket is closed. See tools.ietf.org.\n\n\t\n\nBIGINT\n\nconnections\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nhttp\n\n\t\n\nNumber of connections established via HTTP\n\n\t\n\nOBJECT\n\n\n\n\nhttp['open']\n\n\t\n\nThe currently open connections established via HTTP\n\n\t\n\nBIGINT\n\n\n\n\nhttp['total']\n\n\t\n\nThe total number of connections that have been established via HTTP over the life time of a CrateDB node\n\n\t\n\nBIGINT\n\n\n\n\npsql\n\n\t\n\nNumber of connections established via PostgreSQL protocol\n\n\t\n\nOBJECT\n\n\n\n\npsql['open']\n\n\t\n\nThe currently open connections established via PostgreSQL protocol\n\n\t\n\nBIGINT\n\n\n\n\npsql['total']\n\n\t\n\nThe total number of connections that have been established via PostgreSQL protocol over the life time of a CrateDB node\n\n\t\n\nBIGINT\n\n\n\n\ntransport\n\n\t\n\nNumber of connections established via Transport protocol\n\n\t\n\nOBJECT\n\n\n\n\ntransport['open']\n\n\t\n\nThe currently open connections established via Transport protocol\n\n\t\n\nBIGINT\n\nprocess\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nprocess\n\n\t\n\nStatistics about the CrateDB process.\n\n\t\n\nOBJECT\n\n\n\n\nprocess['open_file_descriptors']\n\n\t\n\nNumber of currently open file descriptors used by the CrateDB process.\n\n\t\n\nBIGINT\n\n\n\n\nprocess['max_open_file_descriptors']\n\n\t\n\nThe maximum number of open file descriptors CrateDB can use.\n\n\t\n\nBIGINT\n\n\n\n\nprocess['probe_timestamp']\n\n\t\n\nThe system UNIX timestamp at the moment of the probe collection.\n\n\t\n\nBIGINT\n\n\n\n\nprocess['cpu']\n\n\t\n\nInformation about the CPU usage of the CrateDB process.\n\n\t\n\nOBJECT\n\n\n\n\nprocess['cpu']['percent']\n\n\t\n\nThe CPU usage of the CrateDB JVM process given in percent.\n\n\t\n\nSMALLINT\n\nThe CPU information values are cached for 1s. They might differ from the actual values at query time. Use the probe timestamp to get the time of the collect. When analyzing the CPU usage over time, always use process['probe_timestamp'] to calculate the time difference between 2 probes.\n\nNote\n\nIf one of the queried nodes is not responding within three seconds it returns null every column except id and name. This behaviour could be used to detect hanging nodes.\n\nNode checks\n\nThe table sys.node_checks exposes a list of internal node checks and results of their validation.\n\nThe table schema is the following:\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe unique check ID.\n\n\t\n\nINTEGER\n\n\n\n\nnode_id\n\n\t\n\nThe unique node ID.\n\n\t\n\nTEXT\n\n\n\n\nseverity\n\n\t\n\nThe level of severity. The higher the value of the field the higher severity.\n\n\t\n\nINTEGER\n\n\n\n\ndescription\n\n\t\n\nThe description message for the setting check.\n\n\t\n\nTEXT\n\n\n\n\npassed\n\n\t\n\nThe flag determines whether the check for the setting has passed.\n\n\t\n\nBOOLEAN\n\n\n\n\nacknowledged\n\n\t\n\nThe flag determines whether the check for this setting has been acknowledged by the user in order to ignored the value of passed column. This column can be updated.\n\n\t\n\nBOOLEAN\n\nExample query:\n\ncr> select id, node_id, description from sys.node_checks order by id, node_id;\n+----+---------...-+--------------------------------------------------------------...-+\n| id | node_id     | description                                                      |\n+----+---------...-+--------------------------------------------------------------...-+\n|  1 | ...         | It has been detected that the 'gateway.expected_data_nodes' s... |\n|  2 | ...         | The cluster setting 'gateway.recover_after_data_nodes' (or th... |\n|  3 | ...         | If any of the \"expected data nodes\" recovery settings are set... |\n|  5 | ...         | The high disk watermark is exceeded on the node. The cluster ... |\n|  6 | ...         | The low disk watermark is exceeded on the node. The cluster w... |\n|  7 | ...         | The flood stage disk watermark is exceeded on the node. Table... |\n|  8 | ...         | The amount of shards on the node reached 90 % of the limit of... |\n+----+---------...-+--------------------------------------------------------------...-+\nSELECT 7 rows in set (... sec)\n\nAcknowledge failed checks\n\nIt is possible to acknowledge every check by updating the acknowledged column. By doing this, specially CrateDB’s built-in Admin UI won’t complain anymore about failing checks.\n\nImagine we’ve added a new node to our cluster, but as the gateway.expected_data_nodes column can only be set via config-file or command-line argument, the check for this setting will not pass on the already running nodes until the config-file or command-line argument on these nodes is updated and the nodes are restarted (which is not what we want on a healthy well running cluster).\n\nIn order to make the Admin UI accept a failing check (so the checks label goes green again), we must acknowledge this check by updating it’s acknowledged flag:\n\ncr> update sys.node_checks set acknowledged = true where id = 1;\nUPDATE OK, 1 row affected (... sec)\n\n\nCaution\n\nUpdates on this column are transient, so changed values are lost after the affected node is restarted.\n\nDescription of checked node settings\nRecovery expected data nodes\n\nThis check looks at the gateway.expected_data_nodes setting and checks if its value matches the actual number of data nodes present in the cluster. If the actual number of nodes is below the expected number, the warning is raised to indicate some nodes are down. If the actual number is greater, this is flagged to indicate the setting should be updated.\n\nNote\n\nFor backward compatibility, setting the deprecated gateway.expected_nodes instead is still supported. It counts all nodes, not only data-carrying nodes.\n\nRecovery after data nodes\n\nThis check looks at the gateway.recover_after_data_nodes setting and checks if its value is greater than half the configured expected number, but not greater than the configured expected number.\n\nNote\n\nFor backward compatibility, setting the deprecated gateway.recover_after_nodes instead is still supported.\n\n(E / 2) < R <= E\n\n\nHere, R is the number of recovery nodes and E is the number of expected (data) nodes.\n\nIf recovery is started when some nodes are down, CrateDB proceeds on the basis the nodes that are down may not be coming back, and it will create new replicas and rebalance shards as necessary. This is throttled, and it can be controlled with routing allocation settings, but depending on the context, you may prefer to delay recovery if the nodes are only down for a short period of time, so it is advisable to review the documentation around the settings involved and configure them carefully.\n\nRecovery after time\n\nIf gateway.recover_after_data_nodes is set, then gateway.recover_after_time must not be set to 0s, otherwise the gateway.recover_after_data_nodes setting wouldn’t have any effect.\n\nNote\n\nFor backward compatibility, setting the deprecated gateway.recover_after_nodes instead is still supported.\n\nRouting allocation disk watermark high\n\nThe check for the cluster.routing.allocation.disk.watermark.high setting verifies that the high watermark is not exceeded on the current node. The usage of each disk for configured CrateDB data paths is verified against the threshold setting. If one or more verification fails the check is marked as not passed.\n\nRouting allocation disk watermark low\n\nThe check for the cluster.routing.allocation.disk.watermark.low which controls the low watermark for the node disk usage. The check verifies that the low watermark is not exceeded on the current node. The verification is done against each disk for configured CrateDB data paths. The check is not passed if the verification for one or more disk fails.\n\nMaximum shards per node\n\nThe check verifies that the amount of shards on the current node is less than 90 percent of cluster.max_shards_per_node. Creating new tables or partitions which would push the number of shards beyond 100 % of the limit will be rejected.\n\nShards\n\nThe table sys.shards contains real-time statistics for all shards of all (non-system) tables.\n\nTable schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nnode\n\n\t\n\nInformation about the node the shard is located at.\n\n\t\n\nOBJECT\n\n\n\n\nnode['name']\n\n\t\n\nThe name of the node the shard is located at.\n\n\t\n\nTEXT\n\n\n\n\nnode['id']\n\n\t\n\nThe id of the node the shard is located at.\n\n\t\n\nTEXT\n\n\n\n\nblob_path\n\n\t\n\nPath to the directory which contains the blob files of the shard, or null if the shard is not a blob shard.\n\n\t\n\nTEXT\n\n\n\n\nid\n\n\t\n\nThe shard id. This shard id is managed by the system, ranging from 0 up to the number of configured shards of the table.\n\n\t\n\nINTEGER\n\n\n\n\nmin_lucene_version\n\n\t\n\nShows the oldest Lucene segment version used in this shard.\n\n\t\n\nTEXT\n\n\n\n\nnum_docs\n\n\t\n\nThe total amount of documents within a shard.\n\n\t\n\nBIGINT\n\n\n\n\noprhan_partition\n\n\t\n\nTrue if this shard belongs to an orphaned partition which doesn’t belong to any table anymore.\n\n\t\n\nBOOLEAN\n\n\n\n\npartition_ident\n\n\t\n\nThe partition ident of a partitioned table. Empty for non-partitioned tables.\n\n\t\n\nTEXT\n\n\n\n\npath\n\n\t\n\nPath to the shard directory on the filesystem. This directory contains state and index files.\n\n\t\n\nTEXT\n\n\n\n\nprimary\n\n\t\n\nIndicates if this shard is the primary shard.\n\n\t\n\nBOOLEAN\n\n\n\n\nrecovery\n\n\t\n\nRecovery statistics for a shard.\n\n\t\n\nOBJECT\n\n\n\n\nrecovery['files']\n\n\t\n\nFile recovery statistics\n\n\t\n\nOBJECT\n\n\n\n\nrecovery['files']['percent']\n\n\t\n\nPercentage of files already recovered.\n\n\t\n\nREAL\n\n\n\n\nrecovery['files']['recovered']\n\n\t\n\nNumber of files recovered in the shard. Includes both existing and reused files.\n\n\t\n\nINTEGER\n\n\n\n\nrecovery['files']['reused']\n\n\t\n\nTotal number of files reused from a local copy while recovering the shard.\n\n\t\n\nINTEGER\n\n\n\n\nrecovery['files']['used']\n\n\t\n\nTotal number of files in the shard.\n\n\t\n\nINTEGER\n\n\n\n\nrecovery['size']\n\n\t\n\nRecovery statistics for the shard in bytes\n\n\t\n\nOBJECT\n\n\n\n\nrecovery['size']['percent']\n\n\t\n\nPercentage of bytes already recovered\n\n\t\n\nREAL\n\n\n\n\nrecovery['size']['recovered']\n\n\t\n\nNumber of bytes recovered. Includes both existing and re-used bytes.\n\n\t\n\nBIGINT\n\n\n\n\nrecovery['size']['reused']\n\n\t\n\nNumber of bytes re-used from a local copy while recovering the shard.\n\n\t\n\nBIGINT\n\n\n\n\nrecovery['size']['used']\n\n\t\n\nTotal number of bytes in the shard.\n\n\t\n\nBIGINT\n\n\n\n\nrecovery['stage']\n\n\t\n\nRecovery stage:\n\ninit: Recovery has not started\n\nindex: Reading the Lucene index meta-data and copying bytes from source to destination\n\nstart: Starting the engine, opening the index for use\n\ntranslog: Replaying transaction log\n\nfinalize: Cleanup\n\ndone: Complete\n\n\t\n\nTEXT\n\n\n\n\nrecovery['total_time']\n\n\t\n\nReturns elapsed time from the start of the shard recovery.\n\n\t\n\nBIGINT\n\n\n\n\nrecovery['type']\n\n\t\n\nRecovery type:\n\ngateway\n\nsnapshot\n\nreplica\n\nrelocating\n\n\t\n\nTEXT\n\n\n\n\nrelocating_node\n\n\t\n\nThe id of the node to which the shard is getting relocated to.\n\n\t\n\nTEXT\n\n\n\n\nrouting_state\n\n\t\n\nThe current state of the shard in the routing table. Possible states are:\n\nUNASSIGNED\n\nINITIALIZING\n\nSTARTED\n\nRELOCATING\n\n\t\n\nTEXT\n\n\n\n\nschema_name\n\n\t\n\nThe schema name of the table the shard belongs to\n\n\t\n\nTEXT\n\n\n\n\nsize\n\n\t\n\nThe current size in bytes. This value is cached for a short period and may return slightly outdated values.\n\n\t\n\nBIGINT\n\n\n\n\nstate\n\n\t\n\nThe current state of the shard. Possible states are:\n\nCREATED\n\nRECOVERING\n\nPOST_RECOVERY\n\nSTARTED\n\nRELOCATED\n\nCLOSED\n\nINITIALIZING\n\nUNASSIGNED\n\n\t\n\nTEXT\n\n\n\n\nclosed\n\n\t\n\nThe state of the table associated with the shard.\n\n\t\n\nBOOLEAN\n\n\n\n\ntable_name\n\n\t\n\nThe name of the table this shard belongs to\n\n\t\n\nTEXT\n\n\n\n\nseq_no_stats\n\n\t\n\nContains information about internal sequence numbering and checkpoints for these sequence numbers.\n\n\t\n\nOBJECT\n\n\n\n\nseq_no_stats['max_seq_no']\n\n\t\n\nThe highest sequence number that has been issued so far on the shard.\n\n\t\n\nBIGINT\n\n\n\n\nseq_no_stats['local_checkpoint']\n\n\t\n\nThe highest sequence number for which all lower sequence number of been processed on this shard. Due to concurrent indexing this can be lower than max_seq_no.\n\n\t\n\nBIGINT\n\n\n\n\nseq_no_stats['global_checkpoint']\n\n\t\n\nThe highest sequence number for which the local shard can guarantee that all lower sequence numbers have been processed on all active shard copies.\n\n\t\n\nBIGINT\n\n\n\n\ntranslog_stats\n\n\t\n\nContains information for the translog of the shard.\n\n\t\n\nOBJECT\n\n\n\n\ntranslog_stats['size']\n\n\t\n\nThe current size of the translog file in bytes.\n\n\t\n\nBIGINT\n\n\n\n\ntranslog_stats['uncommitted_size']\n\n\t\n\nThe size in bytes of the translog that has not been committed to Lucene yet.\n\n\t\n\nBIGINT\n\n\n\n\ntranslog_stats['number_of_operations']\n\n\t\n\nThe number of operations recorded in the translog.\n\n\t\n\nINTEGER\n\n\n\n\ntranslog_stats['uncommitted_operations']\n\n\t\n\nThe number of operations in the translog which have not been committed to Lucene yet.\n\n\t\n\nINTEGER\n\n\n\n\nretention_leases\n\n\t\n\nVersioned collection of retention leases.\n\n\t\n\nOBJECT\n\n\n\n\nflush_stats\n\n\t\n\nFlush information. Shard relocation resets this information.\n\n\t\n\nOBJECT\n\n\n\n\nflush_stats['count']\n\n\t\n\nThe total amount of flush operations that happened on the shard.\n\n\t\n\nBIGINT\n\n\n\n\nflush_stats['periodic_count']\n\n\t\n\nThe number of periodic flushes. Each periodic flush also counts as a regular flush. A periodic flush can happen after writes depending on settings like the translog flush threshold.\n\n\t\n\nBIGINT\n\n\n\n\nflush_stats['total_time_ns']\n\n\t\n\nThe total time spent on flush operations on the shard.\n\n\t\n\nBIGINT\n\nNote\n\nThe sys.shards table is subject to Shard table permissions.\n\nExample\n\nFor example, you can query shards like this:\n\ncr> select schema_name as schema,\n...   table_name as t,\n...   id,\n...   partition_ident as p_i,\n...   num_docs as docs,\n...   primary,\n...   relocating_node as r_n,\n...   routing_state as r_state,\n...   state,\n...   orphan_partition as o_p\n... from sys.shards where table_name = 'locations' and id = 1;\n+--------+-----------+----+-----+------+---------+------+---------+---------+-------+\n| schema | t         | id | p_i | docs | primary | r_n  | r_state |  state  | o_p   |\n+--------+-----------+----+-----+------+---------+------+---------+---------+-------+\n| doc    | locations |  1 |     |    8 | TRUE    | NULL | STARTED | STARTED | FALSE |\n+--------+-----------+----+-----+------+---------+------+---------+---------+-------+\nSELECT 1 row in set (... sec)\n\nSegments\n\nThe sys.segments table contains information about the Lucene segments of the shards.\n\nThe segment information is useful to understand the behaviour of the underlying Lucene file structures for troubleshooting and performance optimization of shards.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nsegment_name\n\n\t\n\nName of the segment, derived from the segment generation and used internally to create file names in the directory of the shard.\n\n\t\n\nTEXT\n\n\n\n\nshard_id\n\n\t\n\nID of the effected shard.\n\n\t\n\nINTEGER\n\n\n\n\ntable_schema\n\n\t\n\nSchema name of the table of the shard.\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nTable name of the shard.\n\n\t\n\nTEXT\n\n\n\n\npartition_ident\n\n\t\n\nThe partition ident of a partitioned table. Empty for non-partitioned tables.\n\n\t\n\nTEXT\n\n\n\n\nnode\n\n\t\n\nInformation about the node the shard is located at.\n\n\t\n\nOBJECT\n\n\n\n\nnode['name']\n\n\t\n\nThe name of the node the shard is located at.\n\n\t\n\nTEXT\n\n\n\n\nnode['id']\n\n\t\n\nThe id of the node the shard is located at.\n\n\t\n\nTEXT\n\n\n\n\ngeneration\n\n\t\n\nGeneration number of the segment, increments for each segment written.\n\n\t\n\nLONG\n\n\n\n\nnum_docs\n\n\t\n\nNumber of non-deleted Lucene documents in this segment.\n\n\t\n\nINTEGER\n\n\n\n\ndeleted_docs\n\n\t\n\nNumber of deleted Lucene documents in this segment.\n\n\t\n\nINTEGER\n\n\n\n\nsize\n\n\t\n\nDisk space used by the segment in bytes.\n\n\t\n\nLONG\n\n\n\n\nmemory\n\n\t\n\nUnavailable starting from CrateDB 5.0. Always returns -1.\n\n\t\n\nLONG\n\n\n\n\ncommitted\n\n\t\n\nIndicates if the segments are synced to disk. Segments that are synced can survive a hard reboot.\n\n\t\n\nBOOLEAN\n\n\n\n\nprimary\n\n\t\n\nDescribes if this segment is part of a primary shard.\n\n\t\n\nBOOLEAN\n\n\n\n\nsearch\n\n\t\n\nIndicates if the segment is searchable. If false, the segment has most likely been written to disk but needs a refresh to be searchable.\n\n\t\n\nBOOLEAN\n\n\n\n\nversion\n\n\t\n\nVersion of Lucene used to write the segment.\n\n\t\n\nTEXT\n\n\n\n\ncompound\n\n\t\n\nIf true, Lucene merges all files from the segment into a single file to save file descriptors.\n\n\t\n\nBOOLEAN\n\n\n\n\nattributes\n\n\t\n\nContains information about whether high compression was enabled.\n\n\t\n\nOBJECT\n\nNote\n\nThe information in the sys.segments table is expensive to calculate and therefore this information should be retrieved with awareness that it can have performance implications on the cluster.\n\nNote\n\nThe sys.shards table is subject to Shard table permissions.\n\nJobs, operations, and logs\n\nTo let you inspect the activities currently taking place in a cluster, CrateDB provides system tables that let you track current cluster jobs and operations. See Jobs Table and Operations Table.\n\nJobs and operations that finished executing are additionally recorded in memory. There are two retention policies available to control how many records should be kept.\n\nOne option is to configure the maximum number of records which should be kept. Once the configured table size is reached, the older log records are deleted as newer records are added. This is configurable using stats.jobs_log_size and stats.operations_log_size.\n\nAnother option is to configure an expiration time for the records. In this case, the records in the logs tables are periodically cleared if they are older than the expiry time. This behaviour is configurable using stats.jobs_log_expiration and stats.operations_log_expiration.\n\nIn addition to these retention policies, there is a memory limit in place preventing these tables from taking up too much memory. The amount of memory that can be used to store the jobs can be configured using stats.breaker.log.jobs.limit and stats.breaker.log.operations.limit. If the memory limit is reached, an error message will be logged and the log table will be cleared completely.\n\nIt is also possible to define a filter which must match for jobs to be recorded after they finished executing. This can be useful to only record slow queries or queries that failed due to an error. This filter can be configured using the stats.jobs_log_filer setting.\n\nFurthermore, there is a second filter setting which also results in a log entry in the regular CrateDB log file for all finished jobs that match this filter. This can be configured using stats.jobs_log_persistent_filter. This could be used to create a persistent slow query log.\n\nJobs\n\nThe sys.jobs table is a constantly updated view of all jobs that are currently being executed in the cluster.\n\nTable schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe job UUID.\n\nThis job ID is generated by the system.\n\n\t\n\nTEXT\n\n\n\n\nnode\n\n\t\n\nInformation about the node that created the job.\n\n\t\n\nOBJECT\n\n\n\n\nnode['id']\n\n\t\n\nThe id of the node.\n\n\t\n\nTEXT\n\n\n\n\nnode['name']\n\n\t\n\nThe name of the node.\n\n\t\n\nTEXT\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the job started.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nstmt\n\n\t\n\nShows the data query or manipulation statement represented by this job.\n\n\t\n\nTEXT\n\n\n\n\nusername\n\n\t\n\nThe user who is executing the statement.\n\n\t\n\nTEXT\n\nThe field username corresponds to the SESSION_USER that is performing the query:\n\ncr> select stmt, username, started from sys.jobs where stmt like 'sel% from %jobs%';\n+---------------------------------------------------------------------------------+----------+-...-----+\n| stmt                                                                            | username | started |\n+---------------------------------------------------------------------------------+----------+-...-----+\n| select stmt, username, started from sys.jobs where stmt like 'sel% from %jobs%' | crate    | ...     |\n+---------------------------------------------------------------------------------+----------+-...-----+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nIf the user management module is not available, the username is given as crate.\n\nEvery request that queries data or manipulates data is considered a “job” if it is a valid query. Requests that are not valid queries (for example, a request that tries to query a non-existent table) will not show up as jobs.\n\nNote\n\nThe sys.jobs table is subject to sys jobs tables permissions.\n\nJobs metrics\n\nThe sys.jobs_metrics table provides an overview of the query latency in the cluster. Jobs metrics are not persisted across node restarts.\n\nThe metrics are aggregated for each node and each unique classification of the statements.\n\nNote\n\nIn order to reduce the memory requirements for these metrics, the times are statistically sampled and therefore may have slight inaccuracies. In addition, durations are only tracked up to 10 minutes. Statements taking longer than that are capped to 10 minutes.\n\nsys.jobs_metrics Table schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nnode\n\n\t\n\nAn object containing the id and name of the node on which the metrics have been sampled.\n\n\t\n\nOBJECT\n\n\n\n\nclassification\n\n\t\n\nAn object containing the statement classification.\n\n\t\n\nOBJECT\n\n\n\n\nclassification['type']\n\n\t\n\nThe general type of the statement. Types are: INSERT, SELECT, UPDATE, DELETE, COPY, DDL, and MANAGEMENT.\n\n\t\n\nTEXT\n\n\n\n\nclassification['labels']\n\n\t\n\nLabels are only available for certain statement types that can be classified more accurately than just by their type.\n\n\t\n\nTEXT_ARRAY\n\n\n\n\ntotal_count\n\n\t\n\nTotal number of queries executed\n\n\t\n\nBIGINT\n\n\n\n\nfailed_count\n\n\t\n\nTotal number of queries that failed to complete successfully.\n\n\t\n\nBIGINT\n\n\n\n\nsum_of_durations\n\n\t\n\nSum of durations in ms of all executed queries per statement type.\n\n\t\n\nBIGINT\n\n\n\n\nstdev\n\n\t\n\nThe standard deviation of the query latencies\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\nmean\n\n\t\n\nThe mean query latency in ms\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\nmax\n\n\t\n\nThe maximum query latency in ms\n\n\t\n\nBIGINT\n\n\n\n\nmin\n\n\t\n\nThe minimum query latency in ms\n\n\t\n\nBIGINT\n\n\n\n\npercentiles\n\n\t\n\nAn object containing different percentiles\n\n\t\n\nOBJECT\n\nClassification\n\nCertain statement types (such as SELECT statements) have additional labels in their classification. These labels are the names of the logical plan operators that are involved in the query.\n\nFor example, the following UNION statement:\n\nSELECT name FROM t1 where id = 1\nUNION ALL\nSELECT name FROM t2 where id < 2\n\n\nwould result in the following labels:\n\nUnion` for the UNION ALL\n\nGet for the left SELECT\n\nCollect for the right SELECT\n\nNote\n\nLabels may be subject to change as they only represent internal properties of the statement!\n\nOperations\n\nThe sys.operations table is a constantly updated view of all operations that are currently being executed in the cluster:\n\ncr> select node['name'], job_id, name, used_bytes from sys.operations\n... order by name limit 1;\n+--------------+--------...-+-----...-+------------+\n| node['name'] | job_id     | name    | used_bytes |\n+--------------+--------...-+-----...-+------------+\n| crate        | ...        | ...     | ...        |\n+--------------+--------...-+-----...-+------------+\nSELECT 1 row in set (... sec)\n\n\nAn operation is a node-specific sub-component of a job (for when a job involves multi-node processing). Jobs that do not require multi-node processing will not produce any operations.\n\nTable schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe operation UUID.\n\nThis operation ID is generated by the system.\n\n\t\n\nTEXT\n\n\n\n\njob_id\n\n\t\n\nThe job id this operation belongs to.\n\n\t\n\nTEXT\n\n\n\n\nname\n\n\t\n\nThe name of the operation.\n\n\t\n\nTEXT\n\n\n\n\nnode\n\n\t\n\nInformation about the node that created the operation.\n\n\t\n\nOBJECT\n\n\n\n\nnode['id']\n\n\t\n\nThe id of the node.\n\n\t\n\nTEXT\n\n\n\n\nnode['name']\n\n\t\n\nThe name of the node.\n\n\t\n\nTEXT\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the operation started.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nused_bytes\n\n\t\n\nCurrently loaded amount of data by the operation.\n\n\t\n\nBIGINT\n\nNote\n\nIn some cases, operations are generated for internal CrateDB work that does not directly correspond to a user request. These entries do not have corresponding entries in sys.jobs.\n\nLogs\n\nThe sys.jobs and sys.operations tables have corresponding log tables: sys.jobs_log and sys.operations_log.\n\nsys.jobs_log Table schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe job ID.\n\n\t\n\nTEXT\n\n\n\n\nended\n\n\t\n\nThe point in time when the job finished.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nerror\n\n\t\n\nIf the job encountered an error, this will hold the error message.\n\n\t\n\nTEXT\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the job started.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nstmt\n\n\t\n\nShows the data query or manipulation statement executed by the job.\n\n\t\n\nTEXT\n\n\n\n\nusername\n\n\t\n\nThe user who executed the statement.\n\n\t\n\nTEXT\n\n\n\n\nclassification\n\n\t\n\nAn object containing the statement classification.\n\n\t\n\nOBJECT\n\n\n\n\nclassification['type']\n\n\t\n\nThe general type of the statement. Types are: INSERT, SELECT, UPDATE, DELETE,``COPY``, DDL, and MANAGEMENT.\n\n\t\n\nTEXT\n\n\n\n\nclassification['labels']\n\n\t\n\nLabels are only available for certain statement types that can be classified more accurately than just by their type.\n\n\t\n\nTEXT_ARRAY\n\n\n\n\nnode\n\n\t\n\nInformation about the node that created the job.\n\n\t\n\nOBJECT\n\n\n\n\nnode['id']\n\n\t\n\nThe id of the node.\n\n\t\n\nTEXT\n\n\n\n\nnode['name']\n\n\t\n\nThe name of the node.\n\n\t\n\nTEXT\n\nNote\n\nYou can control which jobs are recorded using the stats.jobs_log_filter\n\nNote\n\nThe sys.jobs_log table is subject to sys jobs tables permissions.\n\nsys.operations_log Table schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe operation ID.\n\n\t\n\nTEXT\n\n\n\n\njob_id\n\n\t\n\nThe job id.\n\n\t\n\nTEXT\n\n\n\n\nended\n\n\t\n\nThe point in time when the operation finished.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nerror\n\n\t\n\nIf the operation encountered an error, this will hold the error message.\n\n\t\n\nTEXT\n\n\n\n\nname\n\n\t\n\nThe name of the operation.\n\n\t\n\nTEXT\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the operation started.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nused_bytes\n\n\t\n\nThe amount of data loaded by the operation.\n\n\t\n\nBIGINT\n\nAfter a job or operation finishes, the corresponding entry will be moved into the corresponding log table:\n\ncr> select id, stmt, username, started, ended, error\n... from sys.jobs_log order by ended desc limit 2;\n+-...+----------------------------------------------...-+----------+-...-----+-...---+-------+\n| id | stmt                                             | username | started | ended | error |\n+-...+----------------------------------------------...-+----------+-...-----+-...---+-------+\n| ...| select node['name'], ...                         | crate    | ...     | ...   |  NULL |\n| ...| select stmt, username, started from sys.jobs ... | crate    | ...     | ...   |  NULL |\n+-...+----------------------------------------------...-+----------+-...-----+-...---+-------+\nSELECT 2 rows in set (... sec)\n\n\nInvalid queries are also logged in the sys.jobs_log table, i.e. queries that never make it to the sys.jobs table because they could not be executed.\n\nThe log tables are bound by a fixed size (stats.jobs_log_size) or by an expiration time (stats.jobs_log_expiration)\n\nSee Collecting stats for information on how to configure logs.\n\nCaution\n\nIf you deactivate statistics tracking, the logs tables will be truncated.\n\nCluster checks\n\nThe table sys.checks exposes a list of internal cluster checks and results of their validation.\n\nThe sys.checks table looks like this:\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe unique check id.\n\n\t\n\nINTEGER\n\n\n\n\nseverity\n\n\t\n\nThe level of severity. The higher the value of the field the higher severity.\n\n\t\n\nINTEGER\n\n\n\n\ndescription\n\n\t\n\nThe description message for the setting check.\n\n\t\n\nTEXT\n\n\n\n\npassed\n\n\t\n\nThe flag determines whether the check for the setting has passed.\n\n\t\n\nBOOLEAN\n\nHere’s an example query:\n\ncr> select id, description from sys.checks order by id;\n+----+--------------------------------------------------------------...-+\n| id | description                                                      |\n+----+--------------------------------------------------------------...-+\n|  2 | The total number of partitions of one or more partitioned tab... |\n|  3 | The following tables need to be recreated for compatibility w... |\n+----+--------------------------------------------------------------...-+\nSELECT 2 rows in set (... sec)\n\n\nCluster checks are also indicated in the CrateDB admin console. When all cluster checks (and all Node checks) pass, the Checks icon will be green. Here’s what it looks like when some checks are failing at the CRITICAL severity level:\n\nCurrent Checks\nNumber of partitions\n\nThis check warns if any partitioned table has more than 1000 partitions to detect the usage of a high cardinality field for partitioning.\n\nTables need to be recreated\n\nWarning\n\nDo not attempt to upgrade your cluster to a newer major version if this cluster check is failing. Follow the instructions below to get this cluster check passing.\n\nThis check warns you if your cluster contains tables that you need to reindex before you can upgrade to a future major version of CrateDB.\n\nIf you try to upgrade to a later major CrateDB version without reindexing the tables, CrateDB will refuse to start.\n\nCrateDB table version compatibility scheme\n\nCrateDB maintains backward compatibility for tables created in majorVersion - 1:\n\nTable Origin\n\n\t\n\nCurrent Version\n\n\t\n\nCurrent Version\n\n\t\n\nCurrent Version\n\n\n\t\n\n3.x\n\n\t\n\n4.x\n\n\t\n\n5.x\n\n\n\n\n3.x\n\n\t\n\n✔️\n\n\t\n\n✔️\n\n\t\n\n❌\n\n\n\n\n4.x\n\n\t\n\n❌\n\n\t\n\n✔️\n\n\t\n\n✔️\n\n\n\n\n5.x\n\n\t\n\n❌\n\n\t\n\n❌\n\n\t\n\n✔️\n\nAvoiding reindex using partitioned tables\n\nReindexing tables is an expensive operation which can take a long time. If you are storing time series data for a certain retention period and intend to delete old data, it is possible to use the partitioned tables to avoid reindex operations.\n\nYou will have to use a partition column that denotes time. For example, if you have a retention period of nine months, you could partition a table by a month column. Then, every month, the system will create a new partition. This new partition is created using the active CrateDB version and is compatible with the next major CrateDB version. Now to achieve your goal of avoiding a reindex, you must manually delete any partition older than nine months. If you do that, then after nine months you rolled through all partitions and the remaining nine are compatible with the next major CrateDB version.\n\nHow to reindex\n\nUse SHOW CREATE TABLE to get the schema required to create an empty copy of the table to recreate:\n\ncr> SHOW CREATE TABLE rx.metrics;\n+-----------------------------------------------------+\n| SHOW CREATE TABLE rx.metrics                        |\n+-----------------------------------------------------+\n| CREATE TABLE IF NOT EXISTS \"rx\".\"metrics\" (         |\n|    \"id\" TEXT NOT NULL,                                       |\n|    \"temperature\" REAL,                              |\n|    PRIMARY KEY (\"id\")                               |\n| )                                                   |\n| CLUSTERED BY (\"id\") INTO 4 SHARDS                   |\n| WITH (                                              |\n|    \"allocation.max_retries\" = 5,                    |\n|    \"blocks.metadata\" = false,                       |\n|    \"blocks.read\" = false,                           |\n|    \"blocks.read_only\" = false,                      |\n|    \"blocks.read_only_allow_delete\" = false,         |\n|    \"blocks.write\" = false,                          |\n|    codec = 'default',                               |\n|    column_policy = 'strict',                        |\n|    \"mapping.total_fields.limit\" = 1000,             |\n|    max_ngram_diff = 1,                              |\n|    max_shingle_diff = 3,                            |\n|    number_of_replicas = '0-1',                      |\n|    \"routing.allocation.enable\" = 'all',             |\n|    \"routing.allocation.total_shards_per_node\" = -1, |\n|    \"store.type\" = 'fs',                             |\n|    \"translog.durability\" = 'REQUEST',               |\n|    \"translog.flush_threshold_size\" = 536870912,     |\n|    \"translog.sync_interval\" = 5000,                 |\n|    \"unassigned.node_left.delayed_timeout\" = 60000,  |\n|    \"write.wait_for_active_shards\" = '1'             |\n| )                                                   |\n+-----------------------------------------------------+\nSHOW 1 row in set (... sec)\n\n\nCreate a new temporary table, using the schema returned from SHOW CREATE TABLE:\n\ncr> CREATE TABLE rx.tmp_metrics (id TEXT PRIMARY KEY, temperature REAL);\nCREATE OK, 1 row affected (... sec)\n\n\nCopy the data:\n\ncr> INSERT INTO rx.tmp_metrics (id, temperature) (SELECT id, temperature FROM rx.metrics);\nINSERT OK, 2 rows affected (... sec)\n\n\nSwap the tables:\n\ncr> ALTER CLUSTER SWAP TABLE rx.tmp_metrics TO rx.metrics;\nALTER OK, 1 row affected  (... sec)\n\n\nConfirm the new your_table contains all data and has the new version:\n\ncr> SELECT count(*) FROM rx.metrics;\n+----------+\n| count(*) |\n+----------+\n|        2 |\n+----------+\nSELECT 1 row in set (... sec)\n\ncr> SELECT version['created'] FROM information_schema.tables\n... WHERE table_schema = 'rx' AND table_name = 'metrics';\n+--------------------+\n| version['created'] |\n+--------------------+\n| 5.7.0              |\n+--------------------+\nSELECT 1 row in set (... sec)\n\n\nDrop the old table, as it is now obsolete:\n\ncr> DROP TABLE rx.tmp_metrics;\nDROP OK, 1 row affected  (... sec)\n\n\nAfter you reindexed all tables, this cluster check will pass.\n\nNote\n\nSnapshots of your tables created prior to them being upgraded will not work with future versions of CrateDB. For this reason, you should create a new snapshot for each of your tables. (See Snapshots.)\n\nLicense check\n\nNote\n\nThis check was removed in version 4.5 because CrateDB no longer requires an enterprise license, see also Farewell to the CrateDB Enterprise License.\n\nThis check warns you when your license is close to expiration, is already expired, or if the cluster contains more nodes than allowed by your license. It will yield a MEDIUM alert when your license is valid for less than 15 days and a HIGH alert when your license is valid for less than a day. All other cases, like already expired or max-nodes-violation, it will result in a HIGH alert. We recommend that you request a new license when this check triggers, in order to avoid the situation where operations are rejected due to an invalid license.\n\nHealth\n\nThe sys.health table lists the health of each table and table partition. The health is computed by checking the states of the shard of each table/partition.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\ntable_name\n\n\t\n\nThe table name.\n\n\t\n\nTEXT\n\n\n\n\ntable_schema\n\n\t\n\nThe schema of the table.\n\n\t\n\nTEXT\n\n\n\n\npartition_ident\n\n\t\n\nThe ident of the partition. NULL for non-partitioned tables.\n\n\t\n\nTEXT\n\n\n\n\nhealth\n\n\t\n\nThe health label. Can be RED, YELLOW or GREEN.\n\n\t\n\nTEXT\n\n\n\n\nseverity\n\n\t\n\nThe health as a smallint value. Useful when ordering on health.\n\n\t\n\nSMALLINT\n\n\n\n\nmissing_shards\n\n\t\n\nThe number of not assigned or started shards.\n\n\t\n\nINTEGER\n\n\n\n\nunderreplicated_shards\n\n\t\n\nThe number of shards which are not fully replicated.\n\n\t\n\nINTEGER\n\nBoth missing_shards and underreplicated_shards might return -1 if the cluster is in an unhealthy state that prevents the exact number from being calculated. This could be the case when the cluster can’t elect a master, because there are not enough eligible nodes available.\n\ncr> select * from sys.health order by severity desc, table_name;\n+--------+----------------+-----------------+----------+------------+--------------+------------------------+\n| health | missing_shards | partition_ident | severity | table_name | table_schema | underreplicated_shards |\n+--------+----------------+-----------------+----------+------------+--------------+------------------------+\n| GREEN  |              0 |            NULL |        1 | locations  | doc          |                      0 |\n| GREEN  |              0 |            NULL |        1 | quotes     | doc          |                      0 |\n+--------+----------------+-----------------+----------+------------+--------------+------------------------+\nSELECT 2 rows in set (... sec)\n\n\nThe health with the highest severity will always define the health of the query scope.\n\nExample of getting a cluster health (health of all tables):\n\ncr> select health from sys.health order by severity desc limit 1;\n+--------+\n| health |\n+--------+\n| GREEN  |\n+--------+\nSELECT 1 row in set (... sec)\n\nHealth definition\n\nHealth\n\n\t\n\nDescription\n\n\n\n\nRED\n\n\t\n\nAt least one primary shard is missing (primary shard not started or unassigned).\n\n\n\n\nYELLOW\n\n\t\n\nAt least one shard is underreplicated (replica shard not started or unassigned).\n\n\n\n\nGREEN\n\n\t\n\nAll primary and replica shards have been started.\n\nNote\n\nThe sys.health table is subject to Shard table permissions as it will expose a summary of table shard states.\n\nRepositories\n\nThe table sys.repositories lists all configured repositories that can be used to create, manage and restore snapshots (see Snapshots).\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nname\n\n\t\n\nThe repository name\n\n\t\n\nTEXT\n\n\n\n\ntype\n\n\t\n\nThe type of the repository determining how and where the repository stores its snapshots.\n\n\t\n\nTEXT\n\n\n\n\nsettings\n\n\t\n\nThe configuration settings the repository has been created with. The specific settings depend on the repository type, see CREATE REPOSITORY.\n\n\t\n\nOBJECT\n\ncr> SELECT name, type, settings FROM sys.repositories\n... ORDER BY name;\n+---------+------+---------------------------------------------------...--+\n| name    | type | settings                                               |\n+---------+------+---------------------------------------------------...--+\n| my_repo | fs   | {\"compress\": \"true\", \"location\": \"repo_location\", ...} |\n+---------+------+---------------------------------------------------...--+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nSensitive user account information will be masked and thus not visible to the user.\n\nSnapshots\n\nThe table sys.snapshots lists all existing snapshots in all configured repositories (see Snapshots).\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nname\n\n\t\n\nThe name of the snapshot\n\n\t\n\nTEXT\n\n\n\n\nrepository\n\n\t\n\nThe name of the repository that contains this snapshot.\n\n\t\n\nTEXT\n\n\n\n\nconcrete_indices\n\n\t\n\nContains the names of all tables and partitions that are contained in this snapshot how they are represented as ES index names.\n\n\t\n\nARRAY(TEXT)\n\n\n\n\ntables\n\n\t\n\nContains the fully qualified names of all tables within the snapshot.\n\n\t\n\nARRAY(TEXT)\n\n\n\n\ntable_partitions\n\n\t\n\nContains the table schema, table name and partition values of partitioned tables within the snapshot.\n\n\t\n\nARRAY(OBJECT)\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the creation of the snapshot started. Changes made after that are not stored in this snapshot.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nfinished\n\n\t\n\nThe point in time when the snapshot creation finished.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nstate\n\n\t\n\nThe current state of the snapshot. One of: IN_PROGRESS, SUCCESS, PARTIAL, or FAILED.\n\n\t\n\nTEXT\n\n\n\n\nversion\n\n\t\n\nAn internal version this snapshot was created with.\n\n\t\n\nTEXT\n\n\n\n\nfailures\n\n\t\n\nA list of failures that occurred while taking the snapshot. If taking the snapshot was successful this is empty.\n\n\t\n\nARRAY(TEXT)\n\nSnapshot/Restore operates on a per-shard basis. Hence, the state column indicates whether all (SUCCESS), some (PARTIAL), or no shards(FAILED) have been backed up. PARTIAL snapshots are the result of some primaries becoming unavailable while taking the snapshot when there are no replicas at hand (cluster state is RED). If there are replicas of the (now unavailable) primaries (cluster state is YELLOW) the snapshot succeeds and all shards are included (state SUCCESS). Building on a PARTIAL snapshot will include all primaries again.\n\nWarning\n\nIn case of a PARTIAL state another snapshot should be created in order to guarantee a full backup! Only SUCCESS includes all shards.\n\nThe concrete_indices column contains the names of all Elasticsearch indices that were stored in the snapshot. A normal CrateDB table maps to one Elasticsearch index, a partitioned table maps to one Elasticsearch index per partition. The mapping follows the following pattern:\n\nCrateDB table / partition name\n\n\t\n\nconcrete_indices entry\n\n\n\n\ndoc.my_table\n\n\t\n\nmy_table\n\n\n\n\nmy_schema.my_table\n\n\t\n\nmy_schema.my_table\n\n\n\n\ndoc.parted_table (value=null)\n\n\t\n\n.partitioned.my_table.0400\n\n\n\n\nmy_schema.parted_table (value=null)\n\n\t\n\nmy_schema..partitioned.my_table.0400\n\ncr> SELECT \"repository\", name, state, concrete_indices\n... FROM sys.snapshots order by \"repository\", name;\n+------------+-------------+---------+-----------------...-+\n| repository | name        | state   | concrete_indices    |\n+------------+-------------+---------+-----------------...-+\n| my_repo    | my_snapshot | SUCCESS | [...]               |\n+------------+-------------+---------+-----------------...-+\nSELECT 1 row in set (... sec)\n\nSnapshot Restore\n\nThe sys.snapshot_restore table contains information about the current state of snapshot restore operations.\n\npg_stats schema\n\nName\n\n\t\n\nDescription\n\n\t\n\nType\n\n\n\n\nid\n\n\t\n\nThe UUID of the restore snapshot operation.\n\n\t\n\nTEXT\n\n\n\n\nrepository\n\n\t\n\nThe name of the repository that contains the snapshot.\n\n\t\n\nTEXT\n\n\n\n\nsnapshot\n\n\t\n\nThe name of the snapshot.\n\n\t\n\nTEXT\n\n\n\n\nstate\n\n\t\n\nThe current state of the snapshot restore operations. Possible states are: INIT, STARTED, SUCCESS, and FAILURE.\n\n\t\n\nTEXT\n\n\n\n\nshards['table_schema']\n\n\t\n\nThe schema name of the table of the shard.\n\n\t\n\nTEXT\n\n\n\n\nshards['table_name']\n\n\t\n\nThe table name of the shard.\n\n\t\n\nTEXT\n\n\n\n\nshards['partition_ident']\n\n\t\n\nThe identifier of the partition of the shard. NULL if the is not partitioned.\n\n\t\n\nTEXT\n\n\n\n\nshards['shard_id']\n\n\t\n\nThe ID of the shard.\n\n\t\n\nINTEGER\n\n\n\n\nshards['state']\n\n\t\n\nThe restore state of the shard. Possible states are: INIT, STARTED, SUCCESS, and FAILURE.\n\n\t\n\nTEXT\n\nTo get more information about the restoring snapshots and shards one can join the sys.snapshot_restore with sys.shards or sys.snapshots table.\n\nSummits\n\nThe sys.summits table contains the information about the mountains in the Alps higher than 2000m. The mountain names from the table are also used to generate random nodes names.\n\nUsers\n\nThe sys.users table contains all existing database users in the cluster.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nname\n\n\t\n\nThe name of the database user.\n\n\t\n\nTEXT\n\n\n\n\nsuperuser\n\n\t\n\nFlag to indicate whether the user is a superuser.\n\n\t\n\nBOOLEAN\n\n\n\n\npassword\n\n\t\n\n******** if there is a password set or NULL if there is not.\n\n\t\n\nTEXT\n\n\n\n\ngranted_roles\n\n\t\n\nA list of parent roles granted to the user\n\n\t\n\nARRAY\n\n\n\n\ngranted_roles[role]\n\n\t\n\nThe name of the role granted to the user\n\n\t\n\nTEXT\n\n\n\n\ngranted_roles[grantor]\n\n\t\n\nThe name of user who granted the role to the user\n\n\t\n\nTEXT\n\n\n\n\njwt\n\n\t\n\nJWT authentication properties\n\n\t\n\nOBJECT\n\n\n\n\njwt[aud]\n\n\t\n\nRecipient that the JWT is intended for\n\n\t\n\nTEXT\n\n\n\n\njwt[iss]\n\n\t\n\nJWK endpoint URL\n\n\t\n\nTEXT\n\n\n\n\njwt[username]\n\n\t\n\nUser name in a third party app\n\n\t\n\nTEXT\n\nRoles\n\nThe sys.roles table contains all existing database roles in the cluster.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nname\n\n\t\n\nThe name of the database user.\n\n\t\n\nTEXT\n\n\n\n\ngranted_roles\n\n\t\n\nA list of parent roles granted to the user\n\n\t\n\nARRAY\n\n\n\n\ngranted_roles[role]\n\n\t\n\nThe name of the role granted to the user\n\n\t\n\nTEXT\n\n\n\n\ngranted_roles[grantor]\n\n\t\n\nThe name of user who granted the role to the user\n\n\t\n\nTEXT\n\nPrivileges\n\nThe sys.privileges table contains all privileges for each user and role of the database.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nclass\n\n\t\n\nThe class on which the privilege applies\n\n\t\n\nTEXT\n\n\n\n\ngrantee\n\n\t\n\nThe name of the database user or role for which the privilege is granted or denied\n\n\t\n\nTEXT\n\n\n\n\ngrantor\n\n\t\n\nThe name of the database user who granted or denied the privilege\n\n\t\n\nTEXT\n\n\n\n\nident\n\n\t\n\nThe name of the database object on which the privilege applies\n\n\t\n\nTEXT\n\n\n\n\nstate\n\n\t\n\nEither GRANT or DENY, which indicates if the user or role has been granted or denied access to the specific database object\n\n\t\n\nARRAY\n\n\n\n\ntype\n\n\t\n\nThe type of access for the specific database object\n\n\t\n\nTEXT\n\nAllocations\n\nThe sys.allocations table contains information about shards and their allocation state. The table contains:\n\nshards that are unassigned and why they are unassigned\n\nshards that are assigned but cannot be moved or rebalanced and why they remain on their current node\n\nIt can help to identify problems if shard allocations behave different than expected, e.g. when a shard stays unassigned or a shard does not move off a node.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\ntable_schema\n\n\t\n\nSchema name of the table of the shard.\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nTable name of the shard.\n\n\t\n\nTEXT\n\n\n\n\npartition_ident\n\n\t\n\nIdentifier of the partition of the shard. NULL if the table is not partitioned.\n\n\t\n\nTEXT\n\n\n\n\nshard_id\n\n\t\n\nID of the effected shard.\n\n\t\n\nINTEGER\n\n\n\n\nnode_id\n\n\t\n\nID of the node on which the shard resides. NULL if the shard is unassigned.\n\n\t\n\nTEXT\n\n\n\n\nprimary\n\n\t\n\nWhether the shard is a primary shard.\n\n\t\n\nBOOLEAN\n\n\n\n\ncurrent_state\n\n\t\n\nCurrent state of the shard. Possible states are: UNASSIGNED, INITIALIZING, STARTED, RELOCATING\n\n\t\n\nTEXT\n\n\n\n\nexplanation\n\n\t\n\nExplanation why the shard cannot be allocated, moved or rebalanced.\n\n\t\n\nTEXT\n\n\n\n\ndecisions\n\n\t\n\nA list of decisions that describe in detail why the shard in the current state.\n\n\t\n\nARRAY\n\n\n\n\ndecisions['node_id']\n\n\t\n\nID of the node of the decision.\n\n\t\n\nTEXT\n\n\n\n\ndecisions['node_name']\n\n\t\n\nName of the node of the decision.\n\n\t\n\nTEXT\n\n\n\n\ndecisions['explanations']\n\n\t\n\nDetailed list of human readable explanations why the node decided whether to allocate or rebalance the shard. Returns NULL if there is no need to rebalance the shard.\n\n\t\n\nARRAY\n\nNote\n\nThe sys.allocations table is subject to Shard table permissions.\n\nShard table permissions\n\nAccessing tables that return shards (sys.shards, sys.allocations) is subjected to the same privileges constraints as the other tables. Namely, in order to query them, the connected user needs to have the DQL privilege on that particular table, either directly or inherited from the SCHEMA or CLUSTER (for more information on privileges inheritance see Hierarchical Inheritance of Privileges).\n\nHowever, being able to query shard returning system tables will not allow the user to retrieve all the rows in the table, as they may contain information related to tables, which the connected user does not have any privileges for. The only rows that will be returned will be the ones the user is allowed to access.\n\nFor example, if the user john has any privilege on the doc.books table but no privilege at all on doc.locations, when john issues a SELECT * FROM sys.shards statement, the shards information related to the doc.locations table will not be returned.\n\nsys jobs tables permissions\n\nAccessing sys.jobs and sys.jobs_log tables is subjected to the same privileges constraints as other tables. To query them, the current user needs to have the DQL privilege on that particular table, either directly or inherited from the SCHEMA or CLUSTER.\n\nA user that doesn’t have superuser privileges is allowed to retrieve only their own job logs entries, while a user with superuser privileges has access to all.\n\npg_stats\n\nThe pg_stats table in the pg_catalog system schema contains statistical data about the contents of the CrateDB cluster.\n\nEntries are periodically created or updated in the interval configured with the stats.service.interval setting.\n\nAlternatively the statistics can also be updated using the ANALYZE command.\n\nThe table contains 1 entry per column for each table in the cluster which has been analyzed.\n\npg_stats schema\n\nName\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\nschemaname\n\n\t\n\ntext\n\n\t\n\nName of the schema containing the table.\n\n\n\n\ntablename\n\n\t\n\ntext\n\n\t\n\nName of the table.\n\n\n\n\nattname\n\n\t\n\ntext\n\n\t\n\nName of the column.\n\n\n\n\ninherited\n\n\t\n\nbool\n\n\t\n\nAlways false in CrateDB; For compatibility with PostgreSQL.\n\n\n\n\nnull_frac\n\n\t\n\nreal\n\n\t\n\nFraction of column entries that are null.\n\n\n\n\navg_width\n\n\t\n\ninteger\n\n\t\n\nAverage size in bytes of column’s entries.\n\n\n\n\nn_distinct\n\n\t\n\nreal\n\n\t\n\nAn approximation of the number of distinct values in a column.\n\n\n\n\nmost_common_vals\n\n\t\n\nstring[]\n\n\t\n\nA list of the most common values in the column. null if no values seem. more common than others.\n\n\n\n\nmost_common_freqs\n\n\t\n\nreal[]\n\n\t\n\nA list of the frequencies of the most common values. The size of the array always matches most_common_vals. If most_common_vals is null this is null as well.\n\n\n\n\nhistogram_bounds\n\n\t\n\nstring[]\n\n\t\n\nA list of values that divide the column’s values into groups of approximately equal population. The values in most_common_vals, if present, are omitted from this histogram calculation.\n\n\n\n\ncorrelation\n\n\t\n\nreal\n\n\t\n\nAlways 0.0. This column exists for PostgreSQL compatibility.\n\n\n\n\nmost_common_elems\n\n\t\n\nstring[]\n\n\t\n\nAlways null. Exists for PostgreSQL compatibility.\n\n\n\n\nmost_common_elem_freqs\n\n\t\n\nreal[]\n\n\t\n\nAlways null. Exists for PostgreSQL compatibility.\n\n\n\n\nelem_count_histogram\n\n\t\n\nreal[]\n\n\t\n\nAlways null. Exists for PostgreSQL compatibility.\n\nNote\n\nNot all data types support creating statistics. So some columns may not show up in the table.\n\npg_publication\n\nThe pg_publication table in the pg_catalog system schema contains all publications created in the cluster.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\noid\n\n\t\n\nRow identifier.\n\n\t\n\nINTEGER\n\n\n\n\npubname\n\n\t\n\nName of the publication.\n\n\t\n\nTEXT\n\n\n\n\npubowner\n\n\t\n\noid of the owner of the publication.\n\n\t\n\nINTEGER\n\n\n\n\npuballtables\n\n\t\n\nWhether this publication includes all tables in the cluster, including tables created in the future.\n\n\t\n\nBOOLEAN\n\n\n\n\npubinsert\n\n\t\n\nWhether INSERT operations are replicated for tables in the publication. Always true.\n\n\t\n\nBOOLEAN\n\n\n\n\npubupdate\n\n\t\n\nWhether UPDATE operations are replicated for tables in the publication. Always true.\n\n\t\n\nBOOLEAN\n\n\n\n\npubdelete\n\n\t\n\nWhether DELETE operations are replicated for tables in the publication. Always true.\n\n\t\n\nBOOLEAN\n\npg_publication_tables\n\nThe pg_publication_tables table in the pg_catalog system schema contains tables replicated by a publication.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\npubname\n\n\t\n\nName of the publication.\n\n\t\n\nTEXT\n\n\n\n\nschemaname\n\n\t\n\nName of the schema containing table.\n\n\t\n\nTEXT\n\n\n\n\ntablename\n\n\t\n\nName of the table.\n\n\t\n\nTEXT\n\npg_subscription\n\nThe pg_subscription table in the pg_catalog system schema contains all subscriptions created in the cluster.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\noid\n\n\t\n\nRow identifier.\n\n\t\n\nINTEGER\n\n\n\n\nsubdbid\n\n\t\n\nnoop value, always 0.\n\n\t\n\nINTEGER\n\n\n\n\nsubname\n\n\t\n\nName of the subscription.\n\n\t\n\nTEXT\n\n\n\n\nsubowner\n\n\t\n\noid of the owner of the subscription.\n\n\t\n\nINTEGER\n\n\n\n\nsubenabled\n\n\t\n\nWhether the subscription is enabled, always true.\n\n\t\n\nBOOLEAN\n\n\n\n\nsubbinary\n\n\t\n\nNoop value, always true.\n\n\t\n\nBOOLEAN\n\n\n\n\nsubstream\n\n\t\n\nNoop value, always true.\n\n\t\n\nBOOLEAN\n\n\n\n\nsubconninfo\n\n\t\n\nConnection string to the publishing cluster.\n\n\t\n\nTEXT\n\n\n\n\nsubslotname\n\n\t\n\nNoop value, always NULL.\n\n\t\n\nTEXT\n\n\n\n\nsubsynccommit\n\n\t\n\nNoop value, always NULL.\n\n\t\n\nTEXT\n\n\n\n\nsubpublications\n\n\t\n\nArray of subscribed publication names. These publications are defined in the publishing cluster.\n\n\t\n\nARRAY\n\npg_subscription_rel\n\nThe pg_subscription_rel table in the pg_catalog system schema contains the state for each replicated relation in each subscription.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nsrsubid\n\n\t\n\nReference to subscription.\n\n\t\n\nINTEGER\n\n\n\n\nsrrelid\n\n\t\n\nReference to relation.\n\n\t\n\nREGCLASS\n\n\n\n\nsrsubstate\n\n\t\n\nReplication state of the relation. State code: i - initializing; d - restoring; r - monitoring, i.e. waiting for new changes; e - error.\n\n\t\n\nTEXT\n\n\n\n\nsrsubstate_reason\n\n\t\n\nError message if there was a replication error for the relation or NULL.\n\n\t\n\nTEXT\n\n\n\n\nsrsublsn\n\n\t\n\nNoop value, always NULL.\n\n\t\n\nLONG"
  },
  {
    "title": "Users and roles management — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/admin/user-management.html",
    "html": "master\nUsers and roles management\n\nUsers and roles account information is stored in the cluster metadata of CrateDB and supports the following statements to create, alter and drop users and roles:\n\nCREATE USER\n\nCREATE ROLE\n\nALTER USER or ALTER ROLE\n\nDROP USER or DROP ROLE\n\nThese statements are database management statements that can be invoked by superusers that already exist in the CrateDB cluster. The CREATE USER, CREATE ROLE, DROP USER and DROP ROLE statements can also be invoked by users with the AL privilege. ALTER USER or ALTER ROLE can be invoked by users to change their own password, without requiring any privilege.\n\nWhen CrateDB is started, the cluster contains one predefined superuser. This user is called crate. It is not possible to create any other superusers.\n\nThe definition of all users and roles, including hashes of their passwords, together with their privileges is backed up together with the cluster’s metadata when a snapshot is created, and it is restored when using the ALL, METADATA, or USERMANAGEMENT keywords with the:ref:sql-restore-snapshot command.\n\nTable of contents\n\nROLES\n\nCREATE ROLE\n\nALTER ROLE\n\nDROP ROLE\n\nList roles\n\nUSERS\n\nCREATE USER\n\nALTER USER\n\nDROP USER\n\nList users\n\nROLES\n\nRoles are entities that are not allowed to login, but can be assigned privileges and they can be granted to other roles, thus creating a role hierarchy, or directly to users. For example, a role myschema_dql_role can be granted with DQL privileges on schema myschema and afterwards the role can be granted to a user, which will automatically inherit those privileges from the myschema_dql_role. A role myschema_dml_role can be granted with DML privileges on schema myschema and can also be granted the role myschema_dql_role, thus gaining also DQL privileges. When myschema_dml_role is granted to a user, this user will automatically have both DQL and DML privileges on myschema.\n\nCREATE ROLE\n\nTo create a new role for the CrateDB database cluster use the CREATE ROLE SQL statement:\n\ncr> CREATE ROLE role_a;\nCREATE OK, 1 row affected (... sec)\n\n\nTip\n\nNewly created roles do not have any privileges. After creating a role, you should configure user privileges.\n\nFor example, to grant all privileges to the role_a user, run:\n\ncr> GRANT ALL PRIVILEGES TO role_a;\nGRANT OK, 4 rows affected (... sec)\n\n\nThe name parameter of the statement follows the principles of an identifier which means that it must be double-quoted if it contains special characters (e.g. whitespace) or if the case needs to be maintained:\n\ncr> CREATE ROLE \"Custom Role\";\nCREATE OK, 1 row affected (... sec)\n\n\nIf a role or user with the name specified in the SQL statement already exists the statement returns an error:\n\ncr> CREATE ROLE \"Custom Role\";\nRoleAlreadyExistsException[Role 'Custom Role' already exists]\n\nALTER ROLE\n\nALTER ROLE and ALTER USER SQL statements are not supported for roles, only for users.\n\nDROP ROLE\n\nTo remove an existing role from the CrateDB database cluster use the DROP ROLE or DROP USER SQL statement:\n\ncr> DROP ROLE role_c;\nDROP OK, 1 row affected (... sec)\n\ncr> DROP USER role_d;\nDROP OK, 1 row affected (... sec)\n\n\nIf a role with the name specified in the SQL statement does not exist, the statement returns an error:\n\ncr> DROP ROLE role_d;\nRoleUnknownException[Role 'role_d' does not exist]\n\nList roles\n\nCrateDB exposes database roles via the read-only Roles system table. The sys.roles table shows all roles in the cluster which can be used to group privileges.\n\nTo list all existing roles query the table:\n\ncr> SELECT name, granted_roles FROM sys.roles order by name;\n+--------+------------------------------------------+\n| name   | granted_roles                            |\n+--------+------------------------------------------+\n| role_a | []                                       |\n| role_b | [{\"grantor\": \"crate\", \"role\": \"role_c\"}] |\n| role_c | []                                       |\n+--------+------------------------------------------+\nSELECT 3 rows in set (... sec)\n\nUSERS\nCREATE USER\n\nTo create a new user for the CrateDB database cluster use the CREATE USER SQL statement:\n\ncr> CREATE USER user_a;\nCREATE OK, 1 row affected (... sec)\n\n\nTip\n\nNewly created users do not have any privileges. After creating a user, you should configure user privileges.\n\nFor example, to grant all privileges to the user_a user, run:\n\ncr> GRANT ALL PRIVILEGES TO user_a;\nGRANT OK, 4 rows affected (... sec)\n\n\nIt can be used to connect to the database cluster using available authentication methods. You can specify the user’s password in the WITH clause of the CREATE statement. This is required if you want to use the Password authentication method:\n\ncr> CREATE USER user_b WITH (password = 'a_secret_password');\nCREATE OK, 1 row affected (... sec)\n\n\nThe username parameter of the statement follows the principles of an identifier which means that it must be double-quoted if it contains special characters (e.g. whitespace) or if the case needs to be maintained:\n\ncr> CREATE USER \"Custom User\";\nCREATE OK, 1 row affected (... sec)\n\n\nIf a user with the username specified in the SQL statement already exists the statement returns an error:\n\ncr> CREATE USER \"Custom User\";\nRoleAlreadyExistsException[Role 'Custom User' already exists]\n\nALTER USER\n\nTo alter the password for an existing user from the CrateDB database cluster use the ALTER ROLE or ALTER USER SQL statements:\n\ncr> ALTER USER user_a SET (password = 'pass');\nALTER OK, 1 row affected (... sec)\n\n\nThe password can be reset (cleared) if specified as NULL:\n\ncr> ALTER USER user_a SET (password = NULL);\nALTER OK, 1 row affected (... sec)\n\n\nNote\n\nThe built-in superuser crate has no password and it is not possible to set a new password for this user.\n\nDROP USER\n\nTo remove an existing user from the CrateDB database cluster use the DROP ROLE or DROP USER SQL statements:\n\ncr> DROP USER user_c;\nDROP OK, 1 row affected (... sec)\n\ncr> DROP ROLE user_d;\nDROP OK, 1 row affected (... sec)\n\n\nIf a user with the username specified in the SQL statement does not exist the statement returns an error:\n\ncr> DROP USER user_d;\nRoleUnknownException[Role 'user_d' does not exist]\n\n\nNote\n\nIt is not possible to drop the built-in superuser crate.\n\nList users\n\nCrateDB exposes database users via the read-only Users system table. The sys.users table shows all users in the cluster which can be used for authentication. The initial superuser crate which is available for all CrateDB clusters is also part of that list.\n\nTo list all existing users query the table:\n\ncr> SELECT name, granted_roles, password, superuser FROM sys.users order by name;\n+--------+----------------------------------------------------------------------------------+----------+-----------+\n| name   | granted_roles                                                                    | password | superuser |\n+--------+----------------------------------------------------------------------------------+----------+-----------+\n| crate  | []                                                                               | NULL     | TRUE      |\n| user_a | [{\"grantor\": \"crate\", \"role\": \"role_a\"}, {\"grantor\": \"crate\", \"role\": \"role_b\"}] | NULL     | FALSE     |\n| user_b | []                                                                               | ******** | FALSE     |\n+--------+----------------------------------------------------------------------------------+----------+-----------+\nSELECT 3 rows in set (... sec)\n\n\nNote\n\nCrateDB also supports retrieving the current connected user using the system information functions: CURRENT_USER, USER and SESSION_USER."
  },
  {
    "title": "Blobs — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/general/blobs.html",
    "html": "master\nBlobs\n\nCrateDB includes support to store binary large objects. By utilizing CrateDB’s cluster features the files can be replicated and sharded just like regular data.\n\nTable of contents\n\nCreating a table for blobs\n\nCustom location for storing blob data\n\nGlobal by configuration\n\nPer blob table setting\n\nList\n\nAltering a blob table\n\nDeleting a blob table\n\nUsing blob tables\n\nUploading\n\nDownloading\n\nDeleting\n\nCreating a table for blobs\n\nBefore adding blobs a blob table must be created. Blob tables can be sharded. This makes it possible to distribute binaries over multiple nodes. Lets use the CrateDB shell crash to issue the SQL statement:\n\nsh$ crash -c \"create blob table myblobs clustered into 3 shards with (number_of_replicas=0)\"\nCREATE OK, 1 row affected (... sec)\n\n\nNow CrateDB is configured to allow blobs to be management under the /_blobs/myblobs endpoint.\n\nCustom location for storing blob data\n\nIt is possible to define a custom directory path for storing blob data which can be completely different than the normal data path. Best use case for this is storing normal data on a fast SSD and blob data on a large cheap spinning disk.\n\nThe custom blob data path can be set either globally by configuration or while creating a blob table. The path can be either absolute or relative and must be creatable/writable by the user CrateDB is running as. A relative path value is relative to CRATE_HOME.\n\nBlob data will be stored under this path with the following layout:\n\n/<blobs.path>/nodes/<NODE_NO>/indices/<INDEX_UUID>/<SHARD_ID>/blobs\n\nGlobal by configuration\n\nJust uncomment or add following entry at the CrateDB configuration in order to define a custom path globally for all blob tables:\n\nblobs.path: /path/to/blob/data\n\n\nAlso see Configuration.\n\nPer blob table setting\n\nIt is also possible to define a custom blob data path per table instead of global by configuration. Also per table setting take precedence over the configuration setting.\n\nSee CREATE BLOB TABLE for details.\n\nCreating a blob table with a custom blob data path:\n\nsh$ crash -c \"create blob table myblobs clustered into 3 shards with (blobs_path='/tmp/crate_blob_data')\" # doctest: +SKIP\nCREATE OK, 1 row affected (... sec)\n\nList\n\nTo list all blobs inside a blob table a SELECT statement can be used:\n\nsh$ crash -c \"select digest, last_modified from blob.myblobs\"\n+------------------------------------------+---------------+\n| digest                                   | last_modified |\n+------------------------------------------+---------------+\n| 4a756ca07e9487f482465a99e8286abc86ba4dc7 | ...           |\n+------------------------------------------+---------------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nTo query blob tables it is necessary to always specify the schema name blob.\n\nAltering a blob table\n\nThe number of replicas a blob table has can be changed using the ALTER BLOB TABLE clause:\n\nsh$ crash -c \"alter blob table myblobs set (number_of_replicas=0)\"\nALTER OK, -1 rows affected (... sec)\n\nDeleting a blob table\n\nBlob tables can be deleted similar to normal tables:\n\nsh$ crash -c \"drop blob table myblobs\"\nDROP OK, 1 row affected (... sec)\n\nUsing blob tables\n\nThe usage of Blob Tables is only supported using the HTTP/HTTPS protocol. This section describes how binaries can be stored, fetched and deleted.\n\nNote\n\nFor the reason of internal optimization any successful request could lead to a 307 Temporary Redirect response.\n\nUploading\n\nTo upload a blob the SHA1 hash of the blob has to be known upfront since this has to be used as the ID of the new blob. For this example we use a fancy Python one-liner to compute the SHA hash:\n\nsh$ python3 -c 'import hashlib;print(hashlib.sha1(\"contents\".encode(\"utf-8\")).hexdigest())'\n4a756ca07e9487f482465a99e8286abc86ba4dc7\n\n\nThe blob can now be uploaded by issuing a PUT request:\n\nsh$ curl -isSX PUT '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7' -d 'contents'\nHTTP/1.1 201 Created\ncontent-length: 0\n\n\nIf a blob already exists with the given hash a 409 Conflict is returned:\n\nsh$ curl -isSX PUT '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7' -d 'contents'\nHTTP/1.1 409 Conflict\ncontent-length: 0\n\nDownloading\n\nTo download a blob simply use a GET request:\n\nsh$ curl -sS '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7'\ncontents\n\n\nIf the blob doesn’t exist a 404 Not Found error is returned:\n\nsh$ curl -isS '127.0.0.1:4200/_blobs/myblobs/e5fa44f2b31c1fb553b6021e7360d07d5d91ff5e'\nHTTP/1.1 404 Not Found\ncontent-length: 0\n\n\nTo determine if a blob exists without downloading it, a HEAD request can be used:\n\nsh$ curl -sS -I '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7'\nHTTP/1.1 200 OK\ncontent-length: 8\naccept-ranges: bytes\nexpires: Thu, 31 Dec 2037 23:59:59 GMT\ncache-control: max-age=315360000\n\n\nNote\n\nThe cache headers for blobs are static and basically allows clients to cache the response forever since the blob is immutable.\n\nDeleting\n\nTo delete a blob simply use a DELETE request:\n\nsh$ curl -isS -XDELETE '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7'\nHTTP/1.1 204 No Content\n\n\nIf the blob doesn’t exist a 404 Not Found error is returned:\n\nsh$ curl -isS -XDELETE '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7'\nHTTP/1.1 404 Not Found\ncontent-length: 0\n"
  },
  {
    "title": "Information schema — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/general/information-schema.html",
    "html": "master\nInformation schema\n\ninformation_schema is a special schema that contains virtual tables which are read-only and can be queried to get information about the state of the cluster.\n\nTable of contents\n\nAccess\n\nVirtual tables\n\ntables\n\nsettings\n\nviews\n\ncolumns\n\ntable_constraints\n\nkey_column_usage\n\ntable_partitions\n\nroutines\n\nschemata\n\nsql_features\n\ncharacter_sets\n\nforeign_servers\n\nforeign_server_options\n\nforeign_tables\n\nforeign_table_options\n\nuser_mappings\n\nuser_mapping_options\n\nAccess\n\nWhen the user management is enabled, accessing the information_schema is open to all users and it does not require any privileges.\n\nHowever, being able to query information_schema tables will not allow the user to retrieve all the rows in the table, as it can contain information related to tables over which the connected user does not have any privileges. The only rows that will be returned will be the ones the user is allowed to access.\n\nFor example, if the user john has any privilege on the doc.books table but no privilege at all on doc.locations, when john issues a SELECT * FROM information_schema.tables statement, the tables information related to the doc.locations table will not be returned.\n\nVirtual tables\ntables\n\nThe information_schema.tables virtual table can be queried to get a list of all available tables and views and their settings, such as number of shards or number of replicas.\n\ncr> SELECT table_schema, table_name, table_type, number_of_shards, number_of_replicas\n... FROM information_schema.tables\n... ORDER BY table_schema ASC, table_name ASC;\n+--------------------+-------------------------+------------+------------------+--------------------+\n| table_schema       | table_name              | table_type | number_of_shards | number_of_replicas |\n+--------------------+-------------------------+------------+------------------+--------------------+\n| doc                | galaxies                | VIEW       |             NULL | NULL               |\n| doc                | locations               | BASE TABLE |                2 | 0                  |\n| doc                | partitioned_table       | BASE TABLE |                4 | 0-1                |\n| doc                | quotes                  | BASE TABLE |                2 | 0                  |\n| information_schema | character_sets          | BASE TABLE |             NULL | NULL               |\n| information_schema | columns                 | BASE TABLE |             NULL | NULL               |\n| information_schema | foreign_server_options  | BASE TABLE |             NULL | NULL               |\n| information_schema | foreign_servers         | BASE TABLE |             NULL | NULL               |\n| information_schema | foreign_table_options   | BASE TABLE |             NULL | NULL               |\n| information_schema | foreign_tables          | BASE TABLE |             NULL | NULL               |\n| information_schema | key_column_usage        | BASE TABLE |             NULL | NULL               |\n| information_schema | referential_constraints | BASE TABLE |             NULL | NULL               |\n| information_schema | routines                | BASE TABLE |             NULL | NULL               |\n| information_schema | schemata                | BASE TABLE |             NULL | NULL               |\n| information_schema | sql_features            | BASE TABLE |             NULL | NULL               |\n| information_schema | table_constraints       | BASE TABLE |             NULL | NULL               |\n| information_schema | table_partitions        | BASE TABLE |             NULL | NULL               |\n| information_schema | tables                  | BASE TABLE |             NULL | NULL               |\n| information_schema | user_mapping_options    | BASE TABLE |             NULL | NULL               |\n| information_schema | user_mappings           | BASE TABLE |             NULL | NULL               |\n| information_schema | views                   | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_am                   | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_attrdef              | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_attribute            | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_class                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_constraint           | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_cursors              | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_database             | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_depend               | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_description          | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_enum                 | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_event_trigger        | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_index                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_indexes              | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_locks                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_namespace            | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_proc                 | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_publication          | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_publication_tables   | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_range                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_roles                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_settings             | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_shdescription        | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_stats                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_subscription         | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_subscription_rel     | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_tables               | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_tablespace           | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_type                 | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_views                | BASE TABLE |             NULL | NULL               |\n| sys                | allocations             | BASE TABLE |             NULL | NULL               |\n| sys                | checks                  | BASE TABLE |             NULL | NULL               |\n| sys                | cluster                 | BASE TABLE |             NULL | NULL               |\n| sys                | health                  | BASE TABLE |             NULL | NULL               |\n| sys                | jobs                    | BASE TABLE |             NULL | NULL               |\n| sys                | jobs_log                | BASE TABLE |             NULL | NULL               |\n| sys                | jobs_metrics            | BASE TABLE |             NULL | NULL               |\n| sys                | node_checks             | BASE TABLE |             NULL | NULL               |\n| sys                | nodes                   | BASE TABLE |             NULL | NULL               |\n| sys                | operations              | BASE TABLE |             NULL | NULL               |\n| sys                | operations_log          | BASE TABLE |             NULL | NULL               |\n| sys                | privileges              | BASE TABLE |             NULL | NULL               |\n| sys                | repositories            | BASE TABLE |             NULL | NULL               |\n| sys                | roles                   | BASE TABLE |             NULL | NULL               |\n| sys                | segments                | BASE TABLE |             NULL | NULL               |\n| sys                | shards                  | BASE TABLE |             NULL | NULL               |\n| sys                | snapshot_restore        | BASE TABLE |             NULL | NULL               |\n| sys                | snapshots               | BASE TABLE |             NULL | NULL               |\n| sys                | summits                 | BASE TABLE |             NULL | NULL               |\n| sys                | users                   | BASE TABLE |             NULL | NULL               |\n+--------------------+-------------------------+------------+------------------+--------------------+\nSELECT 70 rows in set (... sec)\n\n\nThe table also contains additional information such as the specified routing column and partition columns:\n\ncr> SELECT table_name, clustered_by, partitioned_by\n... FROM information_schema.tables\n... WHERE table_schema = 'doc'\n... ORDER BY table_schema ASC, table_name ASC;\n+-------------------+--------------+----------------+\n| table_name        | clustered_by | partitioned_by |\n+-------------------+--------------+----------------+\n| galaxies          | NULL         | NULL           |\n| locations         | id           | NULL           |\n| partitioned_table | _id          | [\"date\"]       |\n| quotes            | id           | NULL           |\n+-------------------+--------------+----------------+\nSELECT 4 rows in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nDescription\n\n\t\n\nData Type\n\n\n\n\nblobs_path\n\n\t\n\nThe data path of the blob table\n\n\t\n\nTEXT\n\n\n\n\nclosed\n\n\t\n\nThe state of the table\n\n\t\n\nBOOLEAN\n\n\n\n\nclustered_by\n\n\t\n\nThe routing column used to cluster the table\n\n\t\n\nTEXT\n\n\n\n\ncolumn_policy\n\n\t\n\nDefines whether the table uses a STRICT or a DYNAMIC Column policy\n\n\t\n\nTEXT\n\n\n\n\nnumber_of_replicas\n\n\t\n\nThe number of replicas the table currently has\n\n\t\n\nINTEGER\n\n\n\n\nnumber_of_shards\n\n\t\n\nThe number of shards the table is currently distributed across\n\n\t\n\nINTEGER\n\n\n\n\npartitioned_by\n\n\t\n\nThe partition columns (used to partition the table)\n\n\t\n\nTEXT\n\n\n\n\nreference_generation\n\n\t\n\nSpecifies how values in the self-referencing column are generated\n\n\t\n\nTEXT\n\n\n\n\nrouting_hash_function\n\n\t\n\nThe name of the hash function used for internal routing\n\n\t\n\nTEXT\n\n\n\n\nself_referencing_column_name\n\n\t\n\nThe name of the column that uniquely identifies each row (always _id)\n\n\t\n\nTEXT\n\n\n\n\nsettings\n\n\t\n\nWITH\n\n\t\n\nOBJECT\n\n\n\n\ntable_catalog\n\n\t\n\nRefers to the table_schema\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nThe name of the table\n\n\t\n\nTEXT\n\n\n\n\ntable_schema\n\n\t\n\nThe name of the schema the table belongs to\n\n\t\n\nTEXT\n\n\n\n\ntable_type\n\n\t\n\nThe type of the table (BASE TABLE for tables, VIEW for views)\n\n\t\n\nTEXT\n\n\n\n\nversion\n\n\t\n\nA collection of version numbers relevant to the table\n\n\t\n\nOBJECT\n\nsettings\n\nTable settings specify configuration parameters for tables. Some settings can be set during Cluster runtime and others are only applied on cluster restart.\n\nThis list of table settings in WITH shows detailed information of each parameter.\n\nTable parameters can be applied with CREATE TABLE on creation of a table. With ALTER TABLE they can be set on already existing tables.\n\nThe following statement creates a new table and sets the refresh interval of shards to 500 ms and sets the shard allocation for primary shards only:\n\ncr> create table parameterized_table (id integer, content text)\n... with (\"refresh_interval\"=500, \"routing.allocation.enable\"='primaries');\nCREATE OK, 1 row affected (... sec)\n\n\nThe settings can be verified by querying information_schema.tables:\n\ncr> select settings['routing']['allocation']['enable'] as alloc_enable,\n...   settings['refresh_interval'] as refresh_interval\n... from information_schema.tables\n... where table_name='parameterized_table';\n+--------------+------------------+\n| alloc_enable | refresh_interval |\n+--------------+------------------+\n| primaries    |              500 |\n+--------------+------------------+\nSELECT 1 row in set (... sec)\n\n\nOn existing tables this needs to be done with ALTER TABLE statement:\n\ncr> alter table parameterized_table\n... set (\"routing.allocation.enable\"='none');\nALTER OK, -1 rows affected (... sec)\n\nviews\n\nThe table information_schema.views contains the name, definition and options of all available views.\n\ncr> SELECT table_schema, table_name, view_definition\n... FROM information_schema.views\n... ORDER BY table_schema ASC, table_name ASC;\n+--------------+------------+-------------------------+\n| table_schema | table_name | view_definition         |\n+--------------+------------+-------------------------+\n| doc          | galaxies   | SELECT                  |\n|              |            |   \"id\"                  |\n|              |            | , \"name\"                |\n|              |            | , \"description\"         |\n|              |            | FROM \"locations\"        |\n|              |            | WHERE \"kind\" = 'Galaxy' |\n+--------------+------------+-------------------------+\nSELECT 1 row in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nDescription\n\n\t\n\nData Type\n\n\n\n\ntable_catalog\n\n\t\n\nThe catalog of the table of the view (refers to table_schema)\n\n\t\n\nTEXT\n\n\n\n\ntable_schema\n\n\t\n\nThe schema of the table of the view\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nThe name of the table of the view\n\n\t\n\nTEXT\n\n\n\n\nview_definition\n\n\t\n\nThe SELECT statement that defines the view\n\n\t\n\nTEXT\n\n\n\n\ncheck_option\n\n\t\n\nNot applicable for CrateDB, always return NONE\n\n\t\n\nTEXT\n\n\n\n\nis_updatable\n\n\t\n\nWhether the view is updatable. Not applicable for CrateDB, always returns FALSE\n\n\t\n\nBOOLEAN\n\n\n\n\nowner\n\n\t\n\nThe user that created the view\n\n\t\n\nTEXT\n\nNote\n\nIf you drop the table of a view, the view will still exist and show up in the information_schema.tables and information_schema.views tables.\n\ncolumns\n\nThis table can be queried to get a list of all available columns of all tables and views and their definition like data type and ordinal position inside the table:\n\ncr> select table_name, column_name, ordinal_position as pos, data_type\n... from information_schema.columns\n... where table_schema = 'doc' and table_name not like 'my_table%'\n... order by table_name asc, column_name asc;\n+-------------------+--------------------------------+-----+--------------------------+\n| table_name        | column_name                    | pos | data_type                |\n+-------------------+--------------------------------+-----+--------------------------+\n| locations         | date                           |   3 | timestamp with time zone |\n| locations         | description                    |   6 | text                     |\n| locations         | id                             |   1 | integer                  |\n| locations         | information                    |  11 | object_array             |\n| locations         | information['evolution_level'] |  13 | smallint                 |\n| locations         | information['population']      |  12 | bigint                   |\n| locations         | inhabitants                    |   7 | object                   |\n| locations         | inhabitants['description']     |   9 | text                     |\n| locations         | inhabitants['interests']       |   8 | text_array               |\n| locations         | inhabitants['name']            |  10 | text                     |\n| locations         | kind                           |   4 | text                     |\n| locations         | landmarks                      |  14 | text_array               |\n| locations         | name                           |   2 | text                     |\n| locations         | position                       |   5 | integer                  |\n| partitioned_table | date                           |   3 | timestamp with time zone |\n| partitioned_table | id                             |   1 | bigint                   |\n| partitioned_table | title                          |   2 | text                     |\n| quotes            | id                             |   1 | integer                  |\n| quotes            | quote                          |   2 | text                     |\n+-------------------+--------------------------------+-----+--------------------------+\nSELECT 19 rows in set (... sec)\n\n\nYou can even query this table’s own columns (attention: this might lead to infinite recursion of your mind, beware!):\n\ncr> select column_name, data_type, ordinal_position\n... from information_schema.columns\n... where table_schema = 'information_schema'\n... and table_name = 'columns' order by column_name asc;\n+--------------------------+------------+------------------+\n| column_name              | data_type  | ordinal_position |\n+--------------------------+------------+------------------+\n| character_maximum_length | integer    |                1 |\n| character_octet_length   | integer    |                2 |\n| character_set_catalog    | text       |                3 |\n| character_set_name       | text       |                4 |\n| character_set_schema     | text       |                5 |\n| check_action             | integer    |                6 |\n| check_references         | text       |                7 |\n| collation_catalog        | text       |                8 |\n| collation_name           | text       |                9 |\n| collation_schema         | text       |               10 |\n| column_default           | text       |               11 |\n| column_details           | object     |               12 |\n| column_details['name']   | text       |               13 |\n| column_details['path']   | text_array |               14 |\n| column_name              | text       |               15 |\n| data_type                | text       |               16 |\n| datetime_precision       | integer    |               17 |\n| domain_catalog           | text       |               18 |\n| domain_name              | text       |               19 |\n| domain_schema            | text       |               20 |\n| generation_expression    | text       |               21 |\n| identity_cycle           | boolean    |               22 |\n| identity_generation      | text       |               23 |\n| identity_increment       | text       |               24 |\n| identity_maximum         | text       |               25 |\n| identity_minimum         | text       |               26 |\n| identity_start           | text       |               27 |\n| interval_precision       | integer    |               28 |\n| interval_type            | text       |               29 |\n| is_generated             | text       |               30 |\n| is_identity              | boolean    |               31 |\n| is_nullable              | boolean    |               32 |\n| numeric_precision        | integer    |               33 |\n| numeric_precision_radix  | integer    |               34 |\n| numeric_scale            | integer    |               35 |\n| ordinal_position         | integer    |               36 |\n| table_catalog            | text       |               37 |\n| table_name               | text       |               38 |\n| table_schema             | text       |               39 |\n| udt_catalog              | text       |               40 |\n| udt_name                 | text       |               41 |\n| udt_schema               | text       |               42 |\n+--------------------------+------------+------------------+\nSELECT 42 rows in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nDescription\n\n\t\n\nData Type\n\n\n\n\ntable_catalog\n\n\t\n\nRefers to the table_schema\n\n\t\n\nTEXT\n\n\n\n\ntable_schema\n\n\t\n\nSchema name containing the table\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nTable Name\n\n\t\n\nTEXT\n\n\n\n\ncolumn_name\n\n\t\n\nColumn Name For fields in object columns this is not an identifier but a path and therefore must not be double quoted when programmatically obtained.\n\n\t\n\nTEXT\n\n\n\n\nordinal_position\n\n\t\n\nThe position of the column within the table\n\n\t\n\nINTEGER\n\n\n\n\nis_nullable\n\n\t\n\nWhether the column is nullable\n\n\t\n\nBOOLEAN\n\n\n\n\ndata_type\n\n\t\n\nThe data type of the column\n\nFor further information see Data types\n\n\t\n\nTEXT\n\n\n\n\ncolumn_default\n\n\t\n\nThe default expression of the column\n\n\t\n\nTEXT\n\n\n\n\ncharacter_maximum_length\n\n\t\n\nIf the data type is a character type then return the declared length limit; otherwise NULL.\n\n\t\n\nINTEGER\n\n\n\n\ncharacter_octet_length\n\n\t\n\nNot implemented (always returns NULL)\n\nPlease refer to TEXT type\n\n\t\n\nINTEGER\n\n\n\n\nnumeric_precision\n\n\t\n\nIndicates the number of significant digits for a numeric data_type. For all other data types this column is NULL.\n\n\t\n\nINTEGER\n\n\n\n\nnumeric_precision_radix\n\n\t\n\nIndicates in which base the value in the column numeric_precision for a numeric data_type is exposed. This can either be 2 (binary) or 10 (decimal). For all other data types this column is NULL.\n\n\t\n\nINTEGER\n\n\n\n\nnumeric_scale\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nINTEGER\n\n\n\n\ndatetime_precision\n\n\t\n\nContains the fractional seconds precision for a timestamp data_type. For all other data types this column is null.\n\n\t\n\nINTEGER\n\n\n\n\ninterval_type\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ninterval_precision\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nINTEGER\n\n\n\n\ncharacter_set_catalog\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncharacter_set_schema\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncharacter_set_name\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncollation_catalog\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncollation_schema\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncollation_name\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ndomain_catalog\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ndomain_schema\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ndomain_name\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nudt_catalog\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nudt_schema\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nudt_name\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncheck_references\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncheck_action\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nINTEGER\n\n\n\n\ngeneration_expression\n\n\t\n\nThe expression used to generate ad column. If the column is not generated NULL is returned.\n\n\t\n\nTEXT\n\n\n\n\nis_generated\n\n\t\n\nReturns ALWAYS or NEVER wether the column is generated or not.\n\n\t\n\nTEXT\n\n\n\n\nis_identity\n\n\t\n\nNot implemented (always returns false)\n\n\t\n\nBOOLEAN\n\n\n\n\nidentity_cycle\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nBOOLEAN\n\n\n\n\nidentity_generation\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nidentity_increment\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nidentity_maximum\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nidentity_minimum\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nidentity_start\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\ntable_constraints\n\nThis table can be queried to get a list of all defined table constraints, their type, name and which table they are defined in.\n\nNote\n\nCurrently only PRIMARY_KEY constraints are supported.\n\ncr> select table_schema, table_name, constraint_name, constraint_type as type\n... from information_schema.table_constraints\n... where table_name = 'tables'\n...   or table_name = 'quotes'\n...   or table_name = 'documents'\n...   or table_name = 'tbl'\n... order by table_schema desc, table_name asc limit 10;\n+--------------------+------------+------------------------+-------------+\n| table_schema       | table_name | constraint_name        | type        |\n+--------------------+------------+------------------------+-------------+\n| information_schema | tables     | tables_pk              | PRIMARY KEY |\n| doc                | quotes     | quotes_pk              | PRIMARY KEY |\n| doc                | quotes     | doc_quotes_id_not_null | CHECK       |\n| doc                | tbl        | doc_tbl_col_not_null   | CHECK       |\n+--------------------+------------+------------------------+-------------+\nSELECT 4 rows in set (... sec)\n\nkey_column_usage\n\nThis table may be queried to retrieve primary key information from all user tables:\n\ncr> select constraint_name, table_name, column_name, ordinal_position\n... from information_schema.key_column_usage\n... where table_name = 'students'\n+-----------------+------------+-------------+------------------+\n| constraint_name | table_name | column_name | ordinal_position |\n+-----------------+------------+-------------+------------------+\n| students_pk     | students   | id          |                1 |\n| students_pk     | students   | department  |                2 |\n+-----------------+------------+-------------+------------------+\nSELECT 2 rows in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nDescription\n\n\t\n\nData Type\n\n\n\n\nconstraint_catalog\n\n\t\n\nRefers to table_catalog\n\n\t\n\nTEXT\n\n\n\n\nconstraint_schema\n\n\t\n\nRefers to table_schema\n\n\t\n\nTEXT\n\n\n\n\nconstraint_name\n\n\t\n\nName of the constraint\n\n\t\n\nTEXT\n\n\n\n\ntable_catalog\n\n\t\n\nRefers to table_schema\n\n\t\n\nTEXT\n\n\n\n\ntable_schema\n\n\t\n\nName of the schema that contains the table that contains the constraint\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nName of the table that contains the constraint\n\n\t\n\nTEXT\n\n\n\n\ncolumn_name\n\n\t\n\nName of the column that contains the constraint\n\n\t\n\nTEXT\n\n\n\n\nordinal_position\n\n\t\n\nPosition of the column within the constraint (starts with 1)\n\n\t\n\nINTEGER\n\ntable_partitions\n\nThis table can be queried to get information about all partitioned tables, Each partition of a table is represented as one row. The row contains the information table name, schema name, partition ident, and the values of the partition. values is a key-value object with the partition column (or columns) as key(s) and the corresponding value as value(s).\n\ncr> insert into a_partitioned_table (id, content) values (1, 'content_a');\nINSERT OK, 1 row affected (... sec)\n\ncr> alter table a_partitioned_table set (number_of_shards=5);\nALTER OK, -1 rows affected (... sec)\n\ncr> insert into a_partitioned_table (id, content) values (2, 'content_b');\nINSERT OK, 1 row affected (... sec)\n\n\nThe following example shows a table where the column content of table a_partitioned_table has been used to partition the table. The table has two partitions. The partitions are introduced when data is inserted where content is content_a, and content_b.:\n\ncr> select table_name, table_schema as schema, partition_ident, \"values\"\n... from information_schema.table_partitions\n... order by table_name, partition_ident;\n+---------------------+--------+--------------------+--------------------------+\n| table_name          | schema | partition_ident    | values                   |\n+---------------------+--------+--------------------+--------------------------+\n| a_partitioned_table | doc    | 04566rreehimst2vc4 | {\"content\": \"content_a\"} |\n| a_partitioned_table | doc    | 04566rreehimst2vc8 | {\"content\": \"content_b\"} |\n+---------------------+--------+--------------------+--------------------------+\nSELECT 2 rows in set (... sec)\n\n\nThe second partition has been created after the number of shards for future partitions have been changed on the partitioned table, so they show 5 instead of 4:\n\ncr> select table_name, partition_ident,\n... number_of_shards, number_of_replicas\n... from information_schema.table_partitions\n... order by table_name, partition_ident;\n+---------------------+--------------------+------------------+--------------------+\n| table_name          | partition_ident    | number_of_shards | number_of_replicas |\n+---------------------+--------------------+------------------+--------------------+\n| a_partitioned_table | 04566rreehimst2vc4 |                4 | 0-1                |\n| a_partitioned_table | 04566rreehimst2vc8 |                5 | 0-1                |\n+---------------------+--------------------+------------------+--------------------+\nSELECT 2 rows in set (... sec)\n\nroutines\n\nThe routines table contains tokenizers, token-filters, char-filters, custom analyzers created by CREATE ANALYZER statements (see Creating a custom analyzer), and functions created by CREATE FUNCTION statements:\n\ncr> select routine_name, routine_type\n... from information_schema.routines\n... group by routine_name, routine_type\n... order by routine_name asc limit 5;\n+----------------------+--------------+\n| routine_name         | routine_type |\n+----------------------+--------------+\n| PathHierarchy        | TOKENIZER    |\n| apostrophe           | TOKEN_FILTER |\n| arabic               | ANALYZER     |\n| arabic_normalization | TOKEN_FILTER |\n| arabic_stem          | TOKEN_FILTER |\n+----------------------+--------------+\nSELECT 5 rows in set (... sec)\n\n\nFor example you can use this table to list existing tokenizers like this:\n\ncr> select routine_name\n... from information_schema.routines\n... where routine_type='TOKENIZER'\n... order by routine_name asc limit 10;\n+----------------+\n| routine_name   |\n+----------------+\n| PathHierarchy  |\n| char_group     |\n| classic        |\n| edge_ngram     |\n| keyword        |\n| letter         |\n| lowercase      |\n| ngram          |\n| path_hierarchy |\n| pattern        |\n+----------------+\nSELECT 10 rows in set (... sec)\n\n\nOr get an overview of how many routines and routine types are available:\n\ncr> select count(*), routine_type\n... from information_schema.routines\n... group by routine_type\n... order by routine_type;\n+----------+--------------+\n| count(*) | routine_type |\n+----------+--------------+\n|       45 | ANALYZER     |\n|        3 | CHAR_FILTER  |\n|       16 | TOKENIZER    |\n|       61 | TOKEN_FILTER |\n+----------+--------------+\nSELECT 4 rows in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nData Type\n\n\n\n\nroutine_name\n\n\t\n\nTEXT\n\n\n\n\nroutine_type\n\n\t\n\nTEXT\n\n\n\n\nroutine_body\n\n\t\n\nTEXT\n\n\n\n\nroutine_schema\n\n\t\n\nTEXT\n\n\n\n\ndata_type\n\n\t\n\nTEXT\n\n\n\n\nis_deterministic\n\n\t\n\nBOOLEAN\n\n\n\n\nroutine_definition\n\n\t\n\nTEXT\n\n\n\n\nspecific_name\n\n\t\n\nTEXT\n\nroutine_name\n\nName of the routine (might be duplicated in case of overloading)\n\nroutine_type\n\nType of the routine. Can be FUNCTION, ANALYZER, CHAR_FILTER, TOKEN_FILTER or TOKEN_FILTER.\n\nroutine_schema\n\nThe schema where the routine was defined. If it doesn’t apply, then NULL.\n\nroutine_body\n\nThe language used for the routine implementation. If it doesn’t apply, then NULL.\n\ndata_type\n\nThe return type of the function. If it doesn’t apply, then NULL.\n\nis_deterministic\n\nIf the routine is deterministic then True, else False (NULL if it doesn’t apply).\n\nroutine_definition\n\nThe function definition (NULL if it doesn’t apply).\n\nspecific_name\n\nUsed to uniquely identify the function in a schema, even if the function is overloaded. Currently the specific name contains the types of the function arguments. As the format might change in the future, it should be only used to compare it to other instances of specific_name.\n\nschemata\n\nThe schemata table lists all existing schemas. The blob, information_schema, and sys schemas are always available. The doc schema is available after the first user table is created.\n\ncr> select schema_name from information_schema.schemata order by schema_name;\n+--------------------+\n| schema_name        |\n+--------------------+\n| blob               |\n| doc                |\n| information_schema |\n| pg_catalog         |\n| sys                |\n+--------------------+\nSELECT 5 rows in set (... sec)\n\nsql_features\n\nThe sql_features table outlines supported and unsupported SQL features of CrateDB based to the current SQL standard (see SQL standard compliance):\n\ncr> select feature_name, is_supported, sub_feature_id, sub_feature_name\n... from information_schema.sql_features\n... where feature_id='F501';\n+--------------------------------+--------------+----------------+--------------------+\n| feature_name                   | is_supported | sub_feature_id | sub_feature_name   |\n+--------------------------------+--------------+----------------+--------------------+\n| Features and conformance views | FALSE        |                |                    |\n| Features and conformance views | TRUE         | 1              | SQL_FEATURES view  |\n| Features and conformance views | FALSE        | 2              | SQL_SIZING view    |\n| Features and conformance views | FALSE        | 3              | SQL_LANGUAGES view |\n+--------------------------------+--------------+----------------+--------------------+\nSELECT 4 rows in set (... sec)\n\n\nName\n\n\t\n\nData Type\n\n\t\n\nNullable\n\n\n\n\nfeature_id\n\n\t\n\nTEXT\n\n\t\n\nNO\n\n\n\n\nfeature_name\n\n\t\n\nTEXT\n\n\t\n\nNO\n\n\n\n\nsub_feature_id\n\n\t\n\nTEXT\n\n\t\n\nNO\n\n\n\n\nsub_feature_name\n\n\t\n\nTEXT\n\n\t\n\nNO\n\n\n\n\nis_supported\n\n\t\n\nTEXT\n\n\t\n\nNO\n\n\n\n\nis_verified_by\n\n\t\n\nTEXT\n\n\t\n\nYES\n\n\n\n\ncomments\n\n\t\n\nTEXT\n\n\t\n\nYES\n\nfeature_id\n\nIdentifier of the feature\n\nfeature_name\n\nDescriptive name of the feature by the Standard\n\nsub_feature_id\n\nIdentifier of the sub feature; If it has zero-length, this is a feature\n\nsub_feature_name\n\nDescriptive name of the sub feature by the Standard; If it has zero-length, this is a feature\n\nis_supported\n\nYES if the feature is fully supported by the current version of CrateDB, NO if not\n\nis_verified_by\n\nIdentifies the conformance test used to verify the claim;\n\nAlways NULL since the CrateDB development group does not perform formal testing of feature conformance\n\ncomments\n\nEither NULL or shows a comment about the supported status of the feature\n\ncharacter_sets\n\nThe character_sets table identifies the character sets available in the current database.\n\nIn CrateDB there is always a single entry listing UTF8:\n\ncr> SELECT character_set_name, character_repertoire FROM information_schema.character_sets;\n+--------------------+----------------------+\n| character_set_name | character_repertoire |\n+--------------------+----------------------+\n| UTF8               | UCS                  |\n+--------------------+----------------------+\nSELECT 1 row in set (... sec)\n\n\nColumn Name\n\n\t\n\nReturn Type\n\n\t\n\nDescription\n\n\n\n\ncharacter_set_catalog\n\n\t\n\nTEXT\n\n\t\n\nNot implemented, this column is always null.\n\n\n\n\ncharacter_set_schema\n\n\t\n\nTEXT\n\n\t\n\nNot implemented, this column is always null.\n\n\n\n\ncharacter_set_name\n\n\t\n\nTEXT\n\n\t\n\nName of the character set.\n\n\n\n\ncharacter_repertoire\n\n\t\n\nTEXT\n\n\t\n\nCharacter repertoire.\n\n\n\n\nform_of_use\n\n\t\n\nTEXT\n\n\t\n\nCharacter encoding form, same as character_set_name.\n\n\n\n\ndefault_collate_catalog\n\n\t\n\nTEXT\n\n\t\n\nName of the database containing the default collation (Always crate).\n\n\n\n\ndefault_collate_schema\n\n\t\n\nTEXT\n\n\t\n\nName of the schema containing the default collation (Always NULL).\n\n\n\n\ndefault_collate_name\n\n\t\n\nTEXT\n\n\t\n\nName of the default collation (Always NULL).\n\nforeign_servers\n\nLists foreign servers created using CREATE SERVER. See Foreign Data Wrappers.\n\nColumn Name\n\n\t\n\nReturn Type\n\n\t\n\nDescription\n\n\n\n\nforeign_server_catalog\n\n\t\n\nTEXT\n\n\t\n\nName of the database of the foreign server. Always crate.\n\n\n\n\nforeign_server_name\n\n\t\n\nTEXT\n\n\t\n\nName of the foreign server.\n\n\n\n\nforeign_data_wrapper_catalog\n\n\t\n\nTEXT\n\n\t\n\nName of the database that contains the foreign-data wrapper. Always crate.\n\n\n\n\nforeign_data_wrapper_name\n\n\t\n\nTEXT\n\n\t\n\nName of the foreign-data wrapper used by the foreign server.\n\n\n\n\nforeign_server_type\n\n\t\n\nTEXT\n\n\t\n\nForeign server type information. Always null.\n\n\n\n\nforeign_server_version\n\n\t\n\nTEXT\n\n\t\n\nForeign server version information. Always null.\n\n\n\n\nauthorization_identifier\n\n\t\n\nTEXT\n\n\t\n\nName of the user who created the server.\n\nforeign_server_options\n\nLists options of foreign servers created using CREATE SERVER. See Foreign Data Wrappers.\n\nColumn Name\n\n\t\n\nReturn Type\n\n\t\n\nDescription\n\n\n\n\nforeign_server_catalog\n\n\t\n\nTEXT\n\n\t\n\nName of the database that the foreign server is defined in. Always crate.\n\n\n\n\nforeign_server_name\n\n\t\n\nTEXT\n\n\t\n\nName of the foreign server.\n\n\n\n\noption_name\n\n\t\n\nTEXT\n\n\t\n\nName of an option.\n\n\n\n\noption_value\n\n\t\n\nTEXT\n\n\t\n\nValue of the option cast to string.\n\nforeign_tables\n\nLists foreign tables created using CREATE FOREIGN TABLE. See Foreign Data Wrappers.\n\nColumn Name\n\n\t\n\nReturn Type\n\n\t\n\nDescription\n\n\n\n\nforeign_table_catalog\n\n\t\n\nTEXT\n\n\t\n\nName of the database where the foreign table is defined in. Always crate.\n\n\n\n\nforeign_table_schema\n\n\t\n\nTEXT\n\n\t\n\nName of the schema that contains the foreign table.\n\n\n\n\nforeign_table_name\n\n\t\n\nTEXT\n\n\t\n\nName of the foreign table.\n\n\n\n\nforeign_server_catalog\n\n\t\n\nTEXT\n\n\t\n\nName of the database where the foreign server is defined in. Always crate.\n\n\n\n\nforeign_server_name\n\n\t\n\nTEXT\n\n\t\n\nName of the foreign server.\n\nforeign_table_options\n\nLists options for foreign tables created using CREATE FOREIGN TABLE. See Foreign Data Wrappers.\n\nColumn Name\n\n\t\n\nReturn Type\n\n\t\n\nDescription\n\n\n\n\nforeign_table_catalog\n\n\t\n\nTEXT\n\n\t\n\nName of the database that contains the foreign table. Always crate.\n\n\n\n\nforeign_table_schema\n\n\t\n\nTEXT\n\n\t\n\nName of the schema that contains the foreign table.\n\n\n\n\nforeign_table_name\n\n\t\n\nTEXT\n\n\t\n\nName of the foreign table.\n\n\n\n\noption_name\n\n\t\n\nTEXT\n\n\t\n\nName of an option.\n\n\n\n\noption_value\n\n\t\n\nTEXT\n\n\t\n\nValue of the option cast to string.\n\nuser_mappings\n\nLists user mappings created for foreign servers. See Foreign Data Wrappers.\n\nColumn Name\n\n\t\n\nReturn Type\n\n\t\n\nDescription\n\n\n\n\nauthorization_identifier\n\n\t\n\nTEXT\n\n\t\n\nName of the user being mapped.\n\n\n\n\nforeign_server_catalog\n\n\t\n\nTEXT\n\n\t\n\nName of the database of the foreign server. Always crate.\n\n\n\n\nforeign_server_name\n\n\t\n\nTEXT\n\n\t\n\nName of the foreign server for this user mapping.\n\nuser_mapping_options\n\nLists the options for user mappings created for foreign servers. See Foreign Data Wrappers.\n\nColumn Name\n\n\t\n\nReturn Type\n\n\t\n\nDescription\n\n\n\n\nauthorization_identifier\n\n\t\n\nTEXT\n\n\t\n\nName of the user being mapped.\n\n\n\n\nforeign_server_catalog\n\n\t\n\nTEXT\n\n\t\n\nName of the database of the foreign server. Always crate.\n\n\n\n\nforeign_server_name\n\n\t\n\nTEXT\n\n\t\n\nName of the foreign server for this user mapping.\n\n\n\n\noption_name\n\n\t\n\nTEXT\n\n\t\n\nName of an option.\n\n\n\n\noption_value\n\n\t\n\nTEXT\n\n\t\n\nValue of the option. The value is visible only to the user being mapped and to superusers otherwise it will show as a NULL."
  },
  {
    "title": "Querying — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/general/dql/index.html",
    "html": "master\nQuerying\n\nThis section provides an overview of how to query CrateDB.\n\nSee Also\n\nGeneral use: Data definition\n\nGeneral use: Data manipulation\n\nTable of contents\n\nSelecting data\nIntroduction\nFROM clause\nJoins\nDISTINCT clause\nWHERE clause\nComparison operators\nArray comparisons\nEXISTS\nContainer data types\nAggregation\nWindow functions\nGROUP BY\nWITH Queries (Common Table Expressions)\nJoins\nCross joins\nInner joins\nOuter joins\nJoin conditions\nAvailable join algorithms\nLimitations\nUnion\nUnion All\nUnion Distinct\nUnion of object types\nUnion of different types\nRefresh\nIntroduction\nMultiple Table Refresh\nPartition Refresh\nFulltext search\nMATCH Predicate\nUsage\nSearching On Multiple Columns\nNegative Search\nFilter By _score\nGeo search\nIntroduction\nMATCH predicate\nExact queries"
  },
  {
    "title": "Optimistic Concurrency Control — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/general/occ.html",
    "html": "master\nOptimistic Concurrency Control\n\nTable of contents\n\nIntroduction\n\nOptimistic update\n\nOptimistic delete\n\nKnown limitations\n\nIntroduction\n\nEven though CrateDB does not support transactions, Optimistic Concurrency Control can be achieved by using the internal system columns _seq_no and _primary_term.\n\nEvery new primary shard row has an initial sequence number of 0. This value is increased by 1 on every insert, delete or update operation the primary shard executes. The primary term will be incremented when a shard is promoted to primary so the user can know if they are executing an update against the most up to date cluster configuration.\n\nIt’s possible to fetch the _seq_no and _primary_term by selecting them:\n\ncr> SELECT id, type, _seq_no, _primary_term FROM sensors ORDER BY 1;\n+-----+-------+---------+---------------+\n| id  | type  | _seq_no | _primary_term |\n+-----+-------+---------+---------------+\n| ID1 | DHT11 |       0 |             1 |\n| ID2 | DHT21 |       0 |             1 |\n+-----+-------+---------+---------------+\nSELECT 2 rows in set (... sec)\n\n\nThese _seq_no and _primary_term values can now be used on updates and deletes.\n\nNote\n\nOptimistic concurrency control only works using the = operator, checking for the exact _seq_no and _primary_term your update or delete is based on.\n\nOptimistic update\n\nQuerying for the correct _seq_no and _primary_term ensures that no concurrent update and cluster configuration change has taken place:\n\ncr> UPDATE sensors SET last_verification = '2020-01-10 09:40'\n... WHERE\n...   id = 'ID1'\n...   AND \"_seq_no\" = 0\n...   AND \"_primary_term\" = 1;\nUPDATE OK, 1 row affected (... sec)\n\n\nUpdating a row with a wrong or outdated sequence number or primary term will not execute the update and results in 0 affected rows:\n\ncr> UPDATE sensors SET last_verification = '2020-01-10 09:40'\n... WHERE\n...   id = 'ID1'\n...   AND \"_seq_no\" = 42\n...   AND \"_primary_term\" = 5;\nUPDATE OK, 0 rows affected (... sec)\n\nOptimistic delete\n\nThe same can be done when deleting a row:\n\ncr> DELETE FROM sensors WHERE id = 'ID2'\n...   AND \"_seq_no\" = 0\n...   AND \"_primary_term\" = 1;\nDELETE OK, 1 row affected (... sec)\n\nKnown limitations\n\nThe _seq_no and _primary_term columns can only be used when specifying the whole primary key in a query. For example, the query below is not possible with the database schema used for testing, because type is not declared as a primary key:\n\ncr> DELETE FROM sensors WHERE type = 'DHT11'\n...   AND \"_seq_no\" = 3\n...   AND \"_primary_term\" = 1;\nUnsupportedFeatureException[\"_seq_no\" and \"_primary_term\" columns can only be used\ntogether in the WHERE clause with equals comparisons and if there are also equals\ncomparisons on primary key columns]\n\n\nIn order to use the optimistic concurrency control mechanism, both the _seq_no and _primary_term columns need to be specified. It is not possible to only specify one of them. For example, the query below will result in an error:\n\ncr> DELETE FROM sensors WHERE id = 'ID1' AND \"_seq_no\" = 3;\nVersioningValidationException[\"_seq_no\" and \"_primary_term\" columns can only be used\ntogether in the WHERE clause with equals comparisons and if there are also equals\ncomparisons on primary key columns]\n\n\nNote\n\nBoth DELETE and UPDATE commands will return a row count of 0, if the given required version does not match the actual version of the relevant row."
  },
  {
    "title": "User-defined functions — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/general/user-defined-functions.html",
    "html": "master\nUser-defined functions\n\nTable of contents\n\nCREATE OR REPLACE\n\nSupported types\n\nOverloading\n\nDeterminism\n\nDROP FUNCTION\n\nSupported languages\n\nJavaScript\n\nJavaScript supported types\n\nWorking with NUMBERS\n\nCREATE OR REPLACE\n\nCrateDB supports user-defined functions. See CREATE FUNCTION for a full syntax description.\n\nCREATE FUNCTION defines a new function:\n\ncr> CREATE FUNCTION my_subtract_function(integer, integer)\n... RETURNS integer\n... LANGUAGE JAVASCRIPT\n... AS 'function my_subtract_function(a, b) { return a - b; }';\nCREATE OK, 1 row affected  (... sec)\n\ncr> SELECT doc.my_subtract_function(3, 1) AS col;\n+-----+\n| col |\n+-----+\n|   2 |\n+-----+\nSELECT 1 row in set (... sec)\n\n\nCREATE OR REPLACE FUNCTION will either create a new function or replace an existing function definition:\n\ncr> CREATE OR REPLACE FUNCTION log10(bigint)\n... RETURNS double precision\n... LANGUAGE JAVASCRIPT\n... AS 'function log10(a) {return Math.log(a)/Math.log(10); }';\nCREATE OK, 1 row affected  (... sec)\n\ncr> SELECT doc.log10(10) AS col;\n+-----+\n| col |\n+-----+\n| 1.0 |\n+-----+\nSELECT 1 row in set (... sec)\n\n\nIt is possible to use named function arguments in the function signature. For example, the calculate_distance function signature has two geo_point arguments named start and end:\n\ncr> CREATE OR REPLACE FUNCTION calculate_distance(\"start\" geo_point, \"end\" geo_point)\n... RETURNS real\n... LANGUAGE JAVASCRIPT\n... AS 'function calculate_distance(start, end) {\n...       return Math.sqrt(\n...            Math.pow(end[0] - start[0], 2),\n...            Math.pow(end[1] - start[1], 2));\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\n\nNote\n\nArgument names are used for query documentation purposes only. You cannot reference arguments by name in the function body.\n\nOptionally, a schema-qualified function name can be defined. If you omit the schema, the current session schema is used:\n\ncr> CREATE OR REPLACE FUNCTION my_schema.log10(bigint)\n... RETURNS double precision\n... LANGUAGE JAVASCRIPT\n... AS 'function log10(a) { return Math.log(a)/Math.log(10); }';\nCREATE OK, 1 row affected  (... sec)\n\n\nNote\n\nIn order to improve the PostgreSQL server compatibility CrateDB allows the creation of user defined functions against the pg_catalog schema. However, the creation of user defined functions against the read-only System information and Information schema schemas is prohibited.\n\nSupported types\n\nFunction arguments and return values can be any of the supported data types. The values passed into a function must strictly correspond to the specified argument data types.\n\nNote\n\nThe value returned by the function will be casted to the return type provided in the definition if required. An exception will be thrown if the cast is not successful.\n\nOverloading\n\nWithin a specific schema, you can overload functions by defining functions with the same name but a different set of arguments:\n\ncr> CREATE FUNCTION my_schema.my_multiply(integer, integer)\n... RETURNS integer\n... LANGUAGE JAVASCRIPT\n... AS 'function my_multiply(a, b) { return a * b; }';\nCREATE OK, 1 row affected  (... sec)\n\n\nThis would overload the my_multiply function with different argument types:\n\ncr> CREATE FUNCTION my_schema.my_multiply(bigint, bigint)\n... RETURNS bigint\n... LANGUAGE JAVASCRIPT\n... AS 'function my_multiply(a, b) { return a * b; }';\nCREATE OK, 1 row affected  (... sec)\n\n\nThis would overload the my_multiply function with more arguments:\n\ncr> CREATE FUNCTION my_schema.my_multiply(bigint, bigint, bigint)\n... RETURNS bigint\n... LANGUAGE JAVASCRIPT\n... AS 'function my_multiply(a, b, c) { return a * b * c; }';\nCREATE OK, 1 row affected  (... sec)\n\n\nCaution\n\nIt is considered bad practice to create functions that have the same name as the CrateDB built-in functions.\n\nNote\n\nIf you call a function without a schema name, CrateDB will look it up in the built-in functions first and only then in the user-defined functions available in the search_path.\n\nTherefore a built-in function with the same name as a user-defined function will hide the latter, even if it contains a different set of arguments. However, such functions can still be called if the schema name is explicitly provided.\n\nDeterminism\n\nCaution\n\nUser-defined functions need to be deterministic, meaning that they must always return the same result value when called with the same argument values, because CrateDB might cache the returned values and reuse the value if the function is called multiple times with the same arguments.\n\nDROP FUNCTION\n\nFunctions can be dropped like this:\n\ncr> DROP FUNCTION doc.log10(bigint);\nDROP OK, 1 row affected  (... sec)\n\n\nAdding IF EXISTS prevents from raising an error if the function doesn’t exist:\n\ncr> DROP FUNCTION IF EXISTS doc.log10(integer);\nDROP OK, 1 row affected  (... sec)\n\n\nOptionally, argument names can be specified within the drop statement:\n\ncr> DROP FUNCTION IF EXISTS doc.calculate_distance(start_point geo_point, end_point geo_point);\nDROP OK, 1 row affected  (... sec)\n\n\nOptionally, you can provide a schema:\n\ncr> DROP FUNCTION my_schema.log10(bigint);\nDROP OK, 1 row affected  (... sec)\n\nSupported languages\n\nCurrently, CrateDB only supports JavaScript for user-defined functions.\n\nJavaScript\n\nThe user defined function JavaScript is compatible with the ECMAScript 2019 specification.\n\nCrateDB uses the GraalVM JavaScript engine as a JavaScript (ECMAScript) language execution runtime. The GraalVM JavaScript engine is a Java application that works on the stock Java Virtual Machines (VMs). The interoperability between Java code (host language) and JavaScript user-defined functions (guest language) is guaranteed by the GraalVM Polyglot API.\n\nPlease note: CrateDB does not use the GraalVM JIT compiler as optimizing compiler. However, the stock host Java VM JIT compilers can JIT-compile, optimize, and execute the GraalVM JavaScript codebase to a certain extent.\n\nThe execution context for guest JavaScript is created with restricted privileges to allow for the safe execution of less trusted guest language code. The guest language application context for each user-defined function is created with default access modifiers, so any access to managed resources is denied. The only exception is the host language interoperability configuration which explicitly allows access to Java lists and arrays. Please refer to GraalVM Security Guide for more detailed information.\n\nAlso, even though user-defined functions implemented with ECMA-compliant JavaScript, objects that are normally accessible with a web browser (e.g. window, console, and so on) are not available.\n\nNote\n\nGraalVM treats objects provided to JavaScript user-defined functions as close as possible to their respective counterparts and therefore by default only a subset of prototype functions are available in user-defined functions. For CrateDB 4.6 and earlier the object prototype was disabled.\n\nPlease refer to the GraalVM JavaScript Compatibility FAQ to learn more about the compatibility.\n\nJavaScript supported types\n\nJavaScript functions can handle all CrateDB data types. However, for some return types the function output must correspond to the certain format.\n\nIf a function requires geo_point as a return type, then the JavaScript function must return a double precision array of size 2, WKT string or GeoJson object.\n\nHere is an example of a JavaScript function returning a double array:\n\ncr> CREATE FUNCTION rotate_point(point geo_point, angle real)\n... RETURNS geo_point\n... LANGUAGE JAVASCRIPT\n... AS 'function rotate_point(point, angle) {\n...       var cos = Math.cos(angle);\n...       var sin = Math.sin(angle);\n...       var x = cos * point[0] - sin * point[1];\n...       var y = sin * point[0] + cos * point[1];\n...       return [x, y];\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\n\nBelow is an example of a JavaScript function returning a WKT string, which will be cast to geo_point:\n\ncr> CREATE FUNCTION symmetric_point(point geo_point)\n... RETURNS geo_point\n... LANGUAGE JAVASCRIPT\n... AS 'function symmetric_point (point, angle) {\n...       var x = - point[0],\n...           y = - point[1];\n...       return \"POINT (\\\" + x + \\\", \\\" + y +\\\")\";\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\n\nSimilarly, if the function specifies the geo_shape return data type, then the JavaScript function should return a GeoJson object or WKT string:\n\ncr> CREATE FUNCTION line(\"start\" array(double precision), \"end\" array(double precision))\n... RETURNS object\n... LANGUAGE JAVASCRIPT\n... AS 'function line(start, end) {\n...        return { \"type\": \"LineString\", \"coordinates\" : [start_point, end_point] };\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\n\nNote\n\nIf the return value of the JavaScript function is undefined, it is converted to NULL.\n\nWorking with NUMBERS\n\nThe JavaScript engine interprets numbers as java.lang.Double, java.lang.Long, or java.lang.Integer, depending on the computation performed. In most cases, this is not an issue, since the return type of the JavaScript function will be cast to the return type specified in the CREATE FUNCTION statement, although cast might result in a loss of precision.\n\nHowever, when you try to cast DOUBLE PRECISION to TIMESTAMP WITH TIME ZONE, it will be interpreted as UTC seconds and will result in a wrong value:\n\ncr> CREATE FUNCTION utc(bigint, bigint, bigint)\n... RETURNS TIMESTAMP WITH TIME ZONE\n... LANGUAGE JAVASCRIPT\n... AS 'function utc(year, month, day) {\n...       return Date.UTC(year, month, day, 0, 0, 0);\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\ncr> SELECT date_format(utc(2016,04,6)) as epoque;\n+------------------------------+\n| epoque                       |\n+------------------------------+\n| 48314-07-22T00:00:00.000000Z |\n+------------------------------+\nSELECT 1 row in set (... sec)\n\n\nTo avoid this behavior, the numeric value should be divided by 1000 before it is returned:\n\ncr> CREATE FUNCTION utc(bigint, bigint, bigint)\n... RETURNS TIMESTAMP WITH TIME ZONE\n... LANGUAGE JAVASCRIPT\n... AS 'function utc(year, month, day) {\n...       return Date.UTC(year, month, day, 0, 0, 0)/1000;\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\ncr> SELECT date_format(utc(2016,04,6)) as epoque;\n+-----------------------------+\n| epoque                      |\n+-----------------------------+\n| 2016-05-06T00:00:00.000000Z |\n+-----------------------------+\nSELECT 1 row in set (... sec)\n"
  },
  {
    "title": "Built-in functions and operators — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/general/builtins/index.html",
    "html": "master\nBuilt-in functions and operators\n\nThis chapter provides an overview of built-in functions and operators.\n\nTable of contents\n\nScalar functions\nString functions\nDate and time functions\nGeo functions\nMathematical functions\nRegular expression functions\nArray functions\nObject functions\nConditional functions and expressions\nSystem information functions\nSpecial functions\nAggregation\nAggregate expressions\nAggregate functions\nLimitations\nArithmetic operators\nBit operators\nTable functions\nScalar functions\nempty_row( )\nunnest( array [ array , ] )\npg_catalog.generate_series(start, stop, [step])\npg_catalog.generate_subscripts(array, dim, [reverse])\nregexp_matches(source, pattern [, flags])\npg_catalog.pg_get_keywords()\ninformation_schema._pg_expandarray(array)\nComparison operators\nBasic operators\nWHERE clause operators\nArray comparisons\nIN (value [, ...])\nANY/SOME (array expression)\nALL (array_expression)\nSubquery expressions\nIN (subquery)\nANY/SOME (subquery)\nALL (subquery)\nWindow functions\nWindow function call\nWindow definition\nGeneral-purpose window functions\nAggregate window functions"
  },
  {
    "title": "Data definition — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/general/ddl/index.html",
    "html": "master\nData definition\n\nThis section provides an overview of how to create tables and perform other data-definition related operations with CrateDB.\n\nSee Also\n\nGeneral use: Data manipulation\n\nGeneral use: Querying\n\nTable of contents\n\nCreating tables\nTable definition\nTable configuration\nData types\nOverview\nPrimitive types\nContainer types\nFLOAT_VECTOR\nGeographic types\nType casting\nPostgreSQL compatibility\nSystem columns\nGenerated columns\nGeneration expressions\nLast modified dates\nPartitioning\nConstraints\nPRIMARY KEY\nNULL\nNOT NULL\nCHECK\nStorage\nColumn store\nPartitioned tables\nIntroduction\nCreation\nInformation schema\nInsert\nUpdate\nDelete\nQuerying\nAlter\nLimitations\nConsistency notes related to concurrent DML statement\nSharding\nIntroduction\nNumber of shards\nRouting\nReplication\nTable configuration\nShard recovery\nUnderreplication\nShard allocation filtering\nSettings\nSpecial attributes\nColumn policy\nstrict\ndynamic\nFulltext indices\nIndex definition\nDisable indexing\nPlain index (default)\nCreating a custom analyzer\nExtending a built-in analyzer\nFulltext analyzers\nOverview\nBuilt-in analyzers\nBuilt-in tokenizers\nBuilt-in token filters\nBuilt-in char filter\nShow Create Table\nViews\nCreating views\nQuerying views\nDropping views\nAltering tables\nUpdating parameters\nAdding columns\nRenaming columns\nClosing and opening tables\nRenaming tables\nReroute shards"
  },
  {
    "title": "Environment variables — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/config/environment.html",
    "html": "master\nEnvironment variables\n\nCrateDB can be configured with some environment variables.\n\nThere are many different ways to set environment variables, depending on how CrateDB is being deployed.\n\nHere is a trivial example:\n\nsh$ export CRATE_HOME=/tmp/crate\nsh$ ./bin/crate\n\n\nHere, we set CRATE_HOME to /tmp/crate, export it so that sub-processes of the shell have access, and then start CrateDB.\n\nCrateDB supports two kinds of environment variables:\n\nApplication variables\n\nJava Virtual Machine (JVM) variables\n\nWe use “application” here to distinguish between CrateDB running as a Java application and the JVM itself, which runs CrateDB.\n\nTable of contents\n\nApplication variables\n\nJVM variables\n\nGeneral\n\nApplication variables\nCRATE_HOME: directory path\n\nThe home directory of the CrateDB installation.\n\nThis directory is used as the root for the configuration directory, data directory, log directory, and so on.\n\nIf you have installed CrateDB from a package, this variable should be set for you.\n\nIf you are installing manually, in most cases, this should be set to the directory from which you would normally execute bin/crate, i.e. the root directory of the basic installation.\n\nJVM variables\nGeneral\nCRATE_JAVA_OPTS: Java options\n\nThe Java options to use when running CrateDB.\n\nFor example, you could change the stack size like this:\n\nCRATE_JAVA_OPTS=-Xss500k\n\n\nSee Also\n\nFor more information about Java options, consult the documentation for Microsoft Windows or Unix-like operating systems.\n\nCRATE_HEAP_SIZE: size\n\nThe Java heap size, i.e. the amount of memory that can be used.\n\nYou can set the heap size to four gigabytes like this:\n\nCRATE_HEAP_SIZE=4g\n\n\nUse g for gigabytes or m for megabytes.\n\nSee Also\n\nAppropriate memory configuration is important for optimal performance.\n\nCRATE_HEAP_DUMP_PATH: file or directory path (default: varies)\n\nThe directory to be used for heap dumps in the case of a crash.\n\nIf a directory path is configured, new heap dumps will be written to that directory every time CrateDB crashes.\n\nIf a file path is configured (i.e. the last node of the path is non-existent or exists and is a file) CrateDB will overwrite that file with a heap dump every time it crashes.\n\nDefault values are as follows:\n\nFor a basic installation, the process working directory\n\nIf you have installed a CrateDB Linux package, /var/lib/crate\n\nWhen running CrateDB on Docker, /data/data\n\nWarning\n\nMake sure there is enough disk space available for heap dumps."
  },
  {
    "title": "Session settings — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/config/session.html",
    "html": "master\nSession settings\n\nTable of contents\n\nUsage\n\nSupported session settings\n\nSession settings only apply to the currently connected client session.\n\nUsage\n\nTo configure a modifiable session setting, use SET, for example:\n\nSET search_path TO myschema, doc;\n\n\nTo retrieve the current value of a session setting, use SHOW e.g:\n\nSHOW search_path;\n\n\nBesides using SHOW, it is also possible to use the current_setting scalar function.\n\nSupported session settings\nsearch_path\nDefault: pg_catalog, doc\nModifiable: yes\n\nThe list of schemas to be searched when a relation is referenced without a schema.\n\nCrateDB will try to resolve an unqualified relation name against the configured search_path by iterating over the configured schemas in the order they were declared. The first matching relation in the search_path is used. CrateDB will report an error if there is no match.\n\nNote\n\nThis setting mirrors the PostgreSQL search_path setting.\n\nSome PostgreSQL clients require access to various tables in the pg_catalog schema. Usually, this is to extract information about built-in data types or functions.\n\nCrateDB implements the system pg_catalog schema and it automatically includes it in the search_path before the configured schemas, unless it is already explicitly in the schema configuration.\n\napplication_name\nDefault: null\nModifiable: yes\n\nAn arbitrary application name that can be set to identify an application that connects to a CrateDB node.\n\nSome clients set this implicitly to their client name.\n\nstatement_timeout\nDefault: '0'\nModifiable: yes\n\nThe maximum duration of any statement before it gets cancelled. If 0 (the default), queries are allowed to run infinitely and don’t get cancelled automatically.\n\nThe value is an INTERVAL with a maximum of 2147483647 milliseconds. That’s roughly 24 days.\n\nmemory.operation_limit\nDefault: 0\nModifiable: yes\n\nThis is an experimental expert setting defining the maximal amount of memory in bytes that an individual operation can consume before triggering an error.\n\n0 means unlimited. In that case only the global circuit breaker limits apply.\n\nThere is no 1:1 mapping from SQL statement to operation. Some SQL statements have no corresponding operation. Other SQL statements can have more than one operation. You can use the sys.operations view to get some insights, but keep in mind that both, operations which are used to execute a query, and their name could change with any release, including hotfix releases.\n\nenable_hashjoin\nDefault: true\nModifiable: yes\n\nAn experimental setting which enables CrateDB to consider whether a JOIN operation should be evaluated using the HashJoin implementation instead of the Nested-Loops implementation.\n\nNote\n\nIt is not always possible or efficient to use the HashJoin implementation. Having this setting enabled, will only add the option of considering it, it will not guarantee it. See also the available join algorithms for more insights on this topic.\n\nerror_on_unknown_object_key\nDefault: true\nModifiable: yes\n\nThis setting controls the behaviour of querying unknown object keys to dynamic objects. CrateDB will throw an error by default if any of the queried object keys are unknown or will return a null if the setting is set to false.\n\ndatestyle\nDefault: ISO\nModifiable: yes\n\nShows the display format for date and time values. Only the ISO style is supported. Optionally provided pattern conventions for the order of date parts (Day, Month, Year) are ignored.\n\nNote\n\nThe session setting currently has no effect in CrateDB and exists for compatibility with PostgreSQL. Trying to set this to a date format style other than ISO will raise an exception.\n\nmax_index_keys\nDefault: 32\nModifiable: no\n\nShows the maximum number of index keys.\n\nNote\n\nThe session setting has no effect in CrateDB and exists for compatibility with PostgreSQL.\n\nmax_identifier_length\nDefault: 255\nModifiable: no\n\nShows the maximum length of identifiers in bytes.\n\nserver_version_num\nDefault: 100500\nModifiable: no\n\nShows the emulated PostgreSQL server version.\n\nserver_version\nDefault: 10.5\nModifiable: no\n\nShows the emulated PostgreSQL server version.\n\nstandard_conforming_strings\nDefault: on\nModifiable: no\n\nCauses '...' strings to treat backslashes literally.\n\noptimizer\nDefault: true\nModifiable: yes\n\nThis setting indicates whether a query optimizer rule is activated. The name of the query optimizer rule has to be provided as a suffix as part of the setting e.g. SET optimizer_rewrite_collect_to_get = false.\n\nNote\n\nThe optimizer setting is for advanced use only and can significantly impact the performance behavior of the queries.\n\noptimizer_eliminate_cross_join\nDefault: true\nModifiable: yes\n\nThis setting indicates if the cross join elimination rule of the optimizer rule is activated.\n\nWarning\n\nExperimental session settings might be removed in the future even in minor feature releases."
  },
  {
    "title": "Data manipulation — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/general/dml.html",
    "html": "master\nData manipulation\n\nThis section provides an overview of how to manipulate data (e.g., inserting rows) with CrateDB.\n\nSee Also\n\nGeneral use: Data definition\n\nGeneral use: Querying\n\nTable of contents\n\nInserting data\n\nInserting data by query\n\nUpserts (ON CONFLICT DO UPDATE SET)\n\nUpdating data\n\nDeleting data\n\nImport and export\n\nImporting data\n\nExample\n\nDetailed error reporting\n\nExporting data\n\nInserting data\n\nInserting data to CrateDB is done by using the SQL INSERT statement.\n\nNote\n\nThe column list is always ordered based on the column position in the CREATE TABLE statement of the table. If the insert columns are omitted, the values in the VALUES clauses must correspond to the table columns in that order.\n\nInserting a row:\n\ncr> insert into locations (id, date, description, kind, name, position)\n... values (\n...   '14',\n...   '2013-09-12T21:43:59.000Z',\n...   'Blagulon Kappa is the planet to which the police are native.',\n...   'Planet',\n...   'Blagulon Kappa',\n...   7\n... );\nINSERT OK, 1 row affected (... sec)\n\n\nWhen inserting rows with the VALUES clause all data is validated in terms of data types compatibility and compliance with defined constraints, and if there are any issues an error message is returned and no rows are inserted.\n\nInserting multiple rows at once (aka. bulk insert) can be done by defining multiple values for the INSERT statement:\n\ncr> insert into locations (id, date, description, kind, name, position) values\n... (\n...   '16',\n...   '2013-09-14T21:43:59.000Z',\n...   'Blagulon Kappa II is the planet to which the police are native.',\n...   'Planet',\n...   'Blagulon Kappa II',\n...   19\n... ),\n... (\n...   '17',\n...   '2013-09-13T16:43:59.000Z',\n...   'Brontitall is a planet with a warm, rich atmosphere and no mountains.',\n...   'Planet',\n...   'Brontitall',\n...   10\n... );\nINSERT OK, 2 rows affected (... sec)\n\n\nWhen inserting into tables containing Generated columns or Base Columns having the Default clause specified, their values can be safely omitted. They are generated upon insert:\n\ncr> CREATE TABLE debit_card (\n...   owner text,\n...   num_part1 integer,\n...   num_part2 integer,\n...   check_sum integer GENERATED ALWAYS AS ((num_part1 + num_part2) * 42),\n...   \"user\" text DEFAULT 'crate'\n... );\nCREATE OK, 1 row affected (... sec)\n\ncr> insert into debit_card (owner, num_part1, num_part2) values\n... ('Zaphod Beeblebrox', 1234, 5678);\nINSERT OK, 1 row affected (... sec)\n\ncr> select * from debit_card;\n+-------------------+-----------+-----------+-----------+-------+\n| owner             | num_part1 | num_part2 | check_sum | user  |\n+-------------------+-----------+-----------+-----------+-------+\n| Zaphod Beeblebrox |      1234 |      5678 |    290304 | crate |\n+-------------------+-----------+-----------+-----------+-------+\nSELECT 1 row in set (... sec)\n\n\nFor Generated columns, if the value is given, it is validated against the generation clause of the column and the currently inserted row:\n\ncr> insert into debit_card (owner, num_part1, num_part2, check_sum) values\n... ('Arthur Dent', 9876, 5432, 642935);\nSQLParseException[Given value 642935 for generated column check_sum does not match calculation ((num_part1 + num_part2) * 42) = 642936]\n\nInserting data by query\n\nIt is possible to insert data using a query instead of values. Column data types of source and target table can differ as long as the values are castable. This gives the opportunity to restructure the tables data, renaming a field, changing a field’s data type or convert a normal table into a partitioned one.\n\nCaution\n\nWhen inserting data from a query, there is no error message returned when rows fail to be inserted, they are instead skipped, and the number of rows affected is decreased to reflect the actual number of rows for which the operation succeeded.\n\nExample of changing a field’s data type, in this case, changing the position data type from integer to smallint:\n\ncr> create table locations2 (\n...     id text primary key,\n...     name text,\n...     date timestamp with time zone,\n...     kind text,\n...     position smallint,\n...     description text\n... ) clustered by (id) into 2 shards with (number_of_replicas = 0);\nCREATE OK, 1 row affected (... sec)\n\ncr> insert into locations2 (id, name, date, kind, position, description)\n... (\n...     select id, name, date, kind, position, description\n...     from locations\n...     where position < 10\n... );\nINSERT OK, 14 rows affected (... sec)\n\n\nExample of creating a new partitioned table out of the locations table with data partitioned by year:\n\ncr> create table locations_parted (\n...     id text primary key,\n...     name text,\n...     year text primary key,\n...     date timestamp with time zone,\n...     kind text,\n...     position integer\n... ) clustered by (id) into 2 shards\n... partitioned by (year) with (number_of_replicas = 0);\nCREATE OK, 1 row affected (... sec)\n\ncr> insert into locations_parted (id, name, year, date, kind, position)\n... (\n...     select\n...         id,\n...         name,\n...         date_format('%Y', date),\n...         date,\n...         kind,\n...         position\n...     from locations\n... );\nINSERT OK, 16 rows affected (... sec)\n\n\nResulting partitions of the last insert by query:\n\ncr> select table_name, partition_ident, values, number_of_shards, number_of_replicas\n... from information_schema.table_partitions\n... where table_name = 'locations_parted'\n... order by partition_ident;\n+------------------+-----------------+------------------+------------------+--------------------+\n| table_name       | partition_ident | values           | number_of_shards | number_of_replicas |\n+------------------+-----------------+------------------+------------------+--------------------+\n| locations_parted | 042j2e9n74      | {\"year\": \"1979\"} |                2 |                  0 |\n| locations_parted | 042j4c1h6c      | {\"year\": \"2013\"} |                2 |                  0 |\n+------------------+-----------------+------------------+------------------+--------------------+\nSELECT 2 rows in set (... sec)\n\n\nNote\n\nlimit, offset and order by are not supported inside the query statement.\n\nUpserts (ON CONFLICT DO UPDATE SET)\n\nThe ON CONFLICT DO UPDATE SET clause is used to update the existing row if inserting is not possible because of a duplicate-key conflict if a document with the same PRIMARY KEY already exists. This is type of operation is commonly referred to as an upsert, being a combination of “update” and “insert”.\n\ncr> SELECT\n...     name,\n...     visits,\n...     extract(year from last_visit) AS last_visit\n... FROM uservisits ORDER BY NAME;\n+----------+--------+------------+\n| name     | visits | last_visit |\n+----------+--------+------------+\n| Ford     |      1 | 2013       |\n| Trillian |      3 | 2013       |\n+----------+--------+------------+\nSELECT 2 rows in set (... sec)\n\ncr> INSERT INTO uservisits (id, name, visits, last_visit) VALUES\n... (\n...     0,\n...     'Ford',\n...     1,\n...     '2015-01-12'\n... ) ON CONFLICT (id) DO UPDATE SET\n...     visits = visits + 1;\nINSERT OK, 1 row affected (... sec)\n\ncr> SELECT\n...     name,\n...     visits,\n...     extract(year from last_visit) AS last_visit\n... FROM uservisits WHERE id = 0;\n+------+--------+------------+\n| name | visits | last_visit |\n+------+--------+------------+\n| Ford |      2 | 2013       |\n+------+--------+------------+\nSELECT 1 row in set (... sec)\n\n\nIt’s possible to refer to values which would be inserted if no duplicate-key conflict occurred, by using the special excluded table. This table is especially useful in multiple-row inserts, to refer to the current rows values:\n\ncr> INSERT INTO uservisits (id, name, visits, last_visit) VALUES\n... (\n...     0,\n...     'Ford',\n...     2,\n...     '2016-01-13'\n... ),\n... (\n...     1,\n...     'Trillian',\n...     5,\n...     '2016-01-15'\n... ) ON CONFLICT (id) DO UPDATE SET\n...     visits = visits + excluded.visits,\n...     last_visit = excluded.last_visit;\nINSERT OK, 2 rows affected (... sec)\n\ncr> SELECT\n...     name,\n...     visits,\n...     extract(year from last_visit) AS last_visit\n... FROM uservisits ORDER BY name;\n+----------+--------+------------+\n| name     | visits | last_visit |\n+----------+--------+------------+\n| Ford     |      4 | 2016       |\n| Trillian |      8 | 2016       |\n+----------+--------+------------+\nSELECT 2 rows in set (... sec)\n\n\nThis can also be done when using a query instead of values:\n\ncr> CREATE TABLE uservisits2 (\n...   id integer primary key,\n...   name text,\n...   visits integer,\n...   last_visit timestamp with time zone\n... ) CLUSTERED BY (id) INTO 2 SHARDS WITH (number_of_replicas = 0);\nCREATE OK, 1 row affected (... sec)\n\ncr> INSERT INTO uservisits2 (id, name, visits, last_visit)\n... (\n...     SELECT id, name, visits, last_visit\n...     FROM uservisits\n... );\nINSERT OK, 2 rows affected (... sec)\n\ncr> INSERT INTO uservisits2 (id, name, visits, last_visit)\n... (\n...     SELECT id, name, visits, last_visit\n...     FROM uservisits\n... ) ON CONFLICT (id) DO UPDATE SET\n...     visits = visits + excluded.visits,\n...     last_visit = excluded.last_visit;\nINSERT OK, 2 rows affected (... sec)\n\ncr> SELECT\n...     name,\n...     visits,\n...     extract(year from last_visit) AS last_visit\n... FROM uservisits ORDER BY name;\n+----------+--------+------------+\n| name     | visits | last_visit |\n+----------+--------+------------+\n| Ford     |      4 | 2016       |\n| Trillian |      8 | 2016       |\n+----------+--------+------------+\nSELECT 2 rows in set (... sec)\n\n\nSee Also\n\nSQL syntax: ON CONFLICT DO UPDATE SET\n\nUpdating data\n\nIn order to update documents in CrateDB the SQL UPDATE statement can be used:\n\ncr> update locations set description = 'Updated description'\n... where name = 'Bartledan';\nUPDATE OK, 1 row affected (... sec)\n\n\nUpdating nested objects is also supported:\n\ncr> update locations set inhabitants['name'] = 'Human' where name = 'Bartledan';\nUPDATE OK, 1 row affected (... sec)\n\n\nIt’s also possible to reference a column within the expression, for example to increment a number like this:\n\ncr> update locations set position = position + 1 where position < 3;\nUPDATE OK, 6 rows affected (... sec)\n\n\nNote\n\nIf the same documents are updated concurrently an VersionConflictException might occur. CrateDB contains a retry logic that tries to resolve the conflict automatically.\n\nDeleting data\n\nDeleting rows in CrateDB is done using the SQL DELETE statement:\n\ncr> delete from locations where position > 3;\nDELETE OK, ... rows affected (... sec)\n\nImport and export\nImporting data\n\nUsing the COPY FROM statement, CrateDB nodes can import data from local files or files that are available over the network.\n\nThe supported data formats are JSON and CSV. The format is inferred from the file extension, if possible. Alternatively the format can also be provided as an option (see WITH). If the format is not provided and cannot be inferred from the file extension, it will be processed as JSON.\n\nJSON files must contain a single JSON object per line.\n\nExample JSON data:\n\n{\"id\": 1, \"quote\": \"Don't panic\"}\n{\"id\": 2, \"quote\": \"Ford, you're turning into a penguin. Stop it.\"}\n\n\nCSV files must contain a header with comma-separated values, which will be added as columns.\n\nExample CSV data:\n\nid,quote\n1,\"Don't panic\"\n2,\"Ford, you're turning into a penguin. Stop it.\"\n\n\nNote\n\nThe COPY FROM statement will convert and validate your data.\n\nValues for generated columns will be computed if the data does not contain them, otherwise they will be imported and validated\n\nFurthermore, column names in your data are considered case sensitive (as if they were quoted in a SQL statement).\n\nFor further information, including how to import data to Partitioned tables, take a look at the COPY FROM reference.\n\nExample\n\nHere’s an example statement:\n\ncr> COPY quotes FROM 'file:///tmp/import_data/quotes.json';\nCOPY OK, 3 rows affected (... sec)\n\n\nThis statement imports data from the /tmp/import_data/quotes.json file into a table named quotes.\n\nNote\n\nThe file you specify must be available on one of the CrateDB nodes. This statement will not work with files that are local to your client.\n\nFor the above statement, every node in the cluster will attempt to import data from a file located at /tmp/import_data/quotes.json relative to the crate process (i.e., if you are running CrateDB inside a container, the file must also be inside the container).\n\nIf you want to import data from a file that on your local computer using COPY FROM, you must first transfer the file to one of the CrateDB nodes.\n\nConsult the COPY FROM reference for additional information.\n\nIf you want to import all files inside the /tmp/import_data directory on every CrateDB node, you can use a wildcard, like so:\n\ncr> COPY quotes FROM '/tmp/import_data/*' WITH (bulk_size = 4);\nCOPY OK, 3 rows affected (... sec)\n\n\nThis wildcard can also be used to only match certain files in a directory:\n\ncr> COPY quotes FROM '/tmp/import_data/qu*.json';\nCOPY OK, 3 rows affected (... sec)\n\nDetailed error reporting\n\nIf the RETURN_SUMMARY clause is specified, a result set containing information about failures and successfully imported records is returned.\n\ncr> COPY locations FROM '/tmp/import_data/locations_with_failure/locations*.json' RETURN SUMMARY;\n +--...--+----------...--------+---------------+-------------+--------------------...-------------------------------------+\n | node  | uri                 | success_count | error_count | errors                                                     |\n +--...--+----------...--------+---------------+-------------+--------------------...-------------------------------------+\n | {...} | .../locations1.json |             6 |           0 | {}                                                         |\n | {...} | .../locations2.json |             5 |           2 | {\"Cannot cast value...{\"count\": ..., \"line_numbers\": ...}} |\n +--...--+----------...--------+---------------+-------------+--------------------...-------------------------------------+\n COPY 2 rows in set (... sec)\n\n\nIf an error happens while processing the URI in general, the error_count and success_count columns will contains NULL values to indicate that no records were processed.\n\ncr> COPY locations FROM '/tmp/import_data/not-existing.json' RETURN SUMMARY;\n +--...--+-----------...---------+---------------+-------------+------------------------...------------------------+\n | node  | uri                   | success_count | error_count | errors                                            |\n +--...--+-----------...---------+---------------+-------------+------------------------...------------------------+\n | {...} | .../not-existing.json |          NULL |        NULL | {\"...not-existing.json (...)\": {\"count\": 1, ...}} |\n +--...--+-----------...---------+---------------+-------------+------------------------...------------------------+\nCOPY 1 row in set (... sec)\n\n\nSee COPY FROM for more information.\n\nExporting data\n\nData can be exported using the COPY TO statement. Data is exported in a distributed way, meaning each node will export its own data.\n\nReplicated data is not exported. So every row of an exported table is stored only once.\n\nThis example shows how to export a given table into files named after the table and shard ID with gzip compression:\n\ncr> REFRESH TABLE quotes;\nREFRESH OK...\n\ncr> COPY quotes TO DIRECTORY '/tmp/' with (compression='gzip');\nCOPY OK, 3 rows affected ...\n\n\nInstead of exporting a whole table, rows can be filtered by an optional WHERE clause condition. This is useful if only a subset of the data needs to be exported:\n\ncr> COPY quotes WHERE match(quote_ft, 'time') TO DIRECTORY '/tmp/' WITH (compression='gzip');\nCOPY OK, 2 rows affected ...\n\n\nFor further details see COPY TO."
  },
  {
    "title": "Logging — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/config/logging.html",
    "html": "master\nLogging\n\nCrateDB supports two kinds of logging:\n\nApplication logging with Log4j\n\nJava Virtual Machine (JVM) garbage collection logging\n\nWe use “application” here to distinguish between CrateDB running as a Java application and the JVM itself, which runs CrateDB.\n\nBecause garbage collection logging is a native feature of the JVM it behaves differently and is configured differently.\n\nTable of contents\n\nApplication logging\n\nLog4j\n\nConfiguration file\n\nLog levels\n\nRun-time configuration\n\nJVM logging\n\nGarbage collection\n\nEnvironment variables\n\nApplication logging\nLog4j\n\nCrateDB uses Log4j.\n\nConfiguration file\n\nYou can configure Log4j with the log4j2.properties file in the CrateDB configuration directory.\n\nThe log4j2.properties file is formatted using YAML and simplifies Log4j configuration by allowing you to use the PropertyConfigurator but without having to tediously repeat the log4j prefix.\n\nHere’s one example:\n\nrootLogger.level = info\nrootLogger.appenderRef.console.ref = console\n\n# log query execution errors for easier debugging\nlogger.action.name = org.crate.action.sql\nlogger.action.level = debug\n\nappender.console.type = Console\nappender.console.name = console\nappender.console.layout.type = PatternLayout\nappender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c{1.}] %marker%m%n\n\n\nAnd here is a snippet of the generated properties ready for use with log4j. You get the point.\n\nSee Also\n\nConsult the PropertyConfigurator documentation or the configuration section of the Log4j documentation for more information.\n\nLog levels\n\nPossible log levels are the same as for Log4j, in order of increasing importance:\n\nTRACE\n\nDEBUG\n\nINFO\n\nWARN\n\nERROR\n\nLog levels must be provided as string literals in the SET statement.\n\nNote\n\nBe careful using the TRACE log level because it’s extremely verbose, can obscure other important log messages and even fill up entire data disks in some cases.\n\nRun-time configuration\n\nIt’s possible to set the log level of loggers at runtime using SET, like so:\n\nSET GLOBAL TRANSIENT \"logger.action\" = 'INFO';\n\n\nIn this example, the log level INFO is applied to the action logger.\n\nIn addition to being able to configure any of the standard loggers, you can configure the root (i.e. default) logger using logger._root.\n\nAs with any setting, you can inspect the current configuration by querying the sys.cluster table.\n\nTip\n\nRun-time logging configuration is particularly useful if you are debugging a problem and you want to increase the log level without restarting nodes.\n\nRun-time logging configuration is applied across the whole cluster, and overrides the start-up configuration defined in each respective log4j2.properties file.\n\nCaution\n\nThe RESET statement is supported but logging configuration is only reset when the whole cluster is restarted.\n\nJVM logging\n\nCrateDB exposes some native JVM logging functionality.\n\nGarbage collection\n\nCrateDB logs JVM garbage collection times using the built-in garbage collection logging of the JVM.\n\nEnvironment variables\n\nThe following environment variables can be used to configure garbage collection logging.\n\nCRATE_DISABLE_GC_LOGGING: boolean integer (default: 0)\n\nWhether to disable garbage collection logging.\n\nSet to 1 to disable.\n\nNote\n\nSince CrateDB 3.0, Garbage collection logging is enabled by default.\n\nCRATE_GC_LOG_DIR: path to logs directory (default: varies)\n\nThe log file directory.\n\nFor a basic installation, the logs directory in the CRATE_HOME directory is the default.\n\nf you have installed a CrateDB Linux package, the default directory is /var/log/crate instead.\n\nCRATE_GC_LOG_SIZE: file size (default: 64m)\n\nMaximum file size of log files before they are rotated.\n\nCRATE_GC_LOG_FILES: number (default: 16)\n\nThe amount of files kept in rotation.\n\nCaution\n\nWith the default configuration of 16 rotated 64 megabyte log files, garbage collection logs will grow to occupy one gigabyte on disk.\n\nMake sure you have enough available disk space for configuration."
  },
  {
    "title": "Cluster-wide settings — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/config/cluster.html",
    "html": "master\nCluster-wide settings\n\nAll current applied cluster settings can be read by querying the sys.cluster.settings column. Most cluster settings can be changed at runtime. This is documented at each setting.\n\nTable of contents\n\nNon-runtime cluster-wide settings\n\nCollecting stats\n\nShard limits\n\nUsage data collector\n\nGraceful stop\n\nBulk operations\n\nDiscovery\n\nUnicast host discovery\n\nDiscovery via DNS\n\nDiscovery on Amazon EC2\n\nRouting allocation\n\nShard balancing\n\nAttribute-based shard allocation\n\nCluster-wide attribute awareness\n\nCluster-wide attribute filtering\n\nDisk-based shard allocation\n\nRecovery\n\nMemory management\n\nQuery circuit breaker\n\nRequest circuit breaker\n\nAccounting circuit breaker\n\nStats circuit breakers\n\nTotal circuit breaker\n\nThread pools\n\nSettings for fixed thread pools\n\nOverload Protection\n\nMetadata\n\nMetadata gateway\n\nLogical Replication\n\nNon-runtime cluster-wide settings\n\nCluster wide settings which cannot be changed at runtime need to be specified in the configuration of each node in the cluster.\n\nCaution\n\nCluster settings specified via node configurations are required to be exactly the same on every node in the cluster for proper operation of the cluster.\n\nCollecting stats\nstats.enabled\nDefault: true\nRuntime: yes\n\nA boolean indicating whether or not to collect statistical information about the cluster.\n\nCaution\n\nThe collection of statistical information incurs a slight performance penalty, as details about every job and operation across the cluster will cause data to be inserted into the corresponding system tables.\n\nstats.jobs_log_size\nDefault: 10000\nRuntime: yes\n\nThe maximum number of job records kept to be kept in the sys.jobs_log table on each node.\n\nA job record corresponds to a single SQL statement to be executed on the cluster. These records are used for performance analytics. A larger job log produces more comprehensive stats, but uses more RAM.\n\nOlder job records are deleted as newer records are added, once the limit is reached.\n\nSetting this value to 0 disables collecting job information.\n\nstats.jobs_log_expiration\nDefault: 0s (disabled)\nRuntime: yes\n\nThe job record expiry time in seconds.\n\nJob records in the sys.jobs_log table are periodically cleared if they are older than the expiry time. This setting overrides stats.jobs_log_size.\n\nIf the value is set to 0, time based log entry eviction is disabled.\n\nNote\n\nIf both the stats.operations_log_size and stats.operations_log_expiration settings are disabled, jobs will not be recorded.\n\nstats.jobs_log_filter\nDefault: true (Include everything)\nRuntime: yes\n\nAn expression to determine if a job should be recorded into sys.jobs_log. The expression must evaluate to a boolean. If it evaluates to true the statement will show up in sys.jobs_log until it’s evicted due to one of the other rules. (expiration or size limit reached).\n\nThe expression may reference all columns contained in sys.jobs_log. A common use case is to include only jobs that took a certain amount of time to execute:\n\ncr> SET GLOBAL \"stats.jobs_log_filter\" = $$ended - started > '5 minutes'::interval$$;\n\nstats.jobs_log_persistent_filter\nDefault: false (Include nothing)\nRuntime: yes\n\nAn expression to determine if a job should also be recorded to the regular CrateDB log. Entries that match this filter will be logged under the StatementLog logger with the INFO level.\n\nThis is similar to stats.jobs_log_filter except that these entries are persisted to the log file. This should be used with caution and shouldn’t be set to an expression that matches many queries as the logging operation will block on IO and can therefore affect performance.\n\nA common use case is to use this for slow query logging.\n\nstats.operations_log_size\nDefault: 10000\nRuntime: yes\n\nThe maximum number of operations records to be kept in the sys.operations_log table on each node.\n\nA job consists of one or more individual operations. Operations records are used for performance analytics. A larger operations log produces more comprehensive stats, but uses more RAM.\n\nOlder operations records are deleted as newer records are added, once the limit is reached.\n\nSetting this value to 0 disables collecting operations information.\n\nstats.operations_log_expiration\nDefault: 0s (disabled)\nRuntime: yes\n\nEntries of sys.operations_log are cleared by a periodically job when they are older than the specified expire time. This setting overrides stats.operations_log_size. If the value is set to 0 the time based log entry eviction is disabled.\n\nNote\n\nIf both settings stats.operations_log_size and stats.operations_log_expiration are disabled, no job information will be collected.\n\nstats.service.interval\nDefault: 24h\nRuntime: yes\n\nDefines the refresh interval to refresh tables statistics used to produce optimal query execution plans.\n\nThis field expects a time value either as a bigint or double precision or alternatively as a string literal with a time suffix (ms, s, m, h, d, w).\n\nIf the value provided is 0 then the refresh is disabled.\n\nCaution\n\nUsing a very small value can cause a high load on the cluster.\n\nstats.service.max_bytes_per_sec\nDefault: 40mb\nRuntime: yes\n\nSpecifies the maximum number of bytes per second that can be read on data nodes to collect statistics. If this is set to a positive number, the underlying I/O operations of the ANALYZE statement are throttled.\n\nIf the value provided is 0 then the throttling is disabled.\n\nShard limits\ncluster.max_shards_per_node\nDefault: 1000\nRuntime: yes\n\nThe maximum amount of shards per node.\n\nAny operations that would result in the creation of additional shard copies that would exceed this limit are rejected.\n\nFor example. If you have 999 shards in the current cluster and you try to create a new table, the create table operation will fail.\n\nSimilarly, if a write operation would lead to the creation of a new partition, the statement will fail.\n\nEach shard on a node requires some memory and increases the size of the cluster state. Having too many shards per node will impact the clusters stability and it is therefore discouraged to raise the limit above 1000.\n\nNote\n\nThe maximum amount of shards per node setting is also used for the Maximum shards per node check.\n\nUsage data collector\n\nThe settings of the Usage Data Collector are read-only and cannot be set during runtime. Please refer to Usage Data Collector to get further information about its usage.\n\nudc.enabled\nDefault: true\nRuntime: no\n\ntrue: Enables the Usage Data Collector.\n\nfalse: Disables the Usage Data Collector.\n\nudc.initial_delay\nDefault: 10m\nRuntime: no\n\nThe delay for first ping after start-up.\n\nThis field expects a time value either as a bigint or double precision or alternatively as a string literal with a time suffix (ms, s, m, h, d, w).\n\nudc.interval\nDefault: 24h\nRuntime: no\n\nThe interval a UDC ping is sent.\n\nThis field expects a time value either as a bigint or double precision or alternatively as a string literal with a time suffix (ms, s, m, h, d, w).\n\nudc.url\nDefault: https://udc.crate.io\nRuntime: no\n\nThe URL the ping is sent to.\n\nGraceful stop\n\nBy default, when the CrateDB process stops it simply shuts down, possibly making some shards unavailable which leads to a red cluster state and lets some queries fail that required the now unavailable shards. In order to safely shutdown a CrateDB node, the graceful stop procedure can be used.\n\nThe following cluster settings can be used to change the shutdown behaviour of nodes of the cluster:\n\ncluster.graceful_stop.min_availability\nDefault: primaries\nRuntime: yes\nAllowed values: none | primaries | full\n\nnone: No minimum data availability is required. The node may shut down even if records are missing after shutdown.\n\nprimaries: At least all primary shards need to be available after the node has shut down. Replicas may be missing.\n\nfull: All records and all replicas need to be available after the node has shut down. Data availability is full.\n\nNote\n\nThis option is ignored if there is only 1 node in a cluster!\n\ncluster.graceful_stop.timeout\nDefault: 2h\nRuntime: yes\n\nDefines the maximum waiting time in milliseconds for the reallocation process to finish. The force setting will define the behaviour when the shutdown process runs into this timeout.\n\nThe timeout expects a time value either as a bigint or double precision or alternatively as a string literal with a time suffix (ms, s, m, h, d, w).\n\ncluster.graceful_stop.force\nDefault: false\nRuntime: yes\n\nDefines whether graceful stop should force stopping of the node if it runs into the timeout which is specified with the cluster.graceful_stop.timeout setting.\n\nBulk operations\n\nSQL DML Statements involving a huge amount of rows like COPY FROM, INSERT or UPDATE can take an enormous amount of time and resources. The following settings change the behaviour of those queries.\n\nbulk.request_timeout\nDefault: 1m\nRuntime: yes\n\nDefines the timeout of internal shard-based requests involved in the execution of SQL DML Statements over a huge amount of rows.\n\nDiscovery\n\nData sharding and work splitting are at the core of CrateDB. This is how we manage to execute very fast queries over incredibly large datasets. In order for multiple CrateDB nodes to work together a cluster needs to be formed. The process of finding other nodes with which to form a cluster is called discovery. Discovery runs when a CrateDB node starts and when a node is not able to reach the master node and continues until a master node is found or a new master node is elected.\n\ndiscovery.seed_hosts\nDefault: 127.0.0.1\nRuntime: no\n\nIn order to form a cluster with CrateDB instances running on other nodes a list of seed master-eligible nodes needs to be provided. This setting should normally contain the addresses of all the master-eligible nodes in the cluster. In order to seed the discovery process the nodes listed here must be live and contactable. This setting contains either an array of hosts or a comma-delimited string. By default a node will bind to the available loopback and scan for local ports between 4300 and 4400 to try to connect to other nodes running on the same server. This default behaviour provides local auto clustering without any configuration. Each value should be in the form of host:port or host (where port defaults to the setting transport.tcp.port).\n\nNote\n\nIPv6 hosts must be bracketed.\n\ncluster.initial_master_nodes\nDefault: not set\nRuntime: no\n\nContains a list of node names, full-qualified hostnames or IP addresses of the master-eligible nodes which will vote in the very first election of a cluster that’s bootstrapping for the first time. By default this is not set, meaning it expects this node to join an already formed cluster. In development mode, with no discovery settings configured, this step is performed by the nodes themselves, but this auto-bootstrapping is designed to aim development and is not safe for production. In production you must explicitly list the names or IP addresses of the master-eligible nodes whose votes should be counted in the very first election.\n\ndiscovery.type\nDefault: zen\nRuntime: no\nAllowed values: zen | single-node\n\nSpecifies whether CrateDB should form a multiple-node cluster. By default, CrateDB discovers other nodes when forming a cluster and allows other nodes to join the cluster later. If discovery.type is set to single-node, CrateDB forms a single-node cluster and the node won’t join any other clusters. This can be useful for testing. It is not recommend to use this for production setups. The single-node mode also skips bootstrap checks.\n\nCaution\n\nIf a node is started without any initial_master_nodes or a discovery_type set to single-node (e.g., the default configuration), it will never join a cluster even if the configuration is subsequently changed.\n\nIt is possible to force the node to forget its current cluster state by using the crate-node CLI tool. However, be aware that this may result in data loss.\n\nUnicast host discovery\n\nAs described above, CrateDB has built-in support for statically specifying a list of addresses that will act as the seed nodes in the discovery process using the discovery.seed_hosts setting.\n\nCrateDB also has support for several different mechanisms of seed nodes discovery. Currently there are two other discovery types: via DNS and via EC2 API.\n\nWhen a node starts up with one of these discovery types enabled, it performs a lookup using the settings for the specified mechanism listed below. The hosts and ports retrieved from the mechanism will be used to generate a list of unicast hosts for node discovery.\n\nThe same lookup is also performed by all nodes in a cluster whenever the master is re-elected (see Cluster Meta Data).\n\ndiscovery.seed_providers\nDefault: not set\nRuntime: no\nAllowed values: srv, ec2\n\nSee also: Discovery.\n\nDiscovery via DNS\n\nCrate has built-in support for discovery via DNS. To enable DNS discovery the discovery.seed_providers setting needs to be set to srv.\n\nThe order of the unicast hosts is defined by the priority, weight and name of each host defined in the SRV record. For example:\n\n_crate._srv.example.com. 3600 IN SRV 2 20 4300 crate1.example.com.\n_crate._srv.example.com. 3600 IN SRV 1 10 4300 crate2.example.com.\n_crate._srv.example.com. 3600 IN SRV 2 10 4300 crate3.example.com.\n\n\nwould result in a list of discovery nodes ordered like:\n\ncrate2.example.com:4300, crate3.example.com:4300, crate1.example.com:4300\n\ndiscovery.srv.query\nRuntime: no\n\nThe DNS query that is used to look up SRV records, usually in the format _service._protocol.fqdn If not set, the service discovery will not be able to look up any SRV records.\n\ndiscovery.srv.resolver\nRuntime: no\n\nThe hostname or IP of the DNS server used to resolve DNS records. If this is not set, or the specified hostname/IP is not resolvable, the default (system) resolver is used.\n\nOptionally a custom port can be specified using the format hostname:port.\n\nDiscovery on Amazon EC2\n\nCrateDB has built-in support for discovery via the EC2 API. To enable EC2 discovery the discovery.seed_providers settings needs to be set to ec2.\n\ndiscovery.ec2.access_key\nRuntime: no\n\nThe access key ID to identify the API calls.\n\ndiscovery.ec2.secret_key\nRuntime: no\n\nThe secret key to identify the API calls.\n\nFollowing settings control the discovery:\n\ndiscovery.ec2.groups\nRuntime: no\n\nA list of security groups; either by ID or name. Only instances with the given group will be used for unicast host discovery.\n\ndiscovery.ec2.any_group\nDefault: true\nRuntime: no\n\nDefines whether all (false) or just any (true) security group must be present for the instance to be used for discovery.\n\ndiscovery.ec2.host_type\nDefault: private_ip\nRuntime: no\nAllowed values: private_ip, public_ip, private_dns, public_dns\n\nDefines via which host type to communicate with other instances.\n\ndiscovery.ec2.availability_zones\nRuntime: no\n\nA list of availability zones. Only instances within the given availability zone will be used for unicast host discovery.\n\ndiscovery.ec2.tag.<name>\nRuntime: no\n\nEC2 instances for discovery can also be filtered by tags using the discovery.ec2.tag. prefix plus the tag name.\n\nE.g. to filter instances that have the environment tags with the value dev your setting will look like: discovery.ec2.tag.environment: dev.\n\ndiscovery.ec2.endpoint\nRuntime: no\n\nIf you have your own compatible implementation of the EC2 API service you can set the endpoint that should be used.\n\nRouting allocation\ncluster.routing.allocation.enable\nDefault: all\nRuntime: yes\nAllowed values: all | none | primaries | new_primaries\n\nall allows all shard allocations, the cluster can allocate all kinds of shards.\n\nnone allows no shard allocations at all. No shard will be moved or created.\n\nprimaries only primaries can be moved or created. This includes existing primary shards.\n\nnew_primaries allows allocations for new primary shards only. This means that for example a newly added node will not allocate any replicas. However it is still possible to allocate new primary shards for new indices. Whenever you want to perform a zero downtime upgrade of your cluster you need to set this value before gracefully stopping the first node and reset it to all after starting the last updated node.\n\nNote\n\nThis allocation setting has no effect on the recovery of primary shards! Even when cluster.routing.allocation.enable is set to none, nodes will recover their unassigned local primary shards immediately after restart.\n\ncluster.routing.rebalance.enable\nDefault: all\nRuntime: yes\nAllowed values: all | none | primaries | replicas\n\nEnables or disables rebalancing for different types of shards:\n\nall allows shard rebalancing for all types of shards.\n\nnone disables shard rebalancing for any types.\n\nprimaries allows shard rebalancing only for primary shards.\n\nreplicas allows shard rebalancing only for replica shards.\n\ncluster.routing.allocation.allow_rebalance\nDefault: indices_all_active\nRuntime: yes\nAllowed values: always | indices_primary_active | indices_all_active\n\nDefines when rebalancing will happen based on the total state of all the indices shards in the cluster.\n\nDefaults to indices_all_active to reduce chatter during initial recovery.\n\ncluster.routing.allocation.cluster_concurrent_rebalance\nDefault: 2\nRuntime: yes\n\nDefines how many concurrent rebalancing tasks are allowed across all nodes.\n\ncluster.routing.allocation.node_initial_primaries_recoveries\nDefault: 4\nRuntime: yes\n\nDefines how many concurrent primary shard recoveries are allowed on a node.\n\nSince primary recoveries use data that is already on disk (as opposed to inter-node recoveries), recovery should be fast and so this setting can be higher than node_concurrent_recoveries.\n\ncluster.routing.allocation.node_concurrent_recoveries\nDefault: 2\nRuntime: yes\n\nDefines how many concurrent recoveries are allowed on a node.\n\nShard balancing\n\nYou can configure how CrateDB attempts to balance shards across a cluster by specifying one or more property weights. CrateDB will consider a cluster to be balanced when no further allowed action can bring the weighted properties of each node closer together.\n\nNote\n\nBalancing may be restricted by other settings (e.g., attribute-based and disk-based shard allocation).\n\ncluster.routing.allocation.balance.shard\nDefault: 0.45f\nRuntime: yes\n\nDefines the weight factor for shards allocated on a node (float). Raising this raises the tendency to equalize the number of shards across all nodes in the cluster.\n\ncluster.routing.allocation.balance.index\nDefault: 0.55f\nRuntime: yes\n\nDefines a factor to the number of shards per index allocated on a specific node (float). Increasing this value raises the tendency to equalize the number of shards per index across all nodes in the cluster.\n\ncluster.routing.allocation.balance.threshold\nDefault: 1.0f\nRuntime: yes\n\nMinimal optimization value of operations that should be performed (non negative float). Increasing this value will cause the cluster to be less aggressive about optimising the shard balance.\n\nAttribute-based shard allocation\n\nYou can control how shards are allocated to specific nodes by setting custom attributes on each node (e.g., server rack ID or node availability zone). After doing this, you can define cluster-wide attribute awareness and then configure cluster-wide attribute filtering.\n\nSee Also\n\nFor an in-depth example of using custom node attributes, check out the multi-zone setup how-to guide.\n\nCluster-wide attribute awareness\n\nTo make use of custom attributes for attribute-based shard allocation, you must configure cluster-wide attribute awareness.\n\ncluster.routing.allocation.awareness.attributes\nRuntime: no\n\nYou may define custom node attributes which can then be used to do awareness based on the allocation of a shard and its replicas.\n\nFor example, let’s say we want to use an attribute named rack_id. We start two nodes with node.attr.rack_id set to rack_one. Then we create a single table with five shards and one replica. The table will be fully deployed on the current nodes (five shards and one replica each, making a total of 10 shards).\n\nNow, if we start two more nodes with node.attr.rack_id set to rack_two, CrateDB will relocate shards to even out the number of shards across the nodes. However, a shard and its replica will not be allocated to nodes sharing the same rack_id value.\n\nThe awareness.attributes setting supports using several values.\n\ncluster.routing.allocation.awareness.force.*.values\nRuntime: no\n\nAttributes on which shard allocation will be forced. Here, * is a placeholder for the awareness attribute, which can be configured using the cluster.routing.allocation.awareness.attributes setting.\n\nFor example, let’s say we configured forced shard allocation for an awareness attribute named zone with values set to zone1, zone2. Start two nodes with node.attr.zone set to zone1. Then, create a table with five shards and one replica. The table will be created, but only five shards will be allocated (with no replicas). The replicas will only be allocated when we start one or more nodes with node.attr.zone set to zone2.\n\nCluster-wide attribute filtering\n\nTo control how CrateDB uses custom attributes for attribute-based shard allocation, you must configure cluster-wide attribute filtering.\n\nNote\n\nCrateDB will retroactively enforce filter definitions. If a new filter would prevent newly created matching shards from being allocated to a node, CrateDB would also move any existing matching shards away from that node.\n\ncluster.routing.allocation.include.*\nRuntime: yes\n\nOnly allocate shards on nodes where at least one of the specified values matches the attribute.\n\nFor example:\n\ncluster.routing.allocation.include.zone: \"zone1,zone2\"`\n\ncluster.routing.allocation.exclude.*\nRuntime: yes\n\nOnly allocate shards on nodes where none of the specified values matches the attribute.\n\nFor example:\n\ncluster.routing.allocation.exclude.zone: \"zone1\"\n\ncluster.routing.allocation.require.*\nRuntime: yes\n\nUsed to specify a number of rules, which all of them must match for a node in order to allocate a shard on it.\n\nDisk-based shard allocation\ncluster.routing.allocation.disk.threshold_enabled\nDefault: true\nRuntime: yes\n\nPrevent shard allocation on nodes depending of the disk usage.\n\ncluster.routing.allocation.disk.watermark.low\nDefault: 85%\nRuntime: yes\n\nDefines the lower disk threshold limit for shard allocations. New shards will not be allocated on nodes with disk usage greater than this value. It can also be set to an absolute bytes value (like e.g. 500mb) to prevent the cluster from allocating new shards on node with less free disk space than this value.\n\ncluster.routing.allocation.disk.watermark.high\nDefault: 90%\nRuntime: yes\n\nDefines the higher disk threshold limit for shard allocations. The cluster will attempt to relocate existing shards to another node if the disk usage on a node rises above this value. It can also be set to an absolute bytes value (like e.g. 500mb) to relocate shards from nodes with less free disk space than this value.\n\ncluster.routing.allocation.disk.watermark.flood_stage\nDefault: 95%\nRuntime: yes\n\nDefines the threshold on which CrateDB enforces a read-only block on every index that has at least one shard allocated on a node with at least one disk exceeding the flood stage.\n\nNote\n\nblocks.read_only_allow_delete setting is automatically reset to FALSE for the tables if the disk space is freed and the threshold is undershot.\n\ncluster.routing.allocation.disk.watermark settings may be defined as percentages or bytes values. However, it is not possible to mix the value types.\n\nBy default, the cluster will retrieve information about the disk usage of the nodes every 30 seconds. This can also be changed by setting the cluster.info.update.interval setting.\n\nNote\n\nThe watermark settings are also used for the Routing allocation disk watermark low and Routing allocation disk watermark high node check.\n\nSetting cluster.routing.allocation.disk.threshold_enabled to false will disable the allocation decider, but the node checks will still be active and warn users about running low on disk space.\n\ncluster.routing.allocation.total_shards_per_node\nDefault: -1\nRuntime: yes\n\nLimits the number of shards that can be allocated per node. A value of -1 means unlimited.\n\nSetting this to 1000, for example, will prevent CrateDB from assigning more than 1000 shards per node. A node with 1000 shards would be excluded from allocation decisions and CrateDB would attempt to allocate shards to other nodes, or leave shards unassigned if no suitable node can be found.\n\nRecovery\nindices.recovery.max_bytes_per_sec\nDefault: 40mb\nRuntime: yes\n\nSpecifies the maximum number of bytes that can be transferred during shard recovery per seconds. Limiting can be disabled by setting it to 0. This setting allows to control the network usage of the recovery process. Higher values may result in higher network utilization, but also faster recovery process.\n\nindices.recovery.retry_delay_state_sync\nDefault: 500ms\nRuntime: yes\n\nDefines the time to wait after an issue caused by cluster state syncing before retrying to recover.\n\nindices.recovery.retry_delay_network\nDefault: 5s\nRuntime: yes\n\nDefines the time to wait after an issue caused by the network before retrying to recover.\n\nindices.recovery.internal_action_timeout\nDefault: 15m\nRuntime: yes\n\nDefines the timeout for internal requests made as part of the recovery.\n\nindices.recovery.internal_action_long_timeout\nDefault: 30m\nRuntime: yes\n\nDefines the timeout for internal requests made as part of the recovery that are expected to take a long time. Defaults to twice internal_action_timeout.\n\nindices.recovery.recovery_activity_timeout\nDefault: 30m\nRuntime: yes\n\nRecoveries that don’t show any activity for more then this interval will fail. Defaults to internal_action_long_timeout.\n\nindices.recovery.max_concurrent_file_chunks\nDefault: 2\nRuntime: yes\n\nControls the number of file chunk requests that can be sent in parallel per recovery. As multiple recoveries are already running in parallel, controlled by cluster.routing.allocation.node_concurrent_recoveries, increasing this expert-level setting might only help in situations where peer recovery of a single shard is not reaching the total inbound and outbound peer recovery traffic as configured by indices.recovery.max_bytes_per_sec, but is CPU-bound instead, typically when using transport-level security or compression.\n\nMemory management\nmemory.allocation.type\nDefault: on-heap\nRuntime: yes\n\nSupported values are on-heap and off-heap. This influences if memory is preferably allocated in the heap space or in the off-heap/direct memory region.\n\nSetting this to off-heap doesn’t imply that the heap won’t be used anymore. Most allocations will still happen in the heap space but some operations will be allowed to utilize off heap buffers.\n\nWarning\n\nUsing off-heap is considered experimental.\n\nmemory.operation_limit\nDefault: 0\nRuntime: yes\n\nDefault value for the memory.operation_limit session setting. Changing the cluster setting will only affect new sessions, not existing sessions.\n\nQuery circuit breaker\n\nThe Query circuit breaker will keep track of the used memory during the execution of a query. If a query consumes too much memory or if the cluster is already near its memory limit it will terminate the query to ensure the cluster keeps working.\n\nindices.breaker.query.limit\nDefault: 60%\nRuntime: yes\n\nSpecifies the limit for the query breaker. Provided values can either be absolute values (interpreted as a number of bytes), byte sizes (like 1mb) or percentage of the heap size (like 12%). A value of -1 disables breaking the circuit while still accounting memory usage.\n\nRequest circuit breaker\n\nThe request circuit breaker allows an estimation of required heap memory per request. If a single request exceeds the specified amount of memory, an exception is raised.\n\nindices.breaker.request.limit\nDefault: 60%\nRuntime: yes\n\nSpecifies the JVM heap limit for the request circuit breaker.\n\nAccounting circuit breaker\n\nTracks things that are held in memory independent of queries. For example the memory used by Lucene for segments.\n\nindices.breaker.accounting.limit\nDefault: 100%\nRuntime: yes\n\nSpecifies the JVM heap limit for the accounting circuit breaker\n\nCaution\n\nThis setting is deprecated and will be removed in a future release.\n\nStats circuit breakers\n\nSettings that control the behaviour of the stats circuit breaker. There are two breakers in place, one for the jobs log and one for the operations log. For each of them, the breaker limit can be set.\n\nstats.breaker.log.jobs.limit\nDefault: 5%\nRuntime: yes\n\nThe maximum memory that can be used from CRATE_HEAP_SIZE for the sys.jobs_log table on each node.\n\nWhen this memory limit is reached, the job log circuit breaker logs an error message and clears the sys.jobs_log table completely.\n\nstats.breaker.log.operations.limit\nDefault: 5%\nRuntime: yes\n\nThe maximum memory that can be used from CRATE_HEAP_SIZE for the sys.operations_log table on each node.\n\nWhen this memory limit is reached, the operations log circuit breaker logs an error message and clears the sys.operations_log table completely.\n\nTotal circuit breaker\nindices.breaker.total.limit\nDefault: 95%\nRuntime: yes\n\nThe maximum memory that can be used by all aforementioned circuit breakers together.\n\nEven if an individual circuit breaker doesn’t hit its individual limit, queries might still get aborted if several circuit breakers together would hit the memory limit configured in indices.breaker.total.limit.\n\nThread pools\n\nEvery node uses a number of thread pools to schedule operations, each pool is dedicated to specific operations. The most important pools are:\n\nwrite: Used for write operations like index, update or delete. The type defaults to fixed.\n\nsearch: Used for read operations like SELECT statements. The type defaults to fixed.\n\nget: Used for some specific read operations. For example on tables like sys.shards or sys.nodes. The type defaults to fixed.\n\nrefresh: Used for refresh operations. The type defaults to scaling.\n\ngeneric: For internal tasks like cluster state management. The type defaults to scaling.\n\nlogical_replication: For logical replication operations. The type defaults to fixed.\n\nIn addition to those pools, there are also netty worker threads which are used to process network requests and many CPU bound actions like query analysis and optimization.\n\nThe thread pool settings are expert settings which you generally shouldn’t need to touch. They are dynamically sized depending on the number of available CPU cores. If you’re running multiple services on the same machine you instead should change the processors setting.\n\nIncreasing the number of threads for a pool can result in degraded performance due to increased context switching and higher memory footprint.\n\nIf you observe idle CPU cores increasing the thread pool size is rarely the right course of action, instead it can be a sign that:\n\nOperations are blocked on disk IO. Increasing the thread pool size could result in more operations getting queued and blocked on disk IO without increasing throughput but decreasing it due to more memory pressure and additional garbage collection activity.\n\nIndividual operations running single threaded. Not all tasks required to process a SQL statement can be further subdivided and processed in parallel, but many operations default to use one thread per shard. Because of this, you can consider increasing the number of shards of a table to increase the parallelism of a single individual statement and increase CPU core utilization. As an alternative you can try increasing the concurrency on the client side, to have CrateDB process more SQL statements in parallel.\n\nthread_pool.<name>.type\nRuntime: no\nAllowed values: fixed | scaling\n\nfixed holds a fixed size of threads to handle the requests. It also has a queue for pending requests if no threads are available.\n\nscaling ensures that a thread pool holds a dynamic number of threads that are proportional to the workload.\n\nSettings for fixed thread pools\n\nIf the type of a thread pool is set to fixed there are a few optional settings.\n\nthread_pool.<name>.size\nRuntime: no\n\nNumber of threads. The default size of the different thread pools depend on the number of available CPU cores.\n\nthread_pool.<name>.queue_size\nDefault write: 200\nDefault search: 1000\nDefault get: 100\nRuntime: no\n\nSize of the queue for pending requests. A value of -1 sets it to unbounded. If you have burst workloads followed by periods of inactivity it can make sense to increase the queue_size to allow a node to buffer more queries before rejecting new operations. But be aware, increasing the queue size if you have sustained workloads will only increase the system’s memory consumption and likely degrade performance.\n\nOverload Protection\n\nOverload protection settings control how many resources operations like INSERT INTO FROM QUERY or COPY can use.\n\nThe values here serve as a starting point for an algorithm that dynamically adapts the effective concurrency limit based on the round-trip time of requests. Whenever one of these settings is updated, the previously calculated effective concurrency is reset.\n\nChanging settings will only effect new operations, already running operations will continue with the previous settings.\n\noverload_protection.dml.initial_concurrency\nDefault: 5\nRuntime: yes\n\nThe initial number of concurrent operations allowed per target node.\n\noverload_protection.dml.min_concurrency\nDefault: 1\nRuntime: yes\n\nThe minimum number of concurrent operations allowed per target node.\n\noverload_protection.dml.max_concurrency\nDefault: 2000\nRuntime: yes\n\nThe maximum number of concurrent operations allowed per target node.\n\noverload_protection.dml.queue_size\nDefault: 200\nRuntime: yes\n\nHow many operations are allowed to queue up.\n\nMetadata\ncluster.info.update.interval\nDefault: 30s\nRuntime: yes\n\nDefines how often the cluster collect metadata information (e.g. disk usages etc.) if no concrete event is triggered.\n\nMetadata gateway\n\nThe following settings can be used to configure the behavior of the metadata gateway.\n\ngateway.expected_nodes\nDefault: -1\nRuntime: no\n\nThe setting gateway.expected_nodes defines the total number of nodes expected in the cluster. It is evaluated together with gateway.recover_after_nodes to decide if the cluster can start with recovery.\n\nCaution\n\nThis setting is deprecated and will be removed in a future version. Use gateway.expected_data_nodes instead.\n\ngateway.expected_data_nodes\nDefault: -1\nRuntime: no\n\nThe setting gateway.expected_data_nodes defines the total number of data nodes expected in the cluster. It is evaluated together with gateway.recover_after_data_nodes to decide if the cluster can start with recovery.\n\ngateway.recover_after_time\nDefault: 5m\nRuntime: no\n\nThe gateway.recover_after_time setting defines the time to wait for the number of nodes set in gateway.expected_data_nodes (or gateway.expected_nodes) to become available, before starting the recovery, once the number of nodes defined in gateway.recover_after_data_nodes (or gateway.recover_after_nodes) has already been reached. This setting is ignored if gateway.expected_data_nodes or gateway.expected_nodes are set to 0 or 1. It also has no effect if gateway.recover_after_data_nodes is set equal to gateway.expected_data_nodes (or gateway.recover_after_nodes is set equal to gateway.expected_nodes). The cluster also proceeds to immediate recovery, and the default 5 minutes waiting time does not apply, if neither this setting nor expected_nodes and expected_data_nodes are explicitly set.\n\ngateway.recover_after_nodes\nDefault: -1\nRuntime: no\n\nThe gateway.recover_after_nodes setting defines the number of nodes that need to join the cluster before the cluster state recovery can start. If this setting is -1 and gateway.expected_nodes is set, all nodes will need to be started before the cluster state recovery can start. Please note that proceeding with recovery when not all nodes are available could trigger the promotion of shards and the creation of new replicas, generating disk and network load, which may be unnecessary. You can use a combination of this setting with gateway.recovery_after_time to mitigate this risk.\n\nCaution\n\nThis setting is deprecated and will be removed in CrateDB 5.0. Use gateway.recover_after_data_nodes instead.\n\ngateway.recover_after_data_nodes\nDefault: -1\nRuntime: no\n\nThe gateway.recover_after_data_nodes setting defines the number of data nodes that need to be started before the cluster state recovery can start. If this setting is -1 and gateway.expected_data_nodes is set, all data nodes will need to be started before the cluster state recovery can start. Please note that proceeding with recovery when not all data nodes are available could trigger the promotion of shards and the creation of new replicas, generating disk and network load, which may be unnecessary. You can use a combination of this setting with gateway.recovery_after_time to mitigate this risk.\n\nLogical Replication\n\nReplication process can be configured by the following settings. Settings are dynamic and can be changed in runtime.\n\nreplication.logical.ops_batch_size\nDefault: 50000\nMin value: 16\nRuntime: yes\n\nMaximum number of operations to replicate from the publisher cluster per poll. Represents a number to advance a sequence.\n\nreplication.logical.reads_poll_duration\nDefault: 50\nRuntime: yes\n\nThe maximum time (in milliseconds) to wait for changes per poll operation. When a subscriber makes another one request to a publisher, it has reads_poll_duration milliseconds to harvest changes from the publisher.\n\nreplication.logical.recovery.chunk_size\nDefault: 1MB\nMin value: 1KB\nMax value: 1GB\nRuntime: yes\n\nChunk size to transfer files during the initial recovery of a replicating table.\n\nreplication.logical.recovery.max_concurrent_file_chunks\nDefault: 2\nMin value: 1\nMax value: 5\nRuntime: yes\n\nControls the number of file chunk requests that can be sent in parallel between clusters during the recovery."
  },
  {
    "title": "[Legacy] CrateDB Clients and Tools — CrateDB: Clients and Tools",
    "url": "https://cratedb.com/docs/crate/clients-tools/en/latest/legacy.html",
    "html": "[Legacy] CrateDB Clients and Tools\n\nCrateDB is a distributed SQL database that makes it simple to store and analyze massive amounts of machine data in real-time.\n\nSee Also\n\nWe are also maintaining a more up-to-date list of CrateDB integration tutorials.\n\nTip\n\nCrateDB supports the PostgreSQL wire protocol. Accordingly, many clients that work with PostgreSQL also work with CrateDB.\n\nYou can try this out for yourself:\n\nConfigure a PostgreSQL connection, but point your client to a CrateDB server instead of a PostgreSQL server\n\nAuthenticate as the crate superuser with no password\n\nSpecify the doc schema, if you are asked for a database name\n\nFor example, your connection string to a local database might look like postgresql://crate@localhost:5432/doc\n\nCheck out the client compatibility notes and implementation differences for information about known limitations.\n\nIf you run into issues, please let us know using the Feedback section at the bottom of this page. We regularly update CrateDB to accomodate new PostgreSQL clients.\n\nClients\n\nBelow is a selection of CrateDB client libraries.\n\nPick your library, and start building!\n\nLanguage\n\n\t\n\nName\n\n\t\n\nMaintainer\n\n\t\n\nOfficial support\n\n\n\n\nC# (.NET)\n\n\t\n\nNpgsql\n\n\t\n\nCommunity\n\n\t\n\n✔️ (>= CrateDB 4.2.0)\n\n\n\n\nC# (.NET)\n\n\t\n\nCrateDB Npgsql fork\n\n\t\n\nCrate.IO\n\n\t\n\n✔️\n\n\n\n\nErlang\n\n\t\n\ncraterl\n\n\t\n\nCrate.IO\n\n\t\n\n\nGo\n\n\t\n\npgx\n\n\t\n\nCommunity\n\n\t\n\n✔️\n\n\n\n\nJava\n\n\t\n\nPostgreSQL JDBC\n\n\t\n\nCommunity\n\n\t\n\n✔️ (>= CrateDB 4.2, with known issues)\n\n\n\n\nJava\n\n\t\n\ncrate-jdbc\n\n\t\n\nCrate.IO\n\n\t\n\n✔️\n\n\n\n\nNode.JS\n\n\t\n\nnode-postgres\n\n\t\n\nCommunity\n\n\t\n\n✔️\n\n\n\n\nNode.JS\n\n\t\n\ncrate-connect\n\n\t\n\nCommunity\n\n\t\n\n\nNode.JS\n\n\t\n\ncratejs\n\n\t\n\nCommunity\n\n\t\n\n\nNode.JS\n\n\t\n\nnode-crate\n\n\t\n\nCommunity\n\n\t\n\n\nPerl\n\n\t\n\nDBD::Crate\n\n\t\n\nCommunity\n\n\t\n\n\nPHP\n\n\t\n\nCrateDB PDO\n\n\t\n\nCrate.IO\n\n\t\n\n✔️\n\n\n\n\nPHP\n\n\t\n\nCrateDB DBAL\n\n\t\n\nCrate.IO\n\n\t\n\n✔️\n\n\n\n\nPHP\n\n\t\n\nCrateDB driver for Laravel\n\n\t\n\nCommunity\n\n\t\n\n\nPython\n\n\t\n\ncrate-python\n\n\t\n\nCrate.IO\n\n\t\n\n✔️\n\n\n\n\nPython\n\n\t\n\nasyncpg\n\n\t\n\nCommunity\n\n\t\n\n✔️\n\n\n\n\nRuby\n\n\t\n\ncrate_ruby\n\n\t\n\nCrate.IO\n\n\t\n\n\nRuby\n\n\t\n\nactiverecord-crate-adaptor\n\n\t\n\nCrate.IO\n\n\t\n\n\nScala\n\n\t\n\ncrate-scala\n\n\t\n\nCommunity\n\n\t\n\n\nScala\n\n\t\n\ncrate-connector\n\n\t\n\nCommunity\n\n\t\nTools\n\nCrateDB integrates with many different tools. Some of these are:\n\nAzure Functions\n\nAn Azure Function is a short-lived, serverless computation that is triggered by external events. Learn how to create a data enrichment pipeline in our tutorial.\n\nGrafana\n\nGrafana allows you to query, visualize, alert on and understand your metrics and time series data. Read our tutorial on how to pair CrateDB with Grafana.\n\nPentaho\n\nPentaho prepares and blends data, delivering business analytics from any source. You can connect to CrateDB clusters by using Pentaho Kettle and the standalone version of our JDBC driver.\n\nR\n\nR is a programming language and software environment geared towards statistical computing. Read our guide on how to create a Machine Learning pipeline.\n\nSQLPad\n\nSQLPad is a web app for writing, running, and visualizing queries. Read our tutorial on how to set up CrateDB with SQLPad.\n\nStreamSets Data Collector\n\nThe StreamSets Data Collector is a lightweight and powerful engine that streams data in real time. Read our guide on building data stream pipelines.\n\nSee Also\n\nFor more tools and tutorials on how to use CrateDB with them, refer to the integrations category on our blog or the integrations section in our documentation.\n\nNote\n\nIf you would like to add items to this page, please get in touch or directly edit this page on GitHub. You will find corresponding links within the topmost right navigation element."
  },
  {
    "title": "Node-specific settings — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/config/node.html",
    "html": "master\nNode-specific settings\n\nTable of contents\n\nBasics\n\nNode types\n\nGeneral\n\nNetworking\n\nHosts\n\nPorts\n\nAdvanced TCP settings\n\nTransport settings\n\nPaths\n\nPlug-ins\n\nCPU\n\nMemory\n\nGarbage collection\n\nAuthentication\n\nTrust authentication\n\nHost-based authentication\n\nHBA entries\n\nSecured communications (SSL/TLS)\n\nCross-origin resource sharing (CORS)\n\nBlobs\n\nRepositories\n\nQueries\n\nLegacy\n\nJavaScript language\n\nForeign Data Wrappers\n\nCustom attributes\n\nBasics\ncluster.name\nDefault: crate\nRuntime: no\n\nThe name of the CrateDB cluster the node should join to.\n\nnode.name\nRuntime: no\n\nThe name of the node. If no name is configured a random one will be generated.\n\nNote\n\nNode names must be unique in a CrateDB cluster.\n\nnode.store.allow_mmap\nDefault: true\nRuntime: no\n\nThe setting indicates whether or not memory-mapping is allowed.\n\nNode types\n\nCrateDB supports different types of nodes.\n\nThe following settings can be used to differentiate nodes upon startup:\n\nnode.master\nDefault: true\nRuntime: no\n\nWhether or not this node is able to get elected as master node in the cluster.\n\nnode.data\nDefault: true\nRuntime: no\n\nWhether or not this node will store data.\n\nUsing different combinations of these two settings, you can create four different types of node. Each type of node is differentiated by what types of load it will handle.\n\nTabulating the truth values for node.master and node.data produces a truth table outlining the four different types of node:\n\n\t\n\nMaster\n\n\t\n\nNo master\n\n\n\n\nData\n\n\t\n\nHandle all loads.\n\n\t\n\nHandles client requests and query execution.\n\n\n\n\nNo data\n\n\t\n\nHandles cluster management.\n\n\t\n\nHandles client requests.\n\nNodes marked as node.master will only handle cluster management if they are elected as the cluster master. All other loads are shared equally.\n\nGeneral\nnode.sql.read_only\nDefault: false\nRuntime: no\n\nIf set to true, the node will only allow SQL statements which are resulting in read operations.\n\nstatement_timeout\nDefault: 0\nRuntime: yes\n\nThe maximum duration of any statement before it gets cancelled.\n\nThis value is used as default value for the statement_timeout session setting\n\nIf 0 queries are allowed to run infinitely and don’t get cancelled automatically.\n\nNote\n\nUpdating this setting won’t affect existing sessions, it will only take effect for new sessions.\n\nNetworking\nHosts\nnetwork.host\nDefault: _local_\nRuntime: no\n\nThe IP address CrateDB will bind itself to. This setting sets both the network.bind_host and network.publish_host values.\n\nnetwork.bind_host\nDefault: _local_\nRuntime: no\n\nThis setting determines to which address CrateDB should bind itself to.\n\nnetwork.publish_host\nDefault: _local_\nRuntime: no\n\nThis setting is used by a CrateDB node to publish its own address to the rest of the cluster.\n\nTip\n\nApart from IPv4 and IPv6 addresses there are some special values that can be used for all above settings:\n\n_local_\n\n\t\n\nAny loopback addresses on the system, for example 127.0.0.1.\n\n\n\n\n_site_\n\n\t\n\nAny site-local addresses on the system, for example 192.168.0.1.\n\n\n\n\n_global_\n\n\t\n\nAny globally-scoped addresses on the system, for example 8.8.8.8.\n\n\n\n\n_[INTERFACE]_\n\n\t\n\nAddresses of a network interface, for example _en0_.\n\nPorts\nhttp.port\nRuntime: no\n\nThis defines the TCP port range to which the CrateDB HTTP service will be bound to. It defaults to 4200-4300. Always the first free port in this range is used. If this is set to an integer value it is considered as an explicit single port.\n\nThe HTTP protocol is used for the REST endpoint which is used by all clients except the Java client.\n\nhttp.publish_port\nRuntime: no\n\nThe port HTTP clients should use to communicate with the node. It is necessary to define this setting if the bound HTTP port (http.port) of the node is not directly reachable from outside, e.g. running it behind a firewall or inside a Docker container.\n\ntransport.tcp.port\nRuntime: no\n\nThis defines the TCP port range to which the CrateDB transport service will be bound to. It defaults to 4300-4400. Always the first free port in this range is used. If this is set to an integer value it is considered as an explicit single port.\n\nThe transport protocol is used for internal node-to-node communication.\n\ntransport.publish_port\nRuntime: no\n\nThe port that the node publishes to the cluster for its own discovery. It is necessary to define this setting when the bound tranport port (transport.tcp.port) of the node is not directly reachable from outside, e.g. running it behind a firewall or inside a Docker container.\n\npsql.port\nRuntime: no\n\nThis defines the TCP port range to which the CrateDB Postgres service will be bound to. It defaults to 5432-5532. Always the first free port in this range is used. If this is set to an integer value it is considered as an explicit single port.\n\nAdvanced TCP settings\n\nAny interface that uses TCP (Postgres wire, HTTP & Transport protocols) shares the following settings:\n\nnetwork.tcp.no_delay\nDefault: true\nRuntime: no\n\nEnable or disable the Nagle’s algorithm for buffering TCP packets. Buffering is disabled by default.\n\nnetwork.tcp.keep_alive\nDefault: true\nRuntime: no\n\nConfigures the SO_KEEPALIVE option for sockets, which determines whether they send TCP keepalive probes.\n\nnetwork.tcp.reuse_address\nDefault: true on non-windows machines and false otherwise\nRuntime: no\n\nConfigures the SO_REUSEADDRS option for sockets, which determines whether they should reuse the address.\n\nnetwork.tcp.send_buffer_size\nDefault: -1\nRuntime: no\n\nThe size of the TCP send buffer (SO_SNDBUF socket option). By default not explicitly set.\n\nnetwork.tcp.receive_buffer_size\nDefault: -1\nRuntime: no\n\nThe size of the TCP receive buffer (SO_RCVBUF socket option). By default not explicitly set.\n\nNote\n\nEach setting in this section has its counterpart for HTTP and transport. To provide a protocol specific setting, remove network prefix and use either http or transport instead. For example, no_delay can be configured as http.tcp.no_delay and transport.tcp.no_delay. Please note, that PG interface takes its settings from transport.\n\nTransport settings\ntransport.connect_timeout\nDefault: 30s\nRuntime: no\n\nThe connect timeout for initiating a new connection.\n\ntransport.compress\nDefault: false\nRuntime: no\n\nSet to true to enable compression (DEFLATE) between all nodes.\n\ntransport.ping_schedule\nDefault: -1\nRuntime: no\n\nSchedule a regular application-level ping message to ensure that transport connections between nodes are kept alive. Defaults to -1 (disabled). It is preferable to correctly configure TCP keep-alives instead of using this feature, because TCP keep-alives apply to all kinds of long-lived connections and not just to transport connections.\n\nPaths\n\nNote\n\nRelative paths are relative to CRATE_HOME. Absolute paths override this behavior.\n\npath.conf\nDefault: config\nRuntime: no\n\nFilesystem path to the directory containing the configuration files crate.yml and log4j2.properties.\n\npath.data\nDefault: data\nRuntime: no\n\nFilesystem path to the directory where this CrateDB node stores its data (table data and cluster metadata).\n\nMultiple paths can be set by using a comma separated list and each of these paths will hold full shards (instead of striping data across them). For example:\n\npath.data: /path/to/data1,/path/to/data2\n\n\nWhen CrateDB finds striped shards at the provided locations (from CrateDB <0.55.0), these shards will be migrated automatically on startup.\n\npath.logs\nDefault: logs\nRuntime: no\n\nFilesystem path to a directory where log files should be stored.\n\nCan be used as a variable inside log4j2.properties.\n\nFor example:\n\nappender:\n  file:\n    file: ${path.logs}/${cluster.name}.log\n\npath.repo\nRuntime: no\n\nA list of filesystem or UNC paths where repositories of type fs may be stored.\n\nWithout this setting a CrateDB user could write snapshot files to any directory that is writable by the CrateDB process. To safeguard against this security issue, the possible paths have to be whitelisted here.\n\nSee also location setting of repository type fs.\n\nSee Also\n\nblobs.path\n\nPlug-ins\nplugin.mandatory\nRuntime: no\n\nA list of plug-ins that are required for a node to startup.\n\nIf any plug-in listed here is missing, the CrateDB node will fail to start.\n\nCPU\nprocessors\nRuntime: no\n\nThe number of processors is used to set the size of the thread pools CrateDB is using appropriately. If not set explicitly, CrateDB will infer the number from the available processors on the system.\n\nIn environments where the CPU amount can be restricted (like Docker) or when multiple CrateDB instances are running on the same hardware, the inferred number might be too high. In such a case, it is recommended to set the value explicitly.\n\nMemory\nbootstrap.memory_lock\nDefault: false\nRuntime: no\n\nCrateDB performs poorly when the JVM starts swapping: you should ensure that it never swaps. If set to true, CrateDB will use the mlockall system call on startup to ensure that the memory pages of the CrateDB process are locked into RAM.\n\nGarbage collection\n\nCrateDB logs if JVM garbage collection on different memory pools takes too long. The following settings can be used to adjust these timeouts:\n\nmonitor.jvm.gc.collector.young.warn\nDefault: 1000ms\nRuntime: no\n\nCrateDB will log a warning message if it takes more than the configured timespan to collect the Eden Space (heap).\n\nmonitor.jvm.gc.collector.young.info\nDefault: 700ms\nRuntime: no\n\nCrateDB will log an info message if it takes more than the configured timespan to collect the Eden Space (heap).\n\nmonitor.jvm.gc.collector.young.debug\nDefault: 400ms\nRuntime: no\n\nCrateDB will log a debug message if it takes more than the configured timespan to collect the Eden Space (heap).\n\nmonitor.jvm.gc.collector.old.warn\nDefault: 10000ms\nRuntime: no\n\nCrateDB will log a warning message if it takes more than the configured timespan to collect the Old Gen / Tenured Gen (heap).\n\nmonitor.jvm.gc.collector.old.info\nDefault: 5000ms\nRuntime: no\n\nCrateDB will log an info message if it takes more than the configured timespan to collect the Old Gen / Tenured Gen (heap).\n\nmonitor.jvm.gc.collector.old.debug\nDefault: 2000ms\nRuntime: no\n\nCrateDB will log a debug message if it takes more than the configured timespan to collect the Old Gen / Tenured Gen (heap).\n\nAuthentication\nTrust authentication\nauth.trust.http_default_user\nDefault: crate\nRuntime: no\n\nThe default user that should be used for authentication when clients connect to CrateDB via HTTP protocol and they do not specify a user via the Authorization request header.\n\nauth.trust.http_support_x_real_ip\nDefault: false\nRuntime: no\n\nIf enabled, the HTTP transport will trust the X-Real-IP header sent by the client to determine the client’s IP address. This is useful when CrateDB is running behind a reverse proxy or load-balancer. For improved security, any _local_ IP address (127.0.0.1 and ::1) defined in this header will be ignored.\n\nWarning\n\nEnabling this setting can be a security risk, as it allows clients to impersonate other clients by sending a fake X-Real-IP header.\n\nHost-based authentication\n\nAuthentication settings (auth.host_based.*) are node settings, which means that their values apply only to the node where they are applied and different nodes may have different authentication settings.\n\nauth.host_based.enabled\nDefault: false\nRuntime: no\n\nSetting to enable or disable Host Based Authentication (HBA). It is disabled by default.\n\nHBA entries\n\nThe auth.host_based.config. setting is a group setting that can have zero, one or multiple groups that are defined by their group key (${order}) and their fields (user, address, method, protocol, ssl).\n\n${order}:\nAn identifier that is used as a natural order key when looking up the host\nbased configuration entries. For example, an order key of a will be\nlooked up before an order key of b. This key guarantees that the entry\nlookup order will remain independent from the insertion order of the\nentries.\n\nThe Host-Based Authentication (HBA) setting is a list of predicates that users can specify to restrict or allow access to CrateDB.\n\nThe meaning of the fields of the are as follows:\n\nauth.host_based.config.${order}.user\nRuntime: no\nSpecifies an existing CrateDB username, only crate user (superuser) is\navailable. If no user is specified in the entry, then all existing users\ncan have access.\nauth.host_based.config.${order}.address\nRuntime: no\nThe client machine addresses that the client matches, and which are allowed\nto authenticate. This field may contain an IPv4 address, an IPv6 address or\nan IPv4 CIDR mask. For example: 127.0.0.1 or 127.0.0.1/32. It also\nmay contain a hostname or the special _local_ notation which will match\nboth IPv4 and IPv6 connections from localhost. A hostname specification\nthat starts with a dot (.) matches a suffix of the actual hostname.\nSo .crate.io would match foo.crate.io but not just crate.io. If no address\nis specified in the entry, then access to CrateDB is open for all hosts.\nauth.host_based.config.${order}.method\nRuntime: no\nThe authentication method to use when a connection matches this entry.\nValid values are trust, cert, password and jwt. If no\nmethod is specified, the trust method is used by default.\nSee Trust method, Client certificate authentication method, Password authentication method and\nJWT authentication method for more information about these methods.\nauth.host_based.config.${order}.protocol\nRuntime: no\nSpecifies the protocol for which the authentication entry should be used.\nIf no protocol is specified, then this entry will be valid for all\nprotocols that rely on host based authentication see Trust method).\nauth.host_based.config.${order}.ssl\nDefault: optional\nRuntime: no\nSpecifies whether the client must use SSL/TLS to connect to the cluster.\nIf set to on then the client must be connected through SSL/TLS\notherwise is not authenticated. If set to off then the client must\nnot be connected via SSL/TLS otherwise is not authenticated. Finally\noptional, which is the value when the option is completely skipped,\nmeans that the client can be authenticated regardless of SSL/TLS is used\nor not.\n\nExample of config groups:\n\nauth.host_based.config:\n  entry_a:\n    user: crate\n    address: 127.16.0.0/16\n  entry_b:\n    method: trust\n  entry_3:\n    user: crate\n    address: 172.16.0.0/16\n    method: trust\n    protocol: pg\n    ssl: on\n\nSecured communications (SSL/TLS)\n\nSecured communications via SSL allows you to encrypt traffic between CrateDB nodes and clients connecting to them. Connections are secured using Transport Layer Security (TLS).\n\nssl.http.enabled\nDefault: false\nRuntime: no\n\nSet this to true to enable secure communication between the CrateDB node and the client through SSL via the HTTPS protocol.\n\nssl.psql.enabled\nDefault: false\nRuntime: no\n\nSet this to true to enable secure communication between the CrateDB node and the client through SSL via the PostgreSQL wire protocol.\n\nssl.transport.mode\nDefault: legacy\nRuntime: no\n\nFor communication between nodes, choose:\n\noff\n\nSSL cannot be used\n\nlegacy\n\nSSL is not used. If HBA is enabled, transport connections won’t be verified Any reachable host can establish a connection.\n\non\n\nSSL must be used\n\nssl.keystore_filepath\nRuntime: no\n\nThe full path to the node keystore file.\n\nssl.keystore_password\nRuntime: no\n\nThe password used to decrypt the keystore file defined with ssl.keystore_filepath.\n\nssl.keystore_key_password\nRuntime: no\n\nThe password entered at the end of the keytool -genkey command.\n\nNote\n\nOptionally trusted CA certificates can be stored separately from the node’s keystore into a truststore for CA certificates.\n\nssl.truststore_filepath\nRuntime: no\n\nThe full path to the node truststore file. If not defined, then only a keystore will be used.\n\nssl.truststore_password\nRuntime: no\n\nThe password used to decrypt the truststore file defined with ssl.truststore_filepath.\n\nssl.resource_poll_interval\nDefault: 5m\nRuntime: no\n\nThe frequency at which SSL files such as keystore and truststore are polled for changes.\n\nCross-origin resource sharing (CORS)\n\nMany browsers support the same-origin policy which requires web applications to explicitly allow requests across origins. The cross-origin resource sharing settings in CrateDB allow for configuring these.\n\nhttp.cors.enabled\nDefault: false\nRuntime: no\n\nEnable or disable cross-origin resource sharing.\n\nhttp.cors.allow-origin\nDefault: <empty>\nRuntime: no\n\nDefine allowed origins of a request. * allows any origin (which can be a substantial security risk) and by prepending a / the string will be treated as a regular expression. For example /https?:\\/\\/crate.io/ will allow requests from https://crate.io and https://crate.io. This setting disallows any origin by default.\n\nhttp.cors.max-age\nDefault: 1728000 (20 days)\nRuntime: no\n\nMax cache age of a preflight request in seconds.\n\nhttp.cors.allow-methods\nDefault: OPTIONS, HEAD, GET, POST, PUT, DELETE\nRuntime: no\n\nAllowed HTTP methods.\n\nhttp.cors.allow-headers\nDefault: X-Requested-With, Content-Type, Content-Length\nRuntime: no\n\nAllowed HTTP headers.\n\nhttp.cors.allow-credentials\nDefault: false\nRuntime: no\n\nAdd the Access-Control-Allow-Credentials header to responses.\n\nBlobs\nblobs.path\nRuntime: no\n\nPath to a filesystem directory where to store blob data allocated for this node.\n\nBy default blobs will be stored under the same path as normal data. A relative path value is interpreted as relative to CRATE_HOME.\n\nRepositories\n\nRepositories are used to backup a CrateDB cluster.\n\nrepositories.url.allowed_urls\nRuntime: no\n\nThis setting only applies to repositories of type url.\n\nWith this setting a list of urls can be specified which are allowed to be used if a repository of type url is created.\n\nWildcards are supported in the host, path, query and fragment parts.\n\nThis setting is a security measure to prevent access to arbitrary resources.\n\nIn addition, the supported protocols can be restricted using the repositories.url.supported_protocols setting.\n\nrepositories.url.supported_protocols\nDefault: http, https, ftp, file and jar\nRuntime: no\n\nA list of protocols that are supported by repositories of type url.\n\nThe jar protocol is used to access the contents of jar files. For more info, see the java JarURLConnection documentation.\n\nSee also the path.repo Setting.\n\nQueries\nindices.query.bool.max_clause_count\nDefault: 8192\nRuntime: no\n\nThis setting limits the number of boolean clauses that can be generated by != ANY(), LIKE ANY(), ILIKE ANY(), NOT LIKE ANY() and NOT ILIKE ANY() operators on arrays in order to prevent users from executing queries that may result in heavy memory consumption causing nodes to crash with OutOfMemory exceptions. Throws TooManyClauses errors when the limit is exceeded.\n\nNote\n\nYou can avoid TooManyClauses errors by increasing this setting. The number of boolean clauses used can be larger than the elements of the array .\n\nLegacy\nlegacy.table_function_column_naming\nDefault: false\nRuntime: no\n\nSince CrateDB 5.0.0, if the table function is not aliased and is returning a single base data typed column, the table function name is used as the column name. This setting can be set in order to use the naming convention prior to 5.0.0.\n\nThe following table functions are affected by this setting:\n\nunnest\n\nregexp_matches\n\ngenerate_series\n\nWhen the setting is set and a single column is expected to be returned, the returned column will be named col1, groups, or col1 respectively.\n\nNote\n\nBeware that if not all nodes in the cluster are consistently set or unset, the behaviour will depend on the node handling the query.\n\nJavaScript language\nlang.js.enabled\nDefault: true\nRuntime: no\n\nSetting to enable or disable JavaScript UDF support.\n\nForeign Data Wrappers\nfdw.allow_local\nDefault: false\nRuntime: no\n\nAllow access to local addresses via Foreign data wrappers for all users.\n\nBy default, only the crate superuser is allowed to access foreign servers that point to localhost.\n\nWarning\n\nChanging this to true can pose a security risk if you do not trust the users with AL permissions on the system. They can create foreign servers, foreign tables and user mappings that allow them to access services running on the same machine as CrateDB as if connected locally - effectively bypassing any restrictions set up via Host-Based Authentication (HBA).\n\nDo not change this if you don’t understand the implications.\n\nCustom attributes\n\nThe node.attr namespace is a bag of custom attributes. Custom attributes can be used to control shard allocation.\n\nYou can create any attribute you want under this namespace, like node.attr.key: value. These attributes use the node.attr namespace to distinguish them from core node attribute like node.name.\n\nCustom attributes are not validated by CrateDB, unlike core node attributes."
  },
  {
    "title": "CrateDB and DataFrame libraries — CrateDB: Clients and Tools",
    "url": "https://cratedb.com/docs/crate/clients-tools/en/latest/connect/df.html",
    "html": "CrateDB and DataFrame libraries\n\nData frame libraries and frameworks which can be used together with CrateDB.\n\n Tutorials\n\nLearn how to use CrateDB together with popular open-source data frame libraries, on behalf of hands-on tutorials and code examples.\n\nDask pandas Polars\n\n SQLAlchemy\n\nCrateDB’s SQLAlchemy dialect implementation provides fundamental infrastructure to integrations with Dask, pandas, and Polars.\n\nORM Guides • ORM Catalog\n\nDask\n\nDask is a parallel computing library for analytics with task scheduling. It is built on top of the Python programming language, making it easy to scale the Python libraries that you know and love, like NumPy, pandas, and scikit-learn.\n\nDask DataFrames help you process large tabular data by parallelizing pandas, either on your laptop for larger-than-memory computing, or on a distributed cluster of computers.\n\nDask Futures, implementing a real-time task framework, allow you to scale generic Python workflows across a Dask cluster with minimal code changes, by extending Python’s concurrent.futures interface.\n\npandas\n\npandas is a fast, powerful, flexible, and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\n\nPandas (stylized as pandas) is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series.\n\nData Model\n\nPandas is built around data structures called Series and DataFrames. Data for these collections can be imported from various file formats such as comma-separated values, JSON, Parquet, SQL database tables or queries, and Microsoft Excel.\n\nA Series is a 1-dimensional data structure built on top of NumPy’s array.\n\nPandas includes support for time series, such as the ability to interpolate values and filter using a range of timestamps.\n\nBy default, a Pandas index is a series of integers ascending from 0, similar to the indices of Python arrays. However, indices can use any NumPy data type, including floating point, timestamps, or strings.\n\nPandas supports hierarchical indices with multiple values per data point. An index with this structure, called a “MultiIndex”, allows a single DataFrame to represent multiple dimensions, similar to a pivot table in Microsoft Excel. Each level of a MultiIndex can be given a unique name.\n\nPolars\n\nPolars is a blazingly fast DataFrames library with language bindings for Rust, Python, Node.js, R, and SQL. Polars is powered by a multithreaded, vectorized query engine, it is open source, and written in Rust.\n\nFast: Written from scratch in Rust and with performance in mind, designed close to the machine, and without external dependencies.\n\nI/O: First class support for all common data storage layers: local, cloud storage & databases.\n\nIntuitive API: Write your queries the way they were intended. Polars, internally, will determine the most efficient way to execute using its query optimizer. Polars’ expressions are intuitive and empower you to write readable and performant code at the same time.\n\nOut of Core: The streaming API allows you to process your results without requiring all your data to be in memory at the same time.\n\nParallel: Polars’ multi-threaded query engine utilises the power of your machine by dividing the workload amongst the available CPU cores without any additional configuration.\n\nVectorized Query Engine: Uses Apache Arrow, a columnar data format, to process your queries in a vectorized manner and SIMD to optimize CPU usage. This enables cache-coherent algorithms and high performance on modern processors.\n\nOpen Source: Polars is and always will be open source. Driven by an active community of developers. Everyone is encouraged to add new features and contribute. It is free to use under the MIT license.\n\nData formats\n\nPolars supports reading and writing to many common data formats. This allows you to easily integrate Polars into your existing data stack.\n\nText: CSV & JSON\n\nBinary: Parquet, Delta Lake, AVRO & Excel\n\nIPC: Feather, Arrow\n\nDatabases: MySQL, Postgres, SQL Server, Sqlite, Redshift & Oracle\n\nCloud Storage: S3, Azure Blob & Azure File"
  },
  {
    "title": "Resiliency — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/concepts/resiliency.html",
    "html": "master\nResiliency\n\nDistributed systems are tricky. All sorts of things can go wrong that are beyond your control. The network can go away, disks can fail, hosts can be terminated unexpectedly. CrateDB tries very hard to cope with these sorts of issues while maintaining availability, consistency, and durability.\n\nHowever, as with any distributed system, sometimes, rarely, things can go wrong.\n\nThankfully, for most use-cases, if you follow best practices, you are extremely unlikely to experience resiliency issues with CrateDB.\n\nSee Also\n\nAppendix: Resiliency Issues\n\nTable of contents\n\nMonitoring cluster status\n\nStorage and consistency\n\nDeployment strategies\n\nMonitoring cluster status\n\nThe Admin UI in CrateDB has a status indicator which can be used to determine the stability and health of a cluster.\n\nA green status indicates that all shards have been replicated, are available, and are not being relocated. This is the lowest risk status for a cluster. The status will turn yellow when there is an elevated risk of encountering issues, due to a network failure or the failure of a node in the cluster.\n\nThe status is updated every few seconds (variable on your cluster ping configuration).\n\nStorage and consistency\n\nCode that expects the behavior of an ACID compliant database like MySQL may not always work as expected with CrateDB.\n\nCrateDB does not support ACID transactions, but instead has atomic operations and eventual consistency at the row level. See also Clustering.\n\nEventual consistency is the trade-off that CrateDB makes in exchange for high-availability that can tolerate most hardware and network failures. So you may observe data from different cluster nodes temporarily falling very briefly out-of-sync with each other, although over time they will become consistent.\n\nFor example, you know a row has been written as soon as you get the INSERT OK message. But that row might not be read back by a subsequent SELECT on a different node until after a table refresh (which typically occurs within one second).\n\nYour applications should be designed to work this storage and consistency model.\n\nDeployment strategies\n\nWhen deploying CrateDB you should carefully weigh your need for high-availability and disaster recovery against operational complexity and expense.\n\nWhich strategy you pick is going to depend on the specifics of your situation.\n\nHere are some considerations:\n\nCrateDB is designed to scale horizontally. Make sure that your machines are fit for purpose, i.e. use SSDs, increase RAM up to 64 GB, and use multiple CPU cores when you can. But if you want to dynamically increase (or decrease) the capacity of your cluster, add (or remove) nodes.\n\nIf availability is a concern, you can add nodes across multiple zones (e.g. different data centers or geographical regions). The more available your CrateDB cluster is, the more likely it is to withstand external failures like a zone going down.\n\nIf data durability or read performance is a concern, you can increase the number of table replicas. More table replicas means a smaller chance of permanent data loss due to hardware failures, in exchange for the use of more disk space and more intra-cluster network traffic.\n\nIf disaster recovery is important, you can take regular snapshots and store those snapshots in cold storage. This safeguards data that has already been successfully written and replicated across the cluster.\n\nCrateDB works well as part of a data pipeline, especially if you’re working with high-volume data. If you have a message queue in front of CrateDB, you can configure it with backups and replay the data flow for a specific timeframe. This can be used to recover from issues that affect your data before it has been successfully written and replicated across the cluster.\n\nIndeed, this is the generally recommended way to recover from any of the rare consistency or data-loss issues you might encounter when CrateDB experiences network or hardware failures (see next section)."
  },
  {
    "title": "Clustering — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/concepts/clustering.html",
    "html": "master\nClustering\n\nThe aim of this document is to describe, on a high level, how the distributed SQL database CrateDB uses a shared nothing architecture to form high- availability, resilient database clusters with minimal effort of configuration.\n\nIt will lay out the core concepts of the shared nothing architecture at the heart of CrateDB. The main difference to a primary-secondary architecture is that every node in the CrateDB cluster can perform every operation - hence all nodes are equal in terms of functionality (see Components of a CrateDB Node) and are configured the same.\n\nTable of contents\n\nComponents of a CrateDB Node\n\nSQL Handler\n\nJob Execution Service\n\nCluster State Service\n\nData storage\n\nMulti-node setup: Clusters\n\nCluster state management\n\nSettings, metadata, and routing\n\nMaster Node Election\n\nDiscovery\n\nNetworking\n\nCluster behavior\n\nApplication use case\n\nComponents of a CrateDB Node\n\nTo understand how a CrateDB cluster works it makes sense to first take a look at the components of an individual node of the cluster.\n\nFigure 1\n\nMultiple interconnected instances of CrateDB form a single database cluster. The components of each node are equal.\n\nFigure 1 shows that in CrateDB each node of a cluster contains the same components that (a) interface with each other, (b) with the same component from a different node and/or (c) with the outside world. These four major components are: SQL Handler, Job Execution Service, Cluster State Service, and Data Storage.\n\nSQL Handler\n\nThe SQL Handler part of a node is responsible for three aspects:\n\nhandling incoming client requests,\n\nparsing and analyzing the SQL statement from the request and\n\ncreating an execution plan based on the analyzed statement (abstract syntax tree)\n\nThe SQL Handler is the only of the four components that interfaces with the “outside world”. CrateDB supports three protocols to handle client requests:\n\nHTTP\n\na Binary Transport Protocol\n\nthe PostgreSQL Wire Protocol\n\nA typical request contains a SQL statement and its corresponding arguments.\n\nJob Execution Service\n\nThe Job Execution Service is responsible for the execution of a plan (“job”). The phases of the job and the resulting operations are already defined in the execution plan. A job usually consists of multiple operations that are distributed via the Transport Protocol to the involved nodes, be it the local node and/or one or multiple remote nodes. Jobs maintain IDs of their individual operations. This allows CrateDB to “track” (or for example “kill”) distributed queries.\n\nCluster State Service\n\nThe three main functions of the Cluster State Service are:\n\ncluster state management,\n\nelection of the master node and\n\nnode discovery, thus being the main component for cluster building (as described in section Multi-node setup: Clusters).\n\nIt communicates using the Binary Transport Protocol.\n\nData storage\n\nThe data storage component handles operations to store and retrieve data from disk based on the execution plan.\n\nIn CrateDB, the data stored in the tables is sharded, meaning that tables are divided and (usually) stored across multiple nodes. Each shard is a separate Lucene index that is stored physically on the filesystem. Reads and writes are operating on a shard level.\n\nMulti-node setup: Clusters\n\nA CrateDB cluster is a set of two or more CrateDB instances (referred to as nodes) running on different hosts which form a single, distributed database.\n\nFor inter-node communication, CrateDB uses a software specific transport protocol that utilizes byte-serialized Plain Old Java Objects (POJOs) and operates on a separate port. That so-called “transport port” must be open and reachable from all nodes in the cluster.\n\nCluster state management\n\nThe cluster state is versioned and all nodes in a cluster keep a copy of the latest cluster state. However, only a single node in the cluster – the master node – is allowed to change the state at runtime.\n\nSettings, metadata, and routing\n\nThe cluster state contains all necessary meta information to maintain the cluster and coordinate operations:\n\nGlobal cluster settings\n\nDiscovered nodes and their status\n\nSchemas of tables\n\nThe status and location of primary and replica shards\n\nWhen the master node updates the cluster state it will publish the new state to all nodes in the cluster and wait for all nodes to respond before processing the next update.\n\nMaster Node Election\n\nIn a CrateDB cluster there can only be one master node at any single time. The cluster only becomes available to serve requests once a master has been elected, and a new election takes place if the current master node becomes unavailable.\n\nBy default, all nodes are master-eligible, but a node setting is available to indicate, if desired, that a node must not take on the role of master.\n\nTo elect a master among the eligible nodes, a majority (floor(half)+1), also known as quorum, is required among a subset of all master-eligible nodes, this subset of nodes is known as the voting configuration. The voting configuration is a list which is persisted as part of the cluster state. It is maintained automatically in a way that makes so that split-brain scenarios are never possible.\n\nEvery time a node joins the cluster, or leaves the cluster, even if it is for a few seconds, CrateDB re-evaluates the voting configuration. If the new number of master-eligible nodes in the cluster is odd, CrateDB will put them all in the voting configuration. If the number is even, CrateDB will exclude one of the master-eligible nodes from the voting configuration.\n\nThe voting configuration is not shrunk below 3 nodes, meaning that if there were 3 nodes in the voting configuration and one of them becomes unavailable, they all stay in the voting configuration and a quorum of 2 nodes is still required. A master node rescinds its role if it cannot contact a quorum of nodes from the latest voting configuration.\n\nWarning\n\nIf you do infrastructure maintenance, please note that as nodes are shutdown or rebooted, they will temporarily leave the voting configuration, and for the cluster to elect a master a quorum is required among the nodes that were last in the voting configuration.\n\nFor instance, if you have a 5-nodes cluster, with all nodes master-eligible, and node 1 is currently the master, and you shutdown node 5, then node 4, then node 3, the cluster will stay available as the voting configuration will have adapted to only have nodes 1, 2, and 3 on it.\n\nIf you then shutdown one more node the cluster will become unavailable as a quorum of 2 nodes is now required and not available. To bring the cluster back online at this point you will require two nodes among 1, 2, and 3. Bringing back nodes 3, 4, and 5, will not be sufficient.\n\nNote\n\nSpecial settings and considerations applied prior to CrateDB version 4.0.0.\n\nDiscovery\n\nThe process of finding, adding and removing nodes is done in the discovery module.\n\nFigure 2\n\nPhases of the node discovery process. n1 and n2 already form a cluster where n1 is the elected master node, n3 joins the cluster. The cluster state update happens in parallel!\n\nNode discovery happens in multiple steps:\n\nCrateDB requires a list of potential host addresses for other CrateDB nodes when it is starting up. That list can either be provided by a static configuration or can be dynamically generated, for example by fetching DNS SRV records, querying the Amazon EC2 API, and so on.\n\nAll potential host addresses are pinged. Nodes which receive the request respond to it with information about the cluster it belongs to, the current master node, and its own node name.\n\nNow that the node knows the master node, it sends a join request. The Primary verifies the incoming request and adds the new node to the cluster state that now contains the complete list of all nodes in the cluster.\n\nThe cluster state is then published across the cluster. This guarantees the common knowledge of the node addition.\n\nCaution\n\nIf a node is started without any initial_master_nodes or a discovery_type set to single-node (e.g., the default configuration), it will never join a cluster even if the configuration is subsequently changed.\n\nIt is possible to force the node to forget its current cluster state by using the crate-node CLI tool. However, be aware that this may result in data loss.\n\nNetworking\n\nIn a CrateDB cluster all nodes have a direct link to all other nodes; this is known as full mesh topology. Due to simplicity reasons every node maintains a one-way connections to every other node in the network. The network topology of a 5 node cluster looks like this:\n\nFigure 3\n\nNetwork topology of a 5 node CrateDB cluster. Each line represents a one-way connection.\n\nThe advantages of a fully connected network are that it provides a high degree of reliability and the paths between nodes are the shortest possible. However, there are limitations in the size of such networked applications because the number of connections (c) grows quadratically with the number of nodes (n):\n\nc = n * (n - 1)\n\nCluster behavior\n\nThe fact that each CrateDB node in a cluster is equal allows applications and users to connect to any node and get the same response for the same operations. As already described in section Components of a CrateDB Node, the SQL handler is responsible for handling incoming client SQL requests, either using the HTTP transport protocol, or the PostgreSQL wire protocol.\n\nThe “handler node” that accepts the client request also returns the response to the client. It does neither redirect nor delegate the request to a different nodes. The handler node parses the incoming request into a syntax tree, analyzes it and creates an execution plan locally. Then the operations of the plan are executed in a distributed manner. The upstream of the final phase of the execution is always the handler which then returns the response to the client.\n\nApplication use case\n\nIn a conventional setup of an application using a primary-secondary database the deployed stack looks similar to this:\n\nFigure 4\n\nConventional deployment of an application-database stack.\n\nHowever, this given setup does not scale because all application servers use the same, single entry point to the database for writes (the application can still read from secondaries) and if that entry point is unavailable the complete stack is broken.\n\nChoosing a shared nothing architecture allows DevOps to deploy their applications in an “elastic” manner without SPoF. The idea is to extend the shared nothing architecture from the database to the application which in most cases is stateless already.\n\nFigure 5\n\nElastic deployment making use of the shared nothing architecture.\n\nIf you deploy an instance of CrateDB together with every application server you will be able to dynamically scale up and down your database backend depending on your needs. The application only needs to communicate to its “bound” CrateDB instance on localhost. The load balancer tracks the health of the hosts and if either the application or the database on a single host fails the complete host will taken out of the load balancing."
  },
  {
    "title": "Machine Learning with CrateDB — CrateDB: Clients and Tools",
    "url": "https://cratedb.com/docs/crate/clients-tools/en/latest/integrate/ml.html",
    "html": "Machine Learning with CrateDB\n\nMachine learning applications and frameworks which can be used together with CrateDB.\n\n Tutorials\n\nLearn how to integrate CrateDB with machine learning frameworks and tools, for MLOps and Vector database operations.\n\nMLOps Vector Store Embeddings Hybrid Search LLM RAG\n\nLangChain\n\n🦜️🔗\n\nLangChain is a framework for developing applications powered by language models, written in Python, and with a strong focus on composability. As a language model integration framework, LangChain’s use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n\nLangChain supports retrieval-augmented generation (RAG), which is a technique for augmenting LLM knowledge with additional, often private or real-time, data, and mixing in “prompt engineering” as the process of structuring text that can be interpreted and understood by a generative AI model. A prompt is natural language text describing the task that an AI should perform.\n\nThe LangChain adapter for CrateDB provides support to use CrateDB as a vector store database, to load documents using LangChain’s DocumentLoader, and also supports LangChain’s conversational memory subsystem.\n\nMLflow\n\nMLflow is an open source platform to manage the whole ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry.\n\nThe MLflow adapter for CrateDB, available through the mlflow-cratedb package, provides support to use CrateDB as a storage database for the MLflow Tracking subsystem, which is about recording and querying experiments, across code, data, config, and results.\n\nPyCaret\n\nPyCaret is an open-source, low-code machine learning library for Python that automates machine learning workflows.\n\nIt is a high-level interface and AutoML wrapper on top of your loved machine learning libraries like scikit-learn, xgboost, ray, lightgbm, and many more. PyCaret provides a universal interface to utilize these libraries without needing to know the details of the underlying model architectures and parameters.\n\nscikit-learn\n\nscikit-learn\n\nMachine Learning in Python.\n\nSimple and efficient tools for predictive data analysis\n\nAccessible to everybody, and reusable in various contexts\n\nBuilt on NumPy, SciPy, and matplotlib\n\npandas\n\nThe open source data analysis and manipulation tool.\n\nPandas is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series.\n\nProject Jupyter\n\nInteractive computing across all programming languages.\n\nJupyterLab is the latest web-based interactive development environment for notebooks, code, and data. Its flexible interface allows users to configure and arrange workflows in data science, scientific computing, computational journalism, and machine learning. A modular design invites extensions to expand and enrich functionality."
  },
  {
    "title": "Storage and consistency — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/concepts/storage-consistency.html",
    "html": "master\nStorage and consistency\n\nThis document provides an overview on how CrateDB stores and distributes state across the cluster and what consistency and durability guarantees are provided.\n\nNote\n\nSince CrateDB heavily relies on Elasticsearch and Lucene for storage and cluster consensus, concepts shown here might look familiar to Elasticsearch users, since the implementation is actually reused from the Elasticsearch code.\n\nTable of contents\n\nData storage\n\nAtomicity at document level\n\nDurability\n\nAddressing documents\n\nConsistency\n\nCluster meta data\n\nData storage\n\nEvery table in CrateDB is sharded, which means that tables are divided and distributed across the nodes of a cluster. Each shard in CrateDB is a Lucene index broken down into segments getting stored on the filesystem. Physically the files reside under one of the configured data directories of a node.\n\nLucene only appends data to segment files, which means that data written to the disc will never be mutated. This makes it easy for replication and recovery, since syncing a shard is simply a matter of fetching data from a specific marker.\n\nAn arbitrary number of replica shards can be configured per table. Every operational replica holds a full synchronized copy of the primary shard.\n\nWith read operations, there is no difference between executing the operation on the primary shard or on any of the replicas. CrateDB randomly assigns a shard when routing an operation. It is possible to configure this behavior if required, see our best practice guide on multi zone setups for more details.\n\nWrite operations are handled differently than reads. Such operations are synchronous over all active replicas with the following flow:\n\nThe primary shard and the active replicas are looked up in the cluster state for the given operation. The primary shard and a quorum of the configured replicas need to be available for this step to succeed.\n\nThe operation is routed to the according primary shard for execution.\n\nThe operation gets executed on the primary shard\n\nIf the operation succeeds on the primary, the operation gets executed on all replicas in parallel.\n\nAfter all replica operations finish the operation result gets returned to the caller.\n\nShould any replica shard fail to write the data or times out in step 5, it’s immediately considered as unavailable.\n\nAtomicity at document level\n\nEach row of a table in CrateDB is a semi structured document which can be nested arbitrarily deep through the use of object and array types.\n\nOperations on documents are atomic. Meaning that a write operation on a document either succeeds as a whole or has no effect at all. This is always the case, regardless of the nesting depth or size of the document.\n\nCrateDB does not provide transactions. Since every document in CrateDB has a version number assigned, which gets increased every time a change occurs, patterns like Optimistic Concurrency Control can help to work around that limitation.\n\nDurability\n\nEach shard has a WAL also known as translog. It guarantees that operations on documents are persisted to disk without having to issue a Lucene-Commit for every write operation. When the translog gets flushed all data is written to the persistent index storage of Lucene and the translog gets cleared.\n\nIn case of an unclean shutdown of a shard, the transactions in the translog are getting replayed upon startup to ensure that all executed operations are permanent.\n\nThe translog is also directly transferred when a newly allocated replica initializes itself from the primary shard. There is no need to flush segments to disc just for replica recovery purposes.\n\nAddressing documents\n\nEvery document has an internal identifier. By default this identifier is derived from the primary key. Documents living in tables without a primary key are assigned a unique auto-generated ID automatically when created.\n\nEach document is routed to one specific shard according to the routing column. All rows that have the same routing column row value are stored in the same shard. The routing column can be specified with the CLUSTERED clause when creating the table. If a primary key has been defined, it will be used as the default routing column, otherwise the internal document ID is used.\n\nWhile transparent to the user, internally there are two ways how CrateDB accesses documents:\n\nget\n\nDirect access by identifier. Only applicable if the routing key and the identifier can be computed from the given query specification. (e.g: the full primary key is defined in the where clause).\n\nThis is the most efficient way to access a document, since only a single shard gets accessed and only a simple index lookup on the _id field has to be done.\n\nsearch\n\nQuery by matching against fields of documents across all candidate shards of the table.\n\nConsistency\n\nCrateDB is eventual consistent for search operations. Search operations are performed on shared IndexReaders which besides other functionality, provide caching and reverse lookup capabilities for shards. An IndexReader is always bound to the Lucene segment it was started from, which means it has to be refreshed in order to see new changes, this is done on a time based manner, but can also be done manually (see refresh). Therefore a search only sees a change if the according IndexReader was refreshed after that change occurred.\n\nIf a query specification results in a get operation, changes are visible immediately. This is achieved by looking up the document in the translog first, which will always have the most recent version of the document. The common update and fetch use-case is therefore possible. If a client updates a row and that row is looked up by its primary key after that update the changes will always be visible, since the information will be retrieved directly from the translog.\n\nNote\n\nDirty reads can occur if the primary shard becomes isolated. The primary will only realize it is isolated once it tries to communicate with its replicas or the master. At that point, a write operation is already committed into the primary and can be read by a concurrent read operation. In order to minimise the window of opportunity for this phenomena, the CrateDB nodes communicate with the master every second (by default) and once they realise no master is known, they will start rejecting write operations.\n\nEvery replica shard is updated synchronously with its primary and always carries the same information. Therefore it does not matter if the primary or a replica shard is accessed in terms of consistency. Only the refresh of the IndexReader affects consistency.\n\nCaution\n\nSome outage conditions can affect these consistency claims. See the resiliency documentation for details.\n\nCluster meta data\n\nCluster meta data is held in the so called “Cluster State”, which contains the following information:\n\nTables schemas.\n\nPrimary and replica shard locations. Basically just a mapping from shard number to the storage node.\n\nStatus of each shard, which tells if a shard is currently ready for use or has any other state like “initializing”, “recovering” or cannot be assigned at all.\n\nInformation about discovered nodes and their status.\n\nConfiguration information.\n\nEvery node has its own copy of the cluster state. However there is only one node allowed to change the cluster state at runtime. This node is called the “master” node and gets auto-elected. The “master” node has no special configuration at all, all nodes are master-eligible by default, and any master-eligible node can be elected as the master. There is also an automatic re-election if the current master node goes down for some reason.\n\nNote\n\nTo avoid a scenario where two masters could be elected due to network partitioning, CrateDB automatically defines a quorum of nodes with which it is possible to elect a master. For details on how this works and further information see Master Node Election.\n\nTo explain the flow of events for any cluster state change, here is an example flow for an ALTER TABLE statement which changes the schema of a table:\n\nA node in the cluster receives the ALTER TABLE request.\n\nThe node sends out a request to the current master node to change the table definition.\n\nThe master node applies the changes locally to the cluster state and sends out a notification to all affected nodes about the change.\n\nThe nodes apply the change, so that they are now in sync with the master.\n\nEvery node might take some local action depending on the type of cluster state change."
  },
  {
    "title": "Build Status — CrateDB: Clients and Tools",
    "url": "https://cratedb.com/docs/crate/clients-tools/en/latest/status.html",
    "html": "Build Status\n\nThe build status of relevant drivers, applications, and integrations for CrateDB, on one page.\n\nIntegrations\n\nEnd-to-end QA integration tests against CrateDB Nightly, on behalf of cratedb-examples.\n\nApplication\t \nDataframe\t \nLanguage\t      \nTesting\t\nTopic\tMachine Learning\n  \nApplications\n\nCI outcomes for a range of applications, frameworks, and libraries connecting to CrateDB.\n\nFramework\t    \nMetrics\t\nOperation\t\nUI\t  \nDrivers\n\nCI outcomes for a range of client drivers connecting your applications to CrateDB.\n\nCrateDB\n\nDrivers and dialects maintained by Crate.io.\n\n    \nPostgreSQL\n\nStandard PostgreSQL drivers for different languages.\n\n     \n\nNote\n\nPlease note that the designated status of drivers maintained by community authors and other maintainers reflect the build status of the development head. It does not mean integration with GA releases isn’t stable. This is reflected within the topmost section of this page."
  },
  {
    "title": "Joins — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/concepts/joins.html",
    "html": "master\nJoins\n\nJoins are essential operations in relational databases. They create a link between rows based on common values and allow the meaningful combination of these rows. CrateDB supports joins and due to its distributed nature allows you to work with large amounts of data.\n\nIn this document we will present the following topics. First, an overview of the existing types of joins and algorithms provided. Then a description of how CrateDB implements them along with the necessary optimizations, which allows us to work with huge datasets.\n\nTable of contents\n\nJoin types\n\nCross join\n\nInner join\n\nEqui Join\n\nOuter join\n\nJoin algorithms\n\nNested loop join\n\nPrimitive nested loop\n\nDistributed nested loop\n\nHash join\n\nBasic algorithm\n\nBlock hash join\n\nSwitch tables optimization\n\nDistributed block hash join\n\nJoin optimizations\n\nQuery then fetch\n\nPush-down query optimization\n\nCross join elimination\n\nJoin types\n\nA join is a relational operation that merges two data sets based on certain properties. Join Types (Inspired by this article) shows which elements appear in which join.\n\nJoin Types\n\nFrom left to right, top to bottom: left join, right join, inner join, outer join, and cross join of a set L and R.\n\nCross join\n\nA cross join returns the Cartesian product of two or more relations. The result of the Cartesian product on the relation L and R consists of all possible permutations of each tuple of the relation L with every tuple of the relation R.\n\nInner join\n\nAn inner join is a join of two or more relations that returns only tuples that satisfy the join condition.\n\nEqui Join\n\nAn equi join is a subset of an inner join and a comparison-based join, that uses equality comparisons in the join condition. The equi join of the relation L and R combines tuple l of relation L with a tuple r of the relation R if the join attributes of both tuples are identical.\n\nOuter join\n\nAn outer join returns a relation consisting of tuples that satisfy the join condition and dangling tuples from both or one of the relations, respectively to the outer join type.\n\nAn outer join can be one of the following types:\n\nLeft outer join returns tuples of the relation L matching tuples of the relation R and dangling tuples of the relation R padded with null values.\n\nRight outer join returns tuples of the relation R matching tuples of the relation L and dangling tuples from the relation L padded with null values.\n\nFull outer join returns matching tuples of both relations and dangling tuples produced by left and right outer joins.\n\nJoin algorithms\n\nCrateDB supports (a) CROSS JOIN, (b) INNER JOIN, (c) EQUI JOIN, (d) LEFT JOIN, (e) RIGHT JOIN and (f) FULL JOIN. All of these join types are executed using the nested loop join algorithm except for the Equi Joins which are executed using the hash join algorithm. Special optimizations, according to the specific use cases, are applied to improve execution performance.\n\nNested loop join\n\nThe nested loop join is the simplest join algorithm. One of the relations is nominated as the inner relation and the other as the outer relation. Each tuple of the outer relation is compared with each tuple of the inner relation and if the join condition is satisfied, the tuples of the relation L and R are concatenated and added into the returned virtual relation:\n\nfor each tuple l ∈ L do\n    for each tuple r ∈ R do\n        if l.a Θ r.b\n            put tuple(l, r) in Q\n\n\nListing 1. Nested loop join algorithm.\n\nPrimitive nested loop\n\nFor joins on some relations, the nested loop operation can be executed directly on the handler node. Specifically for queries involving a CROSS JOIN or joins on system tables /information_schema each shard sends the data to the handler node. Afterwards, this node runs the nested loop, applies limits, etc. and ultimately returns the results. Similarly, joins can be nested, so instead of collecting data from shards the rows can be the result of a previous join or table function.\n\nDistributed nested loop\n\nRelations are usually distributed to different nodes which require the nested loop to acquire the data before being able to join. After finding the locations of the required shards (which is done in the planning stage), the smaller data set (based on the row count) is broadcast amongst all the nodes holding the shards they are joined with.\n\nAfter that, each of the receiving nodes can start running a nested loop on the subset it has just received. Finally, these intermediate results are pushed to the original (handler) node to merge and return the results to the requesting client (see Nodes that are holding the smaller shards broadcast the data to the processing nodes which then return the results to the requesting node.).\n\nNodes that are holding the smaller shards broadcast the data to the processing nodes which then return the results to the requesting node.\n\nQueries can be optimized if they contain (a) ORDER BY, (b) LIMIT, or (c) if INNER/EQUI JOIN. In any of these cases, the nested loop can be terminated earlier:\n\nOrdering allows determining whether there are records left\n\nLimit states the maximum number of rows that are returned\n\nConsequently, the number of rows is significantly reduced allowing the operation to complete much faster.\n\nHash join\n\nThe Hash Join algorithm is used to execute certain types of joins in a more efficient way than Nested Loop.\n\nBasic algorithm\n\nThe operation takes place in one node (the handler node to which the client is connected). The rows of the left relation of the join are read and a hashing algorithm is applied on the fields of the relation which participate in the join condition. The hashing algorithm generates a hash value which is used to store every row of the left relation in the proper position in a hash table.\n\nThen the rows of the right relation are read one-by-one and the same hashing algorithm is applied on the fields that participate in the join condition. The generated hash value is used to make a lookup in the hash table. If no entry is found, the row is skipped and the processing continues with the next row from the right relation. If an entry is found, the join condition is validated (handling hash collisions) and on successful validation the combined tuple of left and right relation is returned.\n\nBasic hash join algorithm\n\nBlock hash join\n\nThe Hash Join algorithm requires a hash table containing all the rows of the left relation to be stored in memory. Therefore, depending on the size of the relation (number of rows) and the size of each row, the size of this hash table might exceed the available memory of the node executing the hash join. To resolve this limitation the rows of the left relation are loaded into the hash table in blocks.\n\nOn every iteration the maximum available size of the hash table is calculated, based on the number of rows and size of each row of the table but also taking into account the available memory for query execution on the node. Once this block-size is calculated the rows of the left relation are processed and inserted into the hash table until the block-size is reached.\n\nThe operation then starts reading the rows of the right relation, process them one-by-one and performs the lookup and the join condition validation. Once all rows from the right relation are processed the hash table is re-initialized based on a new calculation of the block size and a new iteration starts until all rows of the left relation are processed.\n\nWith this algorithm the memory limitation is handled in expense of having to iterate over the rows of the right table multiple times, and it is the default algorithm used for Hash Join execution by CrateDB.\n\nSwitch tables optimization\n\nSince the right table can be processed multiple times (number of rows from left / block-size) the right table should be the smaller (in number of rows) of the two relations participating in the join. Therefore, if originally the right relation is larger than the left the query planner performs a switch to take advantage of this detail and execute the hash join with better performance.\n\nDistributed block hash join\n\nSince CrateDB is a distributed database and a standard deployment consists of at least three nodes and in most case of much more, the Hash Join algorithm execution can be further optimized (performance-wise) by executing it in a distributed manner across the CrateDB cluster.\n\nThe idea is to have the hash join operation executing in multiple nodes of the cluster in parallel and then merge the intermediate results before returning them to the client.\n\nA hashing algorithm is applied on every row of both the left and right relations. On the integer value generated by this hash, a modulo, by the number of nodes in the cluster, is applied and the resulting number defines the node to which this row should be sent. As a result each node of the cluster receives a subset of the whole data set which is ensured (by the hashing and modulo) to contain all candidate matching rows.\n\nEach node in turn performs a block hash join on this subset and sends its result tuples to the handler node (where the client issued the query). Finally, the handler node receives those intermediate results, merges them and applies any pending ORDER BY, LIMIT and OFFSET and sends the final result to the client.\n\nThis algorithm is used by CrateDB for most cases of hash join execution except for joins on complex subqueries that contain LIMIT and/or OFFSET.\n\nDistributed hash join algorithm\n\nJoin optimizations\nQuery then fetch\n\nJoin operations on large relation can be extremely slow especially if the join is executed with a Nested Loop. - which means that the runtime complexity grows quadratically (O(n*m)). Specifically for cross joins this results in large amounts of data sent over the network and loaded into memory at the handler node. CrateDB reduces the volume of data transferred by employing “Query Then Fetch”: First, filtering and ordering are applied (if possible where the data is located) to obtain the required document IDs. Next, as soon as the final data set is ready, CrateDB fetches the selected fields and returns the data to the client.\n\nPush-down query optimization\n\nComplex queries such as Listing 2 require the planner to decide when to filter, sort, and merge in order to efficiently execute the plan. In this case, the query would be split internally into subqueries before running the join. As shown in Figure 5, first filtering (and ordering) is applied to relations L and R on their shards, then the result is directly broadcast to the nodes running the join. Not only will this behavior reduce the number of rows to work with, it also distributes the workload among the nodes so that the (expensive) join operation can run faster.\n\nSELECT L.a, R.x\nFROM L, R\nWHERE L.id = R.id\n  AND L.b > 100\n  AND R.y < 10\nORDER BY L.a\n\n\nListing 2. An INNER JOIN on ids (effectively an EQUI JOIN) which can be optimized.\n\nFigure 5\n\nComplex queries are broken down into subqueries that are run on their shards before joining.\n\nCross join elimination\n\nThe optimizer will try to eliminate cross joins in the query plan by changing the join-order. Cross join elimination replaces a CROSS JOIN with an INNER JOIN if query conditions used in the WHERE clause or other join conditions allow for it. An example:\n\nSELECT *\nFROM t1 CROSS JOIN t2\nINNER JOIN t3\nON t3.z = t1.x AND t3.z = t2.y\n\n\nThe cross join elimination will change the order of the query from t1, t2, t3 to t2, t1, t3 so that each join has a join condition and the CROSS JOIN can be replaced by an INNER JOIN. When reordering, it will try to preserve the original join order as much as possible. If a CROSS JOIN cannot be eliminated, the original join order will be maintained. This optimizer rule can be disabled with the optimizer eliminate cross join session setting:\n\nSET optimizer_eliminate_cross_join = false\n\n\nNote that this setting is experimental, and may change in the future."
  },
  {
    "title": "Business Analytics and Intelligence with CrateDB — CrateDB: Clients and Tools",
    "url": "https://cratedb.com/docs/crate/clients-tools/en/latest/integrate/bi.html",
    "html": "Business Analytics and Intelligence with CrateDB\n\nBusiness analytics applications and frameworks, which can be used together with CrateDB.\n\n Tutorials\n\nGuidelines about integrating CrateDB with business analytics and intelligence software.\n\nBI DataViz PowerBI Rill Tableau\n\nMicrosoft Power BI\n\nPower BI Desktop is a powerful business intelligence tool that provides a set of data analytics and visualizations. Using Power BI Desktop, users can create reports and dashboards from large datasets.\n\nFor connecting to CrateDB with Power BI, you can use the Power Query PostgreSQL connector. Earlier versions used the PostgreSQL ODBC driver. Reports with CrateDB and Power BI Desktop walks you through the process of configuring that correctly.\n\nPower BI Service is an online data analysis and visualization tool, making it possible to publish your dashboards, in order to share them with others. Real Time Reports with CrateDB and Power BI has a corresponding tutorial.\n\n  \n\nSee Also\n\nCrateDB and Power BI\n\nRill\n\nRill is an open-source operational BI framework for effortlessly transforming data sets into powerful, opinionated dashboards using SQL.\n\nUnlike most BI tools, Rill comes with its own embedded in-memory database. Data and compute are co-located, and queries return in milliseconds. So you can pivot, slice, and drill-down into your data instantly.\n\nRill takes a modern approach to Business Intelligence (BI), which is starting to leverage software engineering principles by implementing the concept of BI as code.\n\nThis methodology allows for versioning and tracking, thus improving collaboration on BI projects using code, which is more efficient and scalable than traditional BI tools, also breaking down information and knowledge barriers.\n\nRill’s design principles\n\nFeels good to use – powered by Sveltekit & DuckDB = conversation-fast, not wait-ten-seconds-for-result-set fast\n\nWorks with your local and remote datasets – imports and exports Parquet and CSV (s3, gcs, https, local)\n\nNo more data analysis “side-quests” – helps you build intuition about your dataset through automatic profiling\n\nNo “run query” button required – responds to each keystroke by re-profiling the resulting dataset\n\nRadically simple interactive dashboards – thoughtful, opinionated, interactive dashboard defaults to help you quickly derive insights from your data\n\nDashboards as code – each step from data to dashboard has versioning, Git sharing, and easy project rehydration\n\n \n\nTableau\n\nTableau is a visual business intelligence and analytics software platform. It expresses data by translating drag-and-drop actions into data queries through an intuitive interface.\n\nSee Also\n\nCrateDB and Tableau"
  },
  {
    "title": "CrateDB Npgsql Plug-In — CrateDB Npgsql",
    "url": "https://cratedb.com/docs/npgsql/en/latest/",
    "html": "latest\nCrateDB Npgsql Plug-In\n\nCaution\n\nFor CrateDB versions 4.2 and above, we recommend that you use the stock Npgsql driver instead of this one.\n\nThis software is a legacy plugin for older versions of CrateDB that lacked full compatibility with Npgsql.\n\nFor general help using the official Npgsql .NET data provider for PostgreSQL, please consult the standard Npgsql documentation. For quickly getting started, please also consult the basic demonstration program for using CrateDB with vanilla Npgsql.\n\nTable of contents\n\nGetting started\nInstall\nNext steps\nConnect to CrateDB\nSetup\nThe basics\nNext steps\nAppendices\nData types\n\nSee Also\n\nThe CrateDB Npgsql Plugin is an open source project and it is hosted on GitHub at crate-npgsql."
  },
  {
    "title": "Monitoring and Metrics with CrateDB — CrateDB: Clients and Tools",
    "url": "https://cratedb.com/docs/crate/clients-tools/en/latest/integrate/metrics.html",
    "html": "Monitoring and Metrics with CrateDB\n\nStoring metrics data for the long term is a common need in systems monitoring scenarios. CrateDB offers corresponding integration adapters.\n\n Tutorials\n\nLearn how to use CrateDB together with popular metrics collection agents, brokers, and stores.\n\nLogs Metrics Monitoring Telemetry Prometheus Telegraf\n\nPrometheus\n\nPrometheus is an open-source systems monitoring and alerting toolkit for collecting metrics data from applications and infrastructures.\n\nPrometheus collects and stores its metrics as time series data, i.e. metrics information is stored with the timestamp at which it was recorded, alongside optional key-value pairs called labels.\n\nFeatures\n\nPrometheus’s main features are:\n\na multi-dimensional data model with time series data identified by metric name and key/value pairs\n\nPromQL, a flexible query language to leverage this dimensionality\n\nno reliance on distributed storage; single server nodes are autonomous\n\ntime series collection happens via a pull model over HTTP\n\npushing time series is supported via an intermediary gateway\n\ntargets are discovered via service discovery or static configuration\n\nmultiple modes of graphing and dashboarding support\n\nRemote Endpoints and Storage\n\nThe Prometheus remote endpoints and storage subsystem, based on its remote write and remote read features, allows to transparently send and receive metric samples. It is primarily intended for long term storage.\n\nThis is where CrateDB comes into place. Using the CrateDB Prometheus Adapter, one can easily store the collected metrics data in CrateDB and take advantage of its high ingestion and query speed and friendly UI to massively scale-out Prometheus.\n\nSee Also\n\nCrateDB and Prometheus\n\nCrateDB Prometheus Adapter\n\nTelegraf\n\nTelegraf is a leading open source server agent to help you collect metrics from your stacks, sensors, and systems. More than 200 adapters to connect to other systems leaves nothing to be desired.\n\nTelegraf is a server-based agent for collecting and sending all metrics and events from databases, systems, and IoT sensors. Telegraf is written in Go and compiles into a single binary with no external dependencies, and requires a very minimal memory footprint.\n\nOverview\n\nIoT sensors: Collect critical stateful data (pressure levels, temperature levels, etc.) with popular protocols like MQTT, ModBus, OPC-UA, and Kafka.\n\nDevOps Tools and frameworks: Gather metrics from cloud platforms, containers, and orchestrators like GitHub, Kubernetes, CloudWatch, Prometheus, and more.\n\nSystem telemetry: Metrics from system telemetry like iptables, Netstat, NGINX, and HAProxy help provide a full stack view of your apps.\n\nSee Also\n\nCrateDB and Telegraf"
  },
  {
    "title": "CrateDB PDO Driver — CrateDB PDO",
    "url": "https://cratedb.com/docs/pdo/en/latest/",
    "html": "latest\nCrateDB PDO Driver\n\nA PDO driver for CrateDB.\n\nThe PHP Data Objects (PDO) is a standard PHP extension that defines a common interface for accessing databases in PHP.\n\nNote\n\nThis is a basic CrateDB driver reference.\n\nCheck out the sample application (and the corresponding sample application documentation) for a practical demonstration of this driver in use.\n\nFor general help using PDO, please consult the PDO documentation.\n\nSee Also\n\nThe CrateDB PHP PDO driver is an open source project and is hosted on GitHub.\n\nTable of contents\n\nGetting started\nPrerequisites\nSet up as a dependency\nInstall\nNext steps\nConnect to CrateDB\nData source names\nGet a connection\nCrateDB Cloud\nAdvanced settings\nBulk operations\nNext steps\nAppendices\nData types\nCompatibility"
  },
  {
    "title": "Compatibility — CrateDB: Crash CLI",
    "url": "https://cratedb.com/docs/crate/crash/en/latest/appendices/compatibility.html",
    "html": "latest\nCompatibility\n\nTable of contents\n\nVersion notes\n\nPython\n\nCrateDB\n\nVersion notes\nPython\n\nYou must be running the following versions of Python:\n\nClient Version\n\n\t\n\nPython Version\n\n\n\n\n>= 0.23\n\n\t\n\n>=3.4\n\n\n\n\n0.23.x\n\n\t\n\n3.3\n\n\n\n\n0.23.x\n\n\t\n\n2.7\n\n\n\n\n0.16.x\n\n\t\n\n2.6\n\nNote\n\nIf you are using pip, the highest compatible version of the client will be installed according to the version of Python you are using.\n\nCrateDB\n\nConsult the following table for CrateDB compatibility notes:\n\nClient Version\n\n\t\n\nCrateDB Version\n\n\t\n\nNotes\n\n\n\n\n<= 0.19\n\n\t\n\n>= 0.57\n\n\t\n\nNot supported.\n\n\n\n\nAny\n\n\t\n\n<= 0.38.x\n\n\t\n\nIt is not possible to insert nested arrays or nested objects via Crash. You must use CrateDB REST endpoint or a client library.\n\nCrateDB versions 0.39 and later allow you to insert nested arrays using array constructors, and nested objects using object literals.\n\n\n\n\nAny\n\n\t\n\n>= 2.1.x\n\n\t\n\nClient needs to connect with a valid database user to access CrateDB.\n\nThe default CrateDB user is crate and has no password is set."
  },
  {
    "title": "CrateDB Python Client — CrateDB Python",
    "url": "https://cratedb.com/docs/python/en/latest/",
    "html": "latest\nCrateDB Python Client\n\nTable of contents\n\nIntroduction\n\nDocumentation\n\nProject information\n\nIntroduction\n\nThe Python client library for CrateDB implements the Python Database API Specification v2.0 (PEP 249), and also includes the CrateDB dialect for SQLAlchemy.\n\nThe Python driver can be used to connect to both CrateDB and CrateDB Cloud, and is verified to work on Linux, macOS, and Windows. It is used by the Crash CLI, as well as other libraries and applications connecting to CrateDB from the Python ecosystem. It is verified to work with CPython, but it has also been tested successfully with PyPy.\n\nPlease make sure to also visit the section about Other connectivity options for Python, using the PostgreSQL wire protocol interface of CrateDB.\n\nDocumentation\n\nFor general help about the Python Database API, or SQLAlchemy, please consult PEP 249, the SQLAlchemy tutorial, and the general SQLAlchemy documentation. For more detailed information about how to install the client driver, how to connect to a CrateDB cluster, and how to run queries, consult the resources referenced below.\n\nGetting started\nConnect to CrateDB\nQuery CrateDB\nBlobs\nDB API\n\nInstall package from PyPI.\n\npip install crate\n\n\nConnect to CrateDB instance running on localhost.\n\n# Connect using DB API.\nfrom crate import client\nfrom pprint import pp\n\nquery = \"SELECT country, mountain, coordinates, height FROM sys.summits ORDER BY country;\"\n\nwith client.connect(\"localhost:4200\", username=\"crate\") as connection:\n    cursor = connection.cursor()\n    cursor.execute(query)\n    pp(cursor.fetchall())\n    cursor.close()\n\n\nConnect to CrateDB Cloud.\n\n# Connect using DB API.\nfrom crate import client\nconnection = client.connect(\n    servers=\"https://example.aks1.westeurope.azure.cratedb.net:4200\",\n    username=\"admin\",\n    password=\"<PASSWORD>\")\n\nSQLAlchemy\n\nThe CrateDB dialect for SQLAlchemy offers convenient ORM access and supports CrateDB’s OBJECT, ARRAY, and geospatial data types using GeoJSON, supporting different kinds of GeoJSON geometry objects.\n\nSQLAlchemy support\nIntroduction\nConnecting\nTables\nQuerying\n\nInstall package from PyPI with DB API and SQLAlchemy support.\n\npip install 'crate[sqlalchemy]' pandas\n\n\nConnect to CrateDB instance running on localhost.\n\n# Connect using SQLAlchemy Core.\nimport pkg_resources\nimport sqlalchemy as sa\nfrom pprint import pp\n\npkg_resources.require(\"sqlalchemy>=2.0\")\n\ndburi = \"crate://localhost:4200\"\nquery = \"SELECT country, mountain, coordinates, height FROM sys.summits ORDER BY country;\"\n\nengine = sa.create_engine(dburi, echo=True)\nwith engine.connect() as connection:\n    with connection.execute(sa.text(query)) as result:\n        pp(result.mappings().fetchall())\n\n\nConnect to CrateDB Cloud.\n\n# Connect using SQLAlchemy Core.\nimport sqlalchemy as sa\ndburi = \"crate://admin:<PASSWORD>@example.aks1.westeurope.azure.cratedb.net:4200?ssl=true\"\nengine = sa.create_engine(dburi, echo=True)\n\n\nLoad results into pandas DataFrame.\n\n# Connect using SQLAlchemy Core and pandas.\nimport pandas as pd\nimport sqlalchemy as sa\n\ndburi = \"crate://localhost:4200\"\nquery = \"SELECT * FROM sys.summits ORDER BY country;\"\n\nengine = sa.create_engine(dburi, echo=True)\nwith engine.connect() as connection:\n    df = pd.read_sql(sql=sa.text(query), con=connection)\n    df.info()\n    print(df)\n\nData types\n\nThe DB API driver and the SQLAlchemy dialect support CrateDB’s data types to different degrees. For more information, please consult the Data types and SQLAlchemy extension types documentation pages.\n\nData types\nDatabase API client\nSQLAlchemy\nExamples\n\nThe By example section enumerates concise examples demonstrating the different API interfaces of the CrateDB Python client library. Those are DB API, HTTP, and BLOB interfaces, and the SQLAlchemy dialect.\n\nExecutable code examples are maintained within the cratedb-examples repository.\n\nThe sample application and the corresponding sample application documentation demonstrate the use of the driver on behalf of an example “guestbook” application.\n\nUse CrateDB with pandas has corresponding code snippets about how to connect to CrateDB using pandas, and how to load and export data.\n\nThe Apache Superset and FIWARE QuantumLeap data historian projects.\n\nBy example\nDB API, HTTP, and BLOB interfaces\nSQLAlchemy by example\nProject information\nResources\n\nSource code\n\nDocumentation\n\nPython Package Index (PyPI)\n\nContributions\n\nThe CrateDB Python client library is an open source project, and is managed on GitHub. Every kind of contribution, feedback, or patch, is much welcome. Create an issue or submit a patch if you think we should include a new feature, or to report or fix a bug.\n\nDevelopment\n\nIn order to setup a development environment on your workstation, please head over to the development sandbox documentation. When you see the software tests succeed, you should be ready to start hacking.\n\nPage index\n\nThe full index for all documentation pages can be inspected at CrateDB Python Client – all pages.\n\nLicense\n\nThe project is licensed under the terms of the Apache 2.0 license, like CrateDB itself, see LICENSE."
  },
  {
    "title": "CrateDB DBAL Driver — CrateDB DBAL",
    "url": "https://cratedb.com/docs/dbal/en/latest/",
    "html": "latest\nCrateDB DBAL Driver\n\nA DBAL driver for CrateDB.\n\nDBAL is a PHP database abstraction layer that comes with database schema introspection, schema management, and PDO support.\n\nThis driver also works with Doctrine ORM, an Object-Relational Mapper.\n\nNote\n\nThis is a basic CrateDB driver reference.\n\nFor general help setting up and using Doctrine software, please consult the DBAL documentation or the Doctrine ORM documentation.\n\nSee Also\n\nThe CrateDB PHP DBAL driver is an open source project and is hosted on GitHub.\n\nTable of contents\n\nGetting started\nPrerequisites\nSet up as a dependency\nInstall\nNext steps\nConnect to CrateDB\nAuthentication\nDBAL\nDoctrine ORM\nNext steps\nAppendices\nCrateDB specific table options\nData types\nCompatibility"
  },
  {
    "title": "ETL with CrateDB — CrateDB: Clients and Tools",
    "url": "https://cratedb.com/docs/crate/clients-tools/en/latest/integrate/etl.html",
    "html": "ETL with CrateDB\n\nETL / data pipeline applications and frameworks for transferring data in and out of CrateDB.\n\n Tutorials\n\nLearn how to integrate CrateDB with popular ETL frameworks and applications.\n\nExtract, Transform, Load Data I/O, Import/Export ETL ELT\n\nApache Airflow / Astronomer\n\nApache Airflow is an open source software platform to programmatically author, schedule, and monitor workflows, written in Python. Astronomer offers managed Airflow services on the cloud of your choice, in order to run Airflow with less overhead.\n\nAirflow has a modular architecture and uses a message queue to orchestrate an arbitrary number of workers. Pipelines are defined in Python, allowing for dynamic pipeline generation and on-demand, code-driven pipeline invocation.\n\nPipeline parametrization is using the powerful Jinja templating engine. To extend the system, you can define your own operators and extend libraries to fit the level of abstraction that suits your environment.\n\nSee Also\n\nCrateDB and Apache Airflow\n\nManaged Airflow\nApache Flink\n\nApache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams, written in Java.\n\nFlink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale. It received the 2023 SIGMOD Systems Award.\n\nApache Flink greatly expanded the use of stream data-processing.\n\nManaged Flink\nApache Kafka\n\nApache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications.\n\nSee Also\n\nCrateDB and Apache Kafka\n\nManaged Kafka\ndbt\n\ndbt is an open source tool for transforming data in data warehouses using Python and SQL. It is an SQL-first transformation workflow platform that lets teams quickly and collaboratively deploy analytics code following software engineering best practices like modularity, portability, CI/CD, and documentation.\n\ndbt enables data analysts and engineers to transform their data using the same practices that software engineers use to build applications.\n\nWith dbt, anyone on your data team can safely contribute to production-grade data pipelines.\n\nThe idea is that data engineers make source data available to an environment where dbt projects run, for example with Debezium or with Airflow. Afterwards, data analysts can run their dbt projects against this data to produce models (tables and views) that can be used with a number of BI tools.\n\n  \n\nManaged dbt\nDebezium\n\nDebezium is an open source distributed platform for change data capture. After pointing it at your databases, you are able to subscribe to the event stream of all database update operations.\n\nKestra\n\nKestra is an open source workflow automation and orchestration toolkit with a rich plugin ecosystem. It enables users to automate and manage complex workflows in a streamlined and efficient manner, defining them both declaratively, or imperatively using any scripting language like Python, Bash, or JavaScript.\n\nKestra comes with a user-friendly web-based interface including a live-updating DAG view, allowing users to create, modify, and manage scheduled and event-driven flows without the need for any coding skills.\n\nPlugins are at the core of Kestra’s extensibility. Many plugins are available from the Kestra core team, and creating your own is easy. With plugins, you can add new functionality to Kestra.\n\n  \n\nSee Also\n\nCrateDB and Kestra\n\nNode-RED\n\nNode-RED is a programming tool for wiring together hardware devices, APIs and online services within a low-code programming environment for event-driven applications. It allows orchestrating message flows and transformations through an intuitive web interface.\n\nIt provides a browser-based editor that makes it easy to wire together flows using the wide range of elements called “nodes” from the palette that can be deployed to its runtime in a single-click.\n\nSee Also\n\nCrateDB and Node-RED\n\nManaged Node-RED\nSinger / Meltano\n\nSinger is a composable open source ETL framework and specification, including powerful data extraction and consolidation elements. Meltano is a declarative code-first data integration engine adhering to the Singer specification.\n\nMeltano Hub is the single source of truth to find any Meltano plugins as well as Singer taps and targets.\n\nSQL Server Integration Services\n\nMicrosoft SQL Server Integration Services (SSIS) is a component of the Microsoft SQL Server database software that can be used to perform a broad range of data migration tasks.\n\nSSIS is a platform for data integration and workflow applications. It features a data warehousing tool used for data extraction, transformation, and loading (ETL). The tool may also be used to automate maintenance of SQL Server databases and updates to multidimensional cube data.\n\nIntegration Services can extract and transform data from a wide variety of sources such as XML data files, flat files, and relational data sources, and then load the data into one or more destinations.\n\nIntegration Services includes a rich set of built-in tasks and transformations, graphical tools for building packages, and an SSIS Catalog database to store, run, and manage packages."
  },
  {
    "title": "Response formats — CrateDB: Crash CLI",
    "url": "https://cratedb.com/docs/crate/crash/en/latest/appendices/formats.html",
    "html": "latest\nResponse formats\n\nCrash supports multiple output formats.\n\nYou can select between these output formats using either Commands or Table of contents.\n\nTable of contents\n\ntabular\n\njson\n\njson_row\n\ncsv\n\nraw\n\nmixed\n\ntabular\n\nThis is the default output format.\n\nQuery results are printed as a plain text formatted table.\n\nFor example:\n\n+--------+---------+\n| name   | version |\n+--------+---------+\n| crate1 | 0.46.3  |\n+--------+---------+\n| crate2 | 0.46.3  |\n+--------+---------+\n\njson\n\nQuery results are printed as a JSON formatted object array. Keys hold the column name, and the key value holds the row value.\n\nTip\n\nThis format is useful for dumping results to a file that can be parsed by another tool.\n\nHere’s an example:\n\n[\n  {\n    \"name\": \"crate1\",\n    \"version\": \"0.46.3\"\n  },\n  {\n    \"name\": \"crate2\",\n    \"version\": \"0.46.3\"\n  }\n]\n\njson_row\n\nQuery results are printed as a JSON formatted object array, like the json format. However, each row gets its own line. For example:\n\n{\"name\": \"crate1\", \"version\": \"0.46.3\"}\n{\"name\": \"crate2\", \"version\": \"0.46.3\"}\n\n\nTip\n\nThis format is compatible with COPY FROM for re-importing data.\n\ncsv\n\nQuery results are printed as comma separated values (CSV).\n\nSpecifically:\n\nThe delimiter is a comma (,)\n\nThe quote character is an apostrophe (')\n\nThe escape character is a reverse solidus (\\)\n\nThe first line of the CSV output contains the name of the selected columns:\n\nname,version\ncrate1,0.46.3\ncrate2,0.46.3\n\n\nobject types and array types are returned as a JSON string:\n\nname,settings[\\'udc\\']\ncrate,'{\"enabled\": true, \"initial_delay\": \"10m\"}'\n\nraw\n\nQuery results are printed as the raw JSON produced by the CrateDB Python client library used by Crash.\n\nThis JSON structure provides:\n\nA rows key for holding a list of rows\n\nA cols key for holding a list of column titles\n\nA rowcount key which holds the total number of rows returned\n\nA duration key which holds the total duration of the query execution in seconds\n\nHere’s an example:\n\n{\n  \"rows\": [\n    [\n      \"crate1\",\n      \"0.46.0\"\n    ],\n    [\n      \"crate2\",\n      \"0.46.0\"\n    ]\n  ],\n  \"cols\": [\n    \"name\",\n    \"0.46.3\"\n  ],\n  \"rowcount\": 1,\n  \"duration\": 0.00477246\n}\n\nmixed\n\nQuery results are printed as a plain text formatted table.\n\nHowever, unlike the tabular format, each row (separated by - characters) contains the column title and column value (separated by the | character).\n\nExample:\n\nname    | crate1\nversion | 0.46.3\n---------------------------------------------------------------\nname    | crate2\nversion | 0.46.3\n---------------------------------------------------------------\n"
  },
  {
    "title": "CrateDB legacy JDBC driver — CrateDB JDBC",
    "url": "https://cratedb.com/docs/jdbc/en/latest/",
    "html": "latest\nCrateDB legacy JDBC driver\n\nTable of contents\n\nIntroduction\n\nSynopsis\n\nDetails\n\nDocumentation\n\nProject information\n\nIntroduction\n\nA JDBC driver for CrateDB, based on the PostgreSQL JDBC Driver. It can be used with CrateDB version 0.38.0 and newer.\n\nThis is a JDBC Type 4 driver, adhering to the JDBC 4.1 specification. It is written in pure Java, and communicates with the database using the PostgreSQL Wire Protocol.\n\nJDBC is a standard Java API that provides common interfaces for accessing databases in Java.\n\nSynopsis\n\nConnect to CrateDB instance running on localhost.\n\nimport java.sql.Connection;\nimport java.sql.DriverManager;\n\nConnection conn = DriverManager.getConnection(\"jdbc:crate://localhost:5432/\");\n\n\nConnect to CrateDB Cloud.\n\nimport java.sql.Connection;\nimport java.sql.DriverManager;\nimport java.util.Properties;\n\nProperties connectionProps = new Properties();\nconnectionProps.put(\"user\", \"admin\");\nconnectionProps.put(\"password\", \"<PASSWORD>\");\nconnectionProps.put(\"tcpKeepAlive\", true);\n\nConnection conn = DriverManager.getConnection(\"jdbc:crate://example.aks1.westeurope.azure.cratedb.net:5432/?user=crate\", connectionProps);\n\nDetails\nOverview\n\nFor general purpose use, we recommend to use the official PostgreSQL JDBC Driver.\n\nThis JDBC driver is needed in certain scenarios like the one outlined at Apache Kafka, Apache Flink, and CrateDB. The background is that, when using the postgresql:// JDBC driver prefix, the Apache Flink JDBC Connector will implicitly select the corresponding dialect implementation for PostgreSQL.\n\nIn turn, this will employ a few behaviours that strictly expect a PostgreSQL server on the other end, so that some operations will fail on databases offering wire-compatibility with PostgreSQL, but do not provide certain features like the hstore or jsonb extensions. Also, tools like Dataiku need this driver to implement transaction commands like ROLLBACK as a no-op.\n\nDifferences\n\nThe driver is based upon a fork of the PostgreSQL JDBC Driver, see pgjdbc driver fork. On a high-level perspective, this list enumerates a few behavioral differences.\n\nThe CrateDB driver deserializes objects to a Map, the official one treats them as JSON.\n\nA few metadata functions have been adjusted to better support CrateDB’s type system.\n\nRead up on further details at the Internals section.\n\nDocumentation\n\nFor general help about JDBC, please consult the JDBC tutorial and the JDBC API documentation.\n\nInstallation\nConnect to CrateDB\nAppendices\nData types\nCompatibility notes\nInternals\nExamples\n\nThe Basic example for connecting to CrateDB and CrateDB Cloud using JDBC demonstrates CrateDB’s PostgreSQL wire protocol compatibility by exercising a basic example using both the vanilla pgJDBC Driver and the CrateDB JDBC Driver.\n\nThe sample application and the corresponding sample application documentation demonstrate the use of the driver on behalf of an example “guestbook” application, using Spring Data JDBC.\n\nThe article Build a data ingestion pipeline using Kafka, Flink, and CrateDB, and the accompanying repositories Apache Kafka, Apache Flink, and CrateDB and Flink example jobs for CrateDB.\n\nProject information\nResources\n\nSource code\n\nDocumentation\n\nMaven Repository\n\nContributions\n\nThe CrateDB legacy JDBC driver library is an open source project, and is managed on GitHub. Every kind of contribution, feedback, or patch, is much welcome. Create an issue or submit a patch if you think we should include a new feature, or to report or fix a bug.\n\nDevelopment\n\nIn order to setup a development environment on your workstation, please head over to the development sandbox documentation. When you see the software tests succeed, you should be ready to start hacking.\n\nLicense\n\nThe project is licensed under the terms of the Apache 2.0 license, like CrateDB itself, see LICENSE."
  },
  {
    "title": "Visualize data in CrateDB — CrateDB: Clients and Tools",
    "url": "https://cratedb.com/docs/crate/clients-tools/en/latest/integrate/visualize.html",
    "html": "Visualize data in CrateDB\n\nDashboard and other data visualization applications and toolkits for visualizing data stored inside CrateDB.\n\n Tutorials\n\nGuidelines about data analysis and visualization with CrateDB.\n\nDataViz EDA BI\n\nApache Superset / Preset\n\nApache Superset is an open-source modern data exploration and visualization platform, written in Python.\n\nPreset offers a managed, elevated, and enterprise-grade SaaS for open-source Apache Superset.\n\n \n\nSee Also\n\nCrateDB and Superset\n\nManaged Superset\nCluvio\n\nCluvio is a programmable and interactive dashboarding platform — your analytics cockpit. Run queries, filter your results, choose the most vivid way to display them, and share them with your colleagues and partners without efforts.\n\nCluvio dashboards are interactive, so you can easily change aggregation, select a specific timerange or filter dashboards by any individual attribute of your data.\n\nUse SQL and R to analyze your data and create beautiful, interactive dashboards for your entire company in few minutes.\n\n \n\nDash\n\nDash is a low-code framework for rapidly building data apps in Python, based on Plotly. Built on top of Plotly.js, React and Flask, Dash ties modern UI elements like dropdowns, sliders, and graphs, directly to your analytical Python code.\n\nDash is a trusted Python framework for building ML & data science web apps. Many specialized open-source Dash libraries exist that are tailored for building domain-specific Dash components and applications.\n\n \n\nDash Enterprise\nExplo\n\nExplo is a software platform for personalized and real-time customer facing analytics. Organizations use Explo’s platform services “Explore”, “Host”, “Report builder”, and “Email”, to activate and share data with their customers.\n\nExplo Explore integrates directly into your web portal or application and provides your customers with a complete self-service data toolkit, which can also be used to run white-labeled data portals.\n\n \n\nGrafana\n\nGrafana OSS is the leading open-source metrics visualization tool that helps you build real-time dashboards, graphs, and many other sorts of data visualizations. Grafana Cloud is a fully-managed service offered by Grafana Labs.\n\nGrafana complements CrateDB in monitoring and visualizing large volumes of machine data in real-time.\n\nConnecting to a CrateDB cluster will use the Grafana PostgreSQL data source adapter. The following tutorials outline how to configure Grafana to connect to CrateDB, and how to run a database query.\n\n \n\nSee Also\n\nCrateDB and Grafana\n\nManaged Grafana\nhvPlot and Datashader\n\nhvPlot is a familiar and high-level API for data exploration and visualization. Datashader is a graphics pipeline system for creating meaningful representations of large datasets quickly and flexibly.\n\nIt is used on behalf of the hvPlot package, which is based on HoloViews, from the family of HoloViz packages of the PyViz ecosystem.\n\nWith Datashader, you can “just plot” large datasets and explore them instantly, with no parameter tweaking, magic numbers, subsampling, or approximation, up to the resolution of the display.\n\nhvPlot sources its power in the HoloViz ecosystem. With HoloViews, you get the ability to easily layout and overlay plots, with Panel, you can get more interactive control of your plots with widgets, with DataShader, you can visualize and interactively explore very large data, and with GeoViews, you can create geographic plots.\n\n \n\nMetabase\n\nMetabase is the ultimate data analysis and visualization tool that unlocks the full potential of your data. Build for data and made for everyone, Metabase can be leveraged with no SQL required.\n\nFast analytics with the friendly UX and integrated tooling to let your company explore data on their own.\n\n \n\nSee Also\n\nCrateDB and Metabase\n\nManaged Metabase\nPlotly\n\nPlotly Open Source Graphing Libraries make interactive, publication-quality graphs. Line plots, scatter plots, area charts, bar charts, error bars, box plots, histograms, heatmaps, subplots, multiple-axes, polar charts, bubble charts, and maps.\n\nThe supported programming languages / libraries / frameworks are Python, R, Julia, JavaScript, ggplot2, F#, MATLAB®, and Dash.\n\nBased on Plotly, Dash is a low-code framework for rapidly building data apps in Python.\n\n "
  },
  {
    "title": "CrateDB and ORM libraries — CrateDB: Clients and Tools",
    "url": "https://cratedb.com/docs/crate/clients-tools/en/latest/connect/orm.html",
    "html": "CrateDB and ORM libraries\n\nORM libraries and frameworks which can be used together with CrateDB.\n\n Tutorials\n\nLearn how to use CrateDB together with popular open-source ORM libraries.\n\nORM SQLAlchemy\n\nSQLAlchemy\n\nSQLAlchemy is the Python SQL toolkit and Object Relational Mapper that gives application developers the full power and flexibility of SQL.\n\nPython-based DataFrame and ML libraries, and a few ETL frameworks, are using SQLAlchemy as database adapter library when connecting to RDBMS."
  },
  {
    "title": "The CrateDB Shell — CrateDB: Crash CLI",
    "url": "https://cratedb.com/docs/crate/crash/en/latest/index.html",
    "html": "latest\nThe CrateDB Shell\n\nThe CrateDB Shell (aka Crash) is an interactive command-line interface (CLI) tool for working with CrateDB.\n\nSee Also\n\nCrash is an open source project and is hosted on GitHub.\n\nScreenshots\n\n  \n\nTable of contents\n\nGetting started\nInstallation\nRun\nQuery\nRunning Crash\nCommand-line options\nUser configuration directory\nEnvironment variables\nStatus messages\nUsing a pager program\nCommands\nTroubleshooting\nDebugging connection errors\nSSL connection errors\nAppendices\nResponse formats\nCompatibility"
  },
  {
    "title": "Appendices — CrateDB: Crash CLI",
    "url": "https://cratedb.com/docs/crate/crash/en/latest/appendices/index.html",
    "html": "latest\nAppendices\n\nSupplementary information for the Crash\n\nTable of contents\n\nResponse formats\ntabular\njson\njson_row\ncsv\nraw\nmixed\nCompatibility\nVersion notes"
  },
  {
    "title": "Using command-line programs with CrateDB — CrateDB: Clients and Tools",
    "url": "https://cratedb.com/docs/crate/clients-tools/en/latest/connect/cli.html",
    "html": "Using command-line programs with CrateDB\n\nThis section provides a quick overview about a few CLI programs, and how to use them for connecting to CrateDB clusters. We recommend to use crash, psql, http (HTTPie), or curl.\n\nYou can use them to quickly validate HTTP and PostgreSQL connectivity to your database cluster, or to conduct basic scripting.\n\nBefore running the command-line snippets outlined below, please use the correct settings instead of the placeholder tokens <hostname>, <username> and <password>.\n\nWhen using CrateDB Cloud, <hostname> will be something like <clustername>.{aks1,eks1}.region.{azure,aws}.cratedb.net.\n\ncrash\n\nThe CrateDB Shell is an interactive command-line interface (CLI) tool for working with CrateDB. For more information, see the documentation about crash.\n\nCrateDB and CrateDB Cloud\nCRATEPW=<password> \\\n    crash --hosts 'https://<hostname>:4200' --username '<username>' \\\n    --command \"SELECT 42.42;\"\n\nCrateDB on localhost\npsql\n\npsql is a terminal-based front-end to PostgreSQL. It enables you to type in queries interactively, issue them to PostgreSQL, and see the query results. For more information, see the documentation about psql.\n\nCrateDB and CrateDB Cloud\nPGUSER=<username> PGPASSWORD=<password> \\\n    psql postgresql://<hostname>/crate --command \"SELECT 42.42;\"\n\nCrateDB on localhost\nHTTPie\n\nThe HTTPie CLI is a modern, user-friendly command-line HTTP client with JSON support, colors, sessions, downloads, plugins & more. For more information, see the documentation about HTTPie.\n\nCrateDB and CrateDB Cloud\nhttp https://<username>:<password>@<hostname>:4200/_sql?pretty\" \\\n    stmt=\"SELECT 42.42;\"\n\nCrateDB on localhost\ncurl\n\nThe venerable curl is the ubiquitous command line tool and library for transferring data with URLs. For more information, see the documentation about curl.\n\nThis example combines it with jq, a lightweight and flexible command-line JSON processor.\n\nCrateDB and CrateDB Cloud\necho '{\"stmt\": \"SELECT 42.42;\"}' \\\n    | curl \"https://<username>:<password>@<hostname>:4200/_sql?pretty\" --silent --data @- | jq\n\nCrateDB on localhost"
  },
  {
    "title": "The CrateDB Shell — CrateDB: Crash CLI",
    "url": "https://cratedb.com/docs/crate/crash/en/0.31/index.html",
    "html": "0.31\nThe CrateDB Shell\n\nThe CrateDB Shell (aka Crash) is an interactive command-line interface (CLI) tool for working with CrateDB.\n\nSee Also\n\nCrash is an open source project and is hosted on GitHub.\n\nScreenshots\n\n  \n\nTable of contents\n\nGetting started\nInstallation\nRun\nQuery\nRunning Crash\nCommand-line options\nUser configuration directory\nEnvironment variables\nStatus messages\nUsing a pager program\nCommands\nTroubleshooting\nDebugging connection errors\nSSL connection errors\nAppendices\nResponse formats\nCompatibility"
  },
  {
    "title": "Connect to a CrateDB cluster — CrateDB: Clients and Tools",
    "url": "https://cratedb.com/docs/crate/clients-tools/en/latest/connect/index.html",
    "html": "Connect to a CrateDB cluster\n\nThis documentation section is about connecting your applications to CrateDB and CrateDB Cloud, using database drivers, and compatibility-adapters and -dialects.\n\nProtocol Support\n\nCrateDB supports both the HTTP protocol and the PostgreSQL wire protocol, which ensures that many clients that work with PostgreSQL, will also work with CrateDB. Through corresponding drivers, CrateDB is compatible with ODBC, JDBC, and other database API specifications.\n\nWhile we generally recommend the PostgreSQL interface (PG) for maximum compatibility in PostgreSQL environments, the HTTP interface supports CrateDB bulk operations and CrateDB BLOBs, which are not supported by the PostgreSQL protocol.\n\nThe HTTP protocol can also be used to connect from environments where PostgreSQL-based communication is not applicable.\n\nConfigure\n\nIn order to connect to CrateDB, your application or driver needs to be configured with corresponding connection properties. Please note that different applications and drivers may obtain connection properties in different formats.\n\nCrateDB and CrateDB Cloud\n\nConnection properties\n\nHost\n\n<clustername>.cratedb.net\n\nPort\n\n5432 (PostgreSQL) or 4200 (HTTP)\n\nUser\n\n<username>\n\nPass\n\n<password>\n\nConnection-string examples\n\nA native PostgreSQL connection string. postgresql://<username>@<clustername>.cratedb.net/crate\n\nA connection string for SQLAlchemy or Apache Flink. crate://<username>@<clustername>.cratedb.net/crate\n\nAn HTTP URL to visit Admin UI. https://<username>@<clustername>.cratedb.net:4200/\n\nCrateDB on localhost\n\nTip\n\nSpecify the schema name doc, if you are asked for a database name.\n\nThe default superuser on a vanilla CrateDB cluster is called crate, without a password.\n\nWhen aiming to authenticate properly, please learn about the available authentication options.\n\nClient Libraries\n\nThis section lists drivers and adapters for relevant programming languages, frameworks, and environments.\n\nPostgreSQL\n\nThe drivers listed in this section all use the CrateDB PostgreSQL interface.\n\nDriver/Adapter\n\nDescription\n\nInfo\n\n-\n\nPostgreSQL ODBC\n\nThe official PostgreSQL ODBC Driver. For connecting to CrateDB from any environment that supports it.\n\n.NET\n\nNpgsql\n\nAn open source ADO.NET Data Provider for PostgreSQL, for program written in C#, Visual Basic, and F#.\n\n \n\n.NET\n\nCrateDB Npgsql fork\n\nThis fork of the official driver was needed prior to CrateDB 4.2.\n\nGolang\n\npgx\n\nA pure Go driver and toolkit for PostgreSQL.\n\nJava\n\nPostgreSQL JDBC\n\nThe official PostgreSQL JDBC Driver. For connecting to CrateDB from any environment that supports it.\n\n  \n\nJava\n\nCrateDB PgJDBC fork\n\nFor connecting to CrateDB with specialized type system support and other tweaks. Ignores the ROLLBACK statement and the hstore and jsonb extensions.\n\nNode.js\n\nnode-postgres\n\nA collection of Node.js modules for interfacing with a PostgreSQL database using JavaScript or TypeScript.\n\nHas support for callbacks, promises, async/await, connection pooling, prepared statements, cursors, streaming results, C/C++ bindings, rich type parsing, and more.\n\n \n\nPHP\n\nPDO_PGSQL\n\nFor connecting to CrateDB from PHP, supporting its PDO interface.\n\n \n\nPHP\n\nAMPHP\n\nFor connecting to CrateDB using AMPHP, an Async PostgreSQL client for PHP. AMPHP is a collection of high-quality, event-driven libraries for PHP designed with fibers and concurrency in mind.\n\n \n\nPython\n\nasyncpg\n\nFor connecting to CrateDB from Python, supporting Python’s asyncio.\n\n \n\nPython\n\npsycopg3\n\nFor connecting to CrateDB from Python, supporting Python’s asyncio.\n\n \n\nHTTP\n\nThe drivers listed in this section all use the CrateDB HTTP interface.\n\nDriver/Adapter\n\nDescription\n\nInfo\n\nNode.js\n\nnode-crate\n\nA JavaScript library connecting to the CrateDB HTTP API.\n\n \n\nPHP\n\nCrateDB PDO driver\n\nFor connecting to CrateDB from PHP.\n\n \n\nPHP\n\nCrateDB DBAL adapter\n\nFor connecting to CrateDB from PHP, using DBAL and Doctrine.\n\n \n\nPython\n\nCrateDB Python driver\n\nFor connecting to CrateDB from Python. Has support for CrateDB BLOBs.\n\n  \n\nPython\n\nSQLAlchemy dialect\n\nFor connecting to CrateDB from Python, using SQLAlchemy.\n\n  \n\nRuby\n\nCrateDB Ruby driver\n\nA Ruby client library for CrateDB.\n\n  \n\nRuby\n\nCrateDB ActiveRecord adapter\n\nRuby on Rails ActiveRecord adapter for CrateDB.\n\nTip\n\nPlease visit the Build Status page for an overview about the integration status of the client drivers listed above, and more."
  },
  {
    "title": "Troubleshooting — CrateDB: Crash CLI",
    "url": "https://cratedb.com/docs/crate/crash/en/latest/troubleshooting.html",
    "html": "latest\nTroubleshooting\n\nThis sections enumerates potential connection problems with crash, and how to investigate and resolve them.\n\nDebugging connection errors\n\nIf you are connecting to CrateDB, for example like this:\n\ncrash --hosts 'http://localhost:4200' -U 'admin' -W\n\n\n… and crash responds with a connection error message like this:\n\nCONNECT ERROR\n\n\nyou may want to add the --verbose command line option, in order to find out about the reason why the connection fails. It could be a DNS / name resolution error, or it could be a problem related to SSL termination.\n\nOther than --verbose, you can also use the shorthand version -v:\n\ncrash --hosts 'http://localhost:4200' -U 'admin' -W -v\n\nSSL connection errors\n\nA recent problem outlined SSL connectivity problems when connecting to CrateDB Cloud:\n\ncrash --hosts 'https://MY-CLUSTER-NAME.eks1.eu-west-1.aws.cratedb.net:4200' -U 'admin' -W -v\n\n\nThe verbose output using crash -v signaled a certificate verification error like that:\n\nServer not available, exception: HTTPSConnectionPool(host='MY-CLUSTER-NAME.eks1.eu-west-1.aws.cratedb.net', port=4200):\nMax retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '\n[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)')))\n\n\nIf you are on macOS, the Python Installer offers an easy option to install the required SSL root certificates. Because crash uses Python, this is the right choice to resolve the problem durably.\n\nIn order to install the SSL root certificates retroactively, you can use a command like:\n\n/Applications/Python 3.11/Install Certificates.command\n"
  },
  {
    "title": "CrateDB Ecosystem Catalog — CrateDB: Clients and Tools",
    "url": "https://cratedb.com/docs/crate/clients-tools/en/latest/#",
    "html": "CrateDB Ecosystem Catalog\n\nDatabase drivers, libraries, frameworks, and applications for CrateDB.\n\nAbout CrateDB\n\nCrateDB is a distributed and scalable open-source SQL database based on Lucene, with PostgreSQL compatibility. CrateDB clusters store information in the range of billions of records, and terabytes of data, and run analytics in near real time, even with complex queries. CrateDB can be used for enterprise data warehouse workloads, it works across clouds and scales with your data.\n\nConnectivity\n\nThe canonical set of database drivers, client- and developer-applications, and how to configure them to connect to CrateDB.\n\nJust to name a few, the sections below are about the CrateDB Admin UI, the Crash CLI terminal program, connecting with PostgreSQL’s psql client, the DataGrip, and DBeaver IDE applications, the Java/JDBC/Python drivers, the SQLAlchemy and Flink dialects, and more.\n\n IDE\n\nConnect to CrateDB using a database IDE like DataGrip or DBeaver.\n\n CLI\n\nConnect to CrateDB using command-line based terminal programs.\n\n Drivers\n\nList of HTTP and PostgreSQL client drivers, and tutorials.\n\n DataFrame Libraries\n\nConnectivity with DataFrame libraries like pandas and Dask.\n\n ORM Libraries\n\nConnectivity with ORM libraries like SQLAlchemy.\n\nIntegrations\n\nCrateDB integrates with a diverse set of applications and tools concerned with analytics, visualization, and data wrangling, in the areas of data loading and export (ETL), business intelligence (BI), metrics aggregation and monitoring, machine learning, and more.\n\n Overview\n\nLearn how to use CrateDB with popular applications, frameworks, and tools. All on one page.\n\n ETL\n\nETL applications and frameworks for transferring data in and out of CrateDB.\n\n System Metrics\n\nIntegrations and long-term storage for systems monitoring tools like Prometheus and Telegraf.\n\n Data Visualization\n\nVisualize information in your CrateDB cluster.\n\n Business Intelligence\n\nAnalyze information in your CrateDB cluster.\n\n Machine Learning\n\nAdapters and integrations with machine learning frameworks.\n\nNote\n\nContributions to the pages in this section and subsections are much welcome. If you would like to add items about integrations with other tools to this documentation section, please get in touch, or directly edit this page on GitHub. You will find corresponding links within the topmost right navigation element.\n\nSee Also\n\nLooking for the previous content on this page? Visit [Legacy] CrateDB Clients and Tools."
  },
  {
    "title": "Search — CrateDB: Clients and Tools",
    "url": "https://cratedb.com/docs/crate/clients-tools/en/latest/search.html",
    "html": "Search\n\nFrom here you can search these documents. Enter your search words into the box below and click \"search\". Note that the search function will automatically search for all of the words. Pages containing fewer words won't appear in the result list.\n\nSearch"
  },
  {
    "title": "Using database IDEs with CrateDB — CrateDB: Clients and Tools",
    "url": "https://cratedb.com/docs/crate/clients-tools/en/latest/connect/ide.html",
    "html": "Using database IDEs with CrateDB\n\nMostly through its PostgreSQL interface, CrateDB supports working with popular database IDE (Integrated Development Environment) applications.\n\nDataGrip\n\nDataGrip is a cross-platform database IDE that is tailored to suit the specific needs of professional SQL developers.\n\nConnecting DataGrip to CrateDB uses the vanilla PostgreSQL JDBC Driver, the blog article Blog: Use CrateDB With DataGrip explains how it works.\n\n \n\nCaution\n\nPlease note while the query console works well, you will notice that many UI actions do not work yet. We are tracking corresponding gaps at Tool: DataGrip, and appreciate any contributions to improve the situation.\n\nDBeaver\n\nDBeaver is a multipurpose database tool for developers and database administrators. Because CrateDB provides a JDBC driver, you can access CrateDB with any client tool that supports JDBC drivers.\n\nConnecting DBeaver to CrateDB uses the legacy PostgreSQL JDBC Driver, the blog article Blog: Use CrateDB With DBeaver explains how it works.\n\n "
  },
  {
    "title": "Running Crash — CrateDB: Crash CLI",
    "url": "https://cratedb.com/docs/crate/crash/en/latest/run.html",
    "html": "latest\nRunning Crash\n\nThis document covers the basics of running Crash from the command-line.\n\nNote\n\nFor help using Crash for the first time, check out Getting started.\n\nTable of contents\n\nCommand-line options\n\nUser configuration directory\n\nEnvironment variables\n\nStatus messages\n\nUsing a pager program\n\nCommand-line options\n\nThe crash executable supports multiple command-line options:\n\nArgument\n\n\t\n\nDescription\n\n\n\n-h,\n--help\n\t\n\nPrint the help message and exit.\n\n\n\n-v,\n--verbose\n\t\n\nPrint debug information to STDOUT.\n\n\n\n\n--version\n\n\t\n\nPrint the Crash version and exit.\n\n\n\n\n--sysinfo\n\n\t\n\nPrint system and cluster information.\n\n\n\n-U <USERNAME>,\n--username <USERNAME>\n\t\n\nAuthenticate as <USERNAME>.\n\n\n\n-W,\n--password\n\t\n\nForce a password prompt.\n\nIf not set, a password prompt happens when required.\n\n\n\n-c <STATEMENT>,\n--command <STATEMENT>\n\t\n\nExecute the <STATEMENT> and exit.\n\n\n\n\n--hosts <HOSTS>\n\n\t\n\nConnect to <HOSTS>.\n\n<HOSTS> can be a single host, or it can be a space separated list of hosts.\n\nIf multiple hosts are specified, Crash will attempt to connect to all of them. The command will succeed if at least one connection is successful.\n\n\n\n\n--timeout <TIMEOUT>\n\n\t\n\nConfigure network timeout in “<connect_sec>” or “<connect_sec>,<read_sec>” format.\n\nThe default value is “5,-1”, configuring a connect timeout of five seconds with infinite read timeout.\n\n\n\n\n--history <FILENAME>\n\n\t\n\nUse <FILENAME> as a history file.\n\nDefaults to the crash_history file in the user configuration directory.\n\n\n\n\n--config <FILENAME>\n\n\t\n\nUse <FILENAME> as a configuration file.\n\nDefaults to the crash.cfg file in the user configuration directory.\n\n\n\n\n--format <FORMAT>\n\n\t\n\nThe output <FORMAT> of the SQL response.\n\nAvailable formats are: tabular, raw, json, json_row, csv and mixed.\n\n\n\n\n--schema <SCHEMA>\n\n\t\n\nThe default schema that should be used for statements.\n\n\n\n-A ,\n--no-autocomplete\n\t\n\nDisable SQL keywords autocompletion.\n\nAutocompletion requires a minimum terminal height of eight lines due to size of the dropdown overlay for suggestions. Disabling autocompletion removes this limitation.\n\n\n\n-a ,\n--autocapitalize\n\t\n\nEnable automatic capitalization of SQL keywords while typing.\n\nThis feature is experimental and may be removed in future versions.\n\n\n\n\n--verify-ssl\n\n\t\n\nForce the verification of the server SSL certificate.\n\n\n\n\n--cert-file <FILENAME>\n\n\t\n\nUse <FILENAME> as the client certificate file.\n\n\n\n\n--key-file <FILENAME>\n\n\t\n\nUse <FILENAME> as the client certificate key file.\n\n\n\n\n--ca-cert-file <FILENAME>\n\n\t\n\nUse <FILENAME> as the certificate authority (CA) certificate file (used to verify the server certificate).\n\nExamples\n\nHere’s an example command:\n\nsh$ crash --hosts node1.example.com \\\n                node2.example.com \\\n        -c \"SELECT * FROM sys.nodes\" \\\n        --format json \\\n    > output.json\n\n\nThis command will:\n\nRun crash, which will:\n\nAttempt to connect to node1.example.com and node2.example.com\n\nExecute SELECT * FROM sys.nodes\n\nPrint the results as JSON\n\nRedirect output to the output.json file\n\nTip\n\nInstead of redirecting to a file, you can pipe into a tool like jq for for further processing of the response.\n\nWe can modify this command to use SSL, like so:\n\nsh$ crash --hosts node1.example.com \\\n                node2.example.com \\\n        --verify-ssl true \\\n        --cert-file ~/.certs/client.crt \\\n        --key-file ~/.certs/client.key \\\n        --ca-cert-file ~/.certs/server-ca.crt \\\n        -c \"SELECT * FROM sys.nodes\" \\\n        --format json \\\n    > output.json\n\n\nHere, we’re using:\n\n~/.certs/client.crt as the client certificate\n\n~/.certs/client.key as the client certificate key\n\n~/.certs/server-ca.crt as the server CA certificate\n\nUser configuration directory\n\nThe crash executable looks for its configuration file and history file in the appropriate user configuration directory for your operating system.\n\nFor Linux, that is:\n\n~/.config/Crate\n\n\nFor macOS, it is:\n\n~/Library/Application Support/Crate\n\n\nAnd for Microsoft Windows, it is:\n\nC:\\\\Users\\user\\AppData\\Local\\Crate\\Crate\n\nEnvironment variables\n\nThe crash executable will take configuration from the environment.\n\nAt the moment, only one environment variable is supported.\n\nCRATEPW\n\nThe password to be used if password authentication is necessary.\n\nCaution\n\nStoring passwords in the environment is not always a good idea from a security perspective.\n\nYou can set CRATEPW like so:\n\nsh$ export CRATEPW=<PASSWORD>\n\n\nHere, <PASSWORD> should be replaced with the password you want to use.\n\nFor the duration of your current session, invokations of crash will use this password when needed (unless you force a password prompt with --password or -W).\n\nStatus messages\n\nWhen used interactively, Crash will print a status message after every successfully executed query.\n\nNote\n\nWhen used non-interactively, these messages are omitted.\n\nExamples of non-interactive use include: executing crash in a shell script, redirecting output to a file, or piping output into a another command\n\nIf the query alters rows, the status message looks like this:\n\n<STATEMENT>, <NUMBER> row(s) affected (<DURATION> sec)\n\n\nIf the query returns rows, the message looks like this:\n\n<STATEMENT> <NUMBER> row(s) in set (<DURATION> sec)\n\n\nIn both instances:\n\n<STATEMENT> is the query keyword (e.g. CREATE, INSERT, UPDATE, DELETE, SELECT, and so on)\n\n<NUMBER> is the number of rows (-1 for queries that do not affect any rows or if the row count is unknown)\n\n<DURATION> is the total number of seconds the query took to execute on the cluster\n\nUsing a pager program\n\nYou can use applications like the jless JSON exploration tool or the pspg pager program to view the result sets, by utilizing the \\pager command.\n\nExample\n\nUse those instructions to drill down into the results of your query by exploring it using jless, like outlined within the screencast above:\n\ncr> \\pager jless\ncr> \\format json\ncr> SELECT * FROM sys.nodes;\n"
  },
  {
    "title": "Getting started — CrateDB: Crash CLI",
    "url": "https://cratedb.com/docs/crate/crash/en/latest/getting-started.html",
    "html": "latest\nGetting started\n\nTable of contents\n\nInstallation\n\nPython package\n\nStandalone\n\nLegacy versions\n\nRun\n\nQuery\n\nInstallation\n\nSee Also\n\nConsult the Compatibility appendix for prerequisites information.\n\nPython package\n\nCrash is available as a pip package.\n\nTo install, run:\n\nsh$ pip install crash\n\n\nNow, run it:\n\nsh$ crash\n\n\nTo update, run:\n\nsh$ pip install -U crash\n\nStandalone\n\nCrash is also available as a standalone executable that includes all the necessary dependencies, and can be run as long as Python (>= 3.4) is available on your system.\n\nFirst, download the executable file:\n\nsh$ curl -o crash https://cdn.crate.io/downloads/releases/crash_standalone_latest\n\n\nThen, set the executable bit:\n\nsh$ chmod +x crash\n\n\nNow, run it:\n\nsh$ ./crash\n\n\nIf you would like to run crash from any directory, and without the leading ./, the file has to be in a directory that is on your PATH.\n\nLegacy versions\n\nFor Python 2.7 and 3.3 please download version 0.23.0 from the CDN:\n\nsh$ curl -o crash https://cdn.crate.io/downloads/releases/crash_standalone_0.23.0\n\n\nFor Python 2.6 please download version 0.16.2 from the CDN:\n\nsh$ curl -o crash https://cdn.crate.io/downloads/releases/crash_standalone_0.16.2\n\nRun\n\nYou can start Crash like so:\n\nsh$ crash\n\n\nWhen crash is run without any additional arguments, it will attempt to connect to localhost:4200. To connect to another host, use the --hosts flag. For example:\n\nsh$ crash --host \"198.51.100.1\"\n\n\nIf you are experiencing a connection error, try the --verbose flag:\n\nsh$ crash --verbose\n\n\nWhen run with --verbose, Crash will print useful information about what it is doing to STDOUT. This includes connection attempts and full stack traces (in the case of an error).\n\nSee Also\n\nFor more help, see Running Crash.\n\nQuery\n\nWhen you run Crash, you will see something like this:\n\nThis is an interactive shell. You can type any CrateDB SQL query at the cr> prompt.\n\nQueries are autocompleted as you type:\n\nOnce you have entered your query, hit Enter to run.\n\nYou should see something like this:\n\nSee Also\n\nFor more help, see Commands or Response formats."
  },
  {
    "title": "Commands — CrateDB: Crash CLI",
    "url": "https://cratedb.com/docs/crate/crash/en/latest/commands.html",
    "html": "latest\nCommands\n\nCrash has several built-in client commands that you can run from the prompt.\n\nEvery command starts with a \\ character.\n\nCommand\n\n\t\n\nDescription\n\n\n\n\n\\?\n\n\t\n\nList all available commands.\n\n\n\n\\c <HOSTS>,\n\\connect <HOSTS>\n\t\n\nConnect to <HOSTS>.\n\nSame as --hosts command line option.\n\nHOSTS can be a single host, or it can be a space separated list of hosts.\n\nIf multiple hosts are specified, Crash will attempt to connect to all of them. The command will succeed if at least one connection is successful.\n\n\n\n\n\\dt\n\n\t\n\nPrint a list of tables.\n\nThe list does not include tables in the sys and information_schema schema.\n\n\n\n\n\\format <FORMAT>\n\n\t\n\nSpecifies the output format of the SQL response.\n\nSame as --format command line option.\n\nAvailable <FORMAT> values are: tabular, raw, json, json_row, csv and mixed.\n\n\n\n\n\\q\n\n\t\n\nQuit the CrateDB shell.\n\n\n\n\n\\check <TYPE>\n\n\t\n\nQuery the sys tables for failing checks.\n\nTYPE can be one of the following:\n\nnot set (query for failing cluster and node checks)\n\nnodes (query for failing node checks)\n\ncluster (query for failing cluster checks)\n\n\n\n\n\\pager\n\n\t\n\nUse apps like jless or pspg to view the result sets. See also Using a pager program.\n\n\n\n\n\\r <FILENAME>\n\n\t\n\nReads statements from <FILENAME> and execute them.\n\n\n\n\n\\sysinfo\n\n\t\n\nQuery the sys tables for system and cluster information.\n\n\n\n\n\\autocomplete\n\n\t\n\nTurn autocomplete feature on or off.\n\nWorks as a toggle.\n\n\n\n\n\\autocapitalize\n\n\t\n\nTurn automatic capitalization for SQL keywords or off.\n\nWorks as a toggle."
  },
  {
    "title": "Appendices — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/appendices/index.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n3.3\nAppendices\n\nSupplementary information for the CrateDB reference manual.\n\nTable of Contents\n\nRelease Notes\nVersions\nOlder Versions\nCompatibility\nImplementation Notes\nUnsupported Features and Functions\nSQL Standard Compliance"
  },
  {
    "title": "Monitoring overview — CrateDB: Admin UI",
    "url": "https://cratedb.com/docs/crate/admin-ui/en/latest/monitoring.html",
    "html": "latest\nMonitoring overview\n\nThe CrateDB Admin UI comes with a monitoring overview screen that allows you to monitor key operational statistics.\n\nTable of contents\n\nScreenshots\n\nScreenshots\n\nThe monitoring page has two live charts:\n\nQueries Per Second:\n\nThis chart graphs the number of queries being run across the entire cluster per second broken down into the following types:\n\nOverall:\n\nAll queries.\n\nSelect:\n\nAll SELECT queries.\n\nInsert:\n\nAll INSERT queries.\n\nUpdate:\n\nAll UPDATE queries.\n\nDelete:\n\nAll DELETE queries.\n\nQuery Speed:\n\nThis chart graphs the execution time of queries being run across the entire cluster, broken down into the following types:\n\nOverall:\n\nAll queries.\n\nSelect:\n\nAll SELECT queries.\n\nInsert:\n\nAll INSERT queries.\n\nUpdate:\n\nAll UPDATE queries.\n\nDelete:\n\nAll DELETE queries."
  },
  {
    "title": "Privileges browser — CrateDB: Admin UI",
    "url": "https://cratedb.com/docs/crate/admin-ui/en/latest/privileges.html",
    "html": "latest\nPrivileges browser\n\nThe CrateDB Admin UI comes with a privileges browser that allows you to inspect users and privileges.\n\nTable of contents\n\nScreenshots\n\nFeatures\n\nScreenshots\n\nWhen you first visit the privileges browser, the crate superuser will be selected:\n\nBy selecting a user from the list of all users in left-hand sub-navigation menu, you can view the privileges that have been assigned to that user:\n\nNote\n\nThe privileges browser is read-only.\n\nFor the time being, user administration must be done manually.\n\nFeatures\nUser list:\n\nThe left-hand sub-navigation menu lists all users.\n\nUser filtering:\n\nYou can enter text into the Filter users text input to only show users with a username that matches the entered text.\n\nPermission filtering:\n\nYou can enter text into the Filter privileges text input to only show privileges with an column value that matches the entered text."
  },
  {
    "title": "Help screen — CrateDB: Admin UI",
    "url": "https://cratedb.com/docs/crate/admin-ui/en/latest/help.html",
    "html": "latest\nHelp screen\n\nThe CrateDB Admin UI comes with a help screen that allows you to import some tweets for testing purposes. This screen also includes links to important CrateDB support resources.\n\nTable of contents\n\nScreenshot\n\nFeatures\n\nScreenshot\nFeatures\n\nThis page includes links to various help resources."
  },
  {
    "title": "Appendices — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/index.html",
    "html": "5.6\nAppendices\n\nSupplementary information for the CrateDB reference manual.\n\nTable of contents\n\nRelease Notes\nVersions\nOlder Versions\nSQL compatibility\nImplementation notes\nUnsupported features and functions\nSQL standard compliance\nResiliency Issues\nKnown issues\nFixed issues\nGlossary\nTerms"
  },
  {
    "title": "Search — CrateDB: Crash CLI",
    "url": "https://cratedb.com/docs/crate/crash/en/latest/search.html",
    "html": "latest\nSearch\n\nFrom here you can search these documents. Enter your search words into the box below and click \"search\". Note that the search function will automatically search for all of the words. Pages containing fewer words won't appear in the result list.\n\nSearch"
  },
  {
    "title": "Appendices — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/appendices/index.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n4.8\nAppendices\n\nSupplementary information for the CrateDB reference manual.\n\nTable of contents\n\nRelease Notes\nVersions\nOlder Versions\nSQL compatibility\nImplementation notes\nUnsupported features and functions\nSQL standard compliance\nResiliency Issues\nKnown issues\nFixed issues\nGlossary\nTerms"
  },
  {
    "title": "Views browser — CrateDB: Admin UI",
    "url": "https://cratedb.com/docs/crate/admin-ui/en/latest/views.html",
    "html": "latest\nViews browser\n\nThe CrateDB Admin UI comes with a views browser that allows you to inspect and query stored views.\n\nTable of contents\n\nScreenshots\n\nFeatures\n\nView-level information\n\nColumns information\n\nOther features\n\nScreenshots\n\nHere’s what a simple view looks like:\n\nThe top section on this screen shows you a basic overview. If you select QUERY VIEW you can query the view using the SQL console.\n\nBelow this, you will find the columns information.\n\nFeatures\nView-level information\n\nThis section displays the following view-level information:\n\nName:\n\nThe name of the view.\n\nDefinition:\n\nThe SQL query used to create the view.\n\nColumns information\n\nThis section section displays information about each view column:\n\nName:\n\nThe name of the view column.\n\nType:\n\nThe column data type.\n\nOther features\nDisplay toggle:\n\nDifferent groups of views can be shown or hidden by toggling the corresponding arrow button on the left-hand sub-navigation menu.\n\nView filter:\n\nThe displayed list of views can be filtered by entering text to match against the view name in the Filter views text input."
  },
  {
    "title": "Appendices — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/appendices/index.html",
    "html": "master\nAppendices\n\nSupplementary information for the CrateDB reference manual.\n\nTable of contents\n\nRelease Notes\nVersions\nOlder Versions\nSQL compatibility\nImplementation notes\nUnsupported features and functions\nSQL standard compliance\nResiliency Issues\nKnown issues\nFixed issues\nGlossary\nTerms"
  },
  {
    "title": "Shards browser — CrateDB: Admin UI",
    "url": "https://cratedb.com/docs/crate/admin-ui/en/latest/shards.html",
    "html": "latest\nShards browser\n\nThe CrateDB Admin UI comes with a shards browser that provides you with a visual overview of all the shards in your cluster.\n\nTable of contents\n\nScreenshots\n\nFeatures\n\nScreenshots\n\nHere’s what a simple database might look like:\n\nIn this example, there is one primary column:\n\nThe blob column holds BLOB tables\n\nThe doc column holds regular document tables\n\nThe blob column lists a single table:\n\nmy_blobs\n\nThe doc column lists a single table:\n\ntweets\n\nBelow this, the table has one row per node in the cluster. CrateDB automatically names unnamed nodes. In this instance, there is a single node named Monte Civetta.\n\nIn this example, the Monte Civetta node is holding the following shards:\n\nThree shards for the table named my_blobs\n\nFour shards for the table named tweets\n\nShards are colored to indicate their status and there is a color key at the top of the table.\n\nThe color of the shards in this example indicates that they are all primary shards and have been successfully started.\n\nFeatures\nShard ID display:\n\nYou can toggle the display of shard IDs by selecting or unselecting the Show Shard IDs checkbox.\n\nShard status:\n\nEach shard is colored according to its status:\n\nStarted Primary (bright green):\n\nThis is a primary shard that has been successfully started, allocated to a node, and is available for querying.\n\nStarted Replica (dark green):\n\nThis is a replica shard that has been successfully started, allocated to a node, and is available for querying.\n\nInitializing (dark yellow):\n\nThe shard is being initialized (i.e. being started).\n\nRelocating (dark red):\n\nThe shard is being relocated to another node.\n\nCrateDB automatically rebalances your cluster for you. While a shard is being moved to another node, it is unavailable for use.\n\nUnassigned (gray):\n\nThe shard exists on disk, but has not yet been allocated to a node."
  },
  {
    "title": "The CrateDB Shell — CrateDB: Crash CLI",
    "url": "https://cratedb.com/docs/crate/crash/en/latest/#",
    "html": "latest\nThe CrateDB Shell\n\nThe CrateDB Shell (aka Crash) is an interactive command-line interface (CLI) tool for working with CrateDB.\n\nSee Also\n\nCrash is an open source project and is hosted on GitHub.\n\nScreenshots\n\n  \n\nTable of contents\n\nGetting started\nInstallation\nRun\nQuery\nRunning Crash\nCommand-line options\nUser configuration directory\nEnvironment variables\nStatus messages\nUsing a pager program\nCommands\nTroubleshooting\nDebugging connection errors\nSSL connection errors\nAppendices\nResponse formats\nCompatibility"
  },
  {
    "title": "Appendices — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/appendices/index.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n5.5\nAppendices\n\nSupplementary information for the CrateDB reference manual.\n\nTable of contents\n\nRelease Notes\nVersions\nOlder Versions\nSQL compatibility\nImplementation notes\nUnsupported features and functions\nSQL standard compliance\nResiliency Issues\nKnown issues\nFixed issues\nGlossary\nTerms"
  },
  {
    "title": "Cluster browser — CrateDB: Admin UI",
    "url": "https://cratedb.com/docs/crate/admin-ui/en/latest/cluster.html",
    "html": "latest\nCluster browser\n\nThe CrateDB Admin UI comes with a cluster browser that allows you to inspect all of the nodes that are in your cluster.\n\nTable of contents\n\nScreenshots\n\nFeatures\n\nNode information\n\nOverview\n\nLoad\n\nCPU cores\n\nShards\n\nDisk operations\n\nScreenshots\n\nHere’s what a single node cluster looks like:\n\nIn this example, there is one node named Monte Civetta. When selected from the left-hand sub-navigation menu, information about a node is displayed.\n\nFeatures\n\nThe cluster browser features:\n\nNode listing:\n\nThe list of nodes gives basic summary information for each node: node name, hostname, CrateDB version, and a visual icon indicating whether the node is a master node or not and the node data status health check.\n\nNode list ordering:\n\nThe list of nodes can be ordered by name or node health.\n\nDetailed node information:\n\nSelecting a node from the node list will display details about that particular node.\n\nThe following subsections explain the detailed node information.\n\nNode information\nOverview\nName:\n\nThe name of the node.\n\nCrateDB automatically names unnamed nodes.\n\nHostname:\n\nThe hostname of the node.\n\nCrateDB Version:\n\nThe version of CrateDB the node is running.\n\nThis is important information when performing a rolling upgrade.\n\nHTTP endpoint:\n\nThe URL of the node’s HTTP endpoint.\n\nCPU Usage:\n\nA visual indicator of system CPU utilization.\n\nHeap Usage:\n\nA visual indicator of allocated Java Virtual Machine (JVM) heap utilization.\n\nDisk Usage:\n\nA visual indicator of system disk space utilization.\n\nCrateDB CPU Usage:\n\nA visual indicator of the CrateDB process CPU utilization.\n\nLoad\n1min:\n\nAverage load over one minute.\n\n5min:\n\nAverage load over five minutes.\n\n15min:\n\nAverage load over fifteen minutes.\n\nCPU cores\nCores:\n\nThe number of CPU cores.\n\nShards\nInitializing:\n\nThe number of shards currently being initialized.\n\nStarted:\n\nThe number of started shards.\n\nReallocating:\n\nThe number of shards that are being moved to another node.\n\nPost Recovery:\n\nThe number of shards that have been recovered but have not yet been started.\n\nDisk operations\nRead:\n\nTotal size of reads on the disk in bytes.\n\nThis value is deprecated.\n\nWrite:\n\nTotal size of writes on the disk in bytes.\n\nThis value is deprecated.\n\nReads:\n\nNumber of reads on the disk.\n\nWrites:\n\nNumber of writes on the disk."
  },
  {
    "title": "Client interfaces — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/interfaces/index.html",
    "html": "5.6\nClient interfaces\n\nCrateDB has two primary client interfaces:\n\nHTTP endpoint\nPostgreSQL wire protocol\n\nSee Also\n\nConnecting to CrateDB — Includes an introduction to the web Admin UI, the CrateDB shell, and the CrateDB HTTP endpoint\n\nClient Libraries — Officially supported clients and community supported clients"
  },
  {
    "title": "Search — CrateDB: Admin UI",
    "url": "https://cratedb.com/docs/crate/admin-ui/en/latest/search.html",
    "html": "latest\nSearch\n\nFrom here you can search these documents. Enter your search words into the box below and click \"search\". Note that the search function will automatically search for all of the words. Pages containing fewer words won't appear in the result list.\n\nSearch"
  },
  {
    "title": "Tables browser — CrateDB: Admin UI",
    "url": "https://cratedb.com/docs/crate/admin-ui/en/latest/tables.html",
    "html": "latest\nTables browser\n\nThe CrateDB Admin UI comes with a tables browser that allows you to inspect and query regular document tables as well as BLOB tables.\n\nTable of contents\n\nScreenshots\n\nFeatures\n\nOverview section\n\nSchema section\n\nOther features\n\nScreenshots\n\nHere’s what a simple document table looks like:\n\nThe top section on this screen shows you a basic overview. If you select QUERY TABLE you can query the table using the SQL console.\n\nBelow this, you will find the schema information:\n\nIf you have BLOB tables, you can access them by selecting the Blob Tables menu item on the left-hand sub-navigation menu.\n\nFeatures\nOverview section\n\nThis section displays the following table-level information:\n\nName:\n\nThe name of the table.\n\nHealth:\n\nThe health of the table.\n\nEach table shard has a corresponding health status. The table-level health status always reflects the worst shard status. (Similarly, the cluster-level status always reflects the worst table-level status.)\n\nShards can be one of the following:\n\nGreen:\n\nThe primary shard and all replica shards are allocated to a node.\n\nYellow:\n\nThe primary shard is allocated, but not all replica shards are allocated to a node.\n\nRed:\n\nThis specific shard is not allocated to a node.\n\nConfigured Replicas:\n\nThe number of configured replicas.\n\nConfigured Shards:\n\nThe number of configured shards\n\nStarted Shards:\n\nThe number of started shards allocated to a node and available for querying.\n\nMissing Shards:\n\nThe total number of known shards that are missing on disk.\n\nUnderrepl. Shards:\n\nThe total number of configured replica shards that are currently missing (i.e. not created or re-created yet).\n\nTotal Records:\n\nThe total number of records (i.e. rows) the table has.\n\nUnavailable Records:\n\nThe total number of records that should exist in missing shards.\n\nUnderrepl. Records:\n\nThe total number of records that do not have the required number of replica copies.\n\nSize:\n\nThe total size on disk used by all primary shards.\n\nRecovery:\n\nThe percentage of the recovery process that is complete (i.e. when re-starting a cluster).\n\nSchema section\n\nThis section section displays information about each table column:\n\nName:\n\nThe name of the table column.\n\nType:\n\nThe column data type.\n\nOther features\nDisplay toggle:\n\nDifferent groups of tables can be shown or hidden by toggling the corresponding arrow button on the left-hand sub-navigation menu.\n\nTable filter:\n\nThe displayed list of tables can be filtered by entering text to match against the table name in the Filter tables text input."
  },
  {
    "title": "Client Interfaces — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/interfaces/index.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n3.3\nClient Interfaces\n\nThere are two ways for clients to talk to CrateDB. This section of the documentation covers both from a client implementation perspective.\n\nSee Also\n\nConnecting to CrateDB — Includes an introduction to the web admin UI, the CrateDB shell, and the CrateDB HTTP endpoint\n\nClient Libraries — Officially supported clients and community supported clients\n\nTable of Contents\n\nHTTP Endpoint\nPostgreSQL Wire Protocol"
  },
  {
    "title": "Client interfaces — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/interfaces/index.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n4.8\nClient interfaces\n\nCrateDB has two primary client interfaces:\n\nHTTP endpoint\nPostgreSQL wire protocol\n\nSee Also\n\nConnecting to CrateDB — Includes an introduction to the web Admin UI, the CrateDB shell, and the CrateDB HTTP endpoint\n\nClient Libraries — Officially supported clients and community supported clients"
  },
  {
    "title": "SQL console — CrateDB: Admin UI",
    "url": "https://cratedb.com/docs/crate/admin-ui/en/latest/console.html",
    "html": "latest\nSQL console\n\nThe CrateDB Admin UI comes with an SQL console so that you can execute queries against your cluster directly from your web browser.\n\nTable of contents\n\nScreenshots\n\nFeatures\n\nScreenshots\n\nWhen you first load the console, it will look like this:\n\nAnd here’s what the console looks like after executing a query:\n\nFeatures\nSyntax highlighting:\n\nCrateDB SQL syntax highlighted as you type.\n\nAuto-completion:\n\nCrateDB SQL auto-completion makes suggestions as you type.\n\nResults formatting:\n\nToggle the Format results checkbox to switch between raw JSON and tabulated data.\n\nQuery history:\n\nToggle the Store console history persistently to disable and enable query history. Previous queries can be cycled through by pressing the Up Arrow key. You can clear your query history by selecting the Clear history button.\n\nQuery URLs:\n\nSelect the share icon located in the bottom right-hand corner of the query panel. This will copy a URL to your clipboard that can be saved or shared that will auto-load the corresponding query when visited. Queries are not automatically executed when you visit a URL.\n\nError traces:\n\nToggle the Show error trace checkbox to view a detailed Java stack trace in the event of an execution error."
  },
  {
    "title": "Client interfaces — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/interfaces/index.html",
    "html": "master\nClient interfaces\n\nCrateDB has two primary client interfaces:\n\nHTTP endpoint\nPostgreSQL wire protocol\n\nSee Also\n\nConnecting to CrateDB — Includes an introduction to the web Admin UI, the CrateDB shell, and the CrateDB HTTP endpoint\n\nClient Libraries — Officially supported clients and community supported clients"
  },
  {
    "title": "The CrateDB Admin UI — CrateDB: Admin UI",
    "url": "https://cratedb.com/docs/crate/admin-ui/en/latest/#",
    "html": "latest\nThe CrateDB Admin UI\n\nCrateDB ships with a web administration user interface (or Admin UI).\n\nThe CrateDB Admin UI runs on every CrateDB node. You can use it to inspect and interact with the whole CrateDB cluster in a number of ways.\n\nSee Also\n\nThe CrateDB Admin UI is an open source project and is hosted on GitHub.\n\nTable of contents\n\nConnecting\n\nNavigating\n\nStatus bar\n\nTabs\n\nConnecting\n\nYou can access the Admin UI via HTTP on port 4200:\n\nhttp://HOSTNAME:4200/\n\n\nReplace HOSTNAME with the hostname of the CrateDB node. If CrateDB is running locally, this will be localhost.\n\nNavigate to this URL in a web browser.\n\nTip\n\nIf you access port 4200 via a client library or command-line tool like curl or wget, the request will be handled by the CrateDB Rest API, and the response will be in JSON.\n\nNavigating\n\nThis is what the Admin UI looks like when it first loads:\n\nTake note of the status bar (at the top) and the tabs (down the left side).\n\nStatus bar\n\nAlong the top of the screen, from left to right, the status bar shows:\n\nCluster name\n\nCrateDB version\n\nNumber of nodes in the cluster\n\nHealth checks\n\nData status\n\nGreen – All data is replicated and available\n\nYellow – Some records are unreplicated\n\nRed – Some data is unavailable\n\nCluster status:\n\nGreen – Good configuration\n\nYellow – Some configuration warnings\n\nRed – Some configuration errors\n\nAverage cluster load (for the past 1 minute, 5 minutes, and 15 minutes)\n\nSettings and notifications menu\n\nTabs\n\nOn the left-hand side, from top to bottom, the tabs are:\n\nOverview screen\n\nSQL console\n\nTables browser\n\nViews browser\n\nShards browser\n\nCluster browser\n\nMonitoring overview\n\nPrivileges browser\n\nHelp screen"
  },
  {
    "title": "Client interfaces — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/interfaces/index.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n5.5\nClient interfaces\n\nCrateDB has two primary client interfaces:\n\nHTTP endpoint\nPostgreSQL wire protocol\n\nSee Also\n\nConnecting to CrateDB — Includes an introduction to the web Admin UI, the CrateDB shell, and the CrateDB HTTP endpoint\n\nClient Libraries — Officially supported clients and community supported clients"
  },
  {
    "title": "WITH — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/with.html",
    "html": "5.6\nWITH\n\nTable of contents\n\nSynopsis\n\nDescription\n\nSynopsis\nWITH with_query [, ...] select_query\n\n\nwhere with_query is:\n\nwith_query_name [ ( column_name [, ...] ) ] AS (select_query)\n\n\nand select_query any SELECT clause.\n\nDescription\n\nThe WITH clause allows you to specify one or more subqueries that can be referenced by name in the primary query. The subqueries effectively act as temporary tables or views for the duration of the primary query.\n\nA name (without schema qualification) must be specified for each WITH query. Optionally, a list of column names can be specified; if this is omitted, the column names are inferred from the subquery.\n\nSee Also\n\nWITH Queries (Common Table Expressions)"
  },
  {
    "title": "UPDATE — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/update.html",
    "html": "5.6\nUPDATE\n\nUpdate rows of a table.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nSynopsis\nUPDATE table_ident [ [AS] table_alias ] SET\n    { column_ident = expression } [, ...]\n  [ WHERE condition ]\n  [ RETURNING { * | output_expression [ [ AS ] output_name ] | relation.* } [, ...] ]\n\nDescription\n\nUPDATE changes the values of the specified columns in all rows that satisfy the condition. Only the columns to be modified need be mentioned in the SET clause; columns not explicitly modified retain their previous values.\n\nThe optional RETURNING clause for UPDATE causes the query to return the specified values from each row that was updated. Any expression using the table’s columns can be computed. The new (post-update) values of the table’s columns are used. The syntax of the RETURNING list is identical to that of the output list of SELECT.\n\nParameters\ntable_ident\n\nThe identifier (optionally schema-qualified) of an existing table.\n\ntable_alias\n\nA substitute name for the target table.\n\nWhen an alias is provided, it completely hides the actual name of the table. For example, given UPDATE foo AS f, the remainder of the UPDATE statement must refer to this table as f not foo.\n\ncolumn_ident\n\nThe name of a column in the table identified by table_ident. It is also possible to use object subscript to address the inner fields of an object column and array subscript elements of an array.\n\nexpression\n\nAn expression to assign to the column.\n\ncondition\n\nAn expression that returns a value of type boolean. Only rows for which this expression returns true will be updated.\n\noutput_expression\n\nAn expression to be computed and returned by the UPDATE command after each row is updated. The expression can use any column names of the table or * to return all columns. System columns can also be returned.\n\noutput_name\n\nA name to use for the result of the output expression."
  },
  {
    "title": "VALUES — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/values.html",
    "html": "5.6\nVALUES\n\nVALUES computes a set of rows.\n\nSynopsis\nVALUES ( expression [, ...] ) [, ...]\n\nDescription\n\nVALUES can be used to generate a result set containing constant values.\n\nWhen more than 1 row is specified, all rows must have the same number of elements.\n\nAn example:\n\ncr> VALUES (1, 'one'), (2, 'two'), (3, 'three');\n+------+-------+\n| col1 | col2  |\n+------+-------+\n|    1 | one   |\n|    2 | two   |\n|    3 | three |\n+------+-------+\nVALUES 3 rows in set (... sec)\n\n\nIt is commonly used in INSERT to provide values to insert into a table.\n\nAll expressions within the same column must have the same type or its types can be implicitly converted."
  },
  {
    "title": "START TRANSACTION — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/start-transaction.html",
    "html": "5.6\nSTART TRANSACTION\n\nCrateDB accepts the START TRANSACTION statement for compatibility with the PostgreSQL wire protocol. However, CrateDB does not support transactions and will silently ignore this statement. .. SEEALSO:\n\n:ref:`Appendix: SQL compatibility <appendix-compatibility>`\n\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nSynopsis\nSTART TRANSACTION [ transaction_mode [ , ...] ]\n\n\nWhere transaction_mode is one of:\n\nISOLATION LEVEL isolation_level | (READ WRITE | READ ONLY) | [NOT] DEFERRABLE\n\n\nWhere isolation_level is one of:\n\n{ SERIALIZABLE | REPEATABLE READ | READ COMMITTED | READ UNCOMMITTED }\n\nDescription\n\nCrateDB will silently ignore the START TRANSACTION statement.\n\nNote\n\nFor backwards compatibility reasons, the commas between successive transaction_modes can be omitted.\n\nParameters\ntransaction_mode\n\nThis parameter has no effect."
  },
  {
    "title": "SHOW TABLES — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/show-tables.html",
    "html": "5.6\nSHOW TABLES\n\nLists the tables in the database.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nClauses\n\nLIKE\n\nWHERE\n\nSynopsis\nSHOW TABLES [{FROM | IN} table_schema] [LIKE 'pattern' | WHERE expression]\n\nDescription\n\nSHOW TABLES can be used to retrieve the table names of the database in alphabetical order. The same list can be fetched by querying table names of the information_schema.tables table.\n\nSystem and BLOB tables are only listed when they are explicitly specified in FROM | IN clause.\n\nParameters\ntable_schema\n\nThe name of the schema the tables are appropriate to.\n\nClauses\nLIKE\n\nThe optional LIKE clause matches only on table names and omits schema names. It takes a string pattern as a filter and has an equivalent behavior to LIKE (ILIKE).\n\nWHERE\n\nThe optional WHERE clause defines the condition to be met for a row to be returned."
  },
  {
    "title": "SHOW SCHEMAS — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/show-schemas.html",
    "html": "5.6\nSHOW SCHEMAS\n\nLists the table schemas of the database.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nClauses\n\nLIKE\n\nWHERE\n\nSynopsis\nSHOW SCHEMAS [LIKE 'pattern' | WHERE expression]\n\nDescription\n\nSHOW SCHEMAS can be used to retrieve defined schema names of the database in alphabetical order.\n\nThe same list can be fetched by querying the schema names from the information_schema.schemata table.\n\nClauses\nLIKE\n\nThe optional LIKE clause indicates which schema names to match. It takes a string pattern as a filter and has an equivalent behavior to LIKE (ILIKE).\n\nWHERE\n\nThe optional WHERE clause defines the condition to be met for a row to be returned."
  },
  {
    "title": "SHOW CREATE TABLE — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/show-create-table.html",
    "html": "5.6\nSHOW CREATE TABLE\n\nShows the CREATE TABLE statement that creates the named table.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nSynopsis\nSHOW CREATE TABLE table_ident\n\nDescription\n\nSHOW CREATE TABLE can be used to dump the schema of an existing user-created table.\n\nIt is not possible to invoke SHOW CREATE TABLE on blob tables or system tables.\n\nParameters\ntable_ident\n\nThe name (optionally schema-qualified) of the table which should be printed."
  },
  {
    "title": "SET TRANSACTION — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/set-transaction.html",
    "html": "5.6\nSET TRANSACTION\n\nSets the characteristics of transactions.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nSynopsis\nSET SESSION CHARACTERISTICS AS TRANSACTION transaction_mode [, ...]\nSET TRANSACTION transaction_mode [, ...]\n\n\nwhere transaction_mode is one of:\n\n    ISOLATION LEVEL { SERIALIZABLE | REPEATABLE READ | READ COMMITTED | READ UNCOMMITTED }\n    READ WRITE | READ ONLY\n    [ NOT ] DEFERRABLE\n\nDescription\n\nSET SESSION CHARACTERISTICS sets the default transaction characteristics for subsequent transactions of a session.\n\nAs CrateDB does not support transactions, this command has no effect and will be ignored. The support was added for compatibility reasons."
  },
  {
    "title": "SHOW (session settings) — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/show.html",
    "html": "5.6\nSHOW (session settings)\n\nThe SHOW statement can display the value of either one or all session setting variables. Some of these can also be configured via SET SESSION.\n\nNote\n\nThe SHOW statement for session settings is unrelated to the other SHOW statements like e.g. SHOW TABLES.\n\nTable of contents\n\nSynopsis\n\nParameters\n\nSynopsis\nSHOW { parameter_name | ALL }\n\nParameters\nparameter_name\n\nThe name of the session setting which should be printed. See Session settings for available session settings.\n\nALL\n\nShow the values of all settings."
  },
  {
    "title": "SHOW COLUMNS — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/show-columns.html",
    "html": "5.6\nSHOW COLUMNS\n\nSHOW COLUMNS displays information about columns in a given table.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nClauses\n\nLIKE\n\nWHERE\n\nSynopsis\nSHOW COLUMNS { FROM | IN } table_name [ FROM | IN table_schema ] [ LIKE 'pattern' | WHERE expression ]\n\nDescription\n\nSHOW COLUMNS fetches all column names of a given table and displays their column name and data type. The column names are listed in alphabetical order. More details can be fetched by querying the information_schema.columns table.\n\nParameters\ntable_name\n\nThe name of the table of which the column information is printed.\n\ntable_schema\n\nThe name of the schema the tables are appropriate to.\n\nIf no schema name is specified the default schema is set to doc.\n\nClauses\nLIKE\n\nThe optional LIKE clause indicates which column names to match. It takes a string pattern as a filter and has an equivalent behavior to LIKE (ILIKE).\n\nWHERE\n\nThe optional WHERE clause defines the condition to be met for a row to be returned."
  },
  {
    "title": "SET AND RESET SESSION AUTHORIZATION — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/set-session-authorization.html",
    "html": "5.6\nSET AND RESET SESSION AUTHORIZATION\n\nSet the user of the current session.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nSynopsis\nSET [ SESSION | LOCAL ] SESSION AUTHORIZATION username\nSET [ SESSION | LOCAL ] SESSION AUTHORIZATION DEFAULT\nRESET SESSION AUTHORIZATION\n\nDescription\n\nThese statements set or reset the user of the session to the given or revert the user back to the original authenticated user.\n\nThe session user can be changed only if the initial authenticated user had the superuser privileges. Otherwise, the statement is only accepted if the specified username matches the originally authenticated user.\n\nUsing this statement, a superuser can temporarily become an unprivileged user and later switch back to a superuser. The superuser would switch using SET SESSION AUTHORIZATION '<impersonating_user>' to drop privileges and become impersonating_user. Later the original privileges can be restored using SET SESSION AUTHORIZATION <real_username> or SET SESSION AUTHORIZATION DEFAULT.\n\nSET LOCAL does not have any effect on session. All SET LOCAL statements will be ignored by CrateDB and logged with the INFO logging level.\n\nThe DEFAULT and RESET forms reset the session and current user to be the originally authenticated user.\n\nParameters\nusername\n\nThe user name represented as an identifier or a string literal.\n\nDEFAULT\n\nUsed for resetting the session user to initial session (authenticated) user."
  },
  {
    "title": "SET LICENSE — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/set-license.html",
    "html": "5.6\nSET LICENSE\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nLicense Key\n\nSynopsis\nSET LICENSE license_key\n\nDescription\n\nSET LICENSE is a no-op statement (i.e., does nothing) and is supported for backward compatibility with earlier versions of CrateDB.\n\nParameters\nLicense Key\n\nThe license_key parameter is ignored.\n\nSee Also\n\nCluster license"
  },
  {
    "title": "SELECT — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/select.html",
    "html": "5.6\nSELECT\n\nRetrieve rows from a table.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nThe SELECT List\n\nClauses\n\nOVER\n\nFROM\n\nRelation reference\n\nJoined relation\n\nTable function\n\nSubselect\n\nWHERE\n\nGROUP BY\n\nHAVING\n\nUNION\n\nORDER BY\n\nWINDOW\n\nLIMIT\n\nFETCH\n\nOFFSET\n\nSynopsis\nSELECT [ ALL | DISTINCT ] * | expression [ [ AS ] output_name ] [, ...]\n  [ OVER ( window_definition ) [, ...] ]\n  [ FROM relation ]\n  [ WHERE condition ]\n  [ GROUP BY expression [, ...] [HAVING condition] ]\n  [ UNION [ ALL | DISTINCT ] query_specification ]\n  [ WINDOW window_name AS ( window_definition ) [, ...] ]\n  [ ORDER BY expression [ ASC | DESC ] [ NULLS { FIRST | LAST } ] [, ...] ]\n  [ LIMIT [num_results | ALL] | [ FETCH [FIRST | NEXT] [ROW | ROWS] ONLY ]\n  [ OFFSET start [ROW | ROWS] ]\n\n\nwhere relation is:\n\nrelation_reference | joined_relation | table_function | subselect\n\nDescription\n\nSELECT retrieves rows from a table. The general processing of SELECT is as follows:\n\nThe FROM item points to the table where the data should be retrieved from. If no FROM item is specified, the query is executed against a virtual table with no columns.\n\nIf the WHERE clause is specified, all rows that do not satisfy the condition are eliminated from the output.\n\nIf the GROUP BY clause is specified, the output is combined into groups of rows that match on one or more values.\n\nThe actual output rows are computed using the SELECT output expressions for each selected row or row group.\n\nIf the ORDER BY clause is specified, the returned rows are sorted in the specified order. If ORDER BY is not given, the rows are returned in whatever order the system finds fastest to produce.\n\nIf DISTINCT is specified, one unique row is kept. All other duplicate rows are removed from the result set.\n\nIf the LIMIT or OFFSET clause is specified, the SELECT statement only returns a subset of the result rows.\n\nParameters\nThe SELECT List\n\nThe SELECT list specifies expressions that form the output rows of the SELECT statement. The expressions can (and usually do) refer to columns computed in the FROM clause.\n\nSELECT [ ALL | DISTINCT ] * | expression [ [ AS ] output_name ] [, ...]\n\n\nJust as in a table, every output column of a SELECT has a name. In a simple SELECT, this name is just used to label the column for display. To specify the name to use for an output column, write AS output_name after the column’s expression. (You can omit AS, but only if the desired output name does not match any reserved keyword. For protection against possible future keyword additions, it is recommended that you always either write AS or double-quote the output name.) If you do not specify a column name, a name is chosen automatically by CrateDB. If the column’s expression is a simple column reference, then the chosen name is the same as that column’s name. In more complex cases, a function or type name may be used, or the system may fall back on a generated name.\n\nAn output column’s name can be used to refer to the column’s value in ORDER BY and GROUP BY clauses, but not in the WHERE clause; there you must write out the expression instead.\n\nInstead of an expression, * can be written in the output list as a shorthand for all the columns of the selected rows. Also, you can write table_name.* as a shorthand for the columns coming from just that table. In these cases it is not possible to specify new names with AS; the output column names will be the same as the table columns’ names.\n\nClauses\nOVER\n\nThe OVER clause defines a window.\n\nOVER ( window_definition )\n\n\nThe window_definition determines the partitioning and ordering of rows before the window function is applied.\n\nSee Also\n\nWindow functions: Window definition\n\nFROM\n\nThe FROM clause specifies the source relation for the SELECT:\n\nFROM relation\n\n\nThe relation can be any of the following relations.\n\nRelation reference\n\nA relation_reference is an ident which can either reference a table or a view with an optional alias:\n\nrelation_ident [ [AS] alias ]\n\nrelation_ident\n\nThe name (optionally schema-qualified) of an existing table or view.\n\nalias\n\nA substitute name for the FROM item containing the alias.\n\nAn alias is used for brevity. When an alias is provided, it completely hides the actual name of the relation. For example given FROM foo AS f, the remainder of the SELECT must refer to this FROM item as f not foo.\n\nSee Also\n\nSQL syntax: CREATE TABLE\n\nSQL syntax: CREATE VIEW\n\nJoined relation\n\nA joined_relation is a relation which joins two relations together.\n\nrelation { , | join_type JOIN } relation [ { ON join_condition  |  USING (col_names) } ]\n\njoin_type\n\nLEFT [OUTER], RIGHT [OUTER], FULL [OUTER], CROSS or INNER.\n\njoin_condition\n\nAn expression which specifies which rows in a join are considered a match.\n\nThe join_condition is not applicable for joins of type CROSS and must have a returning value of type boolean.\n\ncol_names\n\nA comma-separated list of column names. The joined relations need to contain the specified columns.\n\nTable function\n\ntable_function is a function that produces a set of rows and has columns.\n\nfunction_call\n\nfunction_call\n\nThe call declaration of the function. Usually in the form of function_name ( [ args ] ).\n\nDepending on the function the parenthesis and arguments are either optional or required.\n\nSee Also\n\nBuilt-ins: Table functions\n\nSubselect\n\nA subselect is another SELECT statement surrounded by parentheses with an alias:\n\n( select_statement ) [ AS ] alias\n\n\nThe subselect behaves like a temporary table that is evaluated at runtime. The clauses of the surrounding SELECT statements are applied on the result of the inner SELECT statement.\n\nselect_statement\n\nA SELECT statement.\n\nalias\n\nAn alias for the subselect.\n\nWHERE\n\nThe optional WHERE clause defines the condition to be met for a row to be returned:\n\nWHERE condition\n\ncondition\n\nA WHERE condition is any expression that evaluates to a result of type boolean.\n\nAny row that does not satisfy this condition will be eliminated from the output. A row satisfies the condition if it returns true when the actual row values are substituted for any variable references.\n\nGROUP BY\n\nThe optional GROUP BY clause will condense all selected rows that share the same values for the grouped expression into a single row.\n\nAggregate expressions, if any are used, are computed across all rows making up each group, producing a separate value for each group.\n\nGROUP BY expression [, ...] [HAVING condition]\n\nexpression\n\nAn arbitrary expression formed from column references of the queried relation that are also present in the result column list. Numeric literals are interpreted as ordinals referencing an output column from the select list.\n\nIt can also reference output columns by name.\n\nIn case of ambiguity, a GROUP BY name will be interpreted as a name of a column from the queried relation rather than an output column name.\n\nHAVING\n\nThe optional HAVING clause defines the condition to be met for values within a resulting row of a GROUP BY clause.\n\ncondition\n\nA HAVING condition is any expression that evaluates to a result of type boolean. Every row for which the condition is not satisfied will be eliminated from the output.\n\nNote\n\nWhen GROUP BY is present, it is not valid for the SELECT list expressions to refer to ungrouped columns except within aggregate functions, since there would otherwise be more than one possible value to return for an ungrouped column.\n\nAdditionally, grouping can only be applied on indexed fields.\n\nSee Also\n\nFulltext indices : Disable indexing\n\nUNION\n\nThe UNION ALL operator combines the result sets of two or more SELECT statements. The two SELECT statements that represent the direct operands of the UNION ALL must produce the same number of columns, and corresponding columns must have a compatible type.\n\nThe result of UNION ALL may contain duplicate rows. Use UNION DISTINCT or UNION to remove duplicates. You can find here sample usages of the variations of UNION.\n\nUNION [ ALL | DISTINCT ] query_specification\n\nquery_specification\n\nCan be any SELECT statement.\n\nORDER BY, LIMIT, and OFFSET can only be applied after the last SELECT statement of the UNION ALL, as they are applied to the complete result of the UNION operation. In order to apply an ORDER BY and/or LIMIT and/or OFFSET to any of the partial SELECT statements, those statements need to become subqueries.\n\nColumn names used in ORDER BY must be position numbers or refer to the outputs of the first SELECT statement, and no functions can be applied on top of the ORDER BY symbols. To achieve more complex ordering, UNION ALL must become a subselect and the more complex ORDER BY should be applied on the outer SELECT wrapping the UNION ALL subselect.\n\nThe ordering of the outcome is not guaranteed unless ORDER BY is used.\n\nORDER BY\n\nThe ORDER BY clause causes the result rows to be sorted according to the specified expression(s).\n\nORDER BY expression [ ASC | DESC ] [ NULLS { FIRST | LAST } ] [, ...]\n\nexpression\n\nCan be the name or ordinal number of an output column, or it can be an arbitrary expression formed from input-column values.\n\nThe optional keyword ASC (ascending) or DESC (descending) after any expression allows to define the direction in which values are sorted. The default is ascending.\n\nIf NULLS FIRST is specified, null values sort before non-null values. If NULLS LAST is specified, null values sort after non-null values. If neither is specified nulls are considered larger than any value. That means the default for ASC is NULLS LAST and the default for DESC is NULLS FIRST.\n\nIf two rows are equal according to the leftmost expression, they are compared according to the next expression and so on. If they are equal according to all specified expressions, they are returned in an implementation-dependent order.\n\nCharacter-string data is sorted by its UTF-8 representation.\n\nNote\n\nSorting can only be applied on indexed fields.\n\nAdditionally, sorting on Geometric points, Geometric shapes, Arrays, and Objects is not supported.\n\nSee Also\n\nFulltext indices : Disable indexing\n\nWINDOW\n\nThe optional WINDOW clause has a form:\n\nWINDOW window_name AS ( window_definition ) [, ...]\n\n\nThe window_name is a name that can be referenced from OVER clauses or subsequent window definitions.\n\nThe window_definition determines the partitioning and ordering of rows before the window function is applied.\n\nSee Also\n\nWindow functions: Window definition\n\nWindow functions: Named windows\n\nLIMIT\n\nThe optional LIMIT clause allows to limit the number of returned result rows:\n\nLIMIT number_of_results\n\nnumber_of_results\n\nSpecifies the maximum number of result rows to return. Must be a non-negative integer literal.\n\nNote\n\nIt is possible for repeated executions of the same LIMIT query to return different subsets of the rows of a table, if there is not an ORDER BY to enforce selection of a deterministic subset.\n\nNote\n\nIf LIMIT ALL is used, then no limit is applied, essentially the query is returning all rows, as if not LIMIT clause is present.\n\nNote\n\nIf number_of_results is null, then no limit is applied, essentially the query is returning all rows, as if not LIMIT clause is present.\n\nFETCH\n\nThe optional FETCH clause allows to limit the number of returned result rows, and is an alternative to the LIMIT clause:\n\nFETCH FIRST number_of_results ROWS ONLY\n\nnumber_of_results\n\nSpecifies the maximum number of result rows to return. Must be a non-negative integer literal.\n\nNote\n\nIt is possible for repeated executions of the same FETCH query to return different subsets of the rows of a table, if there is not an ORDER BY to enforce selection of a deterministic subset.\n\nNote\n\nIf number_of_results is null, then no limit is applied, essentially the query is returning all rows, as if not FETCH clause is present.\n\nNote\n\nLIMIT and FETCH clauses cannot be used together, since they define the same functionality, only one of the two must be present.\n\nOFFSET\n\nThe optional OFFSET clause allows to skip result rows at the beginning:\n\nOFFSET start [ROW | ROWS]\n\nstart\n\nSpecifies the number of rows to skip before starting to return rows. Must be a non-negative integer literal.\n\nNote\n\nThe ROW or ROWS is optional and can be omitted, without affecting the behaviour of OFFSET functionality.\n\nNote\n\nIf start is null, then no offset is applied, essentially the query is returning rows from the 1st one, as if not OFFSET clause is present."
  },
  {
    "title": "SET and RESET — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/set.html",
    "html": "5.6\nSET and RESET\n\nChange and restore settings at runtime. To get an overview of available CrateDB settings, see Configuration. Only settings documented with Runtime: yes can be changed.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nPersistence\n\nSynopsis\nSET [ SESSION | LOCAL ] setting_ident { = | TO } { setting_value | 'setting_value' | DEFAULT }\n\nSET GLOBAL [ PERSISTENT | TRANSIENT ] { setting_ident [ = | TO ] { value | ident } } [, ...]\n\nRESET GLOBAL setting_ident [, ...]\n\nDescription\n\nSET GLOBAL can be used to change a global cluster setting, see Cluster-wide settings, to a different value. Using RESET will reset the cluster setting to its default value or to the setting value defined in the configuration file, if it was set on a node start-up. The global cluster settings can be applied to a cluster using PERSISTENT and TRANSIENT keywords to set a persistent level.\n\nSET/SET SESSION may affect the current session if the setting is supported. Setting the unsupported settings will be ignored and logged with the INFO logging level. See search_path, to get an overview of the supported session setting parameters.\n\nSET LOCAL does not have any effect on CrateDB configurations. All SET LOCAL statements will be ignored by CrateDB and logged with the INFO logging level.\n\nSET SESSION/LOCAL are introduced to be compliant with third-party applications which use the PostgresSQL wire protocol.\n\nParameters\nsetting_ident\n\nThe full qualified setting ident of the setting to set / reset.\n\nvalue\n\nThe value to set for the setting.\n\nident\n\nThe ident to set for the setting.\n\nsetting_value\n\nThe new value for the setting. It can be specified as string constants, identifiers, numbers, or comma-separated list of these, as appropriate for the particular setting.\n\nDEFAULT\n\nUsed for resetting the parameter to its default value.\n\nPersistence\n\nThe default is TRANSIENT. Settings that are set using the TRANSIENT keyword will be discarded if the cluster is stopped or restarted.\n\nUsing the PERSISTENT keyword will persist a value of the setting to a disk, so that the setting will not be discarded if the cluster restarts.\n\nNote\n\nThe persistence keyword can only be used within a SET statement."
  },
  {
    "title": "REVOKE — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/revoke.html",
    "html": "5.6\nREVOKE\n\nRevokes a previously granted privilege on the whole cluster or on a specific object from a user or a role.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nSynopsis\nREVOKE { { DQL | DML | DDL | AL [,...] } | ALL [ PRIVILEGES ] }\n[ON {SCHEMA | TABLE | VIEW} identifier [, ...]]\nFROM name [, ...];\n\nREVOKE role_name_to_revoke [, ...] FROM name [, ...]\n\nDescription\n\nREVOKE is a management statement which comes in two flavours.\n\nThe first one is used to revoke previously granted privileges on a specific object from one or many existing users or roles. ON {SCHEMA | TABLE | VIEW} is optional, if not specified the privilege will be revoked on the CLUSTER level.\n\nThe second one is used to revoke previously granted roles from one or many existing users or roles. Thus, the users or roles loose the privileges which had automatically inherit from those previously granted roles.\n\nFor usages of the REVOKE statement see Privileges.\n\nParameters\nidentifier\n\nThe identifier of the corresponding object.\n\nIf TABLE or VIEW is specified the identifier should include the object’s full qualified name. Otherwise it will be looked up in the current schema.\n\nrole_name_to_revoke\n\nThe name of the role to revoke from another user or role.\n\nname\n\nThe name of an existing user or role."
  },
  {
    "title": "REFRESH — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/refresh.html",
    "html": "5.6\nREFRESH\n\nRefresh one or more tables explicitly.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nClauses\n\nPARTITION\n\nSynopsis\nREFRESH TABLE (table_ident [ PARTITION (partition_column=value [ , ... ])] [, ...] )\n\nDescription\n\nThe REFRESH TABLE command refreshes one or more tables, making all changes made to that table visible to subsequent commands.\n\nThe PARTITION clause can be used to refresh specific partitions of a partitioned table instead of all partitions. The PARTITION clause requires a list of all partition columns as an argument.\n\nIn case the PARTITION clause is omitted all open partitions will be refreshed. Closed partitions are not refreshed.\n\nSee Also\n\nPartitioned tables\n\nFor performance reasons, refreshing tables or partitions should be avoided as it is an expensive operation. By default CrateDB periodically refreshes the tables anyway. See Refresh and refresh_interval for more information about the periodic refreshes.\n\nWithout an explicit REFRESH, other statements like UPDATE, DELETE or SELECT won’t see data until the periodic refresh happens.\n\nAn exception to that are statements which can filter on a primary key with an exact match on all primary key values within a record. For example, looking up a single document in a table with a single primary key column:\n\nWHERE pk = 'ID1'\n\n\nIf the primary key consists of multiple columns it would look like this:\n\nWHERE pk1 = 'ID_PART_1' AND pk2 = 'ID_PART_2'\n\n\nOr if you want to query multiple records:\n\nWHERE pk = 'ID1' OR pk = 'ID2' OR pk = 'ID3'\n\n\nThese kind of filters will result in a primary key lookup. You can use the EXPLAIN statement to verify if this is the case:\n\ncr> CREATE TABLE pk_demo (id int primary key);\nCREATE OK, 1 row affected  (... sec)\n\ncr> EXPLAIN SELECT * FROM pk_demo WHERE id = 1;\n+--------------------------------------------------------+\n| QUERY PLAN                                             |\n+--------------------------------------------------------+\n| Get[doc.pk_demo | id | DocKeys{1} | (id = 1)] (rows=1) |\n+--------------------------------------------------------+\nEXPLAIN 1 row in set (... sec)\n\n\nThis lists a Get operator, which is the internal operator name for a primary key lookup. Compare this with the following output:\n\ncr> EXPLAIN SELECT * FROM pk_demo WHERE id > 1;\n+-------------------------------------------------------+\n| QUERY PLAN                                            |\n+-------------------------------------------------------+\n| Collect[doc.pk_demo | [id] | (id > 1)] (rows=unknown) |\n+-------------------------------------------------------+\nEXPLAIN 1 row in set (... sec)\n\n\nThe filter changed to id > 1, in this case CrateDB can no longer use a primary key lookup and the used operator changed to a Collect operator.\n\nTo avoid the need for manual refreshes it can be useful to make use of primary key lookups, as they see the data even if the table hasn’t been refreshed yet.\n\nSee also Consistency.\n\nParameters\ntable_ident\n\nThe name (optionally schema-qualified) of an existing table that is to be refreshed.\n\nClauses\nPARTITION\n\nIf the table is partitioned, the optional PARTITION clause can be used to refresh one partition exclusively.\n\n[ PARTITION ( partition_column = value [ , ... ] ) ]\n\npartition_column\n\nOne of the column names used for table partitioning.\n\nvalue\n\nThe respective column value.\n\nAll partition columns (specified by the PARTITIONED BY clause) must be listed inside the parentheses along with their respective values using the partition_column = value syntax (separated by commas).\n\nBecause each partition corresponds to a unique set of partition column row values, this clause uniquely identifies a single partition to refresh.\n\nTip\n\nThe SHOW CREATE TABLE statement will show you the complete list of partition columns specified by the PARTITIONED BY clause."
  },
  {
    "title": "OPTIMIZE — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/optimize.html",
    "html": "5.6\nOPTIMIZE\n\nOptimize one or more tables explicitly.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nClauses\n\nPARTITION\n\nWITH\n\nSynopsis\nOPTIMIZE TABLE table_ident [ PARTITION (partition_column=value [ , ... ]) ] [, ...]\n[ WITH ( optimization_parameter [= value] [, ... ] ) ]\n\nDescription\n\nThe OPTIMIZE TABLE command optimizes tables and table partitions by merging the segments of a table or a partition and reducing their number. It is also used to upgrade tables and table partitions to the current version of the storage engine. This command will block until the optimization process is complete. If the connection to CrateDB is lost, the request will continue in the background, and any new requests will block until the previous optimization is complete.\n\nThe PARTITION clause can be used to only optimize specific partitions of a partitioned table. Specified values for all partition columns are required.\n\nIn case the PARTITION clause is omitted all open partitions will be optimized. Closed partitions are not optimized. For performance reasons doing that should be avoided if possible.\n\nSee Partitioned tables for more information on partitioned tables.\n\nFor further information see Optimization.\n\nNote\n\nSystem tables cannot be optimized.\n\nParameters\ntable_ident\n\nThe name (optionally schema-qualified) of an existing table that is to be optimized.\n\nClauses\nPARTITION\n\nIf the table is partitioned, the optional PARTITION clause can be used to optimize one partition exclusively.\n\n[ PARTITION ( partition_column = value [ , ... ] ) ]\n\npartition_column\n\nOne of the column names used for table partitioning.\n\nvalue\n\nThe respective column value.\n\nAll partition columns (specified by the PARTITIONED BY clause) must be listed inside the parentheses along with their respective values using the partition_column = value syntax (separated by commas).\n\nBecause each partition corresponds to a unique set of partition column row values, this clause uniquely identifies a single partition to optimize.\n\nTip\n\nThe SHOW CREATE TABLE statement will show you the complete list of partition columns specified by the PARTITIONED BY clause.\n\nWITH\n\nThe optional WITH clause can specify parameters for the optimization request.\n\n[ WITH ( optimization_parameter [= value] [, ... ] ) ]\n\noptimization_parameter\n\nSpecifies an optional parameter for the optimization request.\n\nAvailable parameters are:\n\nmax_num_segments\n\nThe number of segments to merge to. To fully merge the table or partition set it to 1.\n\nDefaults to simply checking if a merge is necessary, and if so, executes it.\n\nCaution\n\nForcing a merge to a small number of segments can harm query performance if segments become too big.\n\nIf max_num_segments gets omitted, CrateDB will automatically determine the ideal number of segments based on internal criteria.\n\nonly_expunge_deletes\n\nShould the merge process only expunge segments with deletes in it.\n\nIn CrateDB, a row is not deleted from a segment, just marked as deleted. During a merge process of segments, a new segment is created that does not have those deletes. This flag allows to only merge segments that have deletes.\n\nDefaults to false.\n\nflush\n\nInstructs if a flush should be performed after the optimization.\n\nDefaults to true.\n\nupgrade_segments\n\nThis option is deprecated and has no effect anymore."
  },
  {
    "title": "RESTORE SNAPSHOT — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/restore-snapshot.html",
    "html": "5.6\nRESTORE SNAPSHOT\n\nRestore a snapshot into the cluster.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nClauses\n\nPARTITION\n\nWITH\n\nSynopsis\nRESTORE SNAPSHOT repository_name.snapshot_name\n{ ALL |\n  METADATA |\n  TABLE table_ident [ PARTITION (partition_column = value [, ...])] [, ...] |\n  data_section [, ...] }\n[ WITH (restore_parameter [= value], [, ...]) ]\n\n\nwhere data_section:\n\n{  TABLES |\n   VIEWS |\n   USERS |      -- Deprecated, use USERMANAGEMENT instead\n   PRIVILEGES | -- Deprecated, use USERMANAGEMENT instead\n   USERMANAGEMENT |\n   ANALYZERS |\n   UDFS }\n\nDescription\n\nRestore one or more tables, partitions, or metadata from an existing snapshot into the cluster. The snapshot must be given as fully qualified reference with repository_name and snapshot_name.\n\nTo restore everything, use the ALL keyword.\n\nSingle tables (or table partitions) can be restored by using TABLE together with a table_ident and a optional partition reference given the partition_column values.\n\nIt is possible to restore all tables using the TABLES keyword. This will restore all tables but will not restore metadata.\n\nTo restore only the metadata (including views, users, roles, privileges, analyzers, user-defined-functions, and all cluster settings), instead use the METADATA keyword.\n\nA single metadata group can be restored by using the related data_section keyword.\n\nAdditionally, multiple data_section keywords can be used to restore multiple concrete sections at once.\n\nTo cancel a restore operation simply drop the tables that are being restored.\n\nCaution\n\nIf you try to restore a table that already exists, CrateDB will return an error. However, if you try to restore metadata or cluster settings that already exist, they will be overwritten.\n\nParameters\nrepository_name\n\nThe name of the repository of the snapshot to restore as ident.\n\nsnapshot_name\n\nThe name of the snapshot as ident.\n\ntable_ident\n\nThe name (optionally schema-qualified) of an existing table that is to be restored from the snapshot.\n\ndata_section\n\nThe section name of the data to be restored. Multiple sections can be selected. A section cannot be combined with the ALL, METADATA, or TABLE keywords.\n\nClauses\nPARTITION\n\nIf the table is partitioned, the optional PARTITION clause can be used to restore a snapshot from one partition exclusively.\n\n[ PARTITION ( partition_column = value [, ...] ) ]\n\npartition_column\n\nOne of the column names used for table partitioning\n\nvalue\n\nThe respective column value.\n\nAll partition columns (specified by the PARTITIONED BY clause) must be listed inside the parentheses along with their respective values using the partition_column = value syntax (separated by commas).\n\nBecause each partition corresponds to a unique set of partition column row values, this clause uniquely identifies a single partition to restore.\n\nTip\n\nThe SHOW CREATE TABLE statement will show you the complete list of partition columns specified by the PARTITIONED BY clause.\n\nWITH\n[ WITH (restore_parameter [= value], [, ...]) ]\n\n\nThe following configuration parameters can be used to modify how the snapshot is restored to the cluster:\n\nignore_unavailable\n\n(Default false) Per default the restore command fails if a table is given that does not exist in the snapshot. If set to true those missing tables are ignored.\n\nwait_for_completion\n\n(Default: false) By default the request returns once the restore operation started. If set to true the request returns after all selected tables from the snapshot are restored or an error occurred. In order to monitor the restore operation the * sys.shards table can be queried.\n\nschema_rename_pattern\n\n(Default (.+)) Regular expression matching schemas of restored tables. Used to restore table into a different schema. Capture groups () can be used to reuse portions of the table schema and then used in schema_rename_replacement. Default value matches the entire schema name.\n\nschema_rename_replacement\n\n(Default $1) Replacement pattern used to restore table into a different schema. Can include groups, captured in schema_rename_pattern. By default no replacement is happening and tables are restored into their original schemas.\n\nExample: prefix_$1 combined with default schema_rename_pattern adds ‘prefix’ to all restored table schemas.\n\nExample: target combined with default schema_rename_pattern restores all tables into the target schema.\n\ntable_rename_pattern\n\n(Default (.+)) Regular expression matching names of restored tables. Used to rename tables on restoring. Capture groups () can be used to reuse portions of the table name and then used in table_rename_replacement. Default value matches the entire table name.\n\ntable_rename_replacement\n\n(Default $1) Replacement pattern used to rename tables on restoring. Can include groups, captured in table_rename_pattern. By default no replacement is happening and tables are restored with their original names. Example: prefix_$1 combined with default table_rename_pattern adds ‘prefix’ to all restored table names.\n\nCaution\n\nRestore will abort with a failure if there is a name collision after evaluating the rename operations, or if a table with the same name as the rename target already exists."
  },
  {
    "title": "KILL — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/kill.html",
    "html": "5.6\nKILL\n\nKills active jobs in the CrateDB cluster.\n\nNote\n\nThis statement is only available for all users on clusters running CrateDB versions 4.3 and above. Prior version 4.3, the KILL statement can only be run by the crate superuser.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nSynopsis\nKILL (ALL | job_id)\n\nDescription\n\nThe KILL ALL statement kills all active jobs within the CrateDB cluster which are owned by the current user.\n\nThe statement KILL job_id kills the job with a specified job_id if the job was started by the current user.\n\nAn exception to this is the CRATE super-user, which can also kill statements of other users.\n\nBe aware that CrateDB doesn’t have transactions. If an operation which modifies data is killed, it won’t rollback. For example if a update operation is killed it is likely that it updated some documents before being killed. This might leave the data in an inconsistent state. So take care when using KILL.\n\nCertain fast running operations have a small time frame in which they can be killed. For example if you delete a single document by ID the document could be deleted before the KILL command is processed, but the client might receive an error that the operation has been killed because the KILL command processed before the final result is sent to the client.\n\nKILL ALL and KILL job_id return the number of contexts killed per node. For example if the only active query was select * from t and that query is being executed on 3 nodes, then KILL ALL will return 3.\n\nParameters\njob_id\n\nThe UUID of the currently active job that needs to be killed given as a string literal."
  },
  {
    "title": "GRANT — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/grant.html",
    "html": "5.6\nGRANT\n\nGrants privilege to an existing user or role on the whole cluster or on a specific object, or grant one or more roles to a user or role.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nSynopsis\nGRANT { { DQL | DML | DDL | AL [,...] } | ALL [ PRIVILEGES ] }\n[ON {SCHEMA | TABLE | VIEW} identifier [, ...]]\nTO name [, ...]\n\nGRANT role_name_to_grant [, ...] TO name [, ...]\n\nDescription\n\nGRANT is a management statement which comes in two flavours.\n\nThe first one is used to grant one or many privileges on the whole cluster or on a specific object to one or many existing users or roles. ON {SCHEMA | TABLE | VIEW} is optional, if not specified the privilege will be granted on the CLUSTER level.\n\nWith the second one, GRANT can be used to grant one or more roles to one or many others or roles. Thus, the users (or roles) inherit the privileges of the roles which they are granted.\n\nFor usages of the GRANT statement see Privileges.\n\nParameters\nidentifier\n\nThe identifier of the corresponding object.\n\nIf TABLE or VIEW is specified the identifier should include the object’s full qualified name. Otherwise it will be looked up in the current schema.\n\nrole_name_to_grant\n\nThe name of the role to grant to another user or role.\n\nname\n\nThe name of an existing user or role."
  },
  {
    "title": "INSERT — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/insert.html",
    "html": "5.6\nINSERT\n\nYou can use the INSERT statement to insert new rows into a table.\n\nTable of contents\n\nSynopsis\n\nParameters\n\nDescription\n\nON CONFLICT DO UPDATE SET\n\nON CONFLICT DO NOTHING\n\nSynopsis\n\nCrateDB defines the full INSERT syntax as:\n\nINSERT INTO table_ident\n  [ ( column_ident [, ...] ) ]\n  { VALUES ( expression [, ...] ) [, ...] | ( query ) | query }\n  [ ON CONFLICT (column_ident [, ...]) DO UPDATE SET { column_ident = expression [, ...] } |\n    ON CONFLICT [ ( column_ident [, ...] ) ] DO NOTHING ]\n  [ RETURNING { * | output_expression [ [ AS ] output_name ] | relation.* } [, ...] ]\n\nParameters\ntable_ident\n\nThe identifier (optionally schema-qualified) of an existing table.\n\ncolumn_ident\n\nThe name of a column or field in the table_ident table.\n\nexpression\n\nAn expression or value to assign to the corresponding column.\n\nquery\n\nA query (i.e., SELECT) that supplies rows for the statement to insert.\n\noutput_expression\n\nAn expression to be computed and returned by the INSERT statement after each row is updated. This expression can use any of the table column names, the * character to return all table columns, as well as any system columns.\n\noutput_name\n\nA name to use for the result of the output expression.\n\nDescription\n\nThe INSERT statement creates one or more rows specified by value expressions.\n\nYou can list target column names in any order. If you omit the target column names, they default to all columns of the table or up to n columns if there are fewer values in the VALUES clause or query.\n\nCrateDB will order implicitly inferred column names by their ordinal value. The ordinal value depends on the ordering of the columns within the CREATE TABLE statement.\n\nThe values supplied by the VALUES clause or query are associated with the explicit or implicit column list left-to-right.\n\nCrateDB will not fill any column not present in the explicit or implicit column list.\n\nIf the values for any column are not of the correct data type, CrateDB will attempt automatic type conversion.\n\nNote\n\nWhen inserting data from a query, the number of rows affected indicates the number of rows for which the INSERT succeeded. Please refer to Data manipulation for more details.\n\nInserting data from a query uses Overload Protection to ensure other queries can still perform. Please change these settings during large inserts if needed.\n\nThe optional RETURNING clause causes the INSERT statement to compute and return values from each row inserted (or updated, in the case of ON CONFLICT DO UPDATE). You can take advantage of this behavior to obtain values that CrateDB supplied from defaults, such as _id.\n\nCaution\n\nDynamic SELECT statements may produce inconsistent values for insertion when used with the query parameter.\n\nFor example, this use of unnest produces a single column (foo) with incompatible data types (numeric and character, respectively):\n\nSELECT unnest([{foo=1}, {foo='a string'}])\n\n\nThe same problem could happen like this:\n\nINSERT INTO table_a (obj_col) VALUES ({foo=1}), ({foo='a string'})\nINSERT INTO table_a (int_col) (SELECT obj_col['foo'] FROM table_a)\n\n\nIn this example, problems will arise if valid_col is a valid column name, but invalid_col is not:\n\nSELECT unnest([{valid_col='foo', invalid_col='bar'}])\n\n\nAny inserts that were successful before CrateDB encountered an error will remain, but CrateDB will reject the rest, potentially leading to inconsistent data.\n\nUsers need to take special care when inserting data from queries that might produce dynamic values like the ones above.\n\nON CONFLICT DO UPDATE SET\n\nIf your table has a primary key, you can use the ON CONFLICT DO UPDATE SET clause to modify the existing record (instead of inserting a new one) if CrateDB encounters a primary key conflict during the INSERT operation.\n\nSyntax:\n\nON CONFLICT (conflict_target) DO UPDATE SET { assignments }\n\n\nWhere conflict_target can be one or more column identifiers:\n\ncolumn_ident [, ... ]\n\n\nAnd assignments can be one or more column assignments:\n\nassignments = expression [, ... ]\n\n\nNote\n\nCrateDB does not support unique constraints, foreign key constraints, or exclusion constraints (see SQL compatibility: Unsupported features and functions). Therefore, the only constraint capable of producing a conflict that CrateDB supports is a primary key constraint.\n\nWhen using the ON CONFLICT DO UPDATE SET clause with a primary key constraint, the conflict_target must always match the primary key definition.\n\nFor example, if my_table had a primary key col_a, the correct syntax would be:\n\nON CONFLICT (col_a) DO UPDATE SET { assignments }\n\n\nHowever, if my_table had a primary key on both col_a and col_b, the correct syntax would be:\n\nON CONFLICT (col_a, col_b) DO UPDATE SET { assignments }\n\n\nFor example:\n\ncr> INSERT INTO uservisits (id, name, visits, last_visit) VALUES\n... (\n...     0,\n...     'Ford',\n...     1,\n...     '2015-09-12'\n... ) ON CONFLICT (id) DO UPDATE SET\n...     visits = visits + 1;\nINSERT OK, 1 row affected (... sec)\n\n\nThis statement instructs CrateDB to do the following:\n\nAttempt to insert a new uservisits record for user ID 0.\n\nIf the insert would cause a primary key conflict on id (i.e., the user already has a record in the uservists table), update the existing record by incrementing the visits count.\n\nYou can also use a virtual table named excluded to reference values from the failed (i.e., excluded) INSERT record. For example:\n\ncr> INSERT INTO uservisits (id, name, visits, last_visit) VALUES\n... (\n...     0,\n...     'Ford',\n...     1,\n...     '2015-09-12'\n... ) ON CONFLICT (id) DO UPDATE SET\n...     visits = visits + 1,\n...     last_visit = excluded.last_visit;\nINSERT OK, 1 row affected (... sec)\n\n\nThe addition of last_visit = excluded.last_visit instructs CrateDB to overwrite the existing value of last_visits with the attempted insert value.\n\nSee Also\n\nInserting data: Upserts\n\nON CONFLICT DO NOTHING\n\nIf you use the ON CONFLICT DO NOTHING clause, CrateDB will silently ignore rows that would cause a duplicate key conflict (i.e., CrateDB will not insert them and will not produce an error). For example:\n\nINSERT INTO my_table (col_a, col_b) VALUES (1, 42)\nON CONFLICT DO NOTHING\n\n\nIn the statement above, if col_a had a primary key constraint and the value 1 already existed for col_a, CrateDB would not perform an insert.\n\nNote\n\nYou may specify an explicit primary key as the conflict_target (i.e., ON CONFLICT (conflict_target) DO NOTHING), as with ON CONFLICT DO UPDATE SET. However, doing so is optional."
  },
  {
    "title": "FETCH — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/fetch.html",
    "html": "5.6\nFETCH\n\nFetch rows from a cursor.\n\nSynopsis\n\nDescription\n\nParameters\n\ndirection\n\ncount\n\ncursor_name\n\nSynopsis\nFETCH [ direction [ FROM | IN ] ] cursor_name\n\n\nWhere direction can be empty or one of:\n\nNEXT RELATIVE count ABSOLUTE position count ALL FORWARD FORWARD count FORWARD ALL\n\nDescription\n\nFetches rows from a cursor created using DECLARE.\n\nA cursor has a position and each time you use FETCH, the position changes and the rows spanning the position change get returned.\n\nParameters\ndirection\nNEXT\n\nFetch the next row. This is the default\n\nRELATIVE count\n\nFetch count rows relative to the current position.\n\nABSOLUTE position\n\nJumps to the position and returns the row, or an empty result if jumped to a position outside the result set.\n\nJumping backward is only possible if the cursor was created with SCROLL set in DECLARE.\n\ncount\n\nFetch the next count rows\n\nALL\n\nFetch all remaining rows\n\nFORWARD\n\nSame as NEXT\n\nFORWARD count\n\nSame as count\n\nFORWARD ALL\n\nSame as ALL\n\nBACKWARD\n\nMove 1 row back\n\nMoving backward is only possible if the cursor was created with SCROLL set in DECLARE.\n\nBACKWARD count\n\nMove count rows back\n\nMoving backward is only possible if the cursor was created with SCROLL set in DECLARE.\n\ncount\n\nA integer constant, determining which or how many rows to fetch\n\ncursor_name\n\nName of the cursor to fetch rows from."
  },
  {
    "title": "DROP ROLE — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/drop-role.html",
    "html": "5.6\nDROP ROLE\n\nDrop an existing database user or role.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nSynopsis\nDROP ROLE [ IF EXISTS ] name;\n\nDescription\n\nDROP ROLE is a management statement to remove an existing database user or role from the CrateDB cluster.\n\nFor usage of the DROP ROLE statement see Users and roles management.\n\nParameters\nIF EXISTS\n\nDo not fail if the user or role doesn’t exist.\n\nname\n\nThe unique name of the database user or role to be removed.\n\nThe name follows the principles of a SQL identifier (see Key words and identifiers).\n\nNote\n\nIf a role is granted to one ore more other roles and/or users, it cannot be dropped. The role must first be revoked from those roles and/or users."
  },
  {
    "title": "END — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/end.html",
    "html": "5.6\nEND\n\nA synonym for COMMIT.\n\nTable of contents\n\nSynopsis\n\nParameters\n\nDescription\n\nSynopsis\nEND [ WORK | TRANSACTION ]\n\nParameters\n\nWORK TRANSACTION\n\nOptional keywords. They have no effect.\n\nDescription\n\nThe statement commits the current transaction.\n\nAs CrateDB does not support transactions, the only effect of this command is to close all existing cursors WITHOUT HOLD in the current session."
  },
  {
    "title": "DROP VIEW — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/drop-view.html",
    "html": "5.6\nDROP VIEW\n\nDrop one or more views.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nSynopsis\nDROP VIEW [ IF EXISTS ] view_name [ , ... ]\n\nDescription\n\nDROP VIEW drops one or more existing views.\n\nIf a view doesn’t exist an error will be returned, unless IF EXISTS is used, in which case all matching existing views will be dropped.\n\nSee Also\n\nSQL syntax: CREATE VIEW"
  },
  {
    "title": "DROP USER — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/drop-user.html",
    "html": "5.6\nDROP USER\n\nDrop an existing database user or role.\n\nTable of contents\n\nSynopsis\n\nSynopsis\nDROP USER [ IF EXISTS ] username;\n\n\nFor details, see DROP ROLE as the two statements are identical."
  },
  {
    "title": "EXPLAIN — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/explain.html",
    "html": "5.6\nEXPLAIN\n\nExplain or analyze the plan for a given statement.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nSynopsis\nEXPLAIN [ ANALYZE | VERBOSE ] statement\nEXPLAIN [ ( option [, ...] ) ] statement\n\nwhere option is:\n\n    ANALYZE [ boolean ]\n    COSTS [ boolean ]\n    VERBOSE [ boolean ]\n\nDescription\n\nThe EXPLAIN command displays the execution plan that the planner generates for the supplied statement. The plan is returned as a nested object containing the plan tree.\n\nThe VERBOSE option, available through EXPLAIN VERBOSE or EXPLAIN (VERBOSE TRUE), provides a breakdown of the steps performed by the optimizer. An example output looks like this:\n\ncr> EXPLAIN VERBOSE\n... SELECT employees.id\n... FROM employees, departments\n... WHERE employees.dept_id = departments.id AND departments.name = 'IT';\n+------------------------------------+----------------------------------------------------------------------+\n| STEP                               | QUERY PLAN                                                           |\n+------------------------------------+----------------------------------------------------------------------+\n| Initial logical plan               | Eval[id] (rows=0)                                                    |\n|                                    |   └ Filter[(name = 'IT')] (rows=0)                                   |\n|                                    |     └ Join[INNER | (dept_id = id)] (rows=3)                          |\n|                                    |       ├ Collect[doc.employees | [id, dept_id] | true] (rows=18)      |\n|                                    |       └ Collect[doc.departments | [id, name] | true] (rows=6)        |\n| optimizer_move_filter_beneath_join | Eval[id] (rows=3)                                                    |\n|                                    |   └ Join[INNER | (dept_id = id)] (rows=3)                            |\n|                                    |     ├ Collect[doc.employees | [id, dept_id] | true] (rows=18)        |\n|                                    |     └ Filter[(name = 'IT')] (rows=1)                                 |\n|                                    |       └ Collect[doc.departments | [id, name] | true] (rows=6)        |\n| optimizer_rewrite_join_plan        | Eval[id] (rows=3)                                                    |\n|                                    |   └ HashJoin[(dept_id = id)] (rows=3)                                |\n|                                    |     ├ Collect[doc.employees | [id, dept_id] | true] (rows=18)        |\n|                                    |     └ Filter[(name = 'IT')] (rows=1)                                 |\n|                                    |       └ Collect[doc.departments | [id, name] | true] (rows=6)        |\n| optimizer_merge_filter_and_collect | Eval[id] (rows=3)                                                    |\n|                                    |   └ HashJoin[(dept_id = id)] (rows=3)                                |\n|                                    |     ├ Collect[doc.employees | [id, dept_id] | true] (rows=18)        |\n|                                    |     └ Collect[doc.departments | [id, name] | (name = 'IT')] (rows=1) |\n| Final logical plan                 | Eval[id] (rows=3)                                                    |\n|                                    |   └ HashJoin[(dept_id = id)] (rows=3)                                |\n|                                    |     ├ Collect[doc.employees | [id, dept_id] | true] (rows=18)        |\n|                                    |     └ Collect[doc.departments | [id] | (name = 'IT')] (rows=1)       |\n+------------------------------------+----------------------------------------------------------------------+\nEXPLAIN 5 rows in set (... sec)\n\n\nWhen issuing EXPLAIN ANALYZE or EXPLAIN (ANALYZE TRUE) the plan of the statement is executed and timings of the different phases of the plan are returned.\n\nThe COSTS option is by default enabled and can be disabled by issuing EXPLAIN (COSTS FALSE). The output of the execution plan does then exclude the costs for each logical plan.\n\nNote\n\nThe content of the returned plan tree as well as the level of detail of the timings of the different phases should be considered experimental and are subject to change in future versions. Also not all plan nodes provide in-depth details.\n\nThe output of EXPLAIN ANALYZE also includes a break down of the query execution if the statement being explained involves queries which are executed using Lucene.\n\nNote\n\nWhen a query involves an empty partitioned table you will see no breakdown concerning that table until at least one partition is created by inserting a record.\n\nThe output includes verbose low level information per queried shard. Since SQL query expressions do not always have a direct 1:1 mapping to Lucene queries, the output may be more complex but in most cases it should still be possible to identify the most expensive parts of a query expression. Some familiarity with Lucene helps in interpreting the output.\n\nA short excerpt of a query breakdown looks like this:\n\n{\n  \"QueryName\": \"PointRangeQuery\",\n  \"QueryDescription\": \"x:[1 TO 1]\",\n  \"Time\": 0.004096,\n  \"BreakDown\": {\n    \"score\": 0,\n    \"match_count\": 0,\n    \"build_scorer_count\": 0,\n    \"create_weight\": 0.004095,\n    \"next_doc\": 0,\n    \"match\": 0,\n    \"score_count\": 0,\n    \"next_doc_count\": 0,\n    \"create_weight_count\": 1,\n    \"build_scorer\": 0,\n    \"advance_count\": 0,\n    \"advance\": 0\n  }\n}\n\n\nThe time values are in milliseconds. Fields suffixed with _count indicate how often an operation was invoked.\n\nfield\n\n\t\n\ndescription\n\n\n\n\ncreate_weight\n\n\t\n\nA Weight object is created for a query and acts as a temporary object containing state. This metric shows how long this process took.\n\n\n\n\nbuild_scorer\n\n\t\n\nA Scorer object is used to iterate over documents matching the query and generate scores for them. Note that this includes only the time to create the scorer, not that actual time spent on the iteration.\n\n\n\n\nscore\n\n\t\n\nShows the time it takes to score a particular document via its Scorer.\n\n\n\n\nnext_doc\n\n\t\n\nShows the time it takes to determine which document is the next match.\n\n\n\n\nadvance\n\n\t\n\nA lower level version of next_doc.\n\n\n\n\nmatch\n\n\t\n\nSome queries use a two-phase execution, doing an approximation first, and then a second more expensive phase. This metric measures the second phase.\n\nNote\n\nIndividual timings of the different phases and queries that are profiled do not sum up to the Total. This is because there is usually additional initialization that is not measured. Also, certain phases do overlap during their execution.\n\nParameters\nstatement\n\nThe statement for which a plan or plan analysis should be returned.\n\nCurrently only SELECT and COPY FROM statements are supported."
  },
  {
    "title": "DROP SUBSCRIPTION — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/drop-subscription.html",
    "html": "5.6\nDROP SUBSCRIPTION\n\nSee Also\n\nCREATE SUBSCRIPTION\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nSynopsis\nDROP SUBSCRIPTION [ IF EXISTS ] name\n\nDescription\n\nRemoves an existing subscription from the cluster and stops the replication. Existing tables will turn into regular writable tables. It’s not possible to resume dropped subscription.\n\nParameters\nname\n\nThe name of the subscription to be deleted."
  },
  {
    "title": "DROP TABLE — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/drop-table.html",
    "html": "5.6\nDROP TABLE\n\nRemove a table.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nSynopsis\nDROP [BLOB] TABLE [IF EXISTS] table_ident\n\nDescription\n\nDROP TABLE removes tables from the cluster.\n\nUse the BLOB keyword in order to remove a blob table (see Blobs).\n\nIf the IF EXISTS clause is provided, the statement does not fail, if the referenced table does not exists.\n\nParameters\ntable_ident\n\nThe name (optionally schema-qualified) of the table to be removed."
  },
  {
    "title": "DROP SNAPSHOT — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/drop-snapshot.html",
    "html": "5.6\nDROP SNAPSHOT\n\nDelete an existing snapshot and all files referenced only by this snapshot.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nSynopsis\nDROP SNAPSHOT repository_name.snapshot_name\n\nDescription\n\nDelete a snapshot from a repository and all files only referenced by this snapshot.\n\nIf this statement is executed against a snapshot that is currently being created, the creation is aborted and all files created so far are deleted.\n\nParameters\nrepository_name\n\nThe name of the repository the snapshot is stored in as ident.\n\nsnapshot_name\n\nThe name of the snapshot to drop as ident."
  },
  {
    "title": "DROP REPOSITORY — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/drop-repository.html",
    "html": "5.6\nDROP REPOSITORY\n\nYou can use the DROP REPOSITORY statement to de-register a repository.\n\nSee Also\n\nCREATE REPOSITORY\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nSynopsis\nDROP REPOSITORY repository_name;\n\nDescription\n\nWhen a repository is de-registered, it is no longer available for use.\n\nNote\n\nWhen you drop a repository, CrateDB deletes the corresponding record from sys.repositories but does not delete any snapshots from the corresponding backend data storage. If you create a new repository using the same backend data storage, any existing snapshots will become available again.\n\nParameters\nrepository_name\n\nThe name of the repository to de-register."
  },
  {
    "title": "DROP PUBLICATION — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/drop-publication.html",
    "html": "5.6\nDROP PUBLICATION\n\nSee Also\n\nCREATE PUBLICATION\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nSynopsis\nDROP PUBLICATION [ IF EXISTS ] name\n\nDescription\n\nRemoves an existing publication from the cluster. Stops the replication for all existing subscriptions.\n\nNote\n\nReplicated tables on a subscriber cluster will turn into regular writable tables after dropping a publication on a publishing cluster.\n\nParameters\nname\n\nThe name of the publication to be deleted."
  },
  {
    "title": "DROP ANALYZER — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/drop-analyzer.html",
    "html": "5.6\nDROP ANALYZER\n\nRemove a custom analyzer.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nSynopsis\nDROP ANALYZER analyzer_ident\n\nDescription\n\nDROP ANALYZER removes a custom created analyzer from the cluster.\n\nParameters\nanalyzer_ident\n\nThe name of the analyzer to be removed."
  },
  {
    "title": "DISCARD — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/discard.html",
    "html": "5.6\nDISCARD\n\nDiscards session state.\n\nSynopsis\nDISCARD { ALL | PLANS | SEQUENCES | TEMPORARY | TEMP }\n\nDescription\n\nDiscard releases resources within a session.\n\nDISCARD ALL behaves like DEALLOCATE ALL: it deallocates all previously prepared SQL statements.\n\nAll other variants of the statement have no effect since CrateDB does not cache query plans, has no sequences, and no temporary tables."
  },
  {
    "title": "DROP FUNCTION — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/drop-function.html",
    "html": "5.6\nDROP FUNCTION\n\nDrop a function.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nSynopsis\nDROP FUNCTION [ IF EXISTS ] function_name\n    ( [ [ argument_name ] argument_type [, ...] ] )\n\nDescription\n\nDROP FUNCTION drops a user-defined function. The function_name and respective arg_type variables must be specified.\n\nParameters\nIF EXISTS\n\nDo not produce an error if the function doesn’t exist.\n\nfunction_name\n\nThe name of the function to drop.\n\nargument_name\n\nThe name given to an argument.\n\nFunction arguments do not retain names, but you can name them in your query for documentation purposes. Note that DROP FUNCTION will ignore argument names, since only the argument data types are needed to identify the function.\n\nargument_type\n\nThe data type of an argument, if any."
  },
  {
    "title": "DENY — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/deny.html",
    "html": "5.6\nDENY\n\nDenies privilege to an existing user on the whole cluster or on a specific object.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nSynopsis\nDENY { { DQL | DML | DDL | AL [,...] } | ALL [ PRIVILEGES ] }\n[ON {SCHEMA | TABLE} identifier [, ...]]\nTO user_name [, ...];\n\nDescription\n\nDENY is a management statement to deny one or many privileges on a specific object to one or many existing users.\n\nON {SCHEMA | TABLE} is optional, if not specified the privilege will be denied on the CLUSTER level.\n\nFor usage of the DENY statement see Privileges.\n\nParameters\nidentifier\n\nThe identifier of the corresponding object.\n\nIf TABLE is specified the identifier should include the table’s full qualified name. Otherwise the table will be looked up in the current schema.\n\nuser_name\n\nThe name of an existing user."
  },
  {
    "title": "DECLARE — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/declare.html",
    "html": "5.6\nDECLARE\n\nCreate a cursor.\n\nSynopsis\n\nDescription\n\nClauses\n\nWITH | WITHOUT HOLD\n\n[ ASENSITIVE | INSENSITIVE ]\n\n[ NO ] SCROLL\n\nSynopsis\nDECLARE name [ ASENSITIVE | INSENSITIVE ] [ [ NO ] SCROLL ]\nCURSOR [ { WITH | WITHOUT } HOLD ] FOR query\n\n\nWhere name is an arbitrary name for the cursor and query is a SELECT statement.\n\nDescription\n\nA cursor is used to retrieve a small number of rows at a time from a query with a larger result set. After creating a cursor, rows are fetched using FETCH.\n\nDeclared cursors are visible in the pg_catalog.pg_cursors table.\n\nSee Also\n\nFETCH CLOSE\n\nClauses\nWITH | WITHOUT HOLD\n\nDefaults to WITHOUT HOLD, causing a cursor to be bound to the life-time of a transaction. Using WITHOUT HOLD without active transaction (Started with BEGIN) is an error.\n\nWITH HOLD changes the life-time of a cursor to that of the connection.\n\nCommitting a transaction closes all cursors created with WITHOUT HOLD. Closing a connection closes all cursors created within that connection.\n\nNote\n\nCrateDB doesn’t support full transactions. A transaction cannot be rolled back and any write operations within a BEGIN clause may become visible to other statements before committing the transaction.\n\n[ ASENSITIVE | INSENSITIVE ]\n\nThis clause has no effect in CrateDB Cursors in CrateDB are always insensitive.\n\n[ NO ] SCROLL\n\nNO SCROLL (the default) specifies that the cursor can only be used to move forward.\n\nSCROLL allows using a cursor for backward movement but also adds memory overhead."
  },
  {
    "title": "DEALLOCATE — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/deallocate.html",
    "html": "5.6\nDEALLOCATE\n\nDeallocate a prepared statement\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nSynopsis\nDEALLOCATE [PREPARE] { name | ALL }\n\nDescription\n\nDEALLOCATE is used to deallocate a previously prepared SQL statement and free the reserved resources. It is not meant to be explicitly issued by a user but it’s used by some clients (e.g. libpq) over PostgreSQL Wire Protocol as an alternative way of deallocating a prepared statement to the protocol’s Close (F) message.\n\nParameters\nname\n\nThe name of the prepared statement to deallocate.\n\nALL\n\nDeallocate all prepared statements"
  },
  {
    "title": "DELETE — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/delete.html",
    "html": "5.6\nDELETE\n\nDelete rows of a table\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nSynopsis\nDELETE FROM table_ident [ [AS] table_alias ] [ WHERE condition ]\n\nDescription\n\nDELETE deletes rows that satisfy the WHERE clause from the specified table. If the WHERE clause is absent, the effect is to delete all rows in the table. The result is a valid, but empty table.\n\nParameters\ntable_ident\n\nThe name (optionally schema-qualified) of an existing table to delete rows from.\n\ntable_alias\n\nA substitute name for the target table. When an alias is provided, it completely hides the actual name of the table. For example, given DELETE FROM foo AS f, the remainder of the DELETE statement must refer to this table as f not foo.\n\ncondition\n\nAn expression that returns a value of type boolean. Only rows for which this expression returns true will be deleted."
  },
  {
    "title": "CREATE VIEW — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/create-view.html",
    "html": "5.6\nCREATE VIEW\n\nDefine a new view.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nPrivileges\n\nSynopsis\nCREATE [ OR REPLACE ] VIEW view_ident AS query\n\n\nOr\n\nCREATE [ OR REPLACE ] VIEW view_ident AS (query)\n\nWhere query is a SELECT statement.\n\nDescription\n\nCREATE VIEW creates a named definition of a query. This name can be used in other statements instead of a table name to reference the saved query definition. A view is not materialized, instead the query is run every time a view is referenced in a query.\n\nIf OR REPLACE is used, an already existing view with the same name will be replaced.\n\nIf a schema name is given in the view_ident (some_schema.view_name), the view will be created in the specified schema.\n\nTable and view names must be unique within a schema. A view cannot have the name of an already existing table.\n\nViews are read-only. They cannot be used as a target relation in write operations.\n\nSee Also\n\nSQL syntax: DROP VIEW\n\nNote\n\nIf a * is used to select the columns within the views query definition, this * will be resolved at query time like the rest of the query definition. This means if columns are added to the table after the view had been created, these columns will show up in subsequent queries on the view. It is generally recommended to avoid using * in view definitions.\n\nPrivileges\n\nRegular users need to have DDL permissions on the schema in which the view is being created. In addition the user creating the view requires DQL permissions on all relations that occur within the views query definition."
  },
  {
    "title": "CREATE ROLE — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/create-role.html",
    "html": "5.6\nCREATE ROLE\n\nCreate a new database role.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nSynopsis\nCREATE ROLE roleName\n\nDescription\n\nCREATE ROLE is a management statement to create a new database role in the CrateDB cluster. The newly created role does not have any special privileges, and those must be assigned afterwards, for details see the privileges documentation.\n\nNote\n\nROLE is essentially the same as USER with the difference that a ROLE cannot login to the database, and therefore cannot be assigned a password, and can be granted to another USER or ROLE. On the other hand, a USER can login to the database and can also be assigned a password, but cannot be granted to another USER or ROLE.\n\nFor usages of the CREATE ROLE statement see Users and roles management."
  },
  {
    "title": "CREATE USER — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/create-user.html",
    "html": "5.6\nCREATE USER\n\nCreate a new database user.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nClauses\n\nWITH\n\nSynopsis\nCREATE USER username\n[ WITH ( user_parameter = value [, ...]) ] |\n[ [ WITH ] user_parameter [value] [ ... ] ]\n\nDescription\n\nCREATE USER is a management statement to create a new database user in the CrateDB cluster. The newly created user does not have any special privileges, and those must be assigned afterwards, for details see the privileges documentation. The created user can be used to authenticate against CrateDB, see Host-Based Authentication (HBA).\n\nThe statement allows to specify a password for this account. This is not necessary if password authentication is disabled.\n\nNote\n\nUSER is essentially the same as ROLE with the difference that a USER can login to the database and can also be assigned a password, but cannot be granted to another USER or ROLE. On the contrary, a ROLE cannot login to the database, and therefore cannot be assigned a password, but it can be granted to another USER or ROLE.\n\nFor usages of the CREATE USER statement see Users and roles management.\n\nParameters\nusername\n\nThe unique name of the database user.\n\nThe name follows the principles of a SQL identifier (see Key words and identifiers).\n\nClauses\nWITH\n\nThe following user_parameter are supported to define a new user account:\n\npassword\n\nThe password as cleartext entered as string literal. e.g.:\n\nCREATE USER john WITH (password='foo')\n\nCREATE USER john WITH password='foo'\n\nCREATE USER john WITH password 'foo'\n\nCREATE USER john password 'foo'\n"
  },
  {
    "title": "CREATE TABLE AS — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/create-table-as.html",
    "html": "5.6\nCREATE TABLE AS\n\nDefine a new table from existing tables.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nSynopsis\nCREATE TABLE table_ident AS { ( query ) | query }\n\nDescription\n\nCREATE TABLE AS will create a new table and insert rows based on the specified query.\n\nOnly the column names, types, and the output rows will be used from the query. Default values will be assigned to the optional parameters used for the table creation.\n\nFor further details on the default values of the optional parameters, see CREATE TABLE.\n\nParameters\ntable_ident\n\nThe name (optionally schema-qualified) of the table to be created.\n\nquery\n\nA query (SELECT statement) that supplies the rows to be inserted. Refer to the SELECT statement for a description of the syntax."
  },
  {
    "title": "CREATE SUBSCRIPTION — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/create-subscription.html",
    "html": "5.6\nCREATE SUBSCRIPTION\n\nYou can use the CREATE SUBSCRIPTION statement to add a new subscription to the current cluster.\n\nSee Also\n\nDROP SUBSCRIPTION\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nSynopsis\nCREATE SUBSCRIPTION subscription_name\nCONNECTION 'conninfo'\nPUBLICATION publication_name [, ...]\n\nDescription\n\nCreate a new subscription to one or more publications on a publisher. The subscription name must be distinct from the name of any existing subscription in the cluster. The subscription represents a replication connection to the publisher. A logical replication will be started on a publisher once the subscription is enabled, which is by default on creation.\n\nParameters\nsubscription_name\n\nThe name of the new subscription.\n\nCONNECTION ‘conninfo’\n\nThe connection string to the publisher, which is a URL in the following format:\n\ncrate://host:[port]?params\n\n\nParameters are given in the key=value format and separated by &. Example:\n\ncrate://example.com?user=my_user&password=1234&sslmode=disable\n\n\nSupported parameters:\n\nmode: Sets how the subscriber cluster communicates with the publisher cluster. Two modes are supported: sniff (the default) and pg_tunnel.\n\nIn the sniff mode, the subscriber cluster will use the transport protocol to communicate with the other cluster and it will attempt to establish direct connections to each node of the publisher cluster. The port defaults to 4300.\n\nIn the sniff mode, there can be multiple host:port pairs, separated by a comma. Parameters will be the same for all hosts. These hosts are used as initial seed nodes to discover all eligible nodes from the remote cluster. Example:\n\ncrate://example.com:4310,123.123.123.123\n\n\nIn the pg_tunnel mode, the subscriber cluster will initiate the connection using the PostgreSQL wire protocol, and then proceed communicating via the transport protocol, but within the connection established via the PostgreSQL protocol. All requests from the subscriber cluster to the publisher cluster will get routed through a single node. The connection is only established to the first host listed in the connection string. The port defaults to 5432.\n\nParameters supported with both modes:\n\nuser: name of the user who connects to a publishing cluster. Required.\n\npassword: user password.\n\nCaution\n\nThe user specified in the connection string, must have DQL privileges on all tables of the publication on a publisher cluster. Tables, for which the user does not have the DQL privilege, will not be replicated.\n\nParameters supported in the pg_tunnel mode:\n\nsslmode: Configures whether the connection should use SSL. You must have a working SSL setup for the PostgreSQL wire protocol on both the subscriber and publisher cluster.\n\nAllowed values are prefer, require or disable. Defaults to prefer.\n\nPUBLICATION publication_name\n\nNames of the publications on the publisher to subscribe to"
  },
  {
    "title": "CREATE TABLE — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/create-table.html",
    "html": "5.6\nCREATE TABLE\n\nCreate a new table.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nTable elements\n\nBase Columns\n\nDefault clause\n\nGenerated columns\n\nTable constraints\n\nColumn constraints\n\nStorage options\n\nParameters\n\nIF NOT EXISTS\n\nCLUSTERED\n\nPARTITIONED BY\n\nWITH\n\nnumber_of_replicas\n\nnumber_of_routing_shards\n\nrefresh_interval\n\nwrite.wait_for_active_shards\n\nblocks.read_only\n\nblocks.read_only_allow_delete\n\nblocks.read\n\nblocks.write\n\nblocks.metadata\n\nsoft_deletes.enabled\n\nsoft_deletes.retention_lease.period\n\ncodec\n\nstore.type\n\nmapping.total_fields.limit\n\ntranslog.flush_threshold_size\n\ntranslog.sync_interval\n\ntranslog.durability\n\nrouting.allocation.total_shards_per_node\n\nrouting.allocation.enable\n\nallocation.max_retries\n\nrouting.allocation.include.{attribute}\n\nrouting.allocation.require.{attribute}\n\nrouting.allocation.exclude.{attribute}\n\nunassigned.node_left.delayed_timeout\n\ncolumn_policy\n\nmax_ngram_diff\n\nmax_shingle_diff\n\nmerge.scheduler.max_thread_count\n\nSynopsis\nCREATE TABLE [ IF NOT EXISTS ] table_ident ( [\n    {\n        base_column_definition\n      | generated_column_definition\n      | table_constraint\n    }\n    [, ... ] ]\n)\n[ PARTITIONED BY (column_name [, ...] ) ]\n[ CLUSTERED [ BY (routing_column) ] INTO num_shards SHARDS ]\n[ WITH ( table_parameter [= value] [, ... ] ) ]\n\n\nwhere base_column_definition:\n\ncolumn_name data_type\n[ DEFAULT default_expr ]\n[ column_constraint [ ... ] ]  [ storage_options ]\n\n\nwhere generated_column_definition is:\n\ncolumn_name [ data_type ] [ GENERATED ALWAYS ]\nAS [ ( ] generation_expression [ ) ]\n[ column_constraint [ ... ] ]\n\n\nwhere column_constraint is:\n\n{ [ CONSTRAINT constraint_name ] PRIMARY KEY |\n  NULL |\n  NOT NULL |\n  INDEX { OFF | USING { PLAIN |\n                        FULLTEXT [ WITH ( analyzer = analyzer_name ) ]  }\n  [ CONSTRAINT constraint_name ] CHECK (boolean_expression)\n}\n\n\nwhere storage_options is:\n\nSTORAGE WITH ( option = value_expression [, ... ] )\n\n\nand table_constraint is:\n\n{ [ CONSTRAINT constraint_name ] PRIMARY KEY ( column_name [, ... ] ) |\n  INDEX index_name USING FULLTEXT ( column_name [, ... ] )\n       [ WITH ( analyzer = analyzer_name ) ]\n  [ CONSTRAINT constraint_name ] CHECK (boolean_expression)\n}\n\nDescription\n\nCREATE TABLE will create a new, initially empty table.\n\nIf the table_ident does not contain a schema, the table is created in the doc schema. Otherwise it is created in the given schema, which is implicitly created, if it didn’t exist yet.\n\nA table consists of one or more base columns and any number of generated columns and/or table constraints.\n\nThe optional constraint clauses specify constraints (tests) that new or updated rows must satisfy for an INSERT, UPDATE or COPY FROM operation to succeed. A constraint is an SQL object that helps define the set of valid values in the table in various ways.\n\nThere are two ways to define constraints: table constraints and column constraints. A column constraint is defined as part of a column definition. A table constraint definition is not tied to a particular column, and it can encompass more than one column. Every column constraint can also be written as a table constraint; a column constraint is only a notational convenience for use when the constraint only affects one column.\n\nSee Also\n\nData definition: Creating tables\n\nTable elements\nBase Columns\n\nA base column is a persistent column in the table metadata. In relational terms it is an attribute of the tuple of the table-relation. It has a name, a type, an optional default clause and optional constraints.\n\nBase columns are readable and writable (if the table itself is writable). Values for base columns are given in DML statements explicitly or omitted, in which case their value is null.\n\nDefault clause\n\nThe optional default clause defines the default value of the column. The value is inserted when the column is a target of an INSERT or COPY FROM statement that doesn’t contain an explicit value for it.\n\nThe default clause expression is variable-free, it means that subqueries and cross-references to other columns are not allowed.\n\nNote\n\nDefault values are not allowed for columns of type OBJECT:\n\ncr> CREATE TABLE tbl (obj OBJECT DEFAULT {key='foo'})\nSQLParseException[Default values are not allowed for object columns: obj]\n\n\nThey are allowed for sub columns of an object column. If an object column has at least one child with a default expression it will implicitly create the full object unless it’s within an array.\n\nAn example:\n\ncr> CREATE TABLE object_defaults (id int, obj OBJECT AS (key TEXT DEFAULT ''))\nCREATE OK, 1 row affected  (... sec)\n\ncr> INSERT INTO object_defaults (id) VALUES (1)\nINSERT OK, 1 row affected  (... sec)\n\ncr> REFRESH TABLE object_defaults\nREFRESH OK, 1 row affected  (... sec)\n\ncr> SELECT obj FROM object_defaults\n+-------------+\n| obj         |\n+-------------+\n| {\"key\": \"\"} |\n+-------------+\nSELECT 1 row in set (... sec)\n\nGenerated columns\n\nA generated column is a persistent column that is computed as needed from the generation_expression for every INSERT, UPDATE and COPY FROM operation.\n\nThe GENERATED ALWAYS part of the syntax is optional.\n\nNote\n\nA generated column is not a virtual column. The computed value is stored in the table like a base column is. The automatic computation of the value is what makes it different.\n\nSee Also\n\nData definition: Generated columns\n\nTable constraints\n\nTable constraints are constraints that are applied to more than one column or to the table as a whole.\n\nSee Also\n\nGeneral SQL: Table constraints\n\nCHECK constraint\n\nColumn constraints\n\nColumn constraints are constraints that are applied on each column of the table separately.\n\nSee Also\n\nGeneral SQL: Column constraints\n\nCHECK constraint\n\nStorage options\n\nStorage options can be applied on each column of the table separately.\n\nSee Also\n\nData definition: Storage\n\nParameters\ntable_ident\n\nThe name (optionally schema-qualified) of the table to be created.\n\ncolumn_name\n\nThe name of a column to be created in the new table.\n\ndata_type\n\nThe data type of the column. This can include array and object specifiers.\n\ngeneration_expression\n\nAn expression (usually a function call) that is applied in the context of the current row. As such, it can reference other base columns of the table. Referencing other generated columns (including itself) is not supported. The generation expression is evaluated each time a row is inserted or the referenced base columns are updated.\n\nIF NOT EXISTS\n\nIf the optional IF NOT EXISTS clause is used, this statement won’t do anything if the table exists already.\n\nCLUSTERED\n\nThe optional CLUSTERED clause specifies how a table should be distributed across a cluster.\n\n[ CLUSTERED [ BY (routing_column) ] INTO num_shards SHARDS ]\n\nnum_shards\n\nSpecifies the number of shards a table is stored in. Must be greater than 0. If not provided, the number of shards is calculated based on the number of currently active data nodes with the following formula:\n\nnum_shards = max(4, num_data_nodes * 2)\n\n\nNote\n\nThe minimum value of num_shards is set to 4. This means if the calculation of num_shards does not exceeds its minimum it applies the minimum value to each table or partition as default.\n\nrouting_column\n\nSpecify a routing column that determines how rows are sharded.\n\nAll rows that have the same routing_column row value are stored in the same shard. If a primary key has been defined, it will be used as the default routing column, otherwise the internal document ID is used.\n\nSee Also\n\nData definition: Sharding\n\nPARTITIONED BY\n\nThe PARTITIONED clause splits the created table into separate partitions for every distinct combination of row values in the specified partition columns.\n\n[ PARTITIONED BY ( column_name [ , ... ] ) ]\n\ncolumn_name\n\nThe name of a column to be used for partitioning. Multiple columns names can be specified inside the parentheses and must be separated by commas.\n\nThe following restrictions apply:\n\nPartition columns may not be part of the CLUSTERED clause\n\nPartition columns must only contain primitive types\n\nPartition columns may not be inside an object array\n\nPartition columns may not be indexed with a fulltext index with analyzer\n\nIf the table has a PRIMARY KEY constraint, all of the partition columns must be included in the primary key definition\n\nCaution\n\nPartition columns cannot be altered by an UPDATE statement.\n\nWITH\n\nThe optional WITH clause can specify parameters for tables.\n\n[ WITH ( table_parameter [= value] [, ... ] ) ]\n\ntable_parameter\n\nSpecifies an optional parameter for the table.\n\nNote\n\nSome parameters are nested, and therefore need to be wrapped in double quotes in order to be set. For example:\n\nWITH (\"allocation.max_retries\" = 5)\n\n\nNested parameters are those that contain a . between parameter names (e.g. write.wait_for_active_shards).\n\nAvailable parameters are:\n\nnumber_of_replicas\n\nSpecifies the number or range of replicas each shard of a table should have for normal operation, the default is to have 0-1 replica.\n\nThe number of replicas is defined like this:\n\nmin_replicas [ - [ max_replicas ] ]\n\nmin_replicas\n\nThe minimum number of replicas required.\n\nmax_replicas\n\nThe maximum number of replicas.\n\nThe actual maximum number of replicas is max(num_replicas, N-1), where N is the number of data nodes in the cluster. If max_replicas is the string all then it will always be N.\n\nSee Also\n\nReplication\n\nnumber_of_routing_shards\n\nThis number specifies the hashing space that is used internally to distribute documents across shards.\n\nThis is an optional setting that enables users to later on increase the number of shards using ALTER TABLE. It’s not possible to update this setting after table creation.\n\nrefresh_interval\n\nIn CrateDB new written records are not immediately visible. A user has to either invoke the REFRESH statement or wait for an automatic background refresh.\n\nThe interval of this background refresh is specified in milliseconds using this refresh_interval setting.\n\nBy default it’s not specified, which causes tables to be refreshed once every second but only if the table is not idle. A table can become idle if no query accesses it for more than 30 seconds.\n\nIf a table is idle, the periodic refresh is temporarily disabled. A query hitting an idle table will trigger a refresh and enable the periodic refresh again.\n\nWhen refresh_interval is set explicitly, table is refreshed regardless of idle state. Use ALTER TABLE RESET to switch to default 1 second refresh and freeze-on-idle behavior.\n\nvalue\n\nThe refresh interval in milliseconds. A value smaller or equal than 0 turns off the automatic refresh. A value of greater than 0 schedules a periodic refresh of the table.\n\nNote\n\nA refresh_interval of 0 does not guarantee that new writes are NOT visible to subsequent reads. Only the periodic refresh is disabled. There are other internal factors that might trigger a refresh.\n\nNote\n\nOn partitioned tables, the idle mechanism works per partition. This can be useful for time-based partitions where older partitions are rarely queried.\n\nThe downside is that if many partitions are idle and a query activates them, there will be a spike in refresh load. If you’ve such an access pattern, you may want to set an explicit refresh_interval to have a permanent background refresh.\n\nSee Also\n\nQuerying: Refresh\n\nSQL syntax: REFRESH\n\nwrite.wait_for_active_shards\n\nSpecifies the number of shard copies that need to be active for write operations to proceed. If less shard copies are active the operation must wait and retry for up to 30s before timing out.\n\nvalue\n\nall or a positive integer up to the total number of configured shard copies (number_of_replicas + 1).\n\nA value of 1 means only the primary has to be active. A value of 2 means the primary plus one replica shard has to be active, and so on.\n\nThe default value is set to 1.\n\nall is a special value that means all shards (primary + replicas) must be active for write operations to proceed.\n\nIncreasing the number of shard copies to wait for improves the resiliency of the system. It reduces the chance of write operations not writing to the desired number of shard copies, but it does not eliminate the possibility completely, because the check occurs before the write operation starts.\n\nReplica shard copies that missed some writes will be brought up to date by the system eventually, but in case a node holding the primary copy has a system failure, the replica copy couldn’t be promoted automatically as it would lead to data loss since the system is aware that the replica shard didn’t receive all writes. In such a scenario, ALTER TABLE .. REROUTE PROMOTE REPLICA can be used to force the allocation of a stale replica copy to at least recover the data that is available in the stale replica copy.\n\nSay you’ve a 3 node cluster and a table with 1 configured replica. With write.wait_for_active_shards=1 and number_of_replicas=1 a node in the cluster can be restarted without affecting write operations because the primary copies are either active or the replicas can be quickly promoted.\n\nIf write.wait_for_active_shards would be set to 2 instead and a node is stopped, the write operations would block until the replica is fully replicated again or the write operations would timeout in case the replication is not fast enough.\n\nblocks.read_only\n\nAllows to have a read only table.\n\nvalue\n\nTable is read only if value set to true. Allows writes and table settings changes if set to false.\n\nblocks.read_only_allow_delete\n\nAllows to have a read only table that additionally can be deleted.\n\nvalue\n\nTable is read only and can be deleted if value set to true. Allows writes and table settings changes if set to false. This flag should not be set manually as it’s used, in an automated way, by the mechanism that protects CrateDB nodes from running out of available disk space.\n\nWhen a disk on a node exceeds the cluster.routing.allocation.disk.watermark.flood_stage threshold, this block is applied (set to true) to all tables on that affected node. Once you’ve freed disk space again and the threshold is undershot, the setting is automatically reset to false for the affected tables.\n\nSee Also\n\nCluster-wide settings: Disk-based shard allocation\n\nNote\n\nDuring maintenance operations, you might want to temporarily disable reads, writes or table settings changes. To achieve this, please use the corresponding settings blocks.read, blocks.write, blocks.metadata, or blocks.read_only, which must be manually reset after the maintenance operation has been completed.\n\nblocks.read\n\ndisable/enable all the read operations\n\nvalue\n\nSet to true to disable all read operations for a table, otherwise set false.\n\nblocks.write\n\ndisable/enable all the write operations\n\nvalue\n\nSet to true to disable all write operations and table settings modifications, otherwise set false.\n\nblocks.metadata\n\ndisable/enable the table settings modifications.\n\nvalues\n\nDisables the table settings modifications if set to true. If set to false, table settings modifications are enabled.\n\nsoft_deletes.enabled\n\nIndicates whether soft deletes are enabled or disabled.\n\nSoft deletes allow CrateDB to preserve recent deletions within the Lucene index. This information is used for shard recovery.\n\nBefore the introduction of soft deletes, CrateDB had to retain the information in the Translog. Using soft deletes uses less storage than the Translog equivalent and is faster.\n\nSoft deletes are mandatory in CrateDB 5.0, therefore this setting can no longer be modified. It will always be set to true.\n\nThe setting will be removed in CrateDB 6.0.\n\nsoft_deletes.retention_lease.period\n\nThe maximum period for which a retention lease is retained before it is considered expired.\n\nvalue\n\n12h (default). Any positive time value is allowed.\n\nCrateDB sometimes needs to replay operations that were executed on one shard on other shards. For example if a shard copy is temporarily unavailable but write operations to the primary copy continues, the missed operations have to be replayed once the shard copy becomes available again.\n\nIf soft deletes are enabled, CrateDB uses a Lucene feature to preserve recent deletions in the Lucene index so that they can be replayed. Because of that, deleted documents still occupy disk space, which is why CrateDB only preserves certain recently-deleted documents. CrateDB eventually fully discards deleted documents to prevent the index growing larger despite having deleted documents.\n\nCrateDB keeps track of operations it expects to need to replay using a mechanism called shard history retention leases. Retention leases are a mechanism that allows CrateDB to determine which soft-deleted operations can be safely discarded.\n\nIf a shard copy fails, it stops updating its shard history retention lease, indicating that the soft-deleted operations should be preserved for later recovery.\n\nHowever, to prevent CrateDB from holding onto shard retention leases forever, they expire after soft_deletes.retention_lease.period, which defaults to 12h. Once a retention lease has expired CrateDB can again discard soft-deleted operations. In case a shard copy recovers after a retention lease has expired, CrateDB will fall back to copying the whole index since it can no longer replay the missing history.\n\ncodec\n\nBy default data is stored using LZ4 compression. This can be changed to best_compression which uses DEFLATE for a higher compression ratio, at the expense of slower column value lookups.\n\nvalues\n\ndefault or best_compression\n\nstore.type\n\nThe store type setting allows you to control how data is stored and accessed on disk. It’s not possible to update this setting after table creation. The following storage types are supported:\n\nfs\n\nDefault file system implementation. It will pick the best implementation depending on the operating environment, which is currently hybridfs on all supported systems but is subject to change.\n\nniofs\n\nThe NIO FS type stores the shard index on the file system (Lucene NIOFSDirectory) using NIO. It allows multiple threads to read from the same file concurrently.\n\nmmapfs\n\nThe MMap FS type stores the shard index on the file system (Lucene MMapDirectory) by mapping a file into memory (mmap). Memory mapping uses up a portion of the virtual memory address space in your process equal to the size of the file being mapped. Before using this type, be sure you have allowed plenty of virtual address space.\n\nhybridfs\n\nThe hybridfs type is a hybrid of niofs and mmapfs, which chooses the best file system type for each type of file based on the read access pattern. Similarly to mmapfs be sure you have allowed plenty of virtual address space.\n\nIt is possible to restrict the use of the mmapfs and hybridfs store type via the node.store.allow_mmap node setting.\n\nmapping.total_fields.limit\n\nSets the maximum number of columns that is allowed for a table. Default is 1000.\n\nvalue\n\nMaximum amount of fields in the Lucene index mapping. This includes both the user facing mapping (columns) and internal fields.\n\ntranslog.flush_threshold_size\n\nSets size of transaction log prior to flushing.\n\nvalue\n\nSize (bytes) of translog.\n\ntranslog.sync_interval\n\nHow often the translog is fsynced to disk. Defaults to 5s. When setting this interval, please keep in mind that changes logged during this interval and not synced to disk may get lost in case of a failure. This setting only takes effect if translog.durability is set to ASYNC.\n\nvalue\n\nInterval in milliseconds.\n\ntranslog.durability\n\nIf set to ASYNC the translog gets flushed to disk in the background every translog.sync_interval. If set to REQUEST the flush happens after every operation.\n\nvalue\n\nREQUEST (default), ASYNC\n\nrouting.allocation.total_shards_per_node\n\nControls the total number of shards (replicas and primaries) allowed to be allocated on a single node. Defaults to unbounded (-1).\n\nvalue\n\nNumber of shards per node.\n\nrouting.allocation.enable\n\nControls shard allocation for a specific table. Can be set to:\n\nall\n\nAllows shard allocation for all shards. (Default)\n\nprimaries\n\nAllows shard allocation only for primary shards.\n\nnew_primaries\n\nAllows shard allocation only for primary shards for new tables.\n\nnone\n\nNo shard allocation allowed.\n\nallocation.max_retries\n\nDefines the number of attempts to allocate a shard before giving up and leaving the shard unallocated.\n\nvalue\n\nNumber of retries to allocate a shard. Defaults to 5.\n\nrouting.allocation.include.{attribute}\n\nAssign the table to a node whose {attribute} has at least one of the comma-separated values.\n\nSee Also\n\nData definition: Shard allocation filtering\n\nrouting.allocation.require.{attribute}\n\nAssign the table to a node whose {attribute} has all of the comma-separated values.\n\nSee Also\n\nData definition: Shard allocation filtering\n\nrouting.allocation.exclude.{attribute}\n\nAssign the table to a node whose {attribute} has none of the comma-separated values.\n\nSee Also\n\nData definition: Shard allocation filtering\n\nunassigned.node_left.delayed_timeout\n\nDelay the allocation of replica shards which have become unassigned because a node has left. It defaults to 1m to give a node time to restart completely (which can take some time when the node has lots of shards). Setting the timeout to 0 will start allocation immediately. This setting can be changed on runtime in order to increase/decrease the delayed allocation if needed.\n\ncolumn_policy\n\nSpecifies the column policy of the table. The default column policy is strict.\n\nThe column policy is defined like this:\n\nWITH ( column_policy = {'dynamic' | 'strict'} )\n\nstrict\n\nRejecting any column on INSERT, UPDATE or COPY FROM which is not defined in the schema\n\ndynamic\n\nNew columns can be added using INSERT, UPDATE or COPY FROM. New columns added to dynamic tables are, once added, usable as usual columns. One can retrieve them, sort by them and use them in WHERE clauses.\n\nSee Also\n\nData definition: Column policy\n\nmax_ngram_diff\n\nSpecifies the maximum difference between max_ngram and min_ngram when using the NGramTokenizer or the NGramTokenFilter. The default is 1.\n\nmax_shingle_diff\n\nSpecifies the maximum difference between min_shingle_size and max_shingle_size when using the ShingleTokenFilter. The default is 3.\n\nmerge.scheduler.max_thread_count\n\nThe maximum number of threads on a single shard that may be merging at once. Defaults to Math.max(1, Math.min(4, Runtime.getRuntime().availableProcessors() / 2)) which works well for a good solid-state-disk (SSD). If your index is on spinning platter drives instead, decrease this to 1."
  },
  {
    "title": "CREATE FUNCTION — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/create-function.html",
    "html": "5.6\nCREATE FUNCTION\n\nCreate a new function.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nSynopsis\nCREATE [OR REPLACE] FUNCTION function_name\n    ( [ [argument_name] argument_type ] [, ...] ] )\nRETURNS return_type\nLANGUAGE language_name\nAS 'definition'\n\nDescription\n\nCREATE FUNCTION creates a new user-defined function.\n\nCREATE OR REPLACE FUNCTION will either create a new function, or replace an existing function.\n\nThe signature of the function is defined by its schema, name, and input arguments.\n\nYou can overload functions by defining two functions of the same name, but with a different set of input arguments.\n\nParameters\nfunction_name\n\nThe name of the function to create.\n\nargument_name\n\nThe optional name given to an argument. Function arguments do not retain names, but you can name them in your query for documentation purposes.\n\nargument_type\n\nThe data type of a given argument.\n\nreturn_type\n\nThe returned data type of the function. The return type can be any supported type.\n\nlanguage_name\n\nThe registered language which should be used for the function.\n\ndefinition\n\nA string defining the body of the function."
  },
  {
    "title": "CREATE SNAPSHOT — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/create-snapshot.html",
    "html": "5.6\nCREATE SNAPSHOT\n\nCreate a new incremental snapshot inside a repository that contains the current state of the given tables and/or partitions and the cluster metadata.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nClauses\n\nPARTITION\n\nWITH\n\nSynopsis\nCREATE SNAPSHOT repository_name.snapshot_name\n{ TABLE table_ident [ PARTITION (partition_column = value [, ...])] [, ...] | ALL }\n[ WITH (snapshot_parameter [= value], [, ...]) ]\n\nDescription\n\nCreate a new incremental snapshot inside a repository.\n\nA snapshot is a backup of the current state of the given tables and the cluster metadata at the point the CREATE SNAPSHOT query starts executing. Changes made after that are not considered for the snapshot.\n\nA snapshot is fully qualified by its snapshot_name and the name of the repository it should be created in (repository_name). A snapshot_name must be unique per repository.\n\nNote\n\nFor snapshot names the same restrictions as for table names apply.\n\nThis is mainly because snapshot names will likely become stored as file or directory on disc and hence must be valid filenames.\n\nCreating the snapshot operates on primary shards which are not currently relocated. If a shard is being relocated the snapshot of the shard is created when the relocation is completed.\n\nA snapshot only backups the parts of the data that are not yet stored in the given repository by older snapshots, thus creating snapshots is incremental.\n\nSnapshots can include one or more tables, each given as table_ident. It is also possible to include only single partitions given the values of the partition columns.\n\nIf ALL is used, every table in the cluster (except system tables, blob tables and tables in the information_schema schema) as well as all persistent settings and the full cluster metadata is included in the snapshot.\n\nParameters\nrepository_name\n\nThe name of the repository to create the snapshot in as ident.\n\nsnapshot_name\n\nThe name of the snapshot as ident.\n\ntable_ident\n\nThe name (optionally schema-qualified) of an existing table that is to be included in the snapshot.\n\nClauses\nPARTITION\n\nIf the table is partitioned, the optional PARTITION clause can be used to create a snapshot from one partition exclusively.\n\n[ PARTITION ( partition_column = value [ , ... ] ) ]\n\npartition_column\n\nOne of the column names used for table partitioning\n\nvalue\n\nThe respective column value.\n\nAll partition columns (specified by the PARTITIONED BY clause) must be listed inside the parentheses along with their respective values using the partition_column = value syntax (separated by commas).\n\nBecause each partition corresponds to a unique set of partition column row values, this clause uniquely identifies a single partition to snapshot.\n\nTip\n\nThe SHOW CREATE TABLE statement will show you the complete list of partition columns specified by the PARTITIONED BY clause.\n\nWITH\n[ WITH (snapshot_parameter [= value], [, ...]) ]\n\n\nThe following configuration parameters can be used to modify how the snapshot is created:\n\nwait_for_completion\n\n(Default false) By default the request returns once the snapshot creation started. If set to true the request returns after the whole snapshot was created or an error occurred. The sys.snapshots table can be queried to track the snapshot creation progress if wait_for_completion has been set to false.\n\nignore_unavailable\n\n(Default false) If a given table does not exist the command will fail by default. If set to true these tables are ignored and not included in the snapshot."
  },
  {
    "title": "CREATE REPOSITORY — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/create-repository.html",
    "html": "5.6\nCREATE REPOSITORY\n\nYou can use the CREATE REPOSITORY statement to register a new repository that you can use to create, manage, and restore snapshots.\n\nSee Also\n\nDROP REPOSITORY\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nClauses\n\nWITH\n\nTypes\n\nfs\n\ns3\n\nazure\n\nurl\n\nSynopsis\nCREATE REPOSITORY repository_name TYPE type\n[ WITH (parameter_name [= value], [, ...]) ]\n\nDescription\n\nThe CREATE REPOSITORY statement creates a repository with a repository name and repository type. You can configure the different types of repositories WITH additional parameters.\n\nNote\n\nIf the back-end data storage (specific to the repository type) already contains CrateDB snapshots, they will become available to the cluster.\n\nSee Also\n\nSystem information: Repositories\n\nParameters\nrepository_name\n\nThe name of the repository to register.\n\ntype\n\nThe repository type.\n\nCaution\n\nYou cannot change any repository parameters after creating the repository (including parameters set by the WITH clause).\n\nSuppose you want to use new parameters for an existing repository. In that case, you must first drop the repository using the DROP REPOSITORY statement and then recreate it with a new CREATE REPOSITORY statement.\n\nWhen you drop a repository, CrateDB deletes the corresponding record from sys.repositories but does not delete any snapshots from the corresponding backend data storage. If you create a new repository using the same backend data storage, any existing snapshots will become available again.\n\nClauses\nWITH\n\nYou can use the WITH clause to specify one or more repository parameter values:\n\n[ WITH (parameter_name [= value], [, ...]) ]\n\nParameters\n\nThe following parameters apply to all repository types:\n\nmax_restore_bytes_per_sec\n\nThe maximum rate (bytes per second) at which a single CrateDB node will read snapshot data from this repository.\n\nDefault: 40mb\n\nmax_snapshot_bytes_per_sec\n\nThe maximum rate (bytes per second) at which a single CrateDB node will write snapshot data to this repository.\n\nDefault: 40mb\n\nAll other parameters (see the next section) are specific to the repository type.\n\nTypes\n\nCrateDB includes built-in support for the following types:\n\nfs\n\ns3\n\nazure\n\nurl\n\nCrateDB can support additional types via plugins.\n\nfs\n\nAn fs repository stores snapshots on the local file system. If a cluster has multiple nodes, you must use a shared data storage volume mounted locally on all master nodes and data nodes.\n\nNote\n\nTo create fs repositories, you must configure the list of allowed file system paths using the path.repo setting.\n\nParameters\nlocation\nType: text\nRequired\n\nAn absolute or relative path to the directory where CrateDB will store snapshots. If the path is relative, CrateDB will append it to the first entry in the path.repo setting.\n\nWindows UNC paths are allowed if the server name and shares are specified and backslashes are escaped.\n\nThe path must be allowed by the path.repo setting.\n\ncompress\nType: boolean\nDefault: true\n\nWhether CrateDB should compress the metadata part of the snapshot or not.\n\nCrateDB does not compress the actual table data.\n\nchunk_size\nType: bigint or text\nDefault: null\n\nDefines the maximum size of any single file that comprises the snapshot. If set to null, CrateDB will not split big files into smaller chunks. You can specify the chunk size with units (e.g., 1g, 5m, or 9k). If no unit is specified, the unit defaults to bytes.\n\ns3\n\nAn s3 repository stores snapshot on the Amazon Simple Storage Service (Amazon S3).\n\nNote\n\nIf you are using Amazon S3 in conjunction with IAM roles, the access_key and secret_key parameters must be left undefined.\n\nAdditionally, make sure to attach the IAM to each EC2 instance that will run a CrateDB master node or data node. The attached IAM role will provide the necessary credentials when required.\n\nParameters\naccess_key\nType: text\nRequired: false\n\nAccess key used for authentication against Amazon Web Services (AWS).\n\nNote\n\nCrateDB masks this parameter. You cannot query the parameter value from the sys.repositories table.\n\nsecret_key\nType: text\nRequired: false\n\nThe secret key used for authentication against AWS.\n\nNote\n\nCrateDB masks this parameter. You cannot query the parameter value from the sys.repositories table.\n\nendpoint\nType: text\nDefault: The default AWS API endpoint\n\nThe AWS API endpoint to use.\n\nTip\n\nYou can specify a regional endpoint to force the use of a specific AWS region.\n\nprotocol\nType: text\nValues: http, https\nDefault: https\n\nProtocol to use.\n\nbucket\nType: text\n\nName of the Amazon S3 bucket used for storing snapshots.\n\nIf the bucket does not yet exist, CrateDB will attempt to create a new bucket on Amazon S3.\n\nbase_path\nType: text\nDefault: root directory\n\nThe bucket path to use for snapshots.\n\nThe path is relative, so the base_path value must not start with a / character.\n\ncompress\nType: boolean\nDefault: true\n\nWhether CrateDB should compress the metadata part of the snapshot or not.\n\nCrateDB does not compress the actual table data.\n\nchunk_size\nType: bigint or text\nDefault: null\n\nDefines the maximum size of any single file that comprises the snapshot. If set to null, CrateDB will not split big files into smaller chunks. You can specify the chunk size with units (e.g., 1g, 5m, or 9k). If no unit is specified, the unit defaults to bytes.\n\nreadonly\nType: boolean\nDefault: false\n\nIf true, the repository is read-only.\n\nserver_side_encryption\nType: boolean\nDefault: false\n\nIf true, files are server-side encrypted by AWS using the AES256 algorithm.\n\nbuffer_size\nType: text\nDefault: 5mb\nMinimum: 5mb\n\nIf a chunk is smaller than buffer_size, CrateDB will upload the chunk with a single request.\n\nIf a chunk exceeds buffer_size, CrateDB will split the chunk into multiple parts of buffer_size length and upload them separately.\n\nmax_retries\nType: integer\nDefault: 3\n\nThe number of retries in case of errors.\n\nuse_throttle_retries\nType: boolean\nDefault: true\n\nWhether CrateDB should throttle retries (i.e., should back off).\n\ncanned_acl\nType: text\nValues: private, public-read, public-read-write, authenticated-read, log-delivery-write, bucket-owner-read, or bucket-owner-full-control\nDefault: private\n\nWhen CrateDB creates new buckets and objects, the specified Canned ACL is added.\n\nstorage_class\nType: text\nValues: standard, reduced_redundancy or standard_ia\nDefault: standard\n\nThe S3 storage class used for objects stored in the repository. This only affects the S3 storage class used for newly created objects in the repository.\n\nazure\n\nAn azure repository stores snapshots on the Azure Blob storage service.\n\nParameters\naccount\nType: text\n\nThe Azure Storage account name.\n\nNote\n\nCrateDB masks this parameter. You cannot query the parameter value from the sys.repositories table.\n\nkey\nType: text\n\nThe Azure Storage account secret key.\n\nNote\n\nCrateDB masks this parameter. You cannot query the parameter value from the sys.repositories table.\n\nendpoint\nType: text\n\nThe Azure Storage account endpoint.\n\nTip\n\nYou can use an sql-create-repo-azure-endpoint to access Azure Storage instances served on private endpoints.\n\nNote\n\nendpoint cannot be used in combination with sql-create-repo-azure-endpoint_suffix.\n\nsecondary_endpoint\nType: text\n\nThe Azure Storage account secondary endpoint.\n\nNote\n\nsecondary_endpoint cannot be used if sql-create-repo-azure-endpoint is not specified.\n\nendpoint_suffix\nType: text\nDefault: core.windows.net\n\nThe Azure Storage account endpoint suffix.\n\nTip\n\nYou can use an endpoint suffix to force the use of a specific Azure service region.\n\ncontainer\nType: text\nDefault: crate-snapshots\n\nThe blob container name.\n\nNote\n\nYou must create the container before creating the repository.\n\nbase_path\nType: text\nDefault: root directory\n\nThe container path to use for snapshots.\n\ncompress\nType: boolean\nDefault: true\n\nWhether CrateDB should compress the metadata part of the snapshot or not.\n\nCrateDB does not compress the actual table data.\n\nchunk_size\nType: bigint or text\nDefault: 256mb\nMaximum: 256mb\nMinimum: 1b\n\nDefines the maximum size of any single file that comprises the snapshot. If set to null, CrateDB will not split big files into smaller chunks. You can specify the chunk size with units (e.g., 1g, 5m, or 9k). If no unit is specified, the unit defaults to bytes.\n\nreadonly\nType: boolean\nDefault: false\n\nIf true, the repository is read-only.\n\nlocation_mode\nType: text\nValues: primary_only, secondary_only, primary_then_secondary, secondary_then_primary\nDefault: primary_only\n\nThe location mode for storing blob data.\n\nNote\n\nIf you set location_mode to secondary_only, readonly will be forced to true.\n\nmax_retries\nType: integer\nDefault: 3\n\nThe number of retries (in the case of failures) before considering the snapshot to be failed.\n\ntimeout\nType: text\nDefault: 30s\n\nThe client side timeout for any single request to Azure.\n\nproxy_type\nType: text\nValues: http, socks, or direct\nDefault: direct\n\nThe type of proxy to use when connecting to Azure.\n\nproxy_host\nType: text\n\nThe hostname of the proxy.\n\nproxy_port\nType: integer\nDefault: 0\n\nThe port number of the proxy.\n\nurl\n\nA url repository provides read-only access to an fs repository via one of the supported network access protocols.\n\nYou can use a url repository to restore snapshots.\n\nParameters\nurl\nType: text\n\nThe root URL of the fs repository.\n\nNote\n\nThe URL must match one of the URLs configured by the repositories.url.allowed_urls setting."
  },
  {
    "title": "CREATE PUBLICATION — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/create-publication.html",
    "html": "5.6\nCREATE PUBLICATION\n\nYou can use the CREATE PUBLICATION statement to add a new publication to the current cluster.\n\nSee Also\n\nALTER PUBLICATION DROP PUBLICATION\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nSynopsis\nCREATE PUBLICATION name\n{ FOR TABLE table_name [, ...] | FOR ALL TABLES }\n\nDescription\n\nAdd a new publication to the current cluster. The publication name must be distinct from the name of any existing publication in the current cluster. A publication represents a group of tables whose data changes can be replicated by other clusters (subscribers) by creating a subscription.\n\nIf neither FOR TABLE nor FOR ALL TABLES is specified, then the publication starts out with an empty set of tables. That is useful if tables are to be added later. The creation of a publication does not start any replication.\n\nParameters\nname\n\nThe name of the new publication.\n\nFOR TABLE\n\nSpecifies a list of tables to add to the publication. The partitions of a partitioned table are always implicitly considered part of the publication.\n\nFOR ALL TABLES\n\nMarks the publication as one that replicates changes for all tables in the cluster, including tables created in the future."
  },
  {
    "title": "CREATE BLOB TABLE — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/create-blob-table.html",
    "html": "5.6\nCREATE BLOB TABLE\n\nCreate a new table for storing Binary Large OBjects (BLOBS).\n\nTable of contents\n\nSynopsis\n\nDescription\n\nClauses\n\nCLUSTERED\n\nWITH\n\nblobs_path\n\nSynopsis\nCREATE BLOB TABLE table_name\n[CLUSTERED INTO num_shards SHARDS]\n[ WITH ( storage_parameter [= value] [, ... ] ) ]\n\nDescription\n\nCREATE BLOB TABLE will create a new table for holding BLOBS.\n\nSee Also\n\nBLOB support\n\nClauses\nCLUSTERED\n\nFollows the same syntax as the CREATE TABLE … CLUSTERED clause, except for what concerns the definition of the sharding key which is not applicable as blob tables are always sharded by the blobs’ digests.\n\nWITH\n\nFollows the same syntax as the CREATE TABLE … WITH clause with the following additional parameter.\n\nblobs_path\n\nSpecifies a custom path for storing blob data of a blob table.\n\nblobs_path\n\nThe custom path for storing blob data as a string literal value or string parameter.\n\nThe path can be either absolute or relative and must be creatable/writable by the user CrateDB is running as. A relative path value is relative to CRATE_HOME. This path take precedence over any global configured value."
  },
  {
    "title": "CREATE ANALYZER — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/create-analyzer.html",
    "html": "5.6\nCREATE ANALYZER\n\nDefine a new fulltext analyzer.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nSynopsis\nCREATE ANALYZER analyzer_name EXTENDS parent_analyzer_name\n    WITH ( override_parameter [= value] [, ... ] )\n\nCREATE ANALYZER analyzer_name (\n    [ TOKENIZER\n      {\n          tokenizer_name\n        | custom_name WITH ( type = tokenizer_name, tokenizer_parameter [= value] [, ... ] )\n      }\n    ]\n    [ TOKEN_FILTERS (\n        {\n            token_filter_name\n          | custom_name WITH ( type = token_filter_name, token_filter_parameter [= value] [, ... ] )\n        }\n        [, ... ]\n      )\n    ]\n    [ CHAR_FILTERS (\n        {\n            char_filter_name\n          | custom_name WITH ( type = char_filter_name, char_filter_parameter [= value] [, ... ] )\n        }\n        [, ... ]\n      )\n    ]\n)\n\nDescription\n\nCREATE ANALYZER specifies a whole analyzer chain for use in fulltext searches. It is possible to extend an existing analyzer or define a new analyzer chain from scratch. For examples and detailed explanation see Creating a custom analyzer.\n\nParameters\nanalyzer_name\n\nThe globally unique name of the analyzer being created.\n\nparent_analyzer_name\n\nThe name of the analyzer to inherit defaults from.\n\noverride_parameter\n\nThe name of a parameter of the parent analyzer which should be assigned a new value to.\n\ntokenizer_name\n\nThe name of a builtin tokenizer to be used.\n\ntokenizer_parameter\n\nA name of a parameter for a given tokenizer.\n\ntoken_filter_name\n\nThe name of a builtin token filter to be used.\n\ntoken_filter_parameter\n\nA name of a parameter for a given token filter.\n\nchar_filter_name\n\nThe name of a builtin char filter to be used.\n\nchar_filter_parameter\n\nA name of a parameter for a given char filter.\n\ncustom_name\n\nA custom unique name needed when defining custom tokenizers/token_filter/char_filter."
  },
  {
    "title": "COPY FROM — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/copy-from.html",
    "html": "5.6\nCOPY FROM\n\nYou can use the COPY FROM statement to copy data from a file into a table.\n\nSee Also\n\nData manipulation: Import and export\n\nSQL syntax: COPY TO\n\nTable of contents\n\nSynopsis\n\nDescription\n\nFile formats\n\nData type checks\n\nParameters\n\nURI globbing\n\nURI schemes\n\nClauses\n\nPARTITION\n\nWITH\n\nRETURN SUMMARY\n\nSynopsis\nCOPY table_identifier\n  [ ( column_ident [, ...] ) ]\n  [ PARTITION (partition_column = value [ , ... ]) ]\n  FROM uri [ WITH ( option = value [, ...] ) ] [ RETURN SUMMARY ]\n\nDescription\n\nA COPY FROM copies data from a URI to the specified table.\n\nThe nodes in the cluster will attempt to read the files available at the URI and import the data.\n\nHere’s an example:\n\ncr> COPY quotes FROM 'file:///tmp/import_data/quotes.json';\nCOPY OK, 3 rows affected (... sec)\n\n\nNote\n\nThe COPY statements use Overload Protection to ensure other queries can still perform. Please change these settings during large inserts if needed.\n\nFile formats\n\nCrateDB accepts both JSON and CSV inputs. The format is inferred from the file extension (.json or .csv respectively) if possible. The format can also be set as an option. If a format is not specified and the format cannot be inferred, the file will be processed as JSON.\n\nJSON files must contain a single JSON object per line and all files must be UTF-8 encoded. Also, any empty lines are skipped.\n\nExample JSON data:\n\n{\"id\": 1, \"quote\": \"Don't panic\"}\n{\"id\": 2, \"quote\": \"Ford, you're turning into a penguin. Stop it.\"}\n\n\nA CSV file may or may not contain a header. See CSV header option for further details.\n\nExample CSV data:\n\nid,quote\n1,\"Don't panic\"\n2,\"Ford, you're turning into a penguin. Stop it.\"\n\n\nExample CSV data with no header:\n\n1,\"Don't panic\"\n2,\"Ford, you're turning into a penguin. Stop it.\"\n\n\nSee also: Importing data.\n\nData type checks\n\nCrateDB checks if the columns’ data types match the types from the import file. It casts the types and will always import the data as in the source file. Furthermore CrateDB will check for all Column constraints.\n\nFor example a WKT string cannot be imported into a column of geo_shape or geo_point type, since there is no implicit cast to the GeoJSON format.\n\nNote\n\nIn case the COPY FROM statement fails, the log output on the node will provide an error message. Any data that has been imported until then has been written to the table and should be deleted before restarting the import.\n\nParameters\ntable_ident\n\nThe name (optionally schema-qualified) of an existing table where the data should be put.\n\ncolumn_ident\n\nUsed in an optional columns declaration, each column_ident is the name of a column in the table_ident table.\n\nThis currently only has an effect if using the CSV file format. See the header section for how it behaves.\n\nuri\n\nAn expression or array of expressions. Each expression must evaluate to a string literal that is a well-formed URI.\n\nURIs must use one of the supported URI schemes. CrateDB supports globbing for the file and s3 URI schemes.\n\nNote\n\nIf the URI scheme is missing, CrateDB assumes the value is a pathname and will prepend the file URI scheme (i.e., file://). So, for example, CrateDB will convert /tmp/file.json to file:///tmp/file.json.\n\nURI globbing\n\nWith file and s3 URI schemes, you can use pathname globbing (i.e., * wildcards) with the COPY FROM statement to construct URIs that can match multiple directories and files.\n\nSuppose you used file:///tmp/import_data/*/*.json as the URI. This URI would match all JSON files located in subdirectories of the /tmp/import_data directory.\n\nSo, for example, these files would match:\n\n/tmp/import_data/foo/1.json\n\n/tmp/import_data/bar/2.json\n\n/tmp/import_data/1/boz.json\n\nCaution\n\nA file named /tmp/import_data/foo/.json would also match the file:///tmp/import_data/*/*.json URI. The * wildcard matches any number of characters, including none.\n\nHowever, these files would not match:\n\n/tmp/import_data/1.json (two few subdirectories)\n\n/tmp/import_data/foo/bar/2.json (too many subdirectories)\n\n/tmp/import_data/1/boz.js (file extension mismatch)\n\nURI schemes\n\nCrateDB supports the following URI schemes:\n\nfile\n\ns3\n\nOther schemes\n\nfile\n\nYou can use the file:// scheme to specify an absolute path to one or more files accessible via the local filesystem of one or more CrateDB nodes.\n\nFor example:\n\nfile:///path/to/dir\n\n\nThe files must be accessible on at least one node and the system user running the crate process must have read access to every file specified. Additionally, only the crate superuser is allowed to use the file:// scheme.\n\nBy default, every node will attempt to import every file. If the file is accessible on multiple nodes, you can set the shared option to true in order to avoid importing duplicates.\n\nUse RETURN SUMMARY to get information about what actions were performed on each node.\n\nTip\n\nIf you are running CrateDB inside a container, the file must be inside the container. If you are using Docker, you may have to configure a Docker volume to accomplish this.\n\nTip\n\nIf you are using Microsoft Windows, you must include the drive letter in the file URI.\n\nFor example:\n\nfile://C:\\/tmp/import_data/quotes.json\n\n\nConsult the Windows documentation for more information.\n\ns3\n\nYou can use the s3:// scheme to access buckets on the Amazon Simple Storage Service (Amazon S3).\n\nFor example:\n\ns3://[<accesskey>:<secretkey>@][<host>:<port>/]<bucketname>/<path>\n\n\nS3 compatible storage providers can be specified by the optional pair of host and port, which defaults to Amazon S3 if not provided.\n\nHere is a more concrete example:\n\nCOPY t FROM 's3://accessKey:secretKey@s3.amazonaws.com:443/myBucket/key/a.json' with (protocol = 'https')\n\n\nIf no credentials are set the s3 client will operate in anonymous mode. See AWS Java Documentation.\n\nUsing the s3:// scheme automatically sets the shared to true.\n\nTip\n\nA secretkey provided by Amazon Web Services can contain characters such as ‘/’, ‘+’ or ‘=’. These characters must be URL encoded. For a detailed explanation read the official AWS documentation.\n\nTo escape a secret key, you can use a snippet like this:\n\nsh$ python -c \"from getpass import getpass; from urllib.parse import quote_plus; print(quote_plus(getpass('secret_key: ')))\"\n\n\nThis will prompt for the secret key and print the encoded variant.\n\nAdditionally, versions prior to 0.51.x use HTTP for connections to S3. Since 0.51.x these connections are using the HTTPS protocol. Please make sure you update your firewall rules to allow outgoing connections on port 443.\n\nOther schemes\n\nIn addition to the schemes above, CrateDB supports all protocols supported by the URL implementation of its JVM (typically http, https, ftp, and jar). Please refer to the documentation of the JVM vendor for an accurate list of supported protocols.\n\nNote\n\nThese schemes do not support wildcard expansion.\n\nClauses\n\nThe COPY FROM statement supports the following clauses:\n\nPARTITION\n\nWITH\n\nRETURN SUMMARY\n\nPARTITION\n\nIf the table is partitioned, the optional PARTITION clause can be used to import data into one partition exclusively.\n\n[ PARTITION ( partition_column = value [ , ... ] ) ]\n\npartition_column\n\nOne of the column names used for table partitioning\n\nvalue\n\nThe respective column value.\n\nAll partition columns (specified by the PARTITIONED BY clause) must be listed inside the parentheses along with their respective values using the partition_column = value syntax (separated by commas).\n\nBecause each partition corresponds to a unique set of partition column row values, this clause uniquely identifies a single partition for import.\n\nTip\n\nThe SHOW CREATE TABLE statement will show you the complete list of partition columns specified by the PARTITIONED BY clause.\n\nCaution\n\nPartitioned tables do not store the row values for the partition columns, hence every row will be imported into the specified partition regardless of partition column values.\n\nWITH\n\nYou can use the optional WITH clause to specify option values.\n\n[ WITH ( option = value [, ...] ) ]\n\n\nThe WITH clause supports the following options:\n\nbulk_size\n\nfail_fast\n\nwait_for_completion\n\nshared\n\nnode_filters\n\nnum_readers\n\ncompression\n\nprotocol\n\noverwrite_duplicates\n\nempty_string_as_null\n\ndelimiter\n\nformat\n\nheader\n\nskip\n\nbulk_size\n\nCrateDB will process the lines it reads from the path in bulks. This option specifies the size of one batch. The provided value must be greater than 0, the default value is 10000.\n\nfail_fast\n\nA boolean value indicating if the COPY FROM operation should abort early after an error. This is best effort and due to the distributed execution, it may continue processing some records before it aborts. Defaults to false.\n\nwait_for_completion\n\nA boolean value indicating if the COPY FROM should wait for the copy operation to complete. If set to false the request returns at once and the copy operation runs in the background. Defaults to true.\n\nshared\n\nThis option should be set to true if the URIs location is accessible by more than one CrateDB node to prevent them from importing the same file.\n\nThe default value depends on the scheme of each URI.\n\nIf an array of URIs is passed to COPY FROM this option will overwrite the default for all URIs.\n\nnode_filters\n\nA filter expression to select the nodes to run the read operation.\n\nIt’s an object in the form of:\n\n{\n    name = '<node_name_regex>',\n    id = '<node_id_regex>'\n}\n\n\nOnly one of the keys is required.\n\nThe name regular expression is applied on the name of all execution nodes, whereas the id regex is applied on the node id.\n\nIf both keys are set, both regular expressions have to match for a node to be included.\n\nIf the shared option is false, a strict node filter might exclude nodes with access to the data leading to a partial import.\n\nTo verify which nodes match the filter, run the statement with EXPLAIN.\n\nnum_readers\n\nThe number of nodes that will read the resources specified in the URI. Defaults to the number of nodes available in the cluster. If the option is set to a number greater than the number of available nodes it will still use each node only once to do the import. However, the value must be an integer greater than 0.\n\nIf shared is set to false this option has to be used with caution. It might exclude the wrong nodes, causing COPY FROM to read no files or only a subset of the files.\n\ncompression\n\nThe default value is null, set to gzip to read gzipped files.\n\nprotocol\n\nUsed for s3 scheme only. It is set to HTTPS by default.\n\noverwrite_duplicates\n\nDefault: false\n\nCOPY FROM by default won’t overwrite rows if a document with the same primary key already exists. Set to true to overwrite duplicate rows.\n\nempty_string_as_null\n\nIf set to true the empty_string_as_null option enables conversion of empty strings into NULL. The default value is false meaning that no action will be taken on empty strings during the COPY FROM execution.\n\nThe option is only supported when using the CSV format, otherwise, it will be ignored.\n\ndelimiter\n\nSpecifies a single one-byte character that separates columns within each line of the file. The default delimiter is ,.\n\nThe option is only supported when using the CSV format, otherwise, it will be ignored.\n\nformat\n\nThis option specifies the format of the input file. Available formats are csv or json. If a format is not specified and the format cannot be guessed from the file extension, the file will be processed as JSON.\n\nheader\n\nUsed to indicate if the first line of a CSV file contains a header with the column names. Defaults to true.\n\nIf set to false, the CSV must not contain column names in the first line and instead the columns declared in the statement are used. If no columns are declared in the statement, it will default to all columns present in the table in their CREATE TABLE declaration order.\n\nIf set to true the first line in the CSV file must contain the column names. You can use the optional column declaration in addition to import only a subset of the data.\n\nIf the statement contains no column declarations, all fields in the CSV are read and if it contains fields where there is no matching column in the table, the behavior depends on the column_policy table setting. If dynamic it implicitly adds new columns, if strict the operation will fail.\n\nAn example of using input file with no header\n\ncr> COPY quotes FROM 'file:///tmp/import_data/quotes.csv' with (format='csv', header=false);\nCOPY OK, 3 rows affected (... sec)\n\nskip\n\nDefault: 0\n\nSetting this option to n skips the first n rows while copying.\n\nNote\n\nCrateDB by default expects a header in CSV files. If you’re using the SKIP option to skip the header, you have to set header = false as well. See header.\n\nRETURN SUMMARY\n\nBy using the optional RETURN SUMMARY clause, a per-node result set will be returned containing information about possible failures and successfully inserted records.\n\n[ RETURN SUMMARY ]\n\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nnode\n\n\t\n\nInformation about the node that has processed the URI resource.\n\n\t\n\nOBJECT\n\n\n\n\nnode['id']\n\n\t\n\nThe id of the node.\n\n\t\n\nTEXT\n\n\n\n\nnode['name']\n\n\t\n\nThe name of the node.\n\n\t\n\nTEXT\n\n\n\n\nuri\n\n\t\n\nThe URI the node has processed.\n\n\t\n\nTEXT\n\n\n\n\nerror_count\n\n\t\n\nThe total number of records which failed. A NULL value indicates a general URI reading error, the error will be listed inside the errors column.\n\n\t\n\nBIGINT\n\n\n\n\nsuccess_count\n\n\t\n\nThe total number of records which were inserted. A NULL value indicates a general URI reading error, the error will be listed inside the errors column.\n\n\t\n\nBIGINT\n\n\n\n\nerrors\n\n\t\n\nContains detailed information about all errors. Limited to at most 25 error messages.\n\n\t\n\nOBJECT\n\n\n\n\nerrors[ERROR_MSG]\n\n\t\n\nContains information about a type of an error.\n\n\t\n\nOBJECT\n\n\n\n\nerrors[ERROR_MSG]['count']\n\n\t\n\nThe number records failed with this error.\n\n\t\n\nBIGINT\n\n\n\n\nerrors[ERROR_MSG]['line_numbers']\n\n\t\n\nThe line numbers of the source URI where the error occurred, limited to the first 50 errors, to avoid buffer pressure on clients.\n\n\t\n\nARRAY"
  },
  {
    "title": "COPY TO — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/copy-to.html",
    "html": "5.6\nCOPY TO\n\nYou can use the COPY TO statement to export table data to a file.\n\nSee Also\n\nData manipulation: Import and export\n\nSQL syntax: COPY FROM\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nClauses\n\nPARTITION\n\nWHERE\n\nTO\n\nURI schemes\n\nWITH\n\nSynopsis\nCOPY table_ident [ PARTITION ( partition_column = value [ , ... ] ) ]\n                 [ ( column [ , ...] ) ]\n                 [ WHERE condition ]\n                 TO DIRECTORY output_uri\n                 [ WITH ( copy_parameter [= value] [, ... ] ) ]\n\nDescription\n\nThe COPY TO command exports the contents of a table to one or more files into a given directory with unique filenames. Each node with at least one shard of the table will export its contents onto their local disk.\n\nThe created files are JSON formatted and contain one table row per line and, due to the distributed nature of CrateDB, will remain on the same nodes where the shards are.\n\nHere’s an example:\n\ncr> COPY quotes TO DIRECTORY '/tmp/' with (compression='gzip');\nCOPY OK, 3 rows affected ...\n\n\nNote\n\nCurrently only user tables can be exported. System tables like sys.nodes and blob tables don’t work with the COPY TO statement.\n\nThe COPY statements use Overload Protection to ensure other queries can still perform. Please change these settings during large inserts if needed.\n\nParameters\ntable_ident\n\nThe name (optionally schema-qualified) of the table to be exported.\n\ncolumn\n\n(optional) A list of column expressions that should be exported.\n\nNote\n\nWhen declaring columns, this changes the output to JSON list format, which is currently not supported by the COPY FROM statement.\n\nClauses\nPARTITION\n\nIf the table is partitioned, the optional PARTITION clause can be used to export data from a one partition exclusively.\n\n[ PARTITION ( partition_column = value [ , ... ] ) ]\n\npartition_column\n\nOne of the column names used for table partitioning.\n\nvalue\n\nThe respective column value.\n\nAll partition columns (specified by the PARTITIONED BY clause) must be listed inside the parentheses along with their respective values using the partition_column = value syntax (separated by commas).\n\nBecause each partition corresponds to a unique set of partition column row values, this clause uniquely identifies a single partition to export.\n\nTip\n\nThe SHOW CREATE TABLE statement will show you the complete list of partition columns specified by the PARTITIONED BY clause.\n\nCaution\n\nThe exported data doesn’t contain the partition columns or the corresponding values because they are not part of the partitioned tables.\n\nIf COPY TO is used on a partitioned table without the PARTITION clause, the partition columns and values will be included in the rows of the exported files. If a partition column is a generated column, it will not be included even if the PARTITION clause is missing.\n\nWHERE\n\nThe WHERE clauses use the same syntax as SELECT statements, allowing partial exports. (see WHERE clause for more information).\n\nTO\n\nThe TO clause allows you to specify an output location.\n\nTO DIRECTORY output_uri\n\nParameters\noutput_uri\n\nAn expression must evaluate to a string literal that is a well-formed URI. URIs must use one of the supported URI schemes.\n\nNote\n\nIf the URI scheme is missing, CrateDB assumes the value is a pathname and will prepend the file URI scheme (i.e., file://). So, for example, CrateDB will convert /tmp/file.json to file:///tmp/file.json.\n\nURI schemes\n\nCrateDB supports the following URI schemes:\n\nfile\n\ns3\n\nfile\n\nYou can use the file:// scheme to specify an absolute path to an output location on the local file system.\n\nFor example:\n\nfile:///path/to/dir\n\n\nTip\n\nIf you are running CrateDB inside a container, the location must be inside the container. If you are using Docker, you may have to configure a Docker volume to accomplish this.\n\nTip\n\nIf you are using Microsoft Windows, you must include the drive letter in the file URI.\n\nFor example:\n\nfile://C:\\/tmp/import_data/quotes.json\n\n\nConsult the Windows documentation for more information.\n\ns3\n\nYou can use the s3:// scheme to access buckets on the Amazon Simple Storage Service (Amazon S3).\n\nFor example:\n\ns3://[<accesskey>:<secretkey>@][<host>:<port>/]<bucketname>/<path>\n\n\nS3 compatible storage providers can be specified by the optional pair of host and port, which defaults to Amazon S3 if not provided.\n\nHere is a more concrete example:\n\nCOPY t TO DIRECTORY 's3://myAccessKey:mySecretKey@s3.amazonaws.com:80/myBucket/key1' with (protocol = 'http')\n\n\nIf no credentials are set the s3 client will operate in anonymous mode. See AWS Java Documentation.\n\nTip\n\nA secretkey provided by Amazon Web Services can contain characters such as ‘/’, ‘+’ or ‘=’. These characters must be URL encoded. For a detailed explanation read the official AWS documentation.\n\nTo escape a secret key, you can use a snippet like this:\n\nsh$ python -c \"from getpass import getpass; from urllib.parse import quote_plus; print(quote_plus(getpass('secret_key: ')))\"\n\n\nThis will prompt for the secret key and print the encoded variant.\n\nAdditionally, versions prior to 0.51.x use HTTP for connections to S3. Since 0.51.x these connections are using the HTTPS protocol. Please make sure you update your firewall rules to allow outgoing connections on port 443.\n\nWITH\n\nYou can use the optional WITH clause to specify copy parameter values.\n\n[ WITH ( copy_parameter [= value] [, ... ] ) ]\n\n\nThe WITH clause supports the following copy parameters:\n\ncompression\n\nprotocol\n\nformat\n\nwait_for_completion\n\ncompression\n\nDefine if and how the exported data should be compressed.\n\nBy default the output is not compressed.\n\nPossible values for the compression setting are:\n\ngzip\n\nUse gzip to compress the data output.\n\nprotocol\n\nUsed for s3 scheme only. It is set to HTTPS by default.\n\nformat\n\nOptional parameter to override default output behavior.\n\nPossible values for the format settings are:\n\njson_object\n\nEach row in the result set is serialized as JSON object and written to an output file where one line contains one object. This is the default behavior if no columns are defined. Use this format to import with COPY FROM.\n\njson_array\n\nEach row in the result set is serialized as JSON array, storing one array per line in an output file. This is the default behavior if columns are defined.\n\nwait_for_completion\n\nA boolean value indicating if the COPY TO should wait for the copy operation to complete. If set to false the request returns at once and the copy operation runs in the background. Defaults to true."
  },
  {
    "title": "COMMIT — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/commit.html",
    "html": "5.6\nCOMMIT\n\nCommit the current transaction\n\nTable of contents\n\nSynopsis\n\nParameters\n\nDescription\n\nSynopsis\nCOMMIT [ WORK | TRANSACTION ]\n\nParameters\n\nWORK TRANSACTION\n\nOptional keywords. They have no effect.\n\nDescription\n\nThe statement commits the current transaction.\n\nAs CrateDB does not support transactions, the only effect of this command is to close all existing cursors WITHOUT HOLD in the current session."
  },
  {
    "title": "CLOSE — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/close.html",
    "html": "5.6\nCLOSE\n\nClose a cursor\n\nSynopsis\n\nDescription\n\nSynopsis\nCLOSE { name | ALL }\n\nDescription\n\nCloses cursors created with DECLARE\n\nCLOSE ALL closes all cursors. CLOSE name closes the cursor identified by its name. CLOSE name on a cursor that does not exist results in an error."
  },
  {
    "title": "BEGIN — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/begin.html",
    "html": "5.6\nBEGIN\n\nStart a transaction block\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nSynopsis\nBEGIN [ WORK | TRANSACTION ] [ transaction_mode [ , ...] ]\n\n\nwhere transaction_mode is one of:\n\nISOLATION LEVEL isolation_level | (READ WRITE | READ ONLY) | [NOT] DEFERRABLE\n\n\nwhere isolation_level is one of:\n\n{ SERIALIZABLE | REPEATABLE READ | READ COMMITTED | READ UNCOMMITTED }\n\nDescription\n\nThe statement starts a transaction block.\n\nAs CrateDB does not support transactions, the only effect of this command is to start a scope in which cursors WITHOUT HOLD can be declared.\n\nNote\n\nCursors WITHOUT HOLD are closed automatically after an END or COMMIT command. There is no nesting and this happens regardless of how many times BEGIN has run.\n\nNote\n\nFor backwards compatibility reasons, the commas between successive transaction_modes can be omitted.\n\nParameters\nWORK | TRANSACTION\n\nOptional key words. They have no effect.\n\ntransaction_mode\n\nThe transactional mode parameter. It has no effect."
  },
  {
    "title": "ANALYZE — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/analyze.html",
    "html": "5.6\nANALYZE\nSynopsis\nANALYZE\n\nDescription\n\nThe ANALYZE command can be used to collect statistics about the contents of the tables in the CrateDB cluster.\n\nThe generated statistics can be viewed in the pg_catalog.pg_stats table.\n\nThe query optimizer uses some of those statistics to generate better execution plans.\n\nThe statistics are also periodically updated. How often can be configured with the stats.service.interval setting.\n\nI/O throughput during collection of statistics can be throttled with the stats.service.max_bytes_per_sec setting. Changes to this setting can be made and take effect while an analysis is in progress, thus it’s possible to adjust optimal value for a concrete setup by trying different values while ANALYZE is running."
  },
  {
    "title": "ALTER USER — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/alter-user.html",
    "html": "5.6\nALTER USER\n\nAlter an existing database user.\n\nTable of contents\n\nSynopsis\n\nSynopsis\nALTER USER | ROLE username\n  SET ( user_parameter = value [, ...] )\n\n\nFor details, see ALTER ROLE as the two statements are identical."
  },
  {
    "title": "Lexical structure — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/general/lexical-structure.html",
    "html": "5.6\nLexical structure\n\nAn SQL input consists of a sequence of commands each of which is a sequence of tokens, terminated by a semicolon (;).\n\nThe syntax of a command defines its set of valid tokens. A token can be a key word, an identifier, a quoted identifier, a literal (or constant), or a special character symbol.\n\nTable of contents\n\nString literal\n\nEscape strings\n\nString literals with C-Style escapes\n\nKey words and identifiers\n\nSpecial characters\n\nComments\n\nString literal\n\nString literals are defined as an arbitrary sequence of characters that are delimited with single quotes ' as defined in ANSI SQL, for example 'This is a string'.\n\nIn addition, CrateDB supports dollar quoted strings to help avoid escaping single quotes within single quoted strings. For example, 'I''m a string' can be re-written as $<tag>$I'm a string$<tag>$, where the matching pair of <tag> can be zero or more characters in length.\n\ncr> select 'I''m a string' = $tag1$I'm a string$tag1$;\n+------+\n| true |\n+------+\n| TRUE |\n+------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nNested dollar quoted strings are currently not supported.\n\nEscape strings\n\nThe escape character in CrateDB is the single-quote '. A character gets escaped when adding a single-quote before it. For example a single quote character within a string literal can be included by writing two adjacent single quotes, e.g., 'Jack''s car'.\n\nNote\n\nTwo adjacent single quotes are not equivalent to the double-quote character \".\n\nString literals with C-Style escapes\n\nIn addition to the escaped character ', CrateDB supports C-Style escaped string sequences. Such a sequence is constructed by prefixing the string literal with the letter E or e, for example, e'hello\\nWorld'. The following escaped sequences are supported:\n\nEscape Sequence\n\n\t\n\nInterpretation\n\n\n\n\n\\b\n\n\t\n\nbackspace\n\n\n\n\n\\f\n\n\t\n\nform feed\n\n\n\n\n\\n\n\n\t\n\nnewline\n\n\n\n\n\\r\n\n\t\n\ncarriage return\n\n\n\n\n\\t\n\n\t\n\ntab\n\n\n\n\n\\o, \\oo, \\ooo (o = [0-7])\n\n\t\n\noctal byte value\n\n\n\n\n\\xh, xhh (h = [0-9,A-F,a-f])\n\n\t\n\nhexadecimal byte value\n\n\n\n\n\\uxxxx, \\Uxxxxxxxx (x = [0-9,A-F,a-f])\n\n\t\n\n16 or 32-bit hexadecimal Unicode character value\n\nFor instance, the escape string literal e'\\u0061\\x61\\141' is equivalent to the 'aaa' string literal.\n\ncr> select e'\\u0061\\x61\\141' as col1;\n+------+\n| col1 |\n+------+\n| aaa  |\n+------+\nSELECT 1 row in set (... sec)\n\n\nAny other character following a backslash is taken literally. Thus, to include a backslash character \\, two adjacent backslashes need to be used (i.e. \\\\).\n\ncr> select e'aa\\\\nbb' as col1;\n+--------+\n| col1   |\n+--------+\n| aa\\nbb |\n+--------+\nSELECT 1 row in set (... sec)\n\n\nFinally, a single quote can be included in an escape string literal by also using the escape backslash character: \\', in addition to the single-quote described in the string literals section.\n\ncr> select e'aa\\'bb' as col1;\n+-------+\n| col1  |\n+-------+\n| aa'bb |\n+-------+\nSELECT 1 row in set (... sec)\n\nKey words and identifiers\n\nThe table below lists all reserved key words in CrateDB. These need to be quoted if used as identifiers:\n\ncr> SELECT word FROM pg_catalog.pg_get_keywords() WHERE catcode = 'R' ORDER BY 1;\n+-------------------+\n| word              |\n+-------------------+\n| add               |\n| all               |\n| alter             |\n| and               |\n| any               |\n| array             |\n| as                |\n| asc               |\n| between           |\n| by                |\n| called            |\n| case              |\n| cast              |\n| column            |\n| constraint        |\n| costs             |\n| create            |\n| cross             |\n| current_date      |\n| current_schema    |\n| current_time      |\n| current_timestamp |\n| current_user      |\n| default           |\n| delete            |\n| deny              |\n| desc              |\n| describe          |\n| directory         |\n| distinct          |\n| drop              |\n| else              |\n| end               |\n| escape            |\n| except            |\n| exists            |\n| extract           |\n| false             |\n| first             |\n| for               |\n| from              |\n| full              |\n| function          |\n| grant             |\n| group             |\n| having            |\n| if                |\n| in                |\n| index             |\n| inner             |\n| input             |\n| insert            |\n| intersect         |\n| into              |\n| is                |\n| join              |\n| last              |\n| left              |\n| like              |\n| limit             |\n| match             |\n| natural           |\n| not               |\n| null              |\n| nulls             |\n| object            |\n| offset            |\n| on                |\n| or                |\n| order             |\n| outer             |\n| persistent        |\n| recursive         |\n| reset             |\n| returns           |\n| revoke            |\n| right             |\n| select            |\n| session_user      |\n| set               |\n| some              |\n| stratify          |\n| table             |\n| then              |\n| transient         |\n| true              |\n| try_cast          |\n| unbounded         |\n| union             |\n| update            |\n| user              |\n| using             |\n| when              |\n| where             |\n| with              |\n+-------------------+\nSELECT 95 rows in set (... sec)\n\n\nTokens such as my_table, id, name, or data in the example below are identifiers, which identify names of tables, columns, and other database objects.\n\nExample:\n\nCREATE TABLE my_table (\n  id INTEGER,\n  name STRING,\n  data OBJECT\n) WITH (number_of_replicas = 0);\n\n\nNote\n\nKey words and unquoted identifiers are case insensitive while quoted identifiers are case sensitive.\n\nThis means that:\n\nselect foo from t;\n\n\nis equivalent to:\n\nselect Foo from t;\n\n\nor:\n\nselect FOO from t;\n\n\nTo query a table named Foo:\n\nselect \"Foo\" from t;\n\n\nA widely used convention is to write key words in uppercase and identifiers in lowercase, such as\n\nALTER TABLE foo ADD COLUMN new_column INTEGER;\n\nINSERT INTO foo (id, name) VALUES (1, 'bar');\n\n\nQuoted identifiers can contain an arbitrary sequence of characters enclosed by double quotes (\"). Quoted identifiers are never keywords, so you can use \"update\" as a table or column name.\n\nSpecial characters\n\nSome non-alphanumeric characters do have a special meaning. For their usage please refer to the sections where the respective syntax elements are described.\n\nSemicolon\n\nThe semicolon (;) terminates an SQL statement. It cannot appear anywhere else within the command, except within a string or quoted identifier.\n\nComma\n\nThe comma (,) is used in various syntactical elements to separate elements of a list.\n\nBrackets\n\nSquare brackets ([]) are used to select elements of arrays and objects, e.g. arr[1] or obj['key'].\n\nAsterisk\n\nThe asterisk (*) is used in some contexts to denote all columns of a table. As an argument in global aggregate functions it has the meaning of any field, e.g. COUNT(*).\n\nPeriod\n\nThe period (.) is used for numeric values and to separate schema and table names, e.g. blob.my_blob_table.\n\nComments\n\nAn SQL statement can contain comments. Single line comments start with a double dash (--) and end at the end of that line. Multi line comments start with /* and end with */.\n\nExample:\n\n/*\n * Retrieve information about all tables in the 'doc' schema.\n */\nSELECT *\n  FROM information_schema.tables\n  WHERE table_schema = 'doc'; -- query information schema for doc tables\n"
  },
  {
    "title": "ALTER ROLE — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/alter-role.html",
    "html": "5.6\nALTER ROLE\n\nAlter an existing database user or role.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nArguments\n\nname\n\nSET\n\nSynopsis\nALTER ROLE name\n  SET ( parameter = value [, ...] )\n\nDescription\n\nALTER ROLE applies a change to an existing database user or role. Only existing superusers or the user itself have the privilege to alter an existing database user.\n\nArguments\nname\n\nThe name by which the user or role is identified inside the database.\n\nSET\n\nChanges a user parameter to a different value. The following parameter are supported to alter an existing user account:\n\npassword\n\nThe password as cleartext entered as string literal.\n\nNULL removes the password from the user.\n\nCaution\n\nPasswords cannot be set for the crate superuser.\n\nFor security reasons it is recommended to authenticate as crate using a client certificate.\n\nNote\n\nPasswords can be set only to existing database users, but not to roles."
  },
  {
    "title": "ALTER TABLE — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/alter-table.html",
    "html": "5.6\nALTER TABLE\n\nAlter an existing table.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nClauses\n\nPARTITION\n\nArguments\n\nSET/RESET\n\nADD COLUMN\n\nDROP COLUMN\n\nRENAME COLUMN\n\nOPEN/CLOSE\n\nRENAME TO\n\nREROUTE\n\nDROP CONSTRAINT\n\nSynopsis\nALTER [ BLOB ] TABLE { ONLY table_ident\n                       | table_ident [ PARTITION (partition_column = value [ , ... ]) ] }\n  { SET ( parameter = value [ , ... ] )\n    | RESET ( parameter [ , ... ] )\n    | { ADD [ COLUMN ] column_name data_type [ column_constraint [ ... ] ] } [, ... ]\n    | { DROP [ COLUMN ] [ IF EXISTS ] column_name } [, ... ]\n    | { RENAME [ COLUMN ] column_name TO new_name } [, ... ]\n    | OPEN\n    | CLOSE\n    | RENAME TO table_ident\n    | REROUTE reroute_option\n    | DROP CONSTRAINT constraint_name\n  }\n\n\nwhere column_constraint is:\n\n{ PRIMARY KEY |\n  NULL |\n  NOT NULL |\n  INDEX { OFF | USING { PLAIN |\n                        FULLTEXT [ WITH ( analyzer = analyzer_name ) ]  } |\n  [ CONSTRAINT constraint_name ] CHECK (boolean_expression)\n}\n\nDescription\n\nALTER TABLE can be used to modify an existing table definition. It provides options to add columns, modify constraints, enabling or disabling table parameters and allows to execute a shard reroute allocation.\n\nUse the BLOB keyword in order to alter a blob table (see Blobs). Blob tables cannot have custom columns which means that the ADD COLUMN keyword won’t work.\n\nWhile altering a partitioned table, using ONLY will apply changes for the table only and not for any possible existing partitions. So these changes will only be applied to new partitions. The ONLY keyword cannot be used together with a PARTITION clause.\n\nSee CREATE TABLE WITH for a list of available parameters.\n\ntable_ident\n\nThe name (optionally schema-qualified) of the table to alter.\n\nClauses\nPARTITION\n\nIf the table is partitioned, the optional PARTITION clause can be used to alter one partition exclusively.\n\n[ PARTITION ( partition_column = value [ , ... ] ) ]\n\npartition_column\n\nOne of the column names used for table partitioning.\n\nvalue\n\nThe respective column value.\n\nAll partition columns (specified by the PARTITIONED BY clause) must be listed inside the parentheses along with their respective values using the partition_column = value syntax (separated by commas).\n\nBecause each partition corresponds to a unique set of partition column row values, this clause uniquely identifies a single partition to alter.\n\nTip\n\nThe SHOW CREATE TABLE statement will show you the complete list of partition columns specified by the PARTITIONED BY clause.\n\nNote\n\nBLOB tables cannot be partitioned and hence this clause cannot be used.\n\nSee Also\n\nPartitioned tables: Alter\n\nArguments\nSET/RESET\n\nCan be used to change a table parameter to a different value. Using RESET will reset the parameter to its default value.\n\nparameter\n\nThe name of the parameter that is set to a new value or its default.\n\nThe supported parameters are listed in the CREATE TABLE WITH CLAUSE documentation. In addition to those, for dynamically changing the number of allocated shards, the parameter number_of_shards can be used. For more info on that, see Changing the number of shards.\n\nADD COLUMN\n\nCan be used to add an additional column to a table. While columns can be added at any time, adding a new generated column is only possible if the table is empty. In addition, adding a base column with Default clause is not supported. It is possible to define a CHECK constraint with the restriction that only the column being added may be used in the boolean expression.\n\ndata_type\n\nData type of the column which should be added.\n\ncolumn_name\n\nName of the column which should be added. This can be a sub-column on an existing OBJECT.\n\nIt’s possible to add multiple columns at once.\n\nDROP COLUMN\n\nCan be used to drop a column from a table.\n\ncolumn_name\n\nName of the column which should be dropped. This can be a sub-column of an OBJECT.\n\nIt’s possible to drop multiple columns at once.\n\nNote\n\nIt’s not allowed to drop a column:\n\nwhich is a system column\n\nwhich is part of a PRIMARY KEY\n\nused in CLUSTERED BY column\n\nused in PARTITIONED BY\n\nis a named index column\n\nused in an named index\n\nis referenced in a generated column\n\nis referenced in a table level constraint with other columns\n\nNote\n\nIt’s not allowed to drop all columns of a table.\n\nNote\n\nDropping columns of a table created before version 5.5 is not supported.\n\nRENAME COLUMN\n\nRenames a column of a table\n\ncolumn_name\n\nName of the column to rename. Supports subscript expressions to rename sub-columns of OBJECT columns.\n\nnew_name\n\nThe new name of the column.\n\nNote\n\nRenaming columns of a table created before version 5.5 is not supported.\n\nOPEN/CLOSE\n\nCan be used to open or close the table.\n\nClosing a table means that all operations, except ALTER TABLE ..., will fail. Operations that fail will not return an error, but they will have no effect. Operations on tables containing closed partitions won’t fail, but those operations will exclude all closed partitions.\n\nRENAME TO\n\nCan be used to rename a table or view, while maintaining its schema and data. If renaming a table, the shards of it become temporarily unavailable.\n\nREROUTE\n\nThe REROUTE command provides various options to manually control the allocation of shards. It allows the enforcement of explicit allocations, cancellations and the moving of shards between nodes in a cluster. See Reroute shards to get the convenient use-cases.\n\nThe row count defines if the reroute or allocation process of a shard was acknowledged or rejected.\n\nNote\n\nPartitioned tables require a PARTITION clause in order to specify a unique shard_id.\n\n[ REROUTE reroute_option]\n\n\nwhere reroute_option is:\n\n{ MOVE SHARD shard_id FROM node TO node\n  | ALLOCATE REPLICA SHARD shard_id ON node\n  | PROMOTE REPLICA SHARD shard_id ON node [ WITH (accept_data_loss = { TRUE | FALSE }) ]\n  | CANCEL SHARD shard_id ON node [ WITH (allow_primary = {TRUE|FALSE}) ]\n}\n\nshard_id\n\nThe shard ID. Ranges from 0 up to the specified number of Shards shards of a table.\n\nnode\n\nThe ID or name of a node within the cluster.\n\nSee Nodes how to gain the unique ID.\n\nREROUTE supports the following options to start/stop shard allocation:\n\nMOVE\n\nA started shard gets moved from one node to another. It requests a table_ident and a shard_id to identify the shard that receives the new allocation. Specify FROM node for the node to move the shard from and TO node to move the shard to.\n\nALLOCATE REPLICA\n\nAllows to force allocation of an unassigned replica shard on a specific node.\n\nPROMOTE REPLICA Force promote a stale replica shard to a primary. In case\n\na node holding a primary copy of a shard had a failure and the replica shards are out of sync, the system won’t promote the replica to primary automatically, as it would result in a silent data loss.\n\nIdeally the node holding the primary copy of the shard would be brought back into the cluster, but if that is not possible due to a permanent system failure, it is possible to accept the potential data loss and force promote a stale replica using this command.\n\nThe parameter accept_data_loss needs to be set to true in order for this command to work. If it is not provided or set to false, the command will error out.\n\nCANCEL\n\nThis cancels the allocation or recovery of a shard_id of a table_ident on a given node. The allow_primary flag indicates if it is allowed to cancel the allocation of a primary shard.\n\nDROP CONSTRAINT\n\nRemoves a CHECK constraint from a table.\n\nALTER TABLE table_ident DROP CONSTRAINT check_name\n\ntable_ident\n\nThe name (optionally schema-qualified) of the table.\n\ncheck_name\n\nThe name of the check constraint to be removed.\n\nWarning\n\nA removed CHECK constraints cannot be re-added to a table once dropped."
  },
  {
    "title": "ALTER PUBLICATION — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/alter-publication.html",
    "html": "5.6\nALTER PUBLICATION\n\nYou can use the ALTER PUBLICATION statement to update the list of published tables on the current cluster.\n\nSee Also\n\nCREATE PUBLICATION DROP PUBLICATION\n\nTable of contents\n\nSynopsis\n\nDescription\n\nParameters\n\nSynopsis\nALTER PUBLICATION name ADD TABLE table_name [, ...]\nALTER PUBLICATION name SET TABLE table_name [, ...]\nALTER PUBLICATION name DROP TABLE table_name [, ...]\n\nDescription\n\nUpdate the list of published tables according to the command. If a table gets deleted from the publication and it has existing subscriptions, the replication of the table stops for all subscribers. Already replicated data remains on the subscribed clusters, therefore subscribers can not re-subscribe again to the tables removed from the publication.\n\nNote\n\nReplicated tables on a subscriber cluster will turn into regular writable tables after excluding them from a publication on a publishing cluster.\n\nParameters\nname\n\nThe name of the publication to be updated.\n\nADD TABLE\n\nAdd one or more tables to the list of existing publications.\n\nDROP TABLE\n\nRemove one or more tables from the list of existing publications.\n\nSET TABLE\n\nReplace the list of existing publications with the new one."
  },
  {
    "title": "SQL Syntax — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/sql/index.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n3.3\nSQL Syntax\n\nCrateDB uses SQL to query documents.\n\nThis section of the documentation provides a full SQL syntax reference.\n\nNote\n\nIf you’re just getting started with a particular part of CrateDB, we recommend you consult the appropriate top-level section of the documentation. The SQL syntax reference assumes a basic familiarity with the relevant parts of CrateDB.\n\nGeneral SQL\nConstraints\nValue Expressions\nLexical Structure\nSQL Statements\nALTER CLUSTER\nALTER TABLE\nALTER USER\nBEGIN\nCOMMIT\nCOPY FROM\nCOPY TO\nCREATE ANALYZER\nCREATE BLOB TABLE\nCREATE FUNCTION\nCREATE INGEST RULE\nCREATE REPOSITORY\nCREATE SNAPSHOT\nCREATE TABLE\nCREATE USER\nCREATE VIEW\nDEALLOCATE\nDELETE\nDENY\nDROP ANALYZER\nDROP FUNCTION\nDROP INGEST RULE\nDROP REPOSITORY\nDROP SNAPSHOT\nDROP TABLE\nDROP USER\nDROP VIEW\nEXPLAIN\nGRANT\nINSERT\nKILL\nOPTIMIZE\nREFRESH\nRESTORE SNAPSHOT\nREVOKE\nSELECT\nSET and RESET\nSET TRANSACTION\nSET LICENSE\nSHOW COLUMNS\nSHOW CREATE TABLE\nSHOW SCHEMAS\nSHOW TABLES\nSHOW (session settings)\nUPDATE"
  },
  {
    "title": "Value expressions — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/general/value-expressions.html",
    "html": "5.6\nValue expressions\n\nA value expression is a combination of one or more values, operators, and functions that evaluate to a single value.\n\nTable of contents\n\nLiteral value\n\nInteger literal value\n\nColumn reference\n\nParameter reference\n\nOperator invocation\n\nSubscripts\n\nArray subscript\n\nObject subscript\n\nRecord subscript\n\nFunction call\n\nType cast\n\nObject constructor\n\nArray constructor\n\nScalar subquery\n\nLiteral value\n\nA literal is a notation to represent a value within a statement.\n\nDifferent types have different notations. The simplest forms are:\n\nboolean literals: true or false\n\nstring literals: 'this is a string literal'\n\nnumeric literals: 42, 42.0 or with underscores 1_000_000\n\ninterval literals: INTERVAL '1' SECOND\n\nSee Also\n\nLexical structure\n\nData types\n\nInteger literal value\n\nInteger literals are a subcategory of literal values and can be on of the: numeric literals, null, a parameter reference, or a CAST/TRY CAST expression, for which the resulting datatype can be converted to an integer, e.g.:\n\n123\nnull\n'10'::int\nCAST(? AS long)\nTRY_CAST(? AS short)\n\nColumn reference\n\nA column reference is the name of a column. It’s represented using an identifier. An identifier is an unquoted or double quoted string.\n\nunquoted: columnname\n\nquoted: \"columnName\"\n\nIt’s also possible to include the name of a table with or without schema and catalog name, or an alias in order to unambiguously identify a column of a specific relation if a statement contains multiple aliases or table definitions:\n\ncrate.myschema.mytable.columnname\nmyschema.mytable.columname\n\n\nor:\n\ntab0.columnname\n\n\nSee Also\n\nLexical structure\n\nNote\n\nAs CrateDB doesn’t support multiple catalogs, only multiple schemas, the only valid catalog name is crate.\n\nParameter reference\n\nA parameter reference is a placeholder for a value.\n\nCrateDB clients usually have some kind of API to provide those values.\n\nParameter references can either be unnumbered or numbered:\n\nQuestion mark as an unnumbered placeholder: select * from t where x = ?\n\n$n as numbered placeholder: select * from t where x = $1 or x = $2\n\nOperator invocation\n\nAn operator can be invoked as a value expression in one of two ways: binary or unary.\n\nThe syntax of a binary operator:\n\nexpression operator expression\n\n\nThe syntax of a unary operator:\n\noperator expression\n\nSubscripts\n\nA subscript expression is an expression which contains a subscript operator ([ ]). It can be used to access a sub value of a composite type value.\n\nArray subscript\n\nThe subscript operator can be used on array expressions to retrieve a single element of an array:\n\narray_expression[ array_index ]\n\n\narray_index is a 1 based integer specifying the position of the element in the array which should be retrieved.\n\nSee Also\n\nArrays within objects\n\nObject subscript\n\nOn object expressions the subscript operator can be used to access an inner element of the object:\n\nobj_expression['key']\n\n\nThe key must be a string literal which is the name of the element which should be retrieved.\n\nSee Also\n\nObjects\n\nRecord subscript\n\nRecord subscript retrieves the value of a field within a record or object. This is similar to object subscripts.\n\nSynopsis:\n\n(record_expression).fieldName\n\n\nExample:\n\ncr> SELECT (information_schema._pg_expandarray(ARRAY['a', 'b'])).n AS n\n+---+\n| n |\n+---+\n| 1 |\n| 2 |\n+---+\nSELECT 2 rows in set (... sec)\n\n\nexpression is an expression of type record or object and key is an identifier that must refer to a field of the record.\n\nFunction call\n\nA function can be invoked with a function call (a process better known as calling the function). The corresponding syntax is the function name optionally followed by zero or more arguments (in the form of value expressions) enclosed by parentheses:\n\nfunction_name[([expression [, expression ... ]])]\n\nType cast\n\nA type cast specifies the conversion from one type to another. The syntax is:\n\nCAST(expression as type)\n\n\nAnother variant to do type casts is try_cast. Instead of raising an error this returns null if a value cannot be converted to the given type:\n\nTRY_CAST(expression as type)\n\n\nSee Also\n\nData types\n\nObject constructor\n\nA object constructor is an expression which builds an object using its arguments.\n\nIt consists of one ore more ident = expression, separated by commas and enclosed in curly brackets:\n\n{ elementNameIdent = valueExpression [, elementNameIdent = valueExpression ...] }\n\n\nSee Also\n\nObject literals\n\nArray constructor\n\nA array constructor is an expression which builds an array. It consists of one or more expressions separated by commas, enclosed in square brackets and optionally prefixed with ARRAY:\n\n[ ARRAY ] '[' expression [, expression ... ] ']'\n\n\nSee Also\n\nArray literals\n\nAnother way to construct an array is by using an ARRAY(subquery) expression as part of the SELECT list of a SELECT statement:\n\nARRAY '(' subquery ')'\n\n\nExample:\n\ncr> select array(select height from sys.summits order by height desc limit 5)\n... as top5_mountains_array;\n+--------------------------------+\n| top5_mountains_array           |\n+--------------------------------+\n| [4808, 4634, 4545, 4527, 4506] |\n+--------------------------------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nArray constructor only supports subqueries returning a single column.\n\nScalar subquery\n\nA scalar subquery (also known as a subquery expression) is a subquery that returns a single value (i.e., one row with one column).\n\nIf zero rows are returned, it will be treated as null value. In the case that more than one row (or more than one column) is returned, CrateDB will treat it as an error.\n\nScalar subqueries can access columns of its immediate parent if addressed via a table alias. Such a subquery is known as correlated subquery.\n\ncr> SELECT (SELECT t.mountain) as m FROM sys.summits t ORDER BY 1 ASC LIMIT 2;\n+--------------+\n| m            |\n+--------------+\n| Acherkogel   |\n| Ackerlspitze |\n+--------------+\nSELECT 2 rows in set (... sec)\n\n\nNote\n\nScalar subqueries are restricted to SELECT, DELETE and UPDATE statements and cannot be used in other statements.\n\nNote\n\nCorrelated subqueries are executed via a “Correlated Join”. A correlated join executes the sub-query for each row in the input relation. If the result set of the outer relation is large this can be slow.\n\nNote\n\nCorrelated subqueries are currently limited to the select list and where clause of a query."
  },
  {
    "title": "Constraints — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/general/constraints.html",
    "html": "5.6\nConstraints\n\nTable of contents\n\nTable constraints\n\nPRIMARY KEY\n\nINDEX\n\nCHECK\n\nColumn constraints\n\nNULL\n\nNOT NULL\n\nTable constraints\n\nTable constraints are constraints that are applied to the table as a whole.\n\nPRIMARY KEY\n\nThe PRIMARY KEY constraint specifies that a column or columns of a table can contain only unique (non-duplicate), non-null values.\n\nUsing columns of type object, geo_point, geo_shape or array as PRIMARY KEY is not supported.\n\nTo use a whole object as PRIMARY KEY each column within the object can be declared as PRIMARY KEY instead.\n\nAdding a PRIMARY KEY column is only possible if the table is empty.\n\nSyntax:\n\n[CONSTRAINT <name>] PRIMARY KEY [ column_name [, ... ] ]\n\n\nFor example, a table with a named PRIMARY KEY constraint can be created with:\n\ncr> CREATE TABLE person (\n...     firstname TEXT,\n...     lastname TEXT,\n...     CONSTRAINT c PRIMARY KEY (firstname, lastname)\n... );\nCREATE OK, 1 row affected  (... sec)\n\n\nThe named PRIMARY KEY constraints can be inlined:\n\ncr> CREATE TABLE person2 (\n...     firstname TEXT CONSTRAINT c PRIMARY KEY,\n...     lastname TEXT CONSTRAINT c PRIMARY KEY\n... );\nCREATE OK, 1 row affected  (... sec)\n\n\nIf a new column is required to be added as a PRIMARY KEY column:\n\ncr> ALTER TABLE person2 ADD COLUMN middleName text PRIMARY KEY;\nALTER OK, -1 rows affected  (... sec)\n\n\nThe PRIMARY KEY constraint can also be unnamed, e.g.:\n\ncr> CREATE TABLE person3 (\n...     firstname TEXT PRIMARY KEY,\n...     lastname TEXT PRIMARY KEY\n... );\nCREATE OK, 1 row affected  (... sec)\n\n\nIf CONSTRAINT <name> is omitted, CrateDB generates a unique name automatically. This name is visible in table_constraints.\n\nWarning\n\nThe verification if the table is empty and the schema update isn’t atomic. That means that it could be possible to add a primary key column to a table that isn’t empty.\n\nIf that is the case queries that contain the primary key columns in the WHERE clause will not behave as expected.\n\nINDEX\n\nThe INDEX constraint specifies a specific index method on one or more columns.\n\nIt is possible to define more than one index per table, whether as a column constraint or a table constraint.\n\nSee Also\n\nFulltext indices\n\nCHECK\n\nThe CHECK constraint specifies that the values of certain columns must satisfy a boolean expression on INSERT and UPDATE.\n\nSyntax:\n\n[CONSTRAINT <check_name>] CHECK (boolean_expression)\n\n\nIf CONSTRAINT <check_name> is omitted, CrateDB generates a unique name automatically. This name is visible in table_constraints. This name can be used with DROP CONSTRAINT to remove the constraint.\n\nThe CONSTRAINT definition can either be inlined with a column, like this:\n\ncr> CREATE TABLE metrics1 (\n...     weight REAL CONSTRAINT weight_is_positive CHECK (weight >= 0)\n... );\nCREATE OK, 1 row affected  (... sec)\n\n\nOr, also inlined, but without explicit name:\n\ncr> CREATE TABLE metrics2 (\n...     weight REAL CHECK (weight >= 0)\n... );\nCREATE OK, 1 row affected  (... sec)\n\n\nOr, on a table level with explicit name:\n\ncr> CREATE TABLE metrics3 (\n...     weight REAL,\n...     CONSTRAINT weight_is_positive CHECK (weight >= 0)\n... );\nCREATE OK, 1 row affected  (... sec)\n\n\nOr without name:\n\ncr> CREATE TABLE metrics4 (\n...     weight REAL,\n...     CHECK (weight >= 0)\n... );\nCREATE OK, 1 row affected  (... sec)\n\n\nYou can reference multiple columns using table constraints:\n\ncr> CREATE TABLE metrics5 (\n...     weight REAL,\n...     qty INTEGER,\n...     CHECK (weight * qty != 1918)\n... );\nCREATE OK, 1 row affected  (... sec)\n\n\nWarning\n\nThe CHECK constraint conditions must be deterministic, always yielding the same result for the same input.\n\nA way to break this is to reference a user-defined function in a CHECK expression, and then change the behavior of that function. Some existing rows in the table could now violate the CHECK constraint. That would cause a subsequent database dump and reload to fail.\n\nNote\n\nTo add a CHECK constraint to a sub-column of an object column you must address the sub-column by it’s full path:\n\ncr> CREATE TABLE metrics6 (properties OBJECT AS (weight INTEGER CHECK (properties['weight'] >= 0)))\nCREATE OK, 1 row affected (... sec)\n\nColumn constraints\n\nColumn constraints are constraints that are applied on each column of the table separately.\n\nThe supported column constraints are:\n\nNOT NULL\n\nPRIMARY KEY\n\nCHECK\n\nNULL\n\nThe NULL constraint specifies that a column of a table can also contain null values.\n\nThe columns that are part of the primary key of a table cannot be declared as NULL.\n\nA column cannot be declared both as NULL and NOT NULL.\n\nNote\n\nNULL constraint is not shown in SHOW CREATE TABLE, as is the default for every column.\n\nNOT NULL\n\nThe NOT NULL constraint specifies that a column of a table can contain only non-null values.\n\nThe columns that are part of the primary key of a table are NOT NULL by default."
  },
  {
    "title": "SQL syntax — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/sql/index.html",
    "html": "4.8\nSQL syntax\n\nYou can use Structured Query Language (SQL) to query your data.\n\nThis section of the documentation provides a complete SQL syntax reference for CrateDB.\n\nNote\n\nFor introductions to CrateDB functionality, we recommend you consult the appropriate top-level section of the documentation. The SQL syntax reference assumes a basic familiarity with the relevant parts of CrateDB.\n\nSee Also\n\nGeneral use: Data definition\n\nGeneral use: Data manipulation\n\nGeneral use: Querying\n\nGeneral use: Built-in functions and operators\n\nGeneral SQL\nConstraints\nValue expressions\nLexical structure\nSQL Statements\nALTER CLUSTER\nALTER PUBLICATION\nALTER TABLE\nALTER USER\nANALYZE\nBEGIN\nSTART TRANSACTION\nCOMMIT\nCOPY FROM\nCOPY TO\nCREATE ANALYZER\nCREATE BLOB TABLE\nCREATE FUNCTION\nCREATE PUBLICATION\nCREATE REPOSITORY\nCREATE SNAPSHOT\nCREATE SUBSCRIPTION\nCREATE TABLE\nCREATE TABLE AS\nCREATE USER\nCREATE VIEW\nDEALLOCATE\nDELETE\nDENY\nDISCARD\nDROP ANALYZER\nDROP FUNCTION\nDROP PUBLICATION\nDROP REPOSITORY\nDROP SNAPSHOT\nDROP SUBSCRIPTION\nDROP TABLE\nDROP USER\nDROP VIEW\nEND\nEXPLAIN\nGRANT\nINSERT\nKILL\nOPTIMIZE\nREFRESH\nRESTORE SNAPSHOT\nREVOKE\nSELECT\nSET and RESET\nSET LICENSE\nSET AND RESET SESSION AUTHORIZATION\nSET TRANSACTION\nSHOW COLUMNS\nSHOW CREATE TABLE\nSHOW SCHEMAS\nSHOW TABLES\nSHOW (session settings)\nUPDATE\nVALUES"
  },
  {
    "title": "SQL syntax — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/sql/index.html",
    "html": "5.5\nSQL syntax\n\nYou can use Structured Query Language (SQL) to query your data.\n\nThis section of the documentation provides a complete SQL syntax reference for CrateDB.\n\nNote\n\nFor introductions to CrateDB functionality, we recommend you consult the appropriate top-level section of the documentation. The SQL syntax reference assumes a basic familiarity with the relevant parts of CrateDB.\n\nSee Also\n\nGeneral use: Data definition\n\nGeneral use: Data manipulation\n\nGeneral use: Querying\n\nGeneral use: Built-in functions and operators\n\nGeneral SQL\nConstraints\nValue expressions\nLexical structure\nSQL Statements\nALTER CLUSTER\nALTER PUBLICATION\nALTER TABLE\nALTER USER\nANALYZE\nBEGIN\nCLOSE\nCOMMIT\nCOPY FROM\nCOPY TO\nCREATE ANALYZER\nCREATE BLOB TABLE\nCREATE FUNCTION\nCREATE PUBLICATION\nCREATE REPOSITORY\nCREATE SNAPSHOT\nCREATE SUBSCRIPTION\nCREATE TABLE\nCREATE TABLE AS\nCREATE USER\nCREATE VIEW\nDEALLOCATE\nDECLARE\nDELETE\nDENY\nDISCARD\nDROP ANALYZER\nDROP FUNCTION\nDROP PUBLICATION\nDROP REPOSITORY\nDROP SNAPSHOT\nDROP SUBSCRIPTION\nDROP TABLE\nDROP USER\nDROP VIEW\nEND\nEXPLAIN\nFETCH\nGRANT\nINSERT\nKILL\nOPTIMIZE\nREFRESH\nRESTORE SNAPSHOT\nREVOKE\nSELECT\nSET and RESET\nSET LICENSE\nSET AND RESET SESSION AUTHORIZATION\nSET TRANSACTION\nSHOW (session settings)\nSHOW COLUMNS\nSHOW CREATE TABLE\nSHOW SCHEMAS\nSHOW TABLES\nSTART TRANSACTION\nUPDATE\nVALUES\nWITH"
  },
  {
    "title": "Authentication Methods — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/auth/methods.html",
    "html": "5.6\nAuthentication Methods\n\nThere are multiple ways to authenticate against CrateDB.\n\nTable of contents\n\nTrust method\n\nTrust authentication over PostgreSQL protocol\n\nTrust authentication over HTTP\n\nPassword authentication method\n\nClient certificate authentication method\n\nTrust method\n\nWhen the trust authentication method is used, the server just takes the username provided by the client as is without further validation. This is useful for any setup where access is controlled by other means, like network restrictions as implemented by Host-Based Authentication (HBA).\n\nTrust authentication over PostgreSQL protocol\n\nThe PostgreSQL Protocol requires a user for every connection which is sent by all client implementations.\n\nTrust authentication over HTTP\n\nThe HTTP implementation extracts the username from the HTTP Basic Authentication request header.\n\nSince a user is always required for trust authentication, it is possible to specify a default user in case that the Authorization header is not set. This is useful to allow clients which do not provide the possibility to set any headers, for example a web browser connecting to the Admin UI.\n\nThe default user can be specified via the auth.trust.http_default_user setting like this:\n\nauth:\n  trust:\n    http_default_user: dustin\n\n\nNote\n\nWhen user management is enabled, the user of the Admin UI needs to be granted the following privileges: DQL on sys.shards, sys.nodes, sys.node_checks, sys.checks, sys.cluster, and sys.jobs_log tables. As well as DQL on the doc schema.\n\nThese DQL privileges are required by the Admin UI to display the cluster health, monitoring, and checks, to list the available nodes in the cluster and to list the tables.\n\nPassword authentication method\n\nWhen the password authentication method is used, the client has to provide a password additionally to the username.\n\nFor HTTP, the password must be encoded together with the username with BASE64_ and sent together prefixed with Basic as string value for the Authorization HTTP header. See also: HTTP Basic Authentication.\n\nThe password is sent from the client to the server in clear text, which means that unless SSL is enabled, the password could potentially be read by anyone sniffing the network.\n\nCrateDB does not store user passwords as clear text!\n\nCrateDB stores user passwords salted with a per-user salt and hashed using the PBKDF2 key derivation function and the SHA-512 hash algorithm.\n\nNote\n\nCrateDB will never leak information about user existence in the case of failed authentication. If you’re receiving an error trying to authenticate, first make sure that the user exists.\n\nClient certificate authentication method\n\nWhen the cert authentication method is used, the client has to connect to CrateDB using SSL with a valid client certificate.\n\nIf connecting via HTTP where the username is optional, the common name will be used as username. In case a username is already provided, it has to match the common name of the certificate. Otherwise the authentication will fail. See Trust method on how to provide a username via HTTP.\n\nThe rule that the common name must match the provided username always applies to the PostgreSQL wire protocol, as there the username isn’t optional.\n\nPlease consult the relevant client documentations for instructions on how to connect using SSL with client certificate.\n\nSee Also\n\nHost-Based Authentication (HBA)\n\nSecured communications (SSL/TLS)"
  },
  {
    "title": "SQL syntax — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/sql/index.html",
    "html": "master\nSQL syntax\n\nYou can use Structured Query Language (SQL) to query your data.\n\nThis section of the documentation provides a complete SQL syntax reference for CrateDB.\n\nNote\n\nFor introductions to CrateDB functionality, we recommend you consult the appropriate top-level section of the documentation. The SQL syntax reference assumes a basic familiarity with the relevant parts of CrateDB.\n\nSee Also\n\nGeneral use: Data definition\n\nGeneral use: Data manipulation\n\nGeneral use: Querying\n\nGeneral use: Built-in functions and operators\n\nGeneral SQL\nConstraints\nValue expressions\nLexical structure\nSQL Statements\nALTER CLUSTER\nALTER PUBLICATION\nALTER TABLE\nALTER ROLE\nALTER USER\nANALYZE\nBEGIN\nCLOSE\nCOMMIT\nCOPY FROM\nCOPY TO\nCREATE ANALYZER\nCREATE BLOB TABLE\nCREATE FOREIGN TABLE\nCREATE FUNCTION\nCREATE PUBLICATION\nCREATE REPOSITORY\nCREATE ROLE\nCREATE SERVER\nCREATE SNAPSHOT\nCREATE SUBSCRIPTION\nCREATE TABLE\nCREATE TABLE AS\nCREATE USER\nCREATE USER MAPPING\nCREATE VIEW\nDEALLOCATE\nDECLARE\nDELETE\nDENY\nDISCARD\nDROP ANALYZER\nDROP FOREIGN TABLE\nDROP FUNCTION\nDROP PUBLICATION\nDROP REPOSITORY\nDROP ROLE\nDROP SERVER\nDROP SNAPSHOT\nDROP SUBSCRIPTION\nDROP TABLE\nDROP USER\nDROP USER MAPPING\nDROP VIEW\nEND\nEXPLAIN\nFETCH\nGRANT\nINSERT\nKILL\nOPTIMIZE\nREFRESH\nRESTORE SNAPSHOT\nREVOKE\nSELECT\nSET and RESET\nSET LICENSE\nSET AND RESET SESSION AUTHORIZATION\nSET TRANSACTION\nSHOW (session settings)\nSHOW COLUMNS\nSHOW CREATE TABLE\nSHOW SCHEMAS\nSHOW TABLES\nSTART TRANSACTION\nUPDATE\nVALUES\nWITH"
  },
  {
    "title": "SQL syntax — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/index.html",
    "html": "5.6\nSQL syntax\n\nYou can use Structured Query Language (SQL) to query your data.\n\nThis section of the documentation provides a complete SQL syntax reference for CrateDB.\n\nNote\n\nFor introductions to CrateDB functionality, we recommend you consult the appropriate top-level section of the documentation. The SQL syntax reference assumes a basic familiarity with the relevant parts of CrateDB.\n\nSee Also\n\nGeneral use: Data definition\n\nGeneral use: Data manipulation\n\nGeneral use: Querying\n\nGeneral use: Built-in functions and operators\n\nGeneral SQL\nConstraints\nValue expressions\nLexical structure\nSQL Statements\nALTER CLUSTER\nALTER PUBLICATION\nALTER TABLE\nALTER ROLE\nALTER USER\nANALYZE\nBEGIN\nCLOSE\nCOMMIT\nCOPY FROM\nCOPY TO\nCREATE ANALYZER\nCREATE BLOB TABLE\nCREATE FUNCTION\nCREATE PUBLICATION\nCREATE REPOSITORY\nCREATE SNAPSHOT\nCREATE SUBSCRIPTION\nCREATE TABLE\nCREATE TABLE AS\nCREATE ROLE\nCREATE USER\nCREATE VIEW\nDEALLOCATE\nDECLARE\nDELETE\nDENY\nDISCARD\nDROP ANALYZER\nDROP FUNCTION\nDROP PUBLICATION\nDROP REPOSITORY\nDROP SNAPSHOT\nDROP SUBSCRIPTION\nDROP TABLE\nDROP ROLE\nDROP USER\nDROP VIEW\nEND\nEXPLAIN\nFETCH\nGRANT\nINSERT\nKILL\nOPTIMIZE\nREFRESH\nRESTORE SNAPSHOT\nREVOKE\nSELECT\nSET and RESET\nSET LICENSE\nSET AND RESET SESSION AUTHORIZATION\nSET TRANSACTION\nSHOW (session settings)\nSHOW COLUMNS\nSHOW CREATE TABLE\nSHOW SCHEMAS\nSHOW TABLES\nSTART TRANSACTION\nUPDATE\nVALUES\nWITH"
  },
  {
    "title": "Host-Based Authentication (HBA) — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/auth/hba.html",
    "html": "5.6\nHost-Based Authentication (HBA)\n\nThis section explains how to configure CrateDB client connection and authentication.\n\nNote\n\nThe stock crate.yml shipped with CrateDB explicitly enables host based authentication and defines a set of basic authentication rules.\n\nIf you do not want to use authentication, set auth.host_based.enabled to false. If authentication is disabled, user management remains active and you must specify the user crate (with an empty password) when connecting via the PostgreSQL protocol. HTTP clients do not have to specify a user because they use the auth.trust.http_default_user if no user is provided.\n\nNon-runtime cluster-wide settings must be configured the same on every node.\n\nTable of contents\n\nAuthentication against CrateDB\n\nAuthenticating as a superuser\n\nAuthenticating to Admin UI\n\nNode-to-node communication\n\nAuthentication against CrateDB\n\nClient access and authentication is configured via the auth.host_based.config setting in the crate.yml file.\n\nThe general format of the auth.host_based.config setting is a map of remote client access entries, where the key of the map defines the order in which the entries are used, which permit authentication to CrateDB. Each entry may contain no, one, or multiple fields. Allowed fields are user, ip or cidr, method, protocol and ssl. The description of these fields can be found in Trust authentication.\n\nWhen a client sends an authentication request, CrateDB matches the provided username, IP address, protocol and connection scheme against these entries to determine which authentication method is required. If no entry matches, the client authentication request will be denied.\n\nTo support proxied clients to authenticate, the X-REAL-IP request header can be used. For security reasons, this is disabled by default as it allows clients to impersonate other clients. To enable this feature, set auth.trust.http_support_x_real_ip to true. If enabled, the X-REAL-IP request header has priority over the actual client IP address.\n\nIf auth.host_based is not set, the host based authentication is disabled. In this case CrateDB trusts all connections and accepts the user provided by the client given that this user exists.\n\nIf the setting auth.host_based is present and the configurations list does not contain any entry, then no client can authenticate.\n\nFor example, a host based configuration can look like this:\n\nauth:\n  host_based:\n    enabled: true\n    config:\n      0:\n        user: mike\n        address: 32.0.0.0/8\n        method: trust\n        protocol: pg\n      a:\n        user: barb\n        address: 172.16.0.0\n        protocol: pg\n        ssl: on\n      b:\n        user: crate\n        address: 32.0.0.0/8\n        method: trust\n      y:\n        user: eleven\n        protocol: pg\n      e:\n        user: dustin\n        address: 172.16.0.0\n        method: trust\n        protocol: http\n      f:\n        user: trinity\n        protocol: http\n        address: 127.0.0.1\n        ssl: off\n      z:\n        method: password\n\n\nNote\n\nIn the auth.host_based.config setting, the order of the entries is defined by the natural order of the group keys of the setting. The authentication method of the first entry that matches the client user and address will be used. If the authentication attempt fails, subsequent entries will not be considered. The entry look-up order is determined by the order identifier of each entry.\n\nIn the example above:\n\n{user: mike, address: 32.0.0.0/8, method: trust, protocol: pg} means that the user mike can authenticate to CrateDB over the PostgreSQL Wire Protocol from any IP address ranging from 32.0.0.0 to 32.255.255.255, using the trust authentication method.\n\n{user: crate, address: 32.0.0.0/8, method: trust} means that the superuser crate can authenticate to CrateDB over the protocols for which authentication is supported from any IP address in the range of 32.0.0.0 to 32.255.255.255, using the trust authentication method.\n\n{user: barb, address: 172.16.0.0, protocol: pg, ssl: on} means that the user barb can authenticate to CrateDB over the PostgreSQL Wire Protocol from the 172.16.0.0 IP Address only if the connection is done over SSL/TLS. Since no authentication method is specified, the trust method will be used by default.\n\nThe entry: {user: eleven, protocol: pg} means that the user eleven can authenticate to CrateDB over the PostgreSQL Wire Protocol from any IP address, using the trust method.\n\n{user: dustin, address: 172.16.0.0, protocol: http, method: trust} means that the user dustin can authenticate to CrateDB over HTTP protocol from the 172.16.0.0 IP Address using the trust method.\n\n{user: trinity, address: 127.0.0.1, protocol: http, ssl: off} means that the user trinity can authenticate to CrateDB over HTTP from the 127.0.0.1 IP Address only if no SSL/TLS connection is used. Since no authentication method is specified, the trust method will be used by default.\n\nAnd finally the entry {method: password} means that any existing user (or superuser) can authenticate to CrateDB from any IP address using the password method for both HTTP and PostgreSQL wire protocol.\n\nNote\n\nFor general help managing users and roles, see Users and roles management.\n\nAuthenticating as a superuser\n\nWhen CrateDB is started, the cluster contains one predefined superuser. This user is called crate.\n\nTo enable trust authentication for the superuser, crate must be specified in the auth.host_based setting, like this:\n\nauth:\n  host_based:\n    enabled: true\n    config:\n      0:\n        user: crate\n\nAuthenticating to Admin UI\n\nWhen trying to access the CrateDB Admin UI, authentication with the user defined with the auth.trust.http_default_user setting (defaults to crate) will be attempted initially. If this authentication attempt fails, the browser will open the standard popup window where the user is asked to fill in credentials. Depending on the HBA configuration, it may be necessary to a username and password, or, alternatively, a username only.\n\nUsers that log in to the Admin UI must be granted DQL` privileges at the CLUSTER level in order to be able to access the various monitoring sections. For example:\n\ncr> GRANT DQL TO admin;\nGRANT OK, 1 row affected (... sec)\n\n\nFor more information, consult the privileges section.\n\nNode-to-node communication\n\nYou can use the Host-Based Authentication mechanism for node-to-node communication.\n\nFor example, if you wanted to configure a multi-zone cluster, you should enable certificate authentication like this:\n\nauth:\n  host_based:\n    enabled: true\n    config:\n      0:\n        protocol: transport\n        ssl: on\n        method: cert\n\n\nNote\n\nCrateDB only supports the trust and cert authentication methods for node-to-node communication."
  },
  {
    "title": "Administration — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/admin/index.html",
    "html": "4.8\nAdministration\n\nThis section of the documentation covers any feature primarily of interest to a database administrator.\n\nTable of contents\n\nSystem information\nRuntime configuration\nUser management\nPrivileges\nAuthentication\nAuthentication Methods\nHost-Based Authentication (HBA)\nSecured communications (SSL/TLS)\nOptimization\nJobs management\nJMX monitoring\nSnapshots\nLogical replication\nCloud discovery\nUsage Data Collector"
  },
  {
    "title": "Administration — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/admin/index.html",
    "html": "3.3\nAdministration\n\nThis section of the documentation covers any feature primarily of interest to a database administrator.\n\nTable of Contents\n\nSystem Information\nRuntime Configuration\nUser Management\nPrivileges\nAuthentication\nAuthentication Methods\nHost Based Authentication (HBA)\nSecured Communications (SSL/TLS)\nIngestion Framework\nIngestion Sources\nIngestion Rules\nOptimization\nJobs Management\nJMX Monitoring\nSnapshots\nCloud Discovery\nUsage Data Collector"
  },
  {
    "title": "Administration — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/index.html",
    "html": "5.6\nAdministration\n\nThis section of the documentation covers any feature primarily of interest to a database administrator.\n\nTable of contents\n\nSystem information\nRuntime configuration\nUsers and roles management\nPrivileges\nAuthentication\nAuthentication Methods\nHost-Based Authentication (HBA)\nSecured communications (SSL/TLS)\nOptimization\nJobs management\nJMX monitoring\nSnapshots\nLogical replication\nCloud discovery\nUsage Data Collector"
  },
  {
    "title": "Administration — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/admin/index.html",
    "html": "master\nAdministration\n\nThis section of the documentation covers any feature primarily of interest to a database administrator.\n\nTable of contents\n\nSystem information\nRuntime configuration\nUsers and roles management\nPrivileges\nAuthentication\nAuthentication Methods\nHost-Based Authentication (HBA)\nSecured communications (SSL/TLS)\nOptimization\nJobs management\nJMX monitoring\nSnapshots\nForeign Data Wrappers\nLogical replication\nCloud discovery\nUsage Data Collector"
  },
  {
    "title": "Administration — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/admin/index.html",
    "html": "5.5\nAdministration\n\nThis section of the documentation covers any feature primarily of interest to a database administrator.\n\nTable of contents\n\nSystem information\nRuntime configuration\nUser management\nPrivileges\nAuthentication\nAuthentication Methods\nHost-Based Authentication (HBA)\nSecured communications (SSL/TLS)\nOptimization\nJobs management\nJMX monitoring\nSnapshots\nLogical replication\nCloud discovery\nUsage Data Collector"
  },
  {
    "title": "Subquery expressions — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/builtins/subquery-expressions.html",
    "html": "5.6\nSubquery expressions\n\nSome operators can be used with an uncorrelated subquery to form a subquery expression that returns a boolean value (i.e., true or false) or NULL.\n\nSee Also\n\nSQL: Value expressions\n\nTable of contents\n\nIN (subquery)\n\nANY/SOME (subquery)\n\nALL (subquery)\n\nIN (subquery)\n\nSyntax:\n\nexpression IN (subquery)\n\n\nThe subquery must produce result rows with a single column only.\n\nHere’s an example:\n\ncr> select name, surname, sex from employees\n... where dept_id in (select id from departments where name = 'Marketing')\n... order by name, surname;\n+--------+----------+-----+\n| name   | surname  | sex |\n+--------+----------+-----+\n| David  | Bowe     | M   |\n| David  | Limb     | M   |\n| Sarrah | Mcmillan | F   |\n| Smith  | Clark    | M   |\n+--------+----------+-----+\nSELECT 4 rows in set (... sec)\n\n\nThe IN operator returns true if any subquery row equals the left-hand operand. Otherwise, it returns false (including the case where the subquery returns no rows).\n\nThe operator returns NULL if:\n\nThe left-hand expression evaluates to NULL\n\nThere are no matching right-hand values and at least one right-hand value is NULL\n\nNote\n\nIN (subquery) is an alias for = ANY (subquery)\n\nANY/SOME (subquery)\n\nSyntax:\n\nexpression comparison ANY | SOME (subquery)\n\n\nHere, comparison can be any basic comparison operator. The subquery must produce result rows with a single column only.\n\nHere’s an example:\n\ncr> select name, population from countries\n... where population > any (select * from unnest([8000000, 22000000, NULL]))\n... order by population, name;\n+--------------+------------+\n| name         | population |\n+--------------+------------+\n| Austria      |    8747000 |\n| South Africa |   55910000 |\n| France       |   66900000 |\n| Turkey       |   79510000 |\n| Germany      |   82670000 |\n+--------------+------------+\nSELECT 5 rows in set (... sec)\n\n\nThe ANY operator returns true if the defined comparison is true for any of the result rows of the right-hand subquery.\n\nThe operator returns false if the comparison returns false for all result rows of the subquery or if the subquery returns no rows.\n\nThe operator returns NULL if:\n\nThe left-hand expression evaluates to NULL\n\nThere are no matching right-hand values and at least one right-hand value is NULL\n\nNote\n\nThe following is not supported:\n\nIS NULL or IS NOT NULL as comparison\n\nMatching as many columns as there are expressions on the left-hand row e.g. (x,y) = ANY (select x, y from t)\n\nALL (subquery)\n\nSyntax:\n\nvalue comparison ALL (subquery)\n\n\nHere, comparison can be any basic comparison operator. The subquery must produce result rows with a single column only.\n\nHere’s an example:\n\ncr> select 100 <> ALL (select height from sys.summits) AS x;\n+------+\n| x    |\n+------+\n| TRUE |\n+------+\nSELECT 1 row in set (... sec)\n\n\nThe ALL operator returns true if the defined comparison is true for all of the result rows of the right-hand subquery.\n\nThe operator returns false if the comparison returns false for any result rows of the subquery.\n\nThe operator returns NULL if:\n\nThe left-hand expression evaluates to NULL\n\nNo comparison returns false and at least one right-hand value is NULL"
  },
  {
    "title": "Window functions — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/builtins/window-functions.html",
    "html": "5.6\nWindow functions\n\nWindow functions are functions which perform a computation across a set of rows which are related to the current row. This is comparable to aggregation functions, but window functions do not cause multiple rows to be grouped into a single row.\n\nTable of contents\n\nWindow function call\n\nSynopsis\n\nWindow definition\n\nOVER\n\nSynopsis\n\nNamed windows\n\nGeneral-purpose window functions\n\nrow_number()\n\nfirst_value(arg)\n\nlast_value(arg)\n\nnth_value(arg, number)\n\nlag(arg [, offset [, default] ])\n\nSynopsis\n\nlead(arg [, offset [, default] ])\n\nSynopsis\n\nrank()\n\nSynopsis\n\ndense_rank()\n\nSynopsis\n\nAggregate window functions\n\nWindow function call\nSynopsis\n\nThe synopsis of a window function call is one of the following\n\nfunction_name ( { * | [ expression [, expression ... ] ] } )\n              [ FILTER ( WHERE condition ) ]\n              [ { RESPECT | IGNORE } NULLS ]\n              over_clause\n\n\nwhere function_name is a name of a general-purpose window or aggregate function and expression is a column reference, scalar function or literal.\n\nIf FILTER is specified, then only the rows that met the WHERE condition are supplied to the window function. Only window functions that are aggregates accept the FILTER clause.\n\nIf IGNORE NULLS option is specified, then the null values are excluded from the window function executions. The window functions that support this option are: lead(arg [, offset [, default] ]), lag(arg [, offset [, default] ]), first_value(arg), last_value(arg), and nth_value(arg, number). If a function supports this option and it is not specified, then RESPECT NULLS is set by default.\n\nThe OVER clause is what declares a function to be a window function.\n\nThe window function call that uses a wildcard instead of an expression as a function argument is supported only by the count(*) aggregate function.\n\nWindow definition\nOVER\nSynopsis\nOVER { window_name | ( [ window_definition ] ) }\n\n\nwhere window_definition has the syntax\n\nwindow_definition:\n   [ window_name ]\n   [ PARTITION BY expression [, ...] ]\n   [ ORDER BY expression [ ASC | DESC ] [ NULLS { FIRST | LAST } ] [, ...] ]\n   [ { RANGE | ROWS } BETWEEN frame_start AND frame_end ]\n\n\nThe window_name refers to window_definition defined in the WINDOW clause.\n\nThe frame_start and frame_end can be one of\n\nUNBOUNDED PRECEDING\noffset PRECEDING\nCURRENT ROW\noffset FOLLOWING\nUNBOUNDED FOLLOWING\n\n\nThe default frame definition is RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW. If frame_end is omitted it defaults to CURRENT ROW.\n\nframe_start cannot be FOLLOWING or UNBOUNDED FOLLOWING and frame_end cannot be PRECEDING or UNBOUNDED PRECEDING.\n\nIn RANGE mode if the frame_start is CURRENT ROW the frame starts with the current row’s first peer (a row that the window’s ORDER BY expression sorts as equal to the current row), while a frame_end of CURRENT ROW means the frame will end with the current’s row last peer row.\n\nIn ROWS mode CURRENT_ROW means the current row.\n\nThe offset PRECEDING and offset FOLLOWING options vary in meaning depending on the frame mode. In ROWS mode, the offset is an integer indicating that the frame start or end is offsetted by that many rows before or after the current row. In RANGE mode, the use of a custom offset option requires that there is exactly one ORDER BY column in the window definition. The frame contains those rows whose ordering column value is no more than offset minus (for PRECEDING) or plus (for FOLLOWING) the current row’s ordering column value. Because the value of offset is subtracted/added to the values of the ordering column, only type combinations that support addition/subtraction operations are allowed. For instance, when the ordering column is of type timestamp, the offset expression can be an interval.\n\nThe OVER clause defines the window containing the appropriate rows which will take part in the window function computation.\n\nAn empty OVER clause defines a window containing all the rows in the result set.\n\nExample:\n\ncr> SELECT dept_id, COUNT(*) OVER() AS cnt FROM employees ORDER BY 1, 2;\n+---------+-----+\n| dept_id | cnt |\n+---------+-----+\n|    4001 |  18 |\n|    4001 |  18 |\n|    4001 |  18 |\n|    4002 |  18 |\n|    4002 |  18 |\n|    4002 |  18 |\n|    4002 |  18 |\n|    4003 |  18 |\n|    4003 |  18 |\n|    4003 |  18 |\n|    4003 |  18 |\n|    4003 |  18 |\n|    4004 |  18 |\n|    4004 |  18 |\n|    4004 |  18 |\n|    4006 |  18 |\n|    4006 |  18 |\n|    4006 |  18 |\n+---------+-----+\nSELECT 18 rows in set (... sec)\n\n\nThe PARTITION BY clause groups the rows within a window into partitions which are processed separately by the window function, each partition in turn becoming a window. If PARTITION BY is not specified, all the rows are considered a single partition.\n\nExample:\n\ncr> SELECT dept_id, ROW_NUMBER() OVER(PARTITION BY dept_id) AS row_num\n... FROM employees ORDER BY 1, 2;\n+---------+---------+\n| dept_id | row_num |\n+---------+---------+\n|    4001 |       1 |\n|    4001 |       2 |\n|    4001 |       3 |\n|    4002 |       1 |\n|    4002 |       2 |\n|    4002 |       3 |\n|    4002 |       4 |\n|    4003 |       1 |\n|    4003 |       2 |\n|    4003 |       3 |\n|    4003 |       4 |\n|    4003 |       5 |\n|    4004 |       1 |\n|    4004 |       2 |\n|    4004 |       3 |\n|    4006 |       1 |\n|    4006 |       2 |\n|    4006 |       3 |\n+---------+---------+\nSELECT 18 rows in set (... sec)\n\n\nIf ORDER BY is supplied the window definition consists of a range of rows starting with the first row in the partition and ending with the current row, plus any subsequent rows that are equal to the current row, which are the current row’s peers.\n\nExample:\n\ncr> SELECT\n...   dept_id,\n...   sex,\n...   COUNT(*) OVER(PARTITION BY dept_id ORDER BY sex) AS cnt\n... FROM employees\n... ORDER BY 1, 2, 3\n+---------+-----+-----+\n| dept_id | sex | cnt |\n+---------+-----+-----+\n|    4001 | M   |   3 |\n|    4001 | M   |   3 |\n|    4001 | M   |   3 |\n|    4002 | F   |   1 |\n|    4002 | M   |   4 |\n|    4002 | M   |   4 |\n|    4002 | M   |   4 |\n|    4003 | M   |   5 |\n|    4003 | M   |   5 |\n|    4003 | M   |   5 |\n|    4003 | M   |   5 |\n|    4003 | M   |   5 |\n|    4004 | F   |   1 |\n|    4004 | M   |   3 |\n|    4004 | M   |   3 |\n|    4006 | F   |   1 |\n|    4006 | M   |   3 |\n|    4006 | M   |   3 |\n+---------+-----+-----+\nSELECT 18 rows in set (... sec)\n\n\nNote\n\nTaking into account the peers concept mentioned above, for an empty OVER clause all the rows in the result set are peers.\n\nNote\n\nAggregation functions will be treated as window functions when used in conjunction with the OVER clause.\n\nNote\n\nWindow definitions order or partitioned by an array column type are currently not supported.\n\nIn the UNBOUNDED FOLLOWING case the window for each row starts with each row and ends with the last row in the current partition. If the current row has peers the window will include (or start with) all the current row peers and end at the upper bound of the partition.\n\nExample:\n\ncr> SELECT\n...   dept_id,\n...   sex,\n...   COUNT(*) OVER(\n...     PARTITION BY dept_id\n...     ORDER BY\n...       sex RANGE BETWEEN CURRENT ROW\n...       AND UNBOUNDED FOLLOWING\n...   ) partitionByDeptOrderBySex\n... FROM employees\n... ORDER BY 1, 2, 3\n+---------+-----+---------------------------+\n| dept_id | sex | partitionbydeptorderbysex |\n+---------+-----+---------------------------+\n|    4001 | M   |                         3 |\n|    4001 | M   |                         3 |\n|    4001 | M   |                         3 |\n|    4002 | F   |                         4 |\n|    4002 | M   |                         3 |\n|    4002 | M   |                         3 |\n|    4002 | M   |                         3 |\n|    4003 | M   |                         5 |\n|    4003 | M   |                         5 |\n|    4003 | M   |                         5 |\n|    4003 | M   |                         5 |\n|    4003 | M   |                         5 |\n|    4004 | F   |                         3 |\n|    4004 | M   |                         2 |\n|    4004 | M   |                         2 |\n|    4006 | F   |                         3 |\n|    4006 | M   |                         2 |\n|    4006 | M   |                         2 |\n+---------+-----+---------------------------+\nSELECT 18 rows in set (... sec)\n\nNamed windows\n\nIt is possible to define a list of named window definitions that can be referenced in OVER clauses. To do this, use the WINDOW clause in the SELECT clause.\n\nNamed windows are particularly useful when the same window definition could be used in multiple OVER clauses. For instance\n\ncr> SELECT\n...   x,\n...   FIRST_VALUE(x) OVER (w) AS \"first\",\n...   LAST_VALUE(x) OVER (w) AS \"last\"\n... FROM (VALUES (1), (2), (3), (4)) AS t(x)\n... WINDOW w AS (ORDER BY x)\n+---+-------+------+\n| x | first | last |\n+---+-------+------+\n| 1 |     1 |    1 |\n| 2 |     1 |    2 |\n| 3 |     1 |    3 |\n| 4 |     1 |    4 |\n+---+-------+------+\nSELECT 4 rows in set (... sec)\n\n\nIf a window_name is specified in the window definition of the OVER clause, then there must be a named window entry that matches the window_name in the window definition list of the WINDOW clause.\n\nIf the OVER clause has its own non-empty window definition and references a window definition from the WINDOW clause, then it can only add clauses from the referenced window, but not overwrite them.\n\ncr> SELECT\n...   x,\n...   LAST_VALUE(x) OVER (w ORDER BY x) AS y\n... FROM (VALUES\n...      (1, 1),\n...      (2, 1),\n...      (3, 2),\n...      (4, 2) ) AS t(x, y)\n... WINDOW w AS (PARTITION BY y)\n+---+---+\n| x | y |\n+---+---+\n| 1 | 1 |\n| 2 | 2 |\n| 3 | 3 |\n| 4 | 4 |\n+---+---+\nSELECT 4 rows in set (... sec)\n\n\nOtherwise, an attempt to override the clauses of the referenced window by the window definition of the OVER clause will result in failure.\n\ncr> SELECT\n...   FIRST_VALUE(x) OVER (w ORDER BY x)\n... FROM (VALUES(1), (2), (3), (4)) as t(x)\n... WINDOW w AS (ORDER BY x)\nSQLParseException[Cannot override ORDER BY clause of window w]\n\n\nIt is not possible to define the PARTITION BY clause in the window definition of the OVER clause if it references a window definition from the WINDOW clause.\n\nThe window definitions in the WINDOW clause cannot define its own window frames, if they are referenced by non-empty window definitions of the OVER clauses.\n\nThe definition of the named window can itself begin with a window_name. In this case all the elements of interconnected named windows will be copied to the window definition of the OVER clause if it references the named window definition that has subsequent window references. The window definitions in the WINDOW clause permits only backward references.\n\ncr> SELECT\n...   x,\n...   ROW_NUMBER() OVER (w) AS y\n... FROM (VALUES\n...      (1, 1),\n...      (3, 2),\n...      (2, 1)) AS t (x, y)\n... WINDOW p AS (PARTITION BY y),\n...        w AS (p ORDER BY x)\n+---+---+\n| x | y |\n+---+---+\n| 1 | 1 |\n| 2 | 2 |\n| 3 | 1 |\n+---+---+\nSELECT 3 rows in set (... sec)\n\nGeneral-purpose window functions\nrow_number()\n\nReturns the number of the current row within its window.\n\nExample:\n\ncr> SELECT\n...  col1,\n...  ROW_NUMBER() OVER(ORDER BY col1) as row_num\n... FROM (VALUES('x'), ('y'), ('z')) AS t;\n+------+---------+\n| col1 | row_num |\n+------+---------+\n| x    |       1 |\n| y    |       2 |\n| z    |       3 |\n+------+---------+\nSELECT 3 rows in set (... sec)\n\nfirst_value(arg)\n\nReturns the argument value evaluated at the first row within the window.\n\nIts return type is the type of its argument.\n\nExample:\n\ncr> SELECT\n...  col1,\n...  FIRST_VALUE(col1) OVER (ORDER BY col1) AS value\n... FROM (VALUES('x'), ('y'), ('y'), ('z')) AS t;\n+------+-------+\n| col1 | value |\n+------+-------+\n| x    | x     |\n| y    | x     |\n| y    | x     |\n| z    | x     |\n+------+-------+\nSELECT 4 rows in set (... sec)\n\nlast_value(arg)\n\nReturns the argument value evaluated at the last row within the window.\n\nIts return type is the type of its argument.\n\nExample:\n\ncr> SELECT\n...  col1,\n...  LAST_VALUE(col1) OVER(ORDER BY col1) AS value\n... FROM (VALUES('x'), ('y'), ('y'), ('z')) AS t;\n+------+-------+\n| col1 | value |\n+------+-------+\n| x    | x     |\n| y    | y     |\n| y    | y     |\n| z    | z     |\n+------+-------+\nSELECT 4 rows in set (... sec)\n\nnth_value(arg, number)\n\nReturns the argument value evaluated at row that is the nth row within the window. NULL is returned if the nth row doesn’t exist in the window.\n\nIts return type is the type of its first argument.\n\nExample:\n\ncr> SELECT\n...  col1,\n...  NTH_VALUE(col1, 3) OVER(ORDER BY col1) AS val\n... FROM (VALUES ('x'), ('y'), ('y'), ('z')) AS t;\n+------+------+\n| col1 | val  |\n+------+------+\n| x    | NULL |\n| y    | y    |\n| y    | y    |\n| z    | y    |\n+------+------+\nSELECT 4 rows in set (... sec)\n\nlag(arg [, offset [, default] ])\nSynopsis\nlag(argument any [, offset integer [, default any]])\n\n\nReturns the argument value evaluated at the row that precedes the current row by the offset within the partition. If there is no such row, the return value is default. If offset or default arguments are missing, they default to 1 and null, respectively.\n\nBoth offset and default are evaluated with respect to the current row.\n\nIf offset is 0, then argument value is evaluated for the current row.\n\nThe default and argument data types must match.\n\nExample:\n\ncr> SELECT\n...   dept_id,\n...   year,\n...   budget,\n...   LAG(budget) OVER(\n...      PARTITION BY dept_id) prev_budget\n... FROM (VALUES\n...      (1, 2017, 45000),\n...      (1, 2018, 35000),\n...      (2, 2017, 15000),\n...      (2, 2018, 65000),\n...      (2, 2019, 12000))\n... as t (dept_id, year, budget);\n+---------+------+--------+-------------+\n| dept_id | year | budget | prev_budget |\n+---------+------+--------+-------------+\n|       1 | 2017 |  45000 |        NULL |\n|       1 | 2018 |  35000 |       45000 |\n|       2 | 2017 |  15000 |        NULL |\n|       2 | 2018 |  65000 |       15000 |\n|       2 | 2019 |  12000 |       65000 |\n+---------+------+--------+-------------+\nSELECT 5 rows in set (... sec)\n\nlead(arg [, offset [, default] ])\nSynopsis\nlead(argument any [, offset integer [, default any]])\n\n\nThe lead function is the counterpart of the lag window function as it allows the evaluation of the argument at rows that follow the current row. lead returns the argument value evaluated at the row that follows the current row by the offset within the partition. If there is no such row, the return value is default. If offset or default arguments are missing, they default to 1 or null, respectively.\n\nBoth offset and default are evaluated with respect to the current row.\n\nIf offset is 0, then argument value is evaluated for the current row.\n\nThe default and argument data types must match.\n\nExample:\n\ncr> SELECT\n...   dept_id,\n...   year,\n...   budget,\n...   LEAD(budget) OVER(\n...      PARTITION BY dept_id) next_budget\n... FROM (VALUES\n...      (1, 2017, 45000),\n...      (1, 2018, 35000),\n...      (2, 2017, 15000),\n...      (2, 2018, 65000),\n...      (2, 2019, 12000))\n... as t (dept_id, year, budget);\n+---------+------+--------+-------------+\n| dept_id | year | budget | next_budget |\n+---------+------+--------+-------------+\n|       1 | 2017 |  45000 |       35000 |\n|       1 | 2018 |  35000 |        NULL |\n|       2 | 2017 |  15000 |       65000 |\n|       2 | 2018 |  65000 |       12000 |\n|       2 | 2019 |  12000 |        NULL |\n+---------+------+--------+-------------+\nSELECT 5 rows in set (... sec)\n\nrank()\nSynopsis\nrank()\n\n\nReturns the rank of every row within a partition of a result set.\n\nWithin each partition, the rank of the first row is 1. Subsequent tied rows are given the same rank, and the potential rank of the next row is incremented. Because of this, ranks may not be sequential.\n\nExample:\n\ncr> SELECT\n...   name,\n...   department_id,\n...   salary,\n...   RANK() OVER (ORDER BY salary desc) as salary_rank\n... FROM (VALUES\n...      ('Bobson Dugnutt', 1, 2000),\n...      ('Todd Bonzalez', 2, 2500),\n...      ('Jess Brewer', 1, 2500),\n...      ('Safwan Buchanan', 1, 1900),\n...      ('Hal Dodd', 1, 2500),\n...      ('Gillian Hawes', 2, 2000))\n... as t (name, department_id, salary);\n+-----------------+---------------+--------+-------------+\n| name            | department_id | salary | salary_rank |\n+-----------------+---------------+--------+-------------+\n| Todd Bonzalez   |             2 |   2500 |           1 |\n| Jess Brewer     |             1 |   2500 |           1 |\n| Hal Dodd        |             1 |   2500 |           1 |\n| Bobson Dugnutt  |             1 |   2000 |           4 |\n| Gillian Hawes   |             2 |   2000 |           4 |\n| Safwan Buchanan |             1 |   1900 |           6 |\n+-----------------+---------------+--------+-------------+\nSELECT 6 rows in set (... sec)\n\ndense_rank()\nSynopsis\ndense_rank()\n\n\nReturns the rank of every row within a partition of a result set, similar to rank. However, unlike rank, dense_rank always returns sequential rank values.\n\nWithin each partition, the rank of the first row is 1. Subsequent tied rows are given the same rank.\n\nExample:\n\ncr> SELECT\n...   name,\n...   department_id,\n...   salary,\n...   DENSE_RANK() OVER (ORDER BY salary desc) as salary_rank\n... FROM (VALUES\n...      ('Bobson Dugnutt', 1, 2000),\n...      ('Todd Bonzalez', 2, 2500),\n...      ('Jess Brewer', 1, 2500),\n...      ('Safwan Buchanan', 1, 1900),\n...      ('Hal Dodd', 1, 2500),\n...      ('Gillian Hawes', 2, 2000))\n... as t (name, department_id, salary);\n+-----------------+---------------+--------+-------------+\n| name            | department_id | salary | salary_rank |\n+-----------------+---------------+--------+-------------+\n| Todd Bonzalez   |             2 |   2500 |           1 |\n| Jess Brewer     |             1 |   2500 |           1 |\n| Hal Dodd        |             1 |   2500 |           1 |\n| Bobson Dugnutt  |             1 |   2000 |           2 |\n| Gillian Hawes   |             2 |   2000 |           2 |\n| Safwan Buchanan |             1 |   1900 |           3 |\n+-----------------+---------------+--------+-------------+\nSELECT 6 rows in set (... sec)\n\nAggregate window functions\n\nSee Aggregation."
  },
  {
    "title": "Comparison operators — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/builtins/comparison-operators.html",
    "html": "5.6\nComparison operators\n\nA comparison operator tests the relationship between two values and returns a corresponding value of true, false, or NULL.\n\nTable of contents\n\nBasic operators\n\nWHERE clause operators\n\nBasic operators\n\nFor simple data types, the following basic operators can be used:\n\nOperator\n\n\t\n\nDescription\n\n\n\n\n<\n\n\t\n\nLess than\n\n\n\n\n>\n\n\t\n\nGreater than\n\n\n\n\n<=\n\n\t\n\nLess than or equal to\n\n\n\n\n>=\n\n\t\n\nGreater than or equal to\n\n\n\n\n=\n\n\t\n\nEqual\n\n\n\n\n<>\n\n\t\n\nNot equal\n\n\n\n\n!=\n\n\t\n\nNot equal (same as <>)\n\nWhen comparing strings, a lexicographical comparison is performed:\n\ncr> select name from locations where name > 'Argabuthon' order by name;\n+------------------------------------+\n| name                               |\n+------------------------------------+\n| Arkintoofle Minor                  |\n| Bartledan                          |\n| Galactic Sector QQ7 Active J Gamma |\n| North West Ripple                  |\n| Outer Eastern Rim                  |\n+------------------------------------+\nSELECT 5 rows in set (... sec)\n\n\nWhen comparing dates, ISO date formats can be used:\n\ncr> select date, position from locations where date <= '1979-10-12' and\n... position < 3 order by position;\n+--------------+----------+\n| date         | position |\n+--------------+----------+\n| 308534400000 |        1 |\n| 308534400000 |        2 |\n+--------------+----------+\nSELECT 2 rows in set (... sec)\n\n\nTip\n\nComparison operators are commonly used to filter rows (e.g., in the WHERE and HAVING clauses of a SELECT statement). However, basic comparison operators can be used as value expressions in any context. For example:\n\ncr> SELECT 1 < 10 as my_column;\n+-----------+\n| my_column |\n+-----------+\n| TRUE      |\n+-----------+\nSELECT 1 row in set (... sec)\n\nWHERE clause operators\n\nWithin a WHERE clause, the following operators can also be used:\n\nOperator\n\n\t\n\nDescription\n\n\n\n\n~ , ~* , !~ , !~*\n\n\t\n\nSee Regular expressions\n\n\n\n\nLIKE (ILIKE)\n\n\t\n\nMatches a part of the given value\n\n\n\n\nNOT\n\n\t\n\nNegates a condition\n\n\n\n\nIS NULL\n\n\t\n\nMatches a null value\n\n\n\n\nIS NOT NULL\n\n\t\n\nMatches a non-null value\n\n\n\n\nip << range\n\n\t\n\nTrue if IP is within the given IP range (using CIDR notation)\n\n\n\n\nx BETWEEN y AND z\n\n\t\n\nShortcut for x >= y AND x <= z\n\nSee Also\n\nArray comparisons\n\nSubquery expressions"
  },
  {
    "title": "Table functions — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/builtins/table-functions.html",
    "html": "5.6\nTable functions\n\nTable functions are functions that produce a set of rows. They can be used in place of a relation in the FROM clause.\n\nIf used within the select list, the table functions will be evaluated per row of the relations in the FROM clause, generating one or more rows which are appended to the result set. If multiple table functions with different amounts of rows are used, NULL values will be returned for the functions that are exhausted.\n\nFor example:\n\ncr> select unnest([1, 2, 3]), unnest([1, 2]);\n+-------------------+----------------+\n| unnest([1, 2, 3]) | unnest([1, 2]) |\n+-------------------+----------------+\n|                 1 |              1 |\n|                 2 |              2 |\n|                 3 |           NULL |\n+-------------------+----------------+\nSELECT 3 rows in set (... sec)\n\n\nNote\n\nTable functions in the select list are executed after aggregations. So aggregations can be used as arguments to table functions, but the other way around is not allowed, unless sub queries are utilized.\n\nFor example:\n\n(SELECT aggregate_func(col) FROM (SELECT table_func(...) AS col) ...)\n\n\nTable of contents\n\nScalar functions\n\nempty_row( )\n\nunnest( array [ array , ] )\n\npg_catalog.generate_series(start, stop, [step])\n\npg_catalog.generate_subscripts(array, dim, [reverse])\n\nregexp_matches(source, pattern [, flags])\n\nFlags\n\nExamples\n\npg_catalog.pg_get_keywords()\n\ninformation_schema._pg_expandarray(array)\n\nScalar functions\n\nA scalar function, when used in the FROM clause in place of a relation, will result in a table of one row and one column, containing the scalar value returned from the function.\n\ncr> SELECT * FROM abs(-5), initcap('hello world');\n+-----+-------------+\n| abs | initcap     |\n+-----+-------------+\n|   5 | Hello World |\n+-----+-------------+\nSELECT 1 row in set (... sec)\n\nempty_row( )\n\nempty_row doesn’t take any argument and produces a table with an empty row and no column.\n\ncr> select * from empty_row();\nSELECT OK, 1 row affected  (... sec)\n\nunnest( array [ array , ] )\n\nunnest takes any number of array parameters and produces a table where each provided array argument results in a column.\n\nThe columns are named colN where N is a number starting at 1.\n\ncr> select * from unnest([1, 2, 3], ['Arthur', 'Trillian', 'Marvin']);\n+------+----------+\n| col1 | col2     |\n+------+----------+\n|    1 | Arthur   |\n|    2 | Trillian |\n|    3 | Marvin   |\n+------+----------+\nSELECT 3 rows in set (... sec)\n\npg_catalog.generate_series(start, stop, [step])\n\nGenerate a series of values from inclusive start to inclusive stop with step increments.\n\nThe argument can be integer or bigint, in which case step is optional and defaults to 1.\n\nstart and stop can also be of type timestamp with time zone or timestamp without time zone in which case step is required and must be of type interval.\n\nThe return value always matches the start / stop types.\n\ncr> SELECT * FROM generate_series(1, 4);\n+-----------------+\n| generate_series |\n+-----------------+\n|               1 |\n|               2 |\n|               3 |\n|               4 |\n+-----------------+\nSELECT 4 rows in set (... sec)\n\ncr> SELECT\n...     x,\n...     date_format('%Y-%m-%d, %H:%i', x)\n...     FROM generate_series('2019-01-01 00:00'::timestamp, '2019-01-04 00:00'::timestamp, '30 hours'::interval) AS t(x);\n+---------------+-----------------------------------+\n|             x | date_format('%Y-%m-%d, %H:%i', x) |\n+---------------+-----------------------------------+\n| 1546300800000 | 2019-01-01, 00:00                 |\n| 1546408800000 | 2019-01-02, 06:00                 |\n| 1546516800000 | 2019-01-03, 12:00                 |\n+---------------+-----------------------------------+\nSELECT 3 rows in set (... sec)\n\npg_catalog.generate_subscripts(array, dim, [reverse])\n\nGenerate the subscripts for the specified dimension dim of the given array. Zero rows are returned for arrays that do not have the requested dimension, or for NULL arrays (but valid subscripts are returned for NULL array elements).\n\nIf reverse is true the subscripts will be returned in reverse order.\n\nThis example takes a one dimensional array of four elements, where elements at positions 1 and 3 are NULL:\n\ncr> SELECT generate_subscripts([NULL, 1, NULL, 2], 1) AS s;\n+---+\n| s |\n+---+\n| 1 |\n| 2 |\n| 3 |\n| 4 |\n+---+\nSELECT 4 rows in set (... sec)\n\n\nThis example returns the reversed list of subscripts for the same array:\n\ncr> SELECT generate_subscripts([NULL, 1, NULL, 2], 1, true) AS s;\n+---+\n| s |\n+---+\n| 4 |\n| 3 |\n| 2 |\n| 1 |\n+---+\nSELECT 4 rows in set (... sec)\n\n\nThis example works on an array of three dimensions. Each of the elements within a given level must be either NULL, or an array of the same size as the other arrays within the same level.\n\ncr> select generate_subscripts([[[1],[2]], [[3],[4]], [[4],[5]]], 2) as s;\n+---+\n| s |\n+---+\n| 1 |\n| 2 |\n+---+\nSELECT 2 rows in set (... sec)\n\nregexp_matches(source, pattern [, flags])\n\nUses the regular expression pattern to match against the source string.\n\nThe result rows have one column:\n\nColumn name\n\n\t\n\nDescription\n\n\n\n\ngroups\n\n\t\n\narray(text)\n\nIf pattern matches source, an array of the matched regular expression groups is returned.\n\nIf no regular expression group was used, the whole pattern is used as a group.\n\nA regular expression group is formed by a subexpression that is surrounded by parentheses. The position of a group is determined by the position of its opening parenthesis.\n\nFor example when matching the pattern \\b([A-Z]) a match for the subexpression ([A-Z]) would create group No. 1. If you want to group items with parentheses, but without grouping, use (?...).\n\nFor example matching the regular expression ([Aa](.+)z) against alcatraz, results in these groups:\n\ngroup 1: alcatraz (from first to last parenthesis or whole pattern)\n\ngroup 2: lcatra (beginning at second parenthesis)\n\nThe regexp_matches function will return all groups as a text array:\n\ncr> select regexp_matches('alcatraz', '(a(.+)z)') as matched;\n+------------------------+\n| matched                |\n+------------------------+\n| [\"alcatraz\", \"lcatra\"] |\n+------------------------+\nSELECT 1 row in set (... sec)\n\ncr> select regexp_matches('alcatraz', 'traz') as matched;\n+----------+\n| matched  |\n+----------+\n| [\"traz\"] |\n+----------+\nSELECT 1 row in set (... sec)\n\n\nThrough array element access functionality, a group can be selected directly. See Arrays within objects for details.\n\ncr> select regexp_matches('alcatraz', '(a(.+)z)')[2] as second_group;\n+--------------+\n| second_group |\n+--------------+\n| lcatra       |\n+--------------+\nSELECT 1 row in set (... sec)\n\nFlags\n\nThis function takes a number of flags as optional third parameter. These flags are given as a string containing any of the characters listed below. Order does not matter.\n\nFlag\n\n\t\n\nDescription\n\n\n\n\ni\n\n\t\n\nenable case insensitive matching\n\n\n\n\nu\n\n\t\n\nenable unicode case folding when used together with i\n\n\n\n\nU\n\n\t\n\nenable unicode support for character classes like \\W\n\n\n\n\ns\n\n\t\n\nmake . match line terminators, too\n\n\n\n\nm\n\n\t\n\nmake ^ and $ match on the beginning or end of a line too.\n\n\n\n\nx\n\n\t\n\npermit whitespace and line comments starting with #\n\n\n\n\nd\n\n\t\n\nonly \\n is considered a line-terminator when using ^, $ and .\n\n\n\n\ng\n\n\t\n\nkeep matching until the end of source, instead of stopping at the first match.\n\nExamples\n\nIn this example the pattern does not match anything in the source and the result is an empty table:\n\ncr> select regexp_matches('foobar', '^(a(.+)z)$') as matched;\n+---------+\n| matched |\n+---------+\n+---------+\nSELECT 0 rows in set (... sec)\n\n\nIn this example we find the term that follows two digits:\n\ncr> select regexp_matches('99 bottles of beer on the wall', '\\d{2}\\s(\\w+).*', 'ixU')\n... as matched;\n+-------------+\n| matched     |\n+-------------+\n| [\"bottles\"] |\n+-------------+\nSELECT 1 row in set (... sec)\n\n\nThis example shows the use of flag g, splitting source into a set of arrays, each containing two entries:\n\ncr>  select regexp_matches('#abc #def #ghi #jkl', '(#[^\\s]*) (#[^\\s]*)', 'g') as matched;\n+------------------+\n| matched          |\n+------------------+\n| [\"#abc\", \"#def\"] |\n| [\"#ghi\", \"#jkl\"] |\n+------------------+\nSELECT 2 rows in set (... sec)\n\npg_catalog.pg_get_keywords()\n\nReturns a list of SQL keywords and their categories.\n\nThe result rows have three columns:\n\nColumn name\n\n\t\n\nDescription\n\n\n\n\nword\n\n\t\n\nThe SQL keyword\n\n\n\n\ncatcode\n\n\t\n\nCode for the category (R for reserved keywords, U for unreserved keywords)\n\n\n\n\ncatdesc\n\n\t\n\nThe description of the category\n\ncr> SELECT * FROM pg_catalog.pg_get_keywords() ORDER BY 1 LIMIT 4;\n+----------+---------+------------+\n| word     | catcode | catdesc    |\n+----------+---------+------------+\n| absolute | U       | unreserved |\n| add      | R       | reserved   |\n| alias    | U       | unreserved |\n| all      | R       | reserved   |\n+----------+---------+------------+\nSELECT 4 rows in set (... sec)\n\ninformation_schema._pg_expandarray(array)\n\nTakes an array and returns a set of value and an index into the array.\n\nColumn name\n\n\t\n\nDescription\n\n\n\n\nx\n\n\t\n\nValue within the array\n\n\n\n\nn\n\n\t\n\nIndex of the value within the array\n\ncr> SELECT information_schema._pg_expandarray(ARRAY['a', 'b']) AS result;\n+----------+\n| result   |\n+----------+\n| [\"a\", 1] |\n| [\"b\", 2] |\n+----------+\nSELECT 2 rows in set (... sec)\n\ncr> SELECT * from information_schema._pg_expandarray(ARRAY['a', 'b']);\n+---+---+\n| x | n |\n+---+---+\n| a | 1 |\n| b | 2 |\n+---+---+\nSELECT 2 rows in set (... sec)\n"
  },
  {
    "title": "Array comparisons — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/builtins/array-comparisons.html",
    "html": "5.6\nArray comparisons\n\nAn array comparison operator test the relationship between a value and an array and return true, false, or NULL.\n\nSee Also\n\nSubquery expressions\n\nTable of contents\n\nIN (value [, ...])\n\nANY/SOME (array expression)\n\nALL (array_expression)\n\nIN (value [, ...])\n\nSyntax:\n\nexpression IN (value [, ...])\n\n\nThe IN operator returns true if the left-hand matches at least one value contained within the right-hand side.\n\nThe operator returns NULL if:\n\nThe left-hand expression evaluates to NULL\n\nThere are no matching right-hand values and at least one right-hand value is NULL\n\nHere’s an example:\n\ncr> SELECT\n...   1 in (1, 2, 3) AS a,\n...   4 in (1, 2, 3) AS b,\n...   5 in (1, 2, null) as c;\n+------+-------+------+\n| a    | b     | c    |\n+------+-------+------+\n| TRUE | FALSE | NULL |\n+------+-------+------+\nSELECT 1 row in set (... sec)\n\nANY/SOME (array expression)\n\nSyntax:\n\nexpression <comparison> ANY | SOME (array_expression)\n\n\nHere, <comparison> can be any basic comparison operator.\n\nAn example:\n\ncr> SELECT\n...   1 = ANY ([1,2,3]) AS a,\n...   4 = ANY ([1,2,3]) AS b;\n+------+-------+\n| a    | b     |\n+------+-------+\n| TRUE | FALSE |\n+------+-------+\nSELECT 1 row in set (... sec)\n\n\nThe ANY operator returns true if the defined comparison is true for any of the values in the right-hand array expression.\n\nIf the right side is a multi-dimension array it is automatically unnested to the required dimension.\n\nAn example:\n\ncr> SELECT\n...   4 = ANY ([[1, 2], [3, 4]]) as a,\n...   5 = ANY ([[1, 2], [3, 4]]) as b,\n...   [1, 2] = ANY ([[1,2], [3, 4]]) as c,\n...   [1, 3] = ANY ([[1,2], [3, 4]]) as d;\n+------+-------+------+-------+\n| a    | b     | c    | d     |\n+------+-------+------+-------+\n| TRUE | FALSE | TRUE | FALSE |\n+------+-------+------+-------+\nSELECT 1 row in set (... sec)\n\n\nThe operator returns false if the comparison returns false for all right-hand values or if there are no right-hand values.\n\nThe operator returns NULL if:\n\nThe left-hand expression evaluates to NULL\n\nThere are no matching right-hand values and at least one right-hand value is NULL\n\nTip\n\nWhen doing NOT <value> = ANY(<array_col>), query performance may be degraded because special handling is required to implement the 3-valued logic. To achieve better performance, consider using the ignore3vl function.\n\nALL (array_expression)\n\nSyntax:\n\nvalue comparison ALL (array_expression)\n\n\nHere, comparison can be any basic comparison operator. Objects and arrays of objects are not supported for either operand.\n\nHere’s an example:\n\ncr> SELECT 1 <> ALL(ARRAY[2, 3, 4]) AS x;\n+------+\n| x    |\n+------+\n| TRUE |\n+------+\nSELECT 1 row in set (... sec)\n\n\nThe ALL operator returns true if the defined comparison is true for all values in the right-hand array expression.\n\nThe operator returns false if the comparison returns false for all right-hand values.\n\nThe operator returns NULL if:\n\nThe left-hand expression evaluates to NULL\n\nNo comparison returns false and at least one right-hand value is NULL"
  },
  {
    "title": "Bit operators — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/builtins/bit-operators.html",
    "html": "5.6\nBit operators\n\nBit operators perform bitwise operations on numeric integral values and bit strings:\n\nOperator\n\n\t\n\nDescription\n\n\n\n\n&\n\n\t\n\nBitwise AND of operands.\n\n\n\n\n|\n\n\t\n\nBitwise OR of operands.\n\n\n\n\n#\n\n\t\n\nBitwise XOR of operands.\n\nHere’s an example that uses all of the available bit operators:\n\ncr> select 1 & 2 | 3 # 4 AS n;\n+---+\n| n |\n+---+\n| 7 |\n+---+\nSELECT 1 row in set (... sec)\n\n\nAnd an example with bit strings:\n\ncr> select B'101' # B'011' AS n;\n+--------+\n| n      |\n+--------+\n| B'110' |\n+--------+\nSELECT 1 row in set (... sec)\n\n\nWhen applied to numeric operands, bit operators always return the data type of the argument with the higher precision.\n\nIf at least one operand is NULL, bit operators return NULL.\n\nWhen applied to BIT strings, operands must have equal length.\n\nNote\n\nBit operators have the same precedence and evaluated from left to right. Use parentheses if you want to ensure a specific order of evaluation."
  },
  {
    "title": "Aggregation — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/builtins/aggregation.html",
    "html": "5.6\nAggregation\n\nWhen selecting data from CrateDB, you can use an aggregate function to calculate a single summary value for one or more columns.\n\nFor example:\n\ncr> SELECT count(*) FROM locations;\n+----------+\n| count(*) |\n+----------+\n|       13 |\n+----------+\nSELECT 1 row in set (... sec)\n\n\nHere, the count(*) function computes the result across all rows.\n\nAggregate functions can be used with the GROUP BY clause. When used like this, an aggregate function returns a single summary value for each grouped collection of column values.\n\nFor example:\n\ncr> SELECT kind, count(*) FROM locations GROUP BY kind;\n+-------------+----------+\n| kind        | count(*) |\n+-------------+----------+\n| Galaxy      |        4 |\n| Star System |        4 |\n| Planet      |        5 |\n+-------------+----------+\nSELECT 3 rows in set (... sec)\n\n\nTip\n\nAggregation works across all the rows that match a query or on all matching rows in every distinct group of a GROUP BY statement. Aggregating SELECT statements without GROUP BY will always return one row.\n\nTable of contents\n\nAggregate expressions\n\nAggregate functions\n\narbitrary(column)\n\nany_value(column)\n\narray_agg(column)\n\navg(column)\n\navg(DISTINCT column)\n\ncount(column)\n\ncount(DISTINCT column)\n\ncount(*)\n\ngeometric_mean(column)\n\nhyperloglog_distinct(column, [precision])\n\nmean(column)\n\nmin(column)\n\nmax(column)\n\nmax_by(returnField, searchField)\n\nmin_by(returnField, searchField)\n\nstddev(column)\n\nstring_agg(column, delimiter)\n\npercentile(column, {fraction | fractions})\n\nsum(column)\n\nvariance(column)\n\nLimitations\n\nAggregate expressions\n\nAn aggregate expression represents the application of an aggregate function across rows selected by a query. Besides the function signature, expressions might contain supplementary clauses and keywords.\n\nThe synopsis of an aggregate expression is one of the following:\n\naggregate_function ( * ) [ FILTER ( WHERE condition ) ]\naggregate_function ( [ DISTINCT ] expression [ , ... ] ) [ FILTER ( WHERE condition ) ]\n\n\nHere, aggregate_function is a name of an aggregate function and expression is a column reference, scalar function or literal.\n\nIf FILTER is specified, then only the rows that met the WHERE clause condition are supplied to the aggregate function.\n\nThe optional DISTINCT keyword is only supported by aggregate functions that explicitly mention its support. Please refer to existing limitations for further information.\n\nThe aggregate expression form that uses a wildcard instead of an expression as a function argument is supported only by the count(*) aggregate function.\n\nAggregate functions\narbitrary(column)\n\nThe arbitrary aggregate function returns a single value of a column. Which value it returns is not defined.\n\nIts return type is the type of its parameter column and can be NULL if the column contains NULL values.\n\nExample:\n\ncr> select arbitrary(position) from locations;\n+---------------------+\n| arbitrary(position) |\n+---------------------+\n| ...                 |\n+---------------------+\nSELECT 1 row in set (... sec)\n\ncr> select arbitrary(name), kind from locations\n... where name != ''\n... group by kind order by kind desc;\n+-...-------------+-------------+\n| arbitrary(name) | kind        |\n+-...-------------+-------------+\n| ...             | Star System |\n| ...             | Planet      |\n| ...             | Galaxy      |\n+-...-------------+-------------+\nSELECT 3 rows in set (... sec)\n\n\nAn example use case is to group a table with many rows per user by user_id and get the username for every group, that means every user. This works as rows with same user_id have the same username. This method performs better than grouping on username as grouping on number types is generally faster than on strings. The advantage is that the arbitrary function does very little to no computation as for example max aggregate function would do.\n\nany_value(column)\n\nany_value is an alias for arbitrary.\n\nExample:\n\ncr> select any_value(x) from unnest([1, 1]) t (x);\n+--------------+\n| any_value(x) |\n+--------------+\n| 1            |\n+--------------+\nSELECT 1 row in set (... sec)\n\narray_agg(column)\n\nThe array_agg aggregate function concatenates all input values into an array.\n\ncr> SELECT array_agg(x) FROM (VALUES (42), (832), (null), (17)) as t (x);\n+---------------------+\n| array_agg(x)        |\n+---------------------+\n| [42, 832, null, 17] |\n+---------------------+\nSELECT 1 row in set (... sec)\n\n\nSee Also\n\nstring_agg(column, delimiter)\n\navg(column)\n\nThe avg and mean aggregate function returns the arithmetic mean, the average, of all values in a column that are not NULL. It accepts all numeric, timestamp and interval types as single argument. For numeric argument type the return type is numeric, for interval argument type the return type is interval and for other argument type the return type is double.\n\nExample:\n\ncr> select avg(position), kind from locations\n... group by kind order by kind;\n+---------------+-------------+\n| avg(position) | kind        |\n+---------------+-------------+\n| 3.25          | Galaxy      |\n| 3.0           | Planet      |\n| 2.5           | Star System |\n+---------------+-------------+\nSELECT 3 rows in set (... sec)\n\n\nThe avg aggregation on the bigint column might result in a precision error if sum of elements exceeds 2^53:\n\ncr> select avg(t.val) from\n... (select unnest([9223372036854775807, 9223372036854775807]) as val) t;\n+-----------------------+\n|              avg(val) |\n+-----------------------+\n| 9.223372036854776e+18 |\n+-----------------------+\nSELECT 1 row in set (... sec)\n\n\nTo address the precision error of the avg aggregation, we cast the aggregation column to the numeric data type:\n\ncr> select avg(t.val :: numeric) from\n... (select unnest([9223372036854775807, 9223372036854775807]) as val) t;\n+---------------------------+\n| avg(cast(val AS numeric)) |\n+---------------------------+\n|       9223372036854775807 |\n+---------------------------+\nSELECT 1 row in set (... sec)\n\navg(DISTINCT column)\n\nThe avg aggregate function also supports the distinct keyword. This keyword changes the behaviour of the function so that it will only average the number of distinct values in this column that are not NULL:\n\ncr> select\n...   avg(distinct position) AS avg_pos,\n...   count(*),\n...   date\n... from locations group by date\n... order by 1 desc, count(*) desc;\n+---------+----------+---------------+\n| avg_pos | count(*) |          date |\n+---------+----------+---------------+\n|     4.0 |        1 | 1367366400000 |\n|     3.6 |        8 | 1373932800000 |\n|     2.0 |        4 |  308534400000 |\n+---------+----------+---------------+\nSELECT 3 rows in set (... sec)\n\ncr> select avg(distinct position) AS avg_pos from locations;\n+---------+\n| avg_pos |\n+---------+\n|     3.5 |\n+---------+\nSELECT 1 row in set (... sec)\n\ncount(column)\n\nIn contrast to the count(*) function the count function used with a column name as parameter will return the number of rows with a non-NULL value in that column.\n\nExample:\n\ncr> select count(name), count(*), date from locations group by date\n... order by count(name) desc, count(*) desc;\n+-------------+----------+---------------+\n| count(name) | count(*) | date          |\n+-------------+----------+---------------+\n| 7           | 8        | 1373932800000 |\n| 4           | 4        | 308534400000  |\n| 1           | 1        | 1367366400000 |\n+-------------+----------+---------------+\nSELECT 3 rows in set (... sec)\n\ncount(DISTINCT column)\n\nThe count aggregate function also supports the distinct keyword. This keyword changes the behaviour of the function so that it will only count the number of distinct values in this column that are not NULL:\n\ncr> select\n...   count(distinct kind) AS num_kind,\n...   count(*),\n...   date\n... from locations group by date\n... order by num_kind, count(*) desc;\n+----------+----------+---------------+\n| num_kind | count(*) |          date |\n+----------+----------+---------------+\n|        1 |        1 | 1367366400000 |\n|        3 |        8 | 1373932800000 |\n|        3 |        4 |  308534400000 |\n+----------+----------+---------------+\nSELECT 3 rows in set (... sec)\n\ncr> select count(distinct kind) AS num_kind from locations;\n+----------+\n| num_kind |\n+----------+\n|        3 |\n+----------+\nSELECT 1 row in set (... sec)\n\ncount(*)\n\nThis aggregate function simply returns the number of rows that match the query.\n\ncount(columName) is also possible, but currently only works on a primary key column. The semantics are the same.\n\nThe return value is always of type bigint.\n\ncr> select count(*) from locations;\n+----------+\n| count(*) |\n+----------+\n| 13       |\n+----------+\nSELECT 1 row in set (... sec)\n\n\ncount(*) can also be used on group by queries:\n\ncr> select count(*), kind from locations group by kind order by kind asc;\n+----------+-------------+\n| count(*) | kind        |\n+----------+-------------+\n| 4        | Galaxy      |\n| 5        | Planet      |\n| 4        | Star System |\n+----------+-------------+\nSELECT 3 rows in set (... sec)\n\ngeometric_mean(column)\n\nThe geometric_mean aggregate function computes the geometric mean, a mean for positive numbers. For details see: Geometric Mean.\n\ngeometric mean is defined on all numeric types and on timestamp. It always returns double values. If a value is negative, all values were null or we got no value at all NULL is returned. If any of the aggregated values is 0 the result will be 0.0 as well.\n\nCaution\n\nDue to java double precision arithmetic it is possible that any two executions of the aggregate function on the same data produce slightly differing results.\n\nExample:\n\ncr> select geometric_mean(position), kind from locations\n... group by kind order by kind;\n+--------------------------+-------------+\n| geometric_mean(position) | kind        |\n+--------------------------+-------------+\n|       2.6321480259049848 | Galaxy      |\n|       2.6051710846973517 | Planet      |\n|       2.213363839400643  | Star System |\n+--------------------------+-------------+\nSELECT 3 rows in set (... sec)\n\nhyperloglog_distinct(column, [precision])\n\nThe hyperloglog_distinct aggregate function calculates an approximate count of distinct non-null values using the HyperLogLog++ algorithm.\n\nThe return value data type is always a bigint.\n\nThe first argument can be a reference to a column of all Primitive types. Container types and Geographic types are not supported.\n\nThe optional second argument defines the used precision for the HyperLogLog++ algorithm. This allows to trade memory for accuracy, valid values are 4 to 18. A precision of 4 uses approximately 16 bytes of memory. Each increase in precision doubles the memory requirement. So precision 5 uses approximately 32 bytes, up to 262144 bytes for precision 18.\n\nThe default value for the precision which is used if the second argument is left out is 14.\n\nExamples:\n\ncr> select hyperloglog_distinct(position) from locations;\n+--------------------------------+\n| hyperloglog_distinct(position) |\n+--------------------------------+\n| 6                              |\n+--------------------------------+\nSELECT 1 row in set (... sec)\n\ncr> select hyperloglog_distinct(position, 4) from locations;\n+-----------------------------------+\n| hyperloglog_distinct(position, 4) |\n+-----------------------------------+\n| 6                                 |\n+-----------------------------------+\nSELECT 1 row in set (... sec)\n\nmean(column)\n\nAn alias for avg(column).\n\nmin(column)\n\nThe min aggregate function returns the smallest value in a column that is not NULL. Its single argument is a column name and its return value is always of the type of that column.\n\nExample:\n\ncr> select min(position), kind\n... from locations\n... where name not like 'North %'\n... group by kind order by min(position) asc, kind asc;\n+---------------+-------------+\n| min(position) | kind        |\n+---------------+-------------+\n| 1             | Planet      |\n| 1             | Star System |\n| 2             | Galaxy      |\n+---------------+-------------+\nSELECT 3 rows in set (... sec)\n\ncr> select min(date) from locations;\n+--------------+\n| min(date)    |\n+--------------+\n| 308534400000 |\n+--------------+\nSELECT 1 row in set (... sec)\n\n\nmin returns NULL if the column does not contain any value but NULL. It is allowed on columns with primitive data types. On text columns it will return the lexicographically smallest.\n\ncr> select min(name), kind from locations\n... group by kind order by kind asc;\n+------------------------------------+-------------+\n| min(name)                          | kind        |\n+------------------------------------+-------------+\n| Galactic Sector QQ7 Active J Gamma | Galaxy      |\n|                                    | Planet      |\n| Aldebaran                          | Star System |\n+------------------------------------+-------------+\nSELECT 3 rows in set (... sec)\n\nmax(column)\n\nIt behaves exactly like min but returns the biggest value in a column that is not NULL.\n\nSome Examples:\n\ncr> select max(position), kind from locations\n... group by kind order by kind desc;\n+---------------+-------------+\n| max(position) | kind        |\n+---------------+-------------+\n| 4             | Star System |\n| 5             | Planet      |\n| 6             | Galaxy      |\n+---------------+-------------+\nSELECT 3 rows in set (... sec)\n\ncr> select max(position) from locations;\n+---------------+\n| max(position) |\n+---------------+\n| 6             |\n+---------------+\nSELECT 1 row in set (... sec)\n\ncr> select max(name), kind from locations\n... group by kind order by max(name) desc;\n+-------------------+-------------+\n| max(name)         | kind        |\n+-------------------+-------------+\n| Outer Eastern Rim | Galaxy      |\n| Bartledan         | Planet      |\n| Altair            | Star System |\n+-------------------+-------------+\nSELECT 3 rows in set (... sec)\n\nmax_by(returnField, searchField)\n\nReturns the value of returnField where searchField has the highest value.\n\nIf there are ties for searchField the result is non-deterministic and can be any of the returnField values of the ties.\n\nNULL values in the searchField don’t count as max but are skipped.\n\nAn Example:\n\ncr> SELECT max_by(mountain, height) FROM sys.summits;\n+--------------------------+\n| max_by(mountain, height) |\n+--------------------------+\n| Mont Blanc               |\n+--------------------------+\nSELECT 1 row in set (... sec)\n\nmin_by(returnField, searchField)\n\nReturns the value of returnField where searchField has the lowest value.\n\nIf there are ties for searchField the result is non-deterministic and can be any of the returnField values of the ties.\n\nNULL values in the searchField don’t count as min but are skipped.\n\nAn Example:\n\ncr> SELECT min_by(mountain, height) FROM sys.summits;\n+--------------------------+\n| min_by(mountain, height) |\n+--------------------------+\n| Puy de Rent              |\n+--------------------------+\nSELECT 1 row in set (... sec)\n\nstddev(column)\n\nThe stddev aggregate function computes the Standard Deviation of the set of non-null values in a column. It is a measure of the variation of data values. A low standard deviation indicates that the values tend to be near the mean.\n\nstddev is defined on all numeric types and on timestamp. It always returns double precision values. If all values were null or we got no value at all NULL is returned.\n\nExample:\n\ncr> select stddev(position), kind from locations\n... group by kind order by kind;\n+--------------------+-------------+\n|   stddev(position) | kind        |\n+--------------------+-------------+\n| 1.920286436967152  | Galaxy      |\n| 1.4142135623730951 | Planet      |\n| 1.118033988749895  | Star System |\n+--------------------+-------------+\nSELECT 3 rows in set (... sec)\n\n\nCaution\n\nDue to java double precision arithmetic it is possible that any two executions of the aggregate function on the same data produce slightly differing results.\n\nstring_agg(column, delimiter)\n\nThe string_agg aggregate function concatenates the input values into a string, where each value is separated by a delimiter.\n\nIf all input values are null, null is returned as a result.\n\ncr> select string_agg(col1, ', ') from (values('a'), ('b'), ('c')) as t;\n+------------------------+\n| string_agg(col1, ', ') |\n+------------------------+\n| a, b, c                |\n+------------------------+\nSELECT 1 row in set (... sec)\n\n\nSee Also\n\narray_agg(column)\n\npercentile(column, {fraction | fractions})\n\nThe percentile aggregate function computes a Percentile over numeric non-null values in a column.\n\nPercentiles show the point at which a certain percentage of observed values occur. For example, the 98th percentile is the value which is greater than 98% of the observed values. The result is defined and computed as an interpolated weighted average. According to that it allows the median of the input data to be defined conveniently as the 50th percentile.\n\nThe function expects a single fraction or an array of fractions and a column name. Independent of the input column data type the result of percentile always returns a double precision. If the value at the specified column is null the row is ignored. Fractions must be double precision values between 0 and 1. When supplied a single fraction, the function will return a single value corresponding to the percentile of the specified fraction:\n\ncr> select percentile(position, 0.95), kind from locations\n... group by kind order by kind;\n+----------------------------+-------------+\n| percentile(position, 0.95) | kind        |\n+----------------------------+-------------+\n|                        6.0 | Galaxy      |\n|                        5.0 | Planet      |\n|                        4.0 | Star System |\n+----------------------------+-------------+\nSELECT 3 rows in set (... sec)\n\n\nWhen supplied an array of fractions, the function will return an array of values corresponding to the percentile of each fraction specified:\n\ncr> select percentile(position, [0.0013, 0.9987]) as perc from locations;\n+------------+\n| perc       |\n+------------+\n| [1.0, 6.0] |\n+------------+\nSELECT 1 row in set (... sec)\n\n\nWhen a query with percentile function won’t match any rows then a null result is returned.\n\nTo be able to calculate percentiles over a huge amount of data and to scale out CrateDB calculates approximate instead of accurate percentiles. The algorithm used by the percentile metric is called TDigest. The accuracy/size trade-off of the algorithm is defined by a single compression parameter which has a constant value of 100. However, there are a few guidelines to keep in mind in this implementation:\n\nExtreme percentiles (e.g. 99%) are more accurate.\n\nFor small sets, percentiles are highly accurate.\n\nIt is difficult to generalize the exact level of accuracy, as it depends on your data distribution and volume of data being aggregated.\n\nsum(column)\n\nReturns the sum of a set of numeric input values that are not NULL. Depending on the argument type a suitable return type is chosen. For interval argument types the return type is interval. For real and double precision argument types the return type is equal to the argument type. For byte, smallint, integer and bigint the return type changes to bigint. If the range of bigint values (-2^64 to 2^64-1) gets exceeded an ArithmeticException will be raised.\n\ncr> select sum(position), kind from locations\n... group by kind order by sum(position) asc;\n+---------------+-------------+\n| sum(position) | kind        |\n+---------------+-------------+\n| 10            | Star System |\n| 13            | Galaxy      |\n| 15            | Planet      |\n+---------------+-------------+\nSELECT 3 rows in set (... sec)\n\ncr> select sum(position) as position_sum from locations;\n+--------------+\n| position_sum |\n+--------------+\n| 38           |\n+--------------+\nSELECT 1 row in set (... sec)\n\ncr> select sum(name), kind from locations group by kind order by sum(name) desc;\nSQLParseException[Cannot cast value `North West Ripple` to type `byte`]\n\n\nIf the sum aggregation on a numeric data type with the fixed length can potentially exceed its range it is possible to handle the overflow by casting the function argument to the numeric type with an arbitrary precision.\n\nThe sum aggregation on the bigint column will result in an overflow in the following aggregation query:\n\ncr> SELECT sum(count)\n... FROM uservisits;\nArithmeticException[long overflow]\n\n\nTo address the overflow of the sum aggregation on the given field, we cast the aggregation column to the numeric data type:\n\ncr> SELECT sum(count::numeric)\n... FROM uservisits;\n+-----------------------------+\n| sum(cast(count AS numeric)) |\n+-----------------------------+\n|         9223372036854775817 |\n+-----------------------------+\nSELECT 1 row in set (... sec)\n\nvariance(column)\n\nThe variance aggregate function computes the Variance of the set of non-null values in a column. It is a measure about how far a set of numbers is spread. A variance of 0.0 indicates that all values are the same.\n\nvariance is defined on all numeric types and on timestamp. It returns a double precision value. If all values were null or we got no value at all NULL is returned.\n\nExample:\n\ncr> select variance(position), kind from locations\n... group by kind order by kind desc;\n+--------------------+-------------+\n| variance(position) | kind        |\n+--------------------+-------------+\n|             1.25   | Star System |\n|             2.0    | Planet      |\n|             3.6875 | Galaxy      |\n+--------------------+-------------+\nSELECT 3 rows in set (... sec)\n\n\nCaution\n\nDue to java double precision arithmetic it is possible that any two executions of the aggregate function on the same data produce slightly differing results.\n\nLimitations\n\nDISTINCT is not supported with aggregations on Joins.\n\nAggregate functions can only be applied to columns with a plain index, which is the default for all primitive type columns."
  },
  {
    "title": "Arithmetic operators — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/builtins/arithmetic.html",
    "html": "5.6\nArithmetic operators\n\nArithmetic operators perform mathematical operations on numeric values (including timestamps):\n\nOperator\n\n\t\n\nDescription\n\n\n\n\n+\n\n\t\n\nAdd one number to another\n\n\n\n\n-\n\n\t\n\nSubtract the second number from the first\n\n\n\n\n*\n\n\t\n\nMultiply the first number with the second\n\n\n\n\n/\n\n\t\n\nDivide the first number by the second\n\n\n\n\n%\n\n\t\n\nFinds the remainder of division of one number by another\n\nHere’s an example that uses all of the available arithmetic operators:\n\ncr> select ((2 * 4.0 - 2 + 1) / 2) % 3 AS n;\n+-----+\n|   n |\n+-----+\n| 0.5 |\n+-----+\nSELECT 1 row in set (... sec)\n\n\nArithmetic operators always return the data type of the argument with the higher precision.\n\nIn the case of division, if both arguments are integers, the result will also be an integer with the fractional part truncated:\n\ncr> select 5 / 2 AS a,  5 / 2.0 AS b;\n+---+-----+\n| a |   b |\n+---+-----+\n| 2 | 2.5 |\n+---+-----+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nThe same restrictions that apply to scalar functions also apply to arithmetic operators."
  },
  {
    "title": "Scalar functions — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/builtins/scalar-functions.html",
    "html": "5.6\nScalar functions\n\nScalar functions are functions that return scalars.\n\nTable of contents\n\nString functions\n\nconcat('first_arg', second_arg, [ parameter , ... ])\n\nconcat_ws('separator', second_arg, [ parameter , ... ])\n\nformat('format_string', parameter, [ parameter , ... ])\n\nsubstr('string', from, [ count ])\n\nsubstr('string' FROM 'pattern')\n\nsubstring(...)\n\nchar_length('string')\n\nlength(text)\n\nbit_length('string')\n\noctet_length('string')\n\nascii(string)\n\nchr(int)\n\nlower('string')\n\nupper('string')\n\ninitcap('string')\n\nsha1('string')\n\nmd5('string')\n\nreplace(text, from, to)\n\ntranslate(string, from, to)\n\ntrim({LEADING | TRAILING | BOTH} 'str_arg_1' FROM 'str_arg_2')\n\nltrim(text, [ trimmingText ])\n\nrtrim(text, [ trimmingText ])\n\nbtrim(text, [ trimmingText ])\n\nquote_ident(text)\n\nleft('string', len)\n\nright('string', len)\n\nlpad('string1', len[, 'string2'])\n\nrpad('string1', len[, 'string2'])\n\nencode(bytea, format)\n\ndecode(text, format)\n\nrepeat(text, integer)\n\nsplit_part(text, text, integer)\n\nparse_uri(text)\n\nparse_url(text)\n\nDate and time functions\n\ndate_trunc('interval', ['timezone',] timestamp)\n\ndate_bin(interval, timestamp, origin)\n\nextract(field from source)\n\nCURRENT_TIME\n\nCURRENT_TIMESTAMP\n\nCURDATE()\n\nCURRENT_DATE\n\nnow()\n\ndate_format([format_string, [timezone,]] timestamp)\n\ntimezone(timezone, timestamp)\n\nto_char(expression, format_string)\n\nage([timestamp,] timestamp)\n\nGeo functions\n\ndistance(geo_point1, geo_point2)\n\nwithin(shape1, shape2)\n\nintersects(geo_shape, geo_shape)\n\nlatitude(geo_point) and longitude(geo_point)\n\ngeohash(geo_point)\n\narea(geo_shape)\n\nMathematical functions\n\nabs(number)\n\nceil(number)\n\nceiling(number)\n\ndegrees(double precision)\n\nexp(number)\n\nfloor(number)\n\nln(number)\n\nlog(x : number, b : number)\n\nmodulus(y, x)\n\nmod(y, x)\n\npower(a: number, b: number)\n\nradians(double precision)\n\nrandom()\n\ngen_random_text_uuid()\n\nround(number)\n\ntrunc(number[, precision])\n\nsqrt(number)\n\nsin(number)\n\nasin(number)\n\ncos(number)\n\nacos(number)\n\ntan(number)\n\ncot(number)\n\natan(number)\n\natan2(y: number, x: number)\n\npi()\n\nRegular expression functions\n\nregexp_replace(source, pattern, replacement [, flags])\n\nFlags\n\nExamples\n\nArray functions\n\narray_append(anyarray, value)\n\narray_cat(first_array, second_array)\n\narray_unique(first_array, [ second_array])\n\narray_difference(first_array, second_array)\n\narray(subquery)\n\narray_upper(anyarray, dimension)\n\narray_length(anyarray, dimension)\n\narray_lower(anyarray, dimension)\n\narray_set(array, index, value)\n\narray_set(source_array, indexes_array, values_array)\n\narray_slice(anyarray, from, to)\n\narray_to_string(anyarray, separator, [ null_string ])\n\nstring_to_array(string, separator, [ null_string ])\n\nseparator\n\nnull_string\n\narray_min(array)\n\narray_position(anycompatiblearray, anycompatible [, integer ] ) → integer\n\narray_max(array)\n\narray_sum(array)\n\narray_avg(array)\n\narray_unnest(nested_array)\n\nObject functions\n\nobject_keys(object)\n\nconcat(object, object)\n\nnull_or_empty(object)\n\nConditional functions and expressions\n\nCASE WHEN ... THEN ... END\n\nif(condition, result [, default])\n\ncoalesce('first_arg', second_arg [, ... ])\n\ngreatest('first_arg', second_arg[ , ... ])\n\nleast('first_arg', second_arg[ , ... ])\n\nnullif('first_arg', second_arg)\n\nSystem information functions\n\nCURRENT_SCHEMA\n\nCURRENT_SCHEMAS(boolean)\n\nCURRENT_USER\n\nUSER\n\nSESSION_USER\n\nhas_database_privilege([user,] database, privilege text)\n\nhas_schema_privilege([user,] schema, privilege text)\n\npg_backend_pid()\n\npg_postmaster_start_time()\n\ncurrent_database()\n\ncurrent_setting(text [,boolean])\n\npg_get_expr()\n\npg_get_partkeydef()\n\npg_get_serial_sequence()\n\npg_encoding_to_char()\n\npg_get_userbyid()\n\npg_typeof()\n\npg_function_is_visible()\n\npg_get_function_result()\n\nversion()\n\ncol_description(integer, integer)\n\nobj_description(integer, text)\n\nformat_type(integer, integer)\n\nSpecial functions\n\nknn_match(float_vector, float_vector, int)\n\nignore3vl(boolean)\n\nString functions\nconcat('first_arg', second_arg, [ parameter , ... ])\n\nConcatenates a variable number of arguments into a single string. It ignores NULL values.\n\nReturns: text\n\ncr> select concat('foo', null, 'bar') AS col;\n+--------+\n| col    |\n+--------+\n| foobar |\n+--------+\nSELECT 1 row in set (... sec)\n\n\nYou can also use the || operator:\n\ncr> select 'foo' || 'bar' AS col;\n+--------+\n| col    |\n+--------+\n| foobar |\n+--------+\nSELECT 1 row in set (... sec)\n\n\nTip\n\nThe concat function can also be used for merging objects: concat(object, object)\n\nconcat_ws('separator', second_arg, [ parameter , ... ])\n\nConcatenates a variable number of arguments into a single string using a separator defined by the first argument. If first argument is NULL the return value is NULL. Remaining NULL arguments are ignored.\n\nReturns: text\n\ncr> select concat_ws(',','foo', null, 'bar') AS col;\n+---------+\n| col     |\n+---------+\n| foo,bar |\n+---------+\nSELECT 1 row in set (... sec)\n\nformat('format_string', parameter, [ parameter , ... ])\n\nFormats a string similar to the C function printf. For details about the format string syntax, see formatter\n\nReturns: text\n\ncr> select format('%s.%s', schema_name, table_name)  AS fqtable\n... from sys.shards\n... where table_name = 'locations'\n... limit 1;\n+---------------+\n| fqtable       |\n+---------------+\n| doc.locations |\n+---------------+\nSELECT 1 row in set (... sec)\n\ncr> select format('%tY', date) AS year\n... from locations\n... group by format('%tY', date)\n... order by 1;\n+------+\n| year |\n+------+\n| 1979 |\n| 2013 |\n+------+\nSELECT 2 rows in set (... sec)\n\nsubstr('string', from, [ count ])\n\nExtracts a part of a string. from specifies where to start and count the length of the part.\n\nReturns: text\n\ncr> select substr('crate.io', 3, 2) AS substr;\n+--------+\n| substr |\n+--------+\n| at     |\n+--------+\nSELECT 1 row in set (... sec)\n\nsubstr('string' FROM 'pattern')\n\nExtract a part from a string that matches a POSIX regular expression pattern.\n\nReturns: text.\n\nIf the pattern contains groups specified via parentheses it returns the first matching group. If the pattern doesn’t match, the function returns NULL.\n\ncr> SELECT\n...   substring('2023-08-07', '[a-z]') as no_match,\n...   substring('2023-08-07', '\\d{4}-\\d{2}-\\d{2}') as full_date,\n...   substring('2023-08-07', '\\d{4}-(\\d{2})-\\d{2}') as month;\n+----------+------------+-------+\n| no_match | full_date  | month |\n+----------+------------+-------+\n| NULL     | 2023-08-07 |    08 |\n+----------+------------+-------+\nSELECT 1 row in set (... sec)\n\nsubstring(...)\n\nAlias for substr('string', from, [ count ]).\n\nchar_length('string')\n\nCounts the number of characters in a string.\n\nReturns: integer\n\ncr> select char_length('crate.io') AS char_length;\n+-------------+\n| char_length |\n+-------------+\n|           8 |\n+-------------+\nSELECT 1 row in set (... sec)\n\n\nEach character counts only once, regardless of its byte size.\n\ncr> select char_length('©rate.io') AS char_length;\n+-------------+\n| char_length |\n+-------------+\n|           8 |\n+-------------+\nSELECT 1 row in set (... sec)\n\nlength(text)\n\nReturns the number of characters in a string.\n\nThe same as char_length.\n\nbit_length('string')\n\nCounts the number of bits in a string.\n\nReturns: integer\n\nNote\n\nCrateDB uses UTF-8 encoding internally, which uses between 1 and 4 bytes per character.\n\ncr> select bit_length('crate.io') AS bit_length;\n+------------+\n| bit_length |\n+------------+\n|         64 |\n+------------+\nSELECT 1 row in set (... sec)\n\ncr> select bit_length('©rate.io') AS bit_length;\n+------------+\n| bit_length |\n+------------+\n|         72 |\n+------------+\nSELECT 1 row in set (... sec)\n\noctet_length('string')\n\nCounts the number of bytes (octets) in a string.\n\nReturns: integer\n\ncr> select octet_length('crate.io') AS octet_length;\n+--------------+\n| octet_length |\n+--------------+\n|            8 |\n+--------------+\nSELECT 1 row in set (... sec)\n\ncr> select octet_length('©rate.io') AS octet_length;\n+--------------+\n| octet_length |\n+--------------+\n|            9 |\n+--------------+\nSELECT 1 row in set (... sec)\n\nascii(string)\n\nReturns the ASCII code of the first character. For UTF-8, returns the Unicode code point of the characters.\n\nReturns: int\n\ncr> SELECT ascii('a') AS a, ascii('🎈') AS b;\n+----+--------+\n|  a |      b |\n+----+--------+\n| 97 | 127880 |\n+----+--------+\nSELECT 1 row in set (... sec)\n\nchr(int)\n\nReturns the character with the given code. For UTF-8 the argument is treated as a Unicode code point.\n\nReturns: string\n\ncr> SELECT chr(65) AS a;\n+---+\n| a |\n+---+\n| A |\n+---+\nSELECT 1 row in set (... sec)\n\nlower('string')\n\nConverts all characters to lowercase. lower does not perform locale-sensitive or context-sensitive mappings.\n\nReturns: text\n\ncr> select lower('TransformMe') AS lower;\n+-------------+\n| lower       |\n+-------------+\n| transformme |\n+-------------+\nSELECT 1 row in set (... sec)\n\nupper('string')\n\nConverts all characters to uppercase. upper does not perform locale-sensitive or context-sensitive mappings.\n\nReturns: text\n\ncr> select upper('TransformMe') as upper;\n+-------------+\n| upper       |\n+-------------+\n| TRANSFORMME |\n+-------------+\nSELECT 1 row in set (... sec)\n\ninitcap('string')\n\nConverts the first letter of each word to upper case and the rest to lower case (capitalize letters).\n\nReturns: text\n\ncr> select initcap('heLlo WORLD') AS initcap;\n+-------------+\n| initcap     |\n+-------------+\n| Hello World |\n+-------------+\n SELECT 1 row in set (... sec)\n\nsha1('string')\n\nReturns: text\n\nComputes the SHA1 checksum of the given string.\n\ncr> select sha1('foo') AS sha1;\n+------------------------------------------+\n| sha1                                     |\n+------------------------------------------+\n| 0beec7b5ea3f0fdbc95d0dd47f3c5bc275da8a33 |\n+------------------------------------------+\nSELECT 1 row in set (... sec)\n\nmd5('string')\n\nReturns: text\n\nComputes the MD5 checksum of the given string.\n\nSee sha1 for an example.\n\nreplace(text, from, to)\n\nReplaces all occurrences of from in text with to.\n\ncr> select replace('Hello World', 'World', 'Stranger') AS hello;\n+----------------+\n| hello          |\n+----------------+\n| Hello Stranger |\n+----------------+\nSELECT 1 row in set (... sec)\n\ntranslate(string, from, to)\n\nPerforms several single-character, one-to-one translation in one operation. It translates string by replacing the characters in the from set, one-to-one positionally, with their counterparts in the to set. If from is longer than to, the function removes the occurrences of the extra characters in from. If there are repeated characters in from, only the first mapping is considered.\n\nSynopsis:\n\ntranslate(string, from, to)\n\n\nExamples:\n\ncr> select translate('Crate', 'Ct', 'Dk') as translation;\n +-------------+\n | translation |\n +-------------+\n | Drake       |\n +-------------+\n SELECT 1 row in set (... sec)\n\ncr> select translate('Crate', 'rCe', 'c') as translation;\n +-------------+\n | translation |\n +-------------+\n | cat         |\n +-------------+\n SELECT 1 row in set (... sec)\n\ntrim({LEADING | TRAILING | BOTH} 'str_arg_1' FROM 'str_arg_2')\n\nRemoves the longest string containing characters from str_arg_1 (' ' by default) from the start, end, or both ends (BOTH is the default) of str_arg_2.\n\nIf any of the two strings is NULL, the result is NULL.\n\nSynopsis:\n\ntrim([ [ {LEADING | TRAILING | BOTH} ] [ str_arg_1 ] FROM ] str_arg_2)\n\n\nExamples:\n\ncr> select trim(BOTH 'ab' from 'abcba') AS trim;\n+------+\n| trim |\n+------+\n| c    |\n+------+\nSELECT 1 row in set (... sec)\n\ncr> select trim('ab' from 'abcba') AS trim;\n+------+\n| trim |\n+------+\n| c    |\n+------+\nSELECT 1 row in set (... sec)\n\ncr> select trim('   abcba   ') AS trim;\n+-------+\n| trim  |\n+-------+\n| abcba |\n+-------+\nSELECT 1 row in set (... sec)\n\nltrim(text, [ trimmingText ])\n\nRemoves set of characters which are matching trimmingText (' ' by default) to the left of text.\n\nIf any of the arguments is NULL, the result is NULL.\n\ncr> select ltrim('xxxzzzabcba', 'xz') AS ltrim;\n+-------+\n| ltrim |\n+-------+\n| abcba |\n+-------+\nSELECT 1 row in set (... sec)\n\nrtrim(text, [ trimmingText ])\n\nRemoves set of characters which are matching trimmingText (' ' by default) to the right of text.\n\nIf any of the arguments is NULL, the result is NULL.\n\ncr> select rtrim('abcbaxxxzzz', 'xz') AS rtrim;\n+-------+\n| rtrim |\n+-------+\n| abcba |\n+-------+\nSELECT 1 row in set (... sec)\n\nbtrim(text, [ trimmingText ])\n\nA combination of ltrim and rtrim, removing the longest string matching trimmingText from both the start and end of text.\n\nIf any of the arguments is NULL, the result is NULL.\n\ncr> select btrim('XXHelloXX', 'XX') AS btrim;\n+-------+\n| btrim |\n+-------+\n| Hello |\n+-------+\nSELECT 1 row in set (... sec)\n\nquote_ident(text)\n\nReturns: text\n\nQuotes a provided string argument. Quotes are added only if necessary. For example, if the string contains non-identifier characters, keywords, or would be case-folded. Embedded quotes are properly doubled.\n\nThe quoted string can be used as an identifier in an SQL statement.\n\ncr> select pg_catalog.quote_ident('Column name') AS quoted;\n+---------------+\n| quoted        |\n+---------------+\n| \"Column name\" |\n+---------------+\nSELECT 1 row in set (... sec)\n\nleft('string', len)\n\nReturns the first len characters of string when len > 0, otherwise all but last len characters.\n\nSynopsis:\n\nleft(string, len)\n\n\nExamples:\n\ncr> select left('crate.io', 5) AS col;\n+-------+\n| col   |\n+-------+\n| crate |\n+-------+\nSELECT 1 row in set (... sec)\n\ncr> select left('crate.io', -3) AS col;\n+-------+\n| col   |\n+-------+\n| crate |\n+-------+\nSELECT 1 row in set (... sec)\n\nright('string', len)\n\nReturns the last len characters in string when len > 0, otherwise all but first len characters.\n\nSynopsis:\n\nright(string, len)\n\n\nExamples:\n\ncr> select right('crate.io', 2) AS col;\n+-----+\n| col |\n+-----+\n| io  |\n+-----+\nSELECT 1 row in set (... sec)\n\ncr> select right('crate.io', -6) AS col;\n+-----+\n| col |\n+-----+\n| io  |\n+-----+\nSELECT 1 row in set (... sec)\n\nlpad('string1', len[, 'string2'])\n\nFill up string1 to length len by prepending the characters string2 (a space by default). If string1 is already longer than len then it is truncated (on the right).\n\nSynopsis:\n\nlpad(string1, len[, string2])\n\n\nExample:\n\ncr> select lpad(' I like CrateDB!!', 41, 'yes! ') AS col;\n+-------------------------------------------+\n| col                                       |\n+-------------------------------------------+\n| yes! yes! yes! yes! yes! I like CrateDB!! |\n+-------------------------------------------+\nSELECT 1 row in set (... sec)\n\nrpad('string1', len[, 'string2'])\n\nFill up string1 to length len by appending the characters string2 (a space by default). If string1 is already longer than len then it is truncated.\n\nSynopsis:\n\nrpad(string1, len[, string2])\n\n\nExample:\n\ncr> select rpad('Do you like Crate?', 38, ' yes!') AS col;\n+----------------------------------------+\n| col                                    |\n+----------------------------------------+\n| Do you like Crate? yes! yes! yes! yes! |\n+----------------------------------------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nIn both cases, the scalar functions lpad and rpad do now accept a length greater than 50000.\n\nencode(bytea, format)\n\nEncode takes a binary string (hex format) and returns a text encoding using the specified format. Supported formats are: base64, hex, and escape. The escape format replaces unprintable characters with octal byte notation like \\nnn. For the reverse function, see decode().\n\nSynopsis:\n\nencode(string1, format)\n\n\nExample:\n\ncr> select encode(E'123\\b\\t56', 'base64') AS col;\n+--------------+\n| col          |\n+--------------+\n| MTIzCAk1Ng== |\n+--------------+\nSELECT 1 row in set (... sec)\n\ndecode(text, format)\n\nDecodes a text encoded string using the specified format and returns a binary string (hex format). Supported formats are: base64, hex, and escape. For the reverse function, see encode().\n\nSynopsis:\n\ndecode(text1, format)\n\n\nExample:\n\ncr> select decode('T\\214', 'escape') AS col;\n+--------+\n| col    |\n+--------+\n| \\x548c |\n+--------+\nSELECT 1 row in set (... sec)\n\nrepeat(text, integer)\n\nRepeats a string the specified number of times.\n\nIf the number of repetitions is equal or less than zero then the function returns an empty string.\n\nReturns: text\n\ncr> select repeat('ab', 3) AS repeat;\n+--------+\n| repeat |\n+--------+\n| ababab |\n+--------+\nSELECT 1 row in set (... sec)\n\nsplit_part(text, text, integer)\n\nSplits a string into parts using a delimiter and returns the part at the given index. The first part is addressed by index 1.\n\nSpecial Cases:\n\nReturns the empty string if the index is greater than the number of parts.\n\nIf any of the arguments is NULL, the result is NULL.\n\nIf the delimiter is the empty string, the input string is considered as consisting of exactly one part.\n\nReturns: text\n\nSynopsis:\n\nsplit_part(string, delimiter, index)\n\n\nExample:\n\ncr> select split_part('ab--cdef--gh', '--', 2) AS part;\n+------+\n| part |\n+------+\n| cdef |\n+------+\nSELECT 1 row in set (... sec)\n\nparse_uri(text)\n\nReturns: object\n\nParses the given URI string and returns an object containing the various components of the URI. The returned object has the following properties:\n\n\"uri\" OBJECT AS (\n    \"scheme\" TEXT,\n    \"userinfo\" TEXT,\n    \"hostname\" TEXT,\n    \"port\" INT,\n    \"path\" TEXT,\n    \"query\" TEXT,\n    \"fragment\" TEXT\n)\n\n\nURI Component\n\n\t\n\nDescription\n\n\n\n\nscheme\n\n\t\n\nThe scheme of the URI (e.g. http, crate, etc.)\n\n\n\n\nuserinfo\n\n\t\n\nThe decoded user-information component of this URI.\n\n\n\n\nhostname\n\n\t\n\nThe hostname or IP address specified in the URI.\n\n\n\n\nport\n\n\t\n\nThe port number specified in the URI\n\n\n\n\npath\n\n\t\n\nThe decoded path specified in the URI.\n\n\n\n\nquery\n\n\t\n\nThe decoded query string specified in the URI\n\n\n\n\nfragment\n\n\t\n\nThe query string specified in the URI\n\nNote\n\nFor URI properties not specified in the input string, null is returned.\n\nSynopsis:\n\nparse_uri(text)\n\n\nExample:\n\ncr> SELECT parse_uri('crate://my_user@cluster.crate.io:5432/doc?sslmode=verify-full') as uri;\n+------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| uri                                                                                                                                                        |\n+------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| {\"fragment\": null, \"hostname\": \"cluster.crate.io\", \"path\": \"/doc\", \"port\": 5432, \"query\": \"sslmode=verify-full\", \"scheme\": \"crate\", \"userinfo\": \"my_user\"} |\n+------------------------------------------------------------------------------------------------------------------------------------------------------------+\nSELECT 1 row in set (... sec)\n\n\nIf you just want to select a specific URI component, you can use the bracket notation on the returned object:\n\ncr> SELECT parse_uri('crate://my_user@cluster.crate.io:5432')['hostname'] as uri_hostname;\n+------------------+\n| uri_hostname     |\n+------------------+\n| cluster.crate.io |\n+------------------+\nSELECT 1 row in set (... sec)\n\nparse_url(text)\n\nReturns: object\n\nParses the given URL string and returns an object containing the various components of the URL. The returned object has the following properties:\n\n\"url\" OBJECT AS (\n    \"scheme\" TEXT,\n    \"userinfo\" TEXT,\n    \"hostname\" TEXT,\n    \"port\" INT,\n    \"path\" TEXT,\n    \"query\" TEXT,\n    \"parameters\" OBJECT AS (\n        \"key1\" ARRAY(TEXT),\n        \"key2\" ARRAY(TEXT)\n    ),\n    \"fragment\" TEXT\n)\n\n\nURL Component\n\n\t\n\nDescription\n\n\n\n\nscheme\n\n\t\n\nThe scheme of the URL (e.g. https, crate, etc.)\n\n\n\n\nuserinfo\n\n\t\n\nThe decoded user-information component of this URL.\n\n\n\n\nhostname\n\n\t\n\nThe hostname or IP address specified in the URL.\n\n\n\n\nport\n\n\t\n\nThe port number specified in the URL. If no port number is specified, the default port for the given scheme will be used.\n\n\n\n\npath\n\n\t\n\nThe decoded path specified in the URL.\n\n\n\n\nquery\n\n\t\n\nThe decoded query string specified in the URL.\n\n\n\n\nparameters\n\n\t\n\nFor each query parameter included in the URL, the parameter property holds an object property that stores an array of decoded text values for that specific query parameter.\n\n\n\n\nfragment\n\n\t\n\nThe decoded fragment specified in the URL\n\nNote\n\nFor URL properties not specified in the input string, null is returned.\n\nSynopsis:\n\nparse_url(text)\n\n\nExample:\n\ncr> SELECT parse_url('https://my_user@cluster.crate.io:8000/doc?sslmode=verify-full') as url;\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| url                                                                                                                                                                                                    |\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| {\"fragment\": null, \"hostname\": \"cluster.crate.io\", \"parameters\": {\"sslmode\": [\"verify-full\"]}, \"path\": \"/doc\", \"port\": 8000, \"query\": \"sslmode=verify-full\", \"scheme\": \"https\", \"userinfo\": \"my_user\"} |\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nSELECT 1 row in set (... sec)\n\n\nIf you just want to select a specific URL component, you can use the bracket notation on the returned object:\n\ncr> SELECT parse_url('https://my_user@cluster.crate.io:5432')['hostname'] as url_hostname;\n+------------------+\n| url_hostname     |\n+------------------+\n| cluster.crate.io |\n+------------------+\nSELECT 1 row in set (... sec)\n\n\nParameter values are always treated as text. There is no conversion of comma-separated parameter values into arrays:\n\ncr> SELECT parse_url('http://crate.io?p1=1,2,3&p1=a&p2[]=1,2,3')['parameters'] as params;\n+-------------------------------------------+\n| params                                    |\n+-------------------------------------------+\n| {\"p1\": [\"1,2,3\", \"a\"], \"p2[]\": [\"1,2,3\"]} |\n+-------------------------------------------+\nSELECT 1 row in set (... sec)\n\nDate and time functions\ndate_trunc('interval', ['timezone',] timestamp)\n\nReturns: timestamp with time zone\n\nLimits a timestamps precision to a given interval.\n\nValid intervals are:\n\nsecond\n\nminute\n\nhour\n\nday\n\nweek\n\nmonth\n\nquarter\n\nyear\n\nValid values for timezone are either the name of a time zone (for example ‘Europe/Vienna’) or the UTC offset of a time zone (for example ‘+01:00’). To get a complete overview of all possible values take a look at the available time zones supported by Joda-Time.\n\nThe following example shows how to use the date_trunc function to generate a day based histogram in the Europe/Moscow timezone:\n\ncr> select\n... date_trunc('day', 'Europe/Moscow', date) as day,\n... count(*) as num_locations\n... from locations\n... group by 1\n... order by 1;\n+---------------+---------------+\n| day           | num_locations |\n+---------------+---------------+\n| 308523600000  | 4             |\n| 1367352000000 | 1             |\n| 1373918400000 | 8             |\n+---------------+---------------+\nSELECT 3 rows in set (... sec)\n\n\nIf you don’t specify a time zone, truncate uses UTC time:\n\ncr> select date_trunc('day', date) as day, count(*) as num_locations\n... from locations\n... group by 1\n... order by 1;\n+---------------+---------------+\n| day           | num_locations |\n+---------------+---------------+\n| 308534400000  | 4             |\n| 1367366400000 | 1             |\n| 1373932800000 | 8             |\n+---------------+---------------+\nSELECT 3 rows in set (... sec)\n\ndate_bin(interval, timestamp, origin)\n\ndate_bin “bins” the input timestamp to the specified interval, aligned with a specified origin.\n\ninterval is an expression of type interval. Timestamp and origin are expressions of type timestamp with time zone or timestamp without time zone. The return type matches the timestamp and origin types and will be either timestamp with time zone or timestamp without time zone.\n\nThe return value marks the beginning of the bin into which the input timestamp is placed.\n\nIf you use an interval with a single unit like 1 second or 1 minute, this function returns the same result as date_trunc.\n\nIntervals with months and/or year units are not allowed.\n\nIf the interval is 1 week, date_bin only returns the same result as date_trunc if the origin is a Monday.\n\nIf at least one argument is NULL, the return value is NULL. The interval cannot be zero. Negative intervals are allowed and are treated the same as positive intervals. Intervals having month or year units are not supported due to varying length of those units.\n\nA timestamp can be binned to an interval of arbitrary length aligned with a custom origin.\n\nExamples:\n\ncr> SELECT date_bin('2 hours'::INTERVAL, ts,\n... '2021-01-01T05:00:00Z'::TIMESTAMP) as bin,\n... date_format('%y-%m-%d %H:%i',\n... date_bin('2 hours'::INTERVAL, ts, '2021-01-01T05:00:00Z'::TIMESTAMP))\n... formatted_bin\n... FROM unnest(ARRAY[\n... '2021-01-01T08:30:10Z',\n... '2021-01-01T08:38:10Z',\n... '2021-01-01T18:18:10Z',\n... '2021-01-01T18:18:10Z'\n... ]::TIMESTAMP[]) as tbl (ts);\n+---------------+----------------+\n|           bin | formatted_bin  |\n+---------------+----------------+\n| 1609484400000 | 21-01-01 07:00 |\n| 1609484400000 | 21-01-01 07:00 |\n| 1609520400000 | 21-01-01 17:00 |\n| 1609520400000 | 21-01-01 17:00 |\n+---------------+----------------+\nSELECT 4 rows in set (... sec)\n\n\nTip\n\n0 can be used as a shortcut for Unix zero as the origin:\n\ncr> select date_bin('2 hours' :: INTERVAL,\n... '2021-01-01T08:30:10Z' :: timestamp without time ZONE, 0) as bin;\n+---------------+\n|           bin |\n+---------------+\n| 1609488000000 |\n+---------------+\nSELECT 1 row in set (... sec)\n\n\nPlease note, that implicit cast treats numbers as is, i.e. as a timestamp in that zone and if timestamp is in non-UTC zone you might want to set numeric origin to the same zone:\n\ncr> select date_bin('4 hours' :: INTERVAL,\n... '2020-01-01T09:00:00+0200'::timestamp with time zone,\n... TIMEZONE('+02:00', 0)) as bin;\n+---------------+\n|           bin |\n+---------------+\n| 1577858400000 |\n+---------------+\nSELECT 1 row in set (... sec)\n\nextract(field from source)\n\nextract is a special expression that translates to a function which retrieves subcolumns such as day, hour or minute from a timestamp or an interval.\n\nThe return type depends on the used field.\n\nExample with timestamp:\n\ncr> select extract(day from '2014-08-23') AS day;\n+-----+\n| day |\n+-----+\n|  23 |\n+-----+\nSELECT 1 row in set (... sec)\n\n\nExample with interval:\n\ncr> select extract(hour from INTERVAL '5 days 12 hours 45 minutes') AS hour;\n+------+\n| hour |\n+------+\n|   12 |\n+------+\nSELECT 1 row in set (... sec)\n\n\nSynopsis:\n\nEXTRACT( field FROM source )\n\nfield\n\nAn identifier or string literal which identifies the part of the timestamp or interval that should be extracted.\n\nsource\n\nAn expression that resolves to an interval, or a timestamp (with or without timezone), or is castable to a timestamp.\n\nNote\n\nWhen extracting from an INTERVAL there is normalization of units, up to days e.g.:\n\ncr> SELECT extract(day from INTERVAL '14 years 1250 days 49 hours') AS days;\n+------+\n| days |\n+------+\n| 1252 |\n+------+\nSELECT 1 row in set (... sec)\n\n\nThe following fields are supported:\n\nCENTURY\nReturn type: integer\ncentury of era\n\nReturns the ISO representation which is a straight split of the date.\n\nYear 2000 century 20 and year 2001 is also century 20. This is different to the GregorianJulian (GJ) calendar system where 2001 would be century 21.\n\nYEAR\nReturn type: integer\nthe year field\nQUARTER\nReturn type: integer\nthe quarter of the year (1 - 4)\nMONTH\nReturn type: integer\nthe month of the year\nWEEK\nReturn type: integer\nthe week of the year\nDAY\nReturn type: integer\nthe day of the month for timestamps, days for intervals\nDAY_OF_MONTH\nReturn type: integer\nsame as day\nDAY_OF_WEEK\nReturn type: integer\nday of the week. Starting with Monday (1) to Sunday (7)\nDOW\nReturn type: integer\nsame as day_of_week\nDAY_OF_YEAR\nReturn type: integer\nthe day of the year (1 - 365 / 366)\nDOY\nReturn type: integer\nsame as day_of_year\nHOUR\nReturn type: integer\nthe hour field\nMINUTE\nReturn type: integer\nthe minute field\nSECOND\nReturn type: integer\nthe second field\nEPOCH\nReturn type: double precision\nThe number of seconds since Jan 1, 1970.\nCan be negative if earlier than Jan 1, 1970.\nCURRENT_TIME\n\nThe CURRENT_TIME expression returns the time in microseconds since midnight UTC at the time the SQL statement was handled. Clock time is looked up at most once within the scope of a single query, to ensure that multiple occurrences of CURRENT_TIME evaluate to the same value.\n\nSynopsis:\n\nCURRENT_TIME [ ( precision ) ]\n\nprecision\n\nMust be a positive integer between 0 and 6. The default value is 6. It determines the number of fractional seconds to output. A value of 0 means the time will have second precision, no fractional seconds (microseconds) are given.\n\nNote\n\nNo guarantee is provided about the accuracy of the underlying clock, results may be limited to millisecond precision, depending on the system.\n\nCURRENT_TIMESTAMP\n\nThe CURRENT_TIMESTAMP expression returns the timestamp in milliseconds since midnight UTC at the time the SQL statement was handled. Therefore, the same timestamp value is returned for every invocation of a single statement.\n\nSynopsis:\n\nCURRENT_TIMESTAMP [ ( precision ) ]\n\nprecision\n\nMust be a positive integer between 0 and 3. The default value is 3. This value determines the number of fractional seconds to output. A value of 0 means the timestamp will have second precision, no fractional seconds (milliseconds) are given.\n\nTip\n\nTo get an offset value of CURRENT_TIMESTAMP (e.g., this same time one day ago), you can add or subtract an interval, like so:\n\nCURRENT_TIMESTAMP - '1 day'::interval\n\n\nNote\n\nIf the CURRENT_TIMESTAMP function is used in Generated columns it behaves slightly different in UPDATE operations. In such a case the actual timestamp of each row update is returned.\n\nCURDATE()\n\nThe CURDATE() scalar function is an alias of the CURRENT_DATE expression.\n\nSynopsis:\n\nCURDATE()\n\nCURRENT_DATE\n\nThe CURRENT_DATE expression returns the date in UTC timezone at the time the SQL statement was handled.\n\nClock time is looked up at most once within the scope of a single query, to ensure that multiple occurrences of CURRENT_DATE evaluate to the same value.\n\nSynopsis:\n\nCURRENT_DATE\n\nnow()\n\nReturns the current date and time in UTC.\n\nThis is the same as current_timestamp\n\nReturns: timestamp with time zone\n\nSynopsis:\n\nnow()\n\ndate_format([format_string, [timezone,]] timestamp)\n\nThe date_format function formats a timestamp as string according to the (optional) format string.\n\nReturns: text\n\nSynopsis:\n\nDATE_FORMAT( [ format_string, [ timezone, ] ] timestamp )\n\n\nThe only mandatory argument is the timestamp value to format. It can be any expression that is safely convertible to timestamp data type with or without timezone.\n\nThe syntax for the format_string is 100% compatible to the syntax of the MySQL date_format function. For reference, the format is listed in detail below:\n\nFormat Specifier\n\n\t\n\nDescription\n\n\n\n\n%a\n\n\t\n\nAbbreviated weekday name (Sun..Sat)\n\n\n\n\n%b\n\n\t\n\nAbbreviated month name (Jan..Dec)\n\n\n\n\n%c\n\n\t\n\nMonth in year, numeric (0..12)\n\n\n\n\n%D\n\n\t\n\nDay of month as ordinal number (1st, 2nd, … 24th)\n\n\n\n\n%d\n\n\t\n\nDay of month, padded to 2 digits (00..31)\n\n\n\n\n%e\n\n\t\n\nDay of month (0..31)\n\n\n\n\n%f\n\n\t\n\nMicroseconds, padded to 6 digits (000000..999999)\n\n\n\n\n%H\n\n\t\n\nHour in 24-hour clock, padded to 2 digits (00..23)\n\n\n\n\n%h\n\n\t\n\nHour in 12-hour clock, padded to 2 digits (01..12)\n\n\n\n\n%I\n\n\t\n\nHour in 12-hour clock, padded to 2 digits (01..12)\n\n\n\n\n%i\n\n\t\n\nMinutes, numeric (00..59)\n\n\n\n\n%j\n\n\t\n\nDay of year, padded to 3 digits (001..366)\n\n\n\n\n%k\n\n\t\n\nHour in 24-hour clock (0..23)\n\n\n\n\n%l\n\n\t\n\nHour in 12-hour clock (1..12)\n\n\n\n\n%M\n\n\t\n\nMonth name (January..December)\n\n\n\n\n%m\n\n\t\n\nMonth in year, numeric, padded to 2 digits (00..12)\n\n\n\n\n%p\n\n\t\n\nAM or PM\n\n\n\n\n%r\n\n\t\n\nTime, 12-hour (hh:mm:ss followed by AM or PM)\n\n\n\n\n%S\n\n\t\n\nSeconds, padded to 2 digits (00..59)\n\n\n\n\n%s\n\n\t\n\nSeconds, padded to 2 digits (00..59)\n\n\n\n\n%T\n\n\t\n\nTime, 24-hour (hh:mm:ss)\n\n\n\n\n%U\n\n\t\n\nWeek number, Sunday as first day of the week, first week of the year (01) is the one starting in this year, week 00 starts in last year (00..53)\n\n\n\n\n%u\n\n\t\n\nWeek number, Monday as first day of the week, first week of the year (01) is the one with at least 4 days in this year (00..53)\n\n\n\n\n%V\n\n\t\n\nWeek number, Sunday as first day of the week, first week of the year (01) is the one starting in this year, uses the week number of the last year, if the week started in last year (01..53)\n\n\n\n\n%v\n\n\t\n\nWeek number, Monday as first day of the week, first week of the year (01) is the one with at least 4 days in this year, uses the week number of the last year, if the week started in last year (01..53)\n\n\n\n\n%W\n\n\t\n\nWeekday name (Sunday..Saturday)\n\n\n\n\n%w\n\n\t\n\nDay of the week (0=Sunday..6=Saturday)\n\n\n\n\n%X\n\n\t\n\nWeek year, Sunday as first day of the week, numeric, four digits; used with %V\n\n\n\n\n%x\n\n\t\n\nWeek year, Monday as first day of the week, numeric, four digits; used with %v\n\n\n\n\n%Y\n\n\t\n\nYear, numeric, four digits\n\n\n\n\n%y\n\n\t\n\nYear, numeric, two digits\n\n\n\n\n%%\n\n\t\n\nA literal ‘%’ character\n\n\n\n\n%x\n\n\t\n\nx, for any ‘x’ not listed above\n\nIf no format_string is given the default format will be used:\n\n%Y-%m-%dT%H:%i:%s.%fZ\n\ncr> select date_format('1970-01-01') as epoque;\n+-----------------------------+\n| epoque                      |\n+-----------------------------+\n| 1970-01-01T00:00:00.000000Z |\n+-----------------------------+\nSELECT 1 row in set (... sec)\n\n\nValid values for timezone are either the name of a time zone (for example ‘Europe/Vienna’) or the UTC offset of a time zone (for example ‘+01:00’). To get a complete overview of all possible values take a look at the available time zones supported by Joda-Time.\n\nThe timezone will be UTC if not provided:\n\ncr> select date_format('%W the %D of %M %Y %H:%i %p', 0) as epoque;\n+-------------------------------------------+\n| epoque                                    |\n+-------------------------------------------+\n| Thursday the 1st of January 1970 00:00 AM |\n+-------------------------------------------+\nSELECT 1 row in set (... sec)\n\ncr> select date_format('%Y/%m/%d %H:%i', 'EST',  0) as est_epoque;\n+------------------+\n| est_epoque       |\n+------------------+\n| 1969/12/31 19:00 |\n+------------------+\nSELECT 1 row in set (... sec)\n\ntimezone(timezone, timestamp)\n\nThe timezone scalar function converts values of timestamp without time zone to/from timestamp with time zone.\n\nSynopsis:\n\nTIMEZONE(timezone, timestamp)\n\n\nIt has two variants depending on the type of timestamp:\n\nType of timestamp\n\n\t\n\nReturn Type\n\n\t\n\nDescription\n\n\n\n\ntimestamp without time zone OR bigint\n\n\t\n\ntimestamp with time zone\n\n\t\n\nTreat given timestamp without time zone as located in the specified timezone\n\n\n\n\ntimestamp with time zone\n\n\t\n\ntimestamp without time zone\n\n\t\n\nConvert given timestamp with time zone to the new timezone with no time zone designation\n\ncr> select\n...     257504400000 as no_tz,\n...     date_format(\n...         '%Y-%m-%d %h:%i', 257504400000\n...     ) as no_tz_str,\n...     timezone(\n...         'Europe/Madrid', 257504400000\n...     ) as in_madrid,\n...     date_format(\n...         '%Y-%m-%d %h:%i',\n...         timezone(\n...             'Europe/Madrid', 257504400000\n...         )\n...     ) as in_madrid_str;\n+--------------+------------------+--------------+------------------+\n|        no_tz | no_tz_str        |    in_madrid | in_madrid_str    |\n+--------------+------------------+--------------+------------------+\n| 257504400000 | 1978-02-28 09:00 | 257500800000 | 1978-02-28 08:00 |\n+--------------+------------------+--------------+------------------+\nSELECT 1 row in set (... sec)\n\ncr> select\n...     timezone(\n...         'Europe/Madrid',\n...         '1978-02-28T10:00:00+01:00'::timestamp with time zone\n...     ) as epoque,\n...     date_format(\n...          '%Y-%m-%d %h:%i',\n...          timezone(\n...              'Europe/Madrid',\n...              '1978-02-28T10:00:00+01:00'::timestamp with time zone\n...          )\n...     ) as epoque_str;\n+--------------+------------------+\n|       epoque | epoque_str       |\n+--------------+------------------+\n| 257508000000 | 1978-02-28 10:00 |\n+--------------+------------------+\nSELECT 1 row in set (... sec)\n\ncr> select\n...     timezone(\n...         'Europe/Madrid',\n...         '1978-02-28T10:00:00+01:00'::timestamp without time zone\n...     ) as epoque,\n...     date_format(\n...         '%Y-%m-%d %h:%i',\n...         timezone(\n...             'Europe/Madrid',\n...             '1978-02-28T10:00:00+01:00'::timestamp without time zone\n...         )\n...     ) as epoque_str;\n+--------------+------------------+\n|       epoque | epoque_str       |\n+--------------+------------------+\n| 257504400000 | 1978-02-28 09:00 |\n+--------------+------------------+\nSELECT 1 row in set (... sec)\n\nto_char(expression, format_string)\n\nThe to_char function converts a timestamp or interval value to a string, based on a given format string.\n\nReturns: text\n\nSynopsis:\n\nTO_CHAR( expression, format_string )\n\n\nHere, expression can be any value with the type of timestamp (with or without a timezone) or interval.\n\nThe syntax for the format_string differs based the type of the expression. For timestamp expressions, the format_string is a template string containing any of the following symbols:\n\nPattern\n\n\t\n\nDescription\n\n\n\n\nHH / HH12\n\n\t\n\nHour of day (01-12)\n\n\n\n\nHH24\n\n\t\n\nHour of day (00-23)\n\n\n\n\nMI\n\n\t\n\nMinute (00-59)\n\n\n\n\nSS\n\n\t\n\nSecond (00-59)\n\n\n\n\nMS\n\n\t\n\nMillisecond (000-999)\n\n\n\n\nUS\n\n\t\n\nMicrosecond (000000-999999)\n\n\n\n\nFF1\n\n\t\n\nTenth of second (0-9)\n\n\n\n\nFF2\n\n\t\n\nHundredth of second (00-99)\n\n\n\n\nFF3\n\n\t\n\nMillisecond (000-999)\n\n\n\n\nFF4\n\n\t\n\nTenth of millisecond (0000-9999)\n\n\n\n\nFF5\n\n\t\n\nHundredth of millisecond (00000-99999)\n\n\n\n\nFF6\n\n\t\n\nMicrosecond (000000-999999)\n\n\n\n\nSSSS / SSSSS\n\n\t\n\nSeconds past midnight (0-86399)\n\n\n\n\nAM / am / PM / pm\n\n\t\n\nMeridiem indicator\n\n\n\n\nA.M. / a.m. / P.M. / p.m.\n\n\t\n\nMeridiem indicator (with periods)\n\n\n\n\nY,YYY\n\n\t\n\n4 digit year with comma\n\n\n\n\nYYYY\n\n\t\n\n4 digit year\n\n\n\n\nYYY\n\n\t\n\nLast 3 digits of year\n\n\n\n\nYY\n\n\t\n\nLast 2 digits of year\n\n\n\n\nY\n\n\t\n\nLast digit of year\n\n\n\n\nIYYY\n\n\t\n\n4 digit ISO-8601 week-numbering year\n\n\n\n\nIYY\n\n\t\n\nLast 3 digits of ISO-8601 week-numbering year\n\n\n\n\nIY\n\n\t\n\nLast 2 digits of ISO-8601 week-numbering year\n\n\n\n\nI\n\n\t\n\nLast digit of ISO-8601 week-numbering year\n\n\n\n\nBC / bc / AD / ad\n\n\t\n\nEra indicator\n\n\n\n\nB.C. / b.c. / A.D. / a.d.\n\n\t\n\nEra indicator with periods\n\n\n\n\nMONTH / Month / month\n\n\t\n\nFull month name (uppercase, capitalized, lowercase) padded to 9 characters\n\n\n\n\nMON / Mon / mon\n\n\t\n\nShort month name (uppercase, capitalized, lowercase) padded to 9 characters\n\n\n\n\nMM\n\n\t\n\nMonth number (01-12)\n\n\n\n\nDAY / Day / day\n\n\t\n\nFull day name (uppercase, capitalized, lowercase) padded to 9 characters\n\n\n\n\nDY / Dy / dy\n\n\t\n\nShort, 3 character day name (uppercase, capitalized, lowercase)\n\n\n\n\nDDD\n\n\t\n\nDay of year (001-366)\n\n\n\n\nIDDD\n\n\t\n\nDay of ISO-8601 week-numbering year, where the first Monday of the first ISO week is day 1 (001-371)\n\n\n\n\nDD\n\n\t\n\nDay of month (01-31)\n\n\n\n\nD\n\n\t\n\nDay of the week, from Sunday (1) to Saturday (7)\n\n\n\n\nID\n\n\t\n\nISO-8601 day of the week, from Monday (1) to Sunday (7)\n\n\n\n\nW\n\n\t\n\nWeek of month (1-5)\n\n\n\n\nWW\n\n\t\n\nWeek number of year (1-53)\n\n\n\n\nIW\n\n\t\n\nWeek number of ISO-8601 week-numbering year (01-53)\n\n\n\n\nCC\n\n\t\n\nCentury\n\n\n\n\nJ\n\n\t\n\nJulian Day\n\n\n\n\nQ\n\n\t\n\nQuarter\n\n\n\n\nRM / rm\n\n\t\n\nMonth in Roman numerals (uppercase, lowercase)\n\n\n\n\nTZ / tz\n\n\t\n\nTime-zone abbreviation (uppercase, lowercase)\n\n\n\n\nTZH\n\n\t\n\nTime-zone hours\n\n\n\n\nTZM\n\n\t\n\nTime-zone minutes\n\n\n\n\nOF\n\n\t\n\nTime-zone offset from UTC\n\nExample:\n\ncr> select\n...     to_char(\n...         timestamp '1970-01-01T17:31:12',\n...         'Day, Month DD - HH12:MI AM YYYY AD'\n...     ) as ts;\n+-----------------------------------------+\n| ts                                      |\n+-----------------------------------------+\n| Thursday, January 01 - 05:31 PM 1970 AD |\n+-----------------------------------------+\nSELECT 1 row in set (... sec)\n\n\nFor interval expressions, the formatting string accepts the same tokens as timestamp expressions. The function then uses the timestamp of the specified interval added to the timestamp of 0000/01/01 00:00:00:\n\ncr> select\n...     to_char(\n...         interval '1 year 3 weeks 200 minutes',\n...         'YYYY MM DD HH12:MI:SS'\n...     ) as interval;\n+---------------------+\n| interval            |\n+---------------------+\n| 0001 01 22 03:20:00 |\n+---------------------+\nSELECT 1 row in set (... sec)\n\nage([timestamp,] timestamp)\n\nReturns: interval between 2 timestamps. Second argument is subtracted from the first one. If at least one argument is NULL, the return value is NULL. If only one timestamp is given, the return value is interval between current_date (at midnight) and the given timestamp.\n\nExample:\n\ncr> select pg_catalog.age('2021-10-21'::timestamp, '2021-10-20'::timestamp)\n... as age;\n+----------------+\n| age            |\n+----------------+\n| 1 day 00:00:00 |\n+----------------+\nSELECT 1 row in set (... sec)\n\ncr> select pg_catalog.age(date_trunc('day', CURRENT_DATE)) as age;\n+----------+\n| age      |\n+----------+\n| 00:00:00 |\n+----------+\nSELECT 1 row in set (... sec)\n\nGeo functions\ndistance(geo_point1, geo_point2)\n\nReturns: double precision\n\nThe distance function can be used to calculate the distance between two points on earth. It uses the Haversine formula which gives great-circle distances between 2 points on a sphere based on their latitude and longitude.\n\nThe return value is the distance in meters.\n\nBelow is an example of the distance function where both points are specified using WKT. See Geographic types for more information on the implicit type casting of geo points:\n\ncr> select distance('POINT (10 20)', 'POINT (11 21)') AS col;\n+-------------------+\n|               col |\n+-------------------+\n| 152354.3209044634 |\n+-------------------+\nSELECT 1 row in set (... sec)\n\n\nThis scalar function can always be used in both the WHERE and ORDER BY clauses. With the limitation that one of the arguments must be a literal and the other argument must be a column reference.\n\nNote\n\nThe algorithm of the calculation which is used when the distance function is used as part of the result column list has a different precision than what is stored inside the index which is utilized if the distance function is part of a WHERE clause.\n\nFor example, if select distance(...) returns 0.0, an equality check with where distance(...) = 0 might not yield anything at all due to the precision difference.\n\nwithin(shape1, shape2)\n\nReturns: boolean\n\nThe within function returns true if shape1 is within shape2. If that is not the case false is returned.\n\nshape1 can either be a geo_shape or a geo_point. shape2 must be a geo_shape.\n\nBelow is an example of the within function which makes use of the implicit type casting from strings in WKT representation to geo point and geo shapes:\n\ncr> select within(\n...   'POINT (10 10)',\n...   'POLYGON ((5 5, 10 5, 10 10, 5 10, 5 5))'\n... ) AS is_within;\n+-----------+\n| is_within |\n+-----------+\n| TRUE      |\n+-----------+\nSELECT 1 row in set (... sec)\n\n\nThis function can always be used within the WHERE clause.\n\nintersects(geo_shape, geo_shape)\n\nReturns: boolean\n\nThe intersects function returns true if both argument shapes share some points or area, they overlap. This also includes two shapes where one lies within the other.\n\nIf false is returned, both shapes are considered disjoint.\n\nExample:\n\ncr> select\n... intersects(\n...   {type='Polygon', coordinates=[\n...         [[13.4252, 52.7096],[13.9416, 52.0997],\n...          [12.7221, 52.1334],[13.4252, 52.7096]]]},\n...   'LINESTRING(13.9636 52.6763, 13.2275 51.9578,\n...               12.9199 52.5830, 11.9970 52.6830)'\n... ) as intersects,\n... intersects(\n...   {type='Polygon', coordinates=[\n...         [[13.4252, 52.7096],[13.9416, 52.0997],\n...          [12.7221, 52.1334],[13.4252, 52.7096]]]},\n...   'LINESTRING (11.0742 49.4538, 11.5686 48.1367)'\n... ) as disjoint;\n+------------+----------+\n| intersects | disjoint |\n+------------+----------+\n| TRUE       | FALSE    |\n+------------+----------+\nSELECT 1 row in set (... sec)\n\n\nDue to a limitation on the Geometric shapes datatype this function cannot be used in the ORDER BY clause.\n\nlatitude(geo_point) and longitude(geo_point)\n\nReturns: double precision\n\nThe latitude and longitude function return the coordinates of latitude or longitude of a point, or NULL if not available. The input must be a column of type geo_point, a valid WKT string or a double precision array. See Geographic types for more information on the implicit type casting of geo points.\n\nExample:\n\ncr> select\n...     mountain,\n...     height,\n...     longitude(coordinates) as \"lon\",\n...     latitude(coordinates) as \"lat\"\n... from sys.summits\n... order by height desc limit 1;\n+------------+--------+---------+---------+\n| mountain   | height |     lon |     lat |\n+------------+--------+---------+---------+\n| Mont Blanc |   4808 | 6.86444 | 45.8325 |\n+------------+--------+---------+---------+\nSELECT 1 row in set (... sec)\n\n\nBelow is an example of the latitude/longitude functions which make use of the implicit type casting from strings to geo point:\n\ncr> select\n...    latitude('POINT (10 20)') AS lat,\n...    longitude([10.0, 20.0]) AS long;\n+------+------+\n|  lat | long |\n+------+------+\n| 20.0 | 10.0 |\n+------+------+\nSELECT 1 row in set (... sec)\n\ngeohash(geo_point)\n\nReturns: text\n\nReturns a GeoHash representation based on full precision (12 characters) of the input point, or NULL if not available. The input has to be a column of type geo_point, a valid WKT string or a double precision array. See Geographic types for more information of the implicit type casting of geo points.\n\nExample:\n\ncr> select\n...     mountain,\n...     height,\n...     geohash(coordinates) as \"geohash\"\n... from sys.summits\n... order by height desc limit 1;\n+------------+--------+--------------+\n| mountain   | height | geohash      |\n+------------+--------+--------------+\n| Mont Blanc |   4808 | u0huspw99j1r |\n+------------+--------+--------------+\nSELECT 1 row in set (... sec)\n\narea(geo_shape)\n\nReturns: double precision\n\nThe area function calculates the area of the input shape in square-degrees. The calculation will use geospatial awareness (AKA geodetic) instead of Euclidean geometry. The input has to be a column of type Geometric shapes, a valid WKT string or GeoJSON. See Geometric shapes for more information.\n\nBelow you can find an example.\n\nExample:\n\ncr> select\n...     round(area('POLYGON ((5 5, 10 5, 10 10, 5 10, 5 5))')) as \"area\";\n+------+\n| area |\n+------+\n|   25 |\n+------+\nSELECT 1 row in set (... sec)\n\nMathematical functions\n\nAll mathematical functions can be used within WHERE and ORDER BY clauses.\n\nabs(number)\n\nReturns the absolute value of the given number in the datatype of the given number:\n\ncr> select abs(214748.0998) AS a, abs(0) AS b, abs(-214748) AS c;\n+-------------+---+--------+\n|           a | b |      c |\n+-------------+---+--------+\n| 214748.0998 | 0 | 214748 |\n+-------------+---+--------+\nSELECT 1 row in set (... sec)\n\nceil(number)\n\nReturns the smallest integer or long value that is not less than the argument.\n\nReturns: bigint or integer\n\nReturn value will be of type integer if the input value is an integer or float. If the input value is of type bigint or double precision the return value will be of type bigint:\n\ncr> select ceil(29.9) AS col;\n+-----+\n| col |\n+-----+\n|  30 |\n+-----+\nSELECT 1 row in set (... sec)\n\nceiling(number)\n\nThis is an alias for ceil.\n\ndegrees(double precision)\n\nConvert the given radians value to degrees.\n\nReturns: double precision\n\ncr> select degrees(0.5) AS degrees;\n+-------------------+\n|           degrees |\n+-------------------+\n| 28.64788975654116 |\n+-------------------+\nSELECT 1 row in set (... sec)\n\nexp(number)\n\nReturns Euler’s number e raised to the power of the given numeric value. The output will be cast to the given input type and thus may loose precision.\n\nReturns: Same as input type.\n\n> select exp(1.0) AS exp;\n+-------------------+\n|               exp |\n+-------------------+\n| 2.718281828459045 |\n+-------------------+\nSELECT 1 row in set (... sec)\n\nfloor(number)\n\nReturns the largest integer or long value that is not greater than the argument.\n\nReturns: bigint or integer\n\nReturn value will be an integer if the input value is an integer or a float. If the input value is of type bigint or double precision the return value will be of type bigint.\n\nSee below for an example:\n\ncr> select floor(29.9) AS floor;\n+-------+\n| floor |\n+-------+\n|    29 |\n+-------+\nSELECT 1 row in set (... sec)\n\nln(number)\n\nReturns the natural logarithm of given number.\n\nReturns: double precision\n\nSee below for an example:\n\ncr> SELECT ln(1) AS ln;\n+-----+\n|  ln |\n+-----+\n| 0.0 |\n+-----+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nAn error is returned for arguments which lead to undefined or illegal results. E.g. ln(0) results in minus infinity, and therefore, an error is returned.\n\nlog(x : number, b : number)\n\nReturns the logarithm of given x to base b.\n\nReturns: double precision\n\nSee below for an example, which essentially is the same as above:\n\ncr> SELECT log(100, 10) AS log;\n+-----+\n| log |\n+-----+\n| 2.0 |\n+-----+\nSELECT 1 row in set (... sec)\n\n\nThe second argument (b) is optional. If not present, base 10 is used:\n\ncr> SELECT log(100) AS log;\n+-----+\n| log |\n+-----+\n| 2.0 |\n+-----+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nAn error is returned for arguments which lead to undefined or illegal results. E.g. log(0) results in minus infinity, and therefore, an error is returned.\n\nThe same is true for arguments which lead to a division by zero, as, e.g., log(10, 1) does.\n\nmodulus(y, x)\n\nReturns the remainder of y/x.\n\nReturns: Same as argument types.\n\ncr> select modulus(5, 4) AS mod;\n+-----+\n| mod |\n+-----+\n|   1 |\n+-----+\nSELECT 1 row in set (... sec)\n\nmod(y, x)\n\nThis is an alias for modulus.\n\npower(a: number, b: number)\n\nReturns the given argument a raised to the power of argument b.\n\nReturns: double precision\n\nThe return type of the power function is always double precision, even when both the inputs are integral types, in order to be consistent across positive and negative exponents (which will yield decimal types).\n\nSee below for an example:\n\ncr> SELECT power(2,3) AS pow;\n+-----+\n| pow |\n+-----+\n| 8.0 |\n+-----+\nSELECT 1 row in set (... sec)\n\nradians(double precision)\n\nConvert the given degrees value to radians.\n\nReturns: double precision\n\ncr> select radians(45.0) AS radians;\n+--------------------+\n|            radians |\n+--------------------+\n| 0.7853981633974483 |\n+--------------------+\nSELECT 1 row in set (... sec)\n\nrandom()\n\nThe random function returns a random value in the range 0.0 <= X < 1.0.\n\nReturns: double precision\n\nNote\n\nEvery call to random will yield a new random number.\n\ngen_random_text_uuid()\n\nReturns a random time based UUID as text. The returned ID is similar to flake IDs and well suited for use as primary key value.\n\nNote that the ID is opaque (i.e., not to be considered meaningful in any way) and the implementation is free to change.\n\nround(number)\n\nIf the input is of type double precision or bigint the result is the closest bigint to the argument, with ties rounding up.\n\nIf the input is of type real or integer the result is the closest integer to the argument, with ties rounding up.\n\nReturns: bigint or integer\n\nSee below for an example:\n\ncr> select round(42.2) AS round;\n+-------+\n| round |\n+-------+\n|    42 |\n+-------+\nSELECT 1 row in set (... sec)\n\ntrunc(number[, precision])\n\nReturns number truncated to the specified precision (decimal places).\n\nWhen precision is not specified, the result’s type is an integer, or bigint. When it is specified, the result’s type is double precision. Notice that trunc(number) and trunc(number, 0) return different result types.\n\nSee below for examples:\n\ncr> select trunc(29.999999, 3) AS trunc;\n+--------+\n|  trunc |\n+--------+\n| 29.999 |\n+--------+\nSELECT 1 row in set (... sec)\n\ncr> select trunc(29.999999) AS trunc;\n+-------+\n| trunc |\n+-------+\n|    29 |\n+-------+\nSELECT 1 row in set (... sec)\n\nsqrt(number)\n\nReturns the square root of the argument.\n\nReturns: double precision\n\nSee below for an example:\n\ncr> select sqrt(25.0) AS sqrt;\n+------+\n| sqrt |\n+------+\n|  5.0 |\n+------+\nSELECT 1 row in set (... sec)\n\nsin(number)\n\nReturns the sine of the argument.\n\nReturns: double precision\n\nSee below for an example:\n\ncr> SELECT sin(1) AS sin;\n+--------------------+\n|                sin |\n+--------------------+\n| 0.8414709848078965 |\n+--------------------+\nSELECT 1 row in set (... sec)\n\nasin(number)\n\nReturns the arcsine of the argument.\n\nReturns: double precision\n\nSee below for an example:\n\ncr> SELECT asin(1) AS asin;\n+--------------------+\n|               asin |\n+--------------------+\n| 1.5707963267948966 |\n+--------------------+\nSELECT 1 row in set (... sec)\n\ncos(number)\n\nReturns the cosine of the argument.\n\nReturns: double precision\n\nSee below for an example:\n\ncr> SELECT cos(1) AS cos;\n+--------------------+\n|                cos |\n+--------------------+\n| 0.5403023058681398 |\n+--------------------+\nSELECT 1 row in set (... sec)\n\nacos(number)\n\nReturns the arccosine of the argument.\n\nReturns: double precision\n\nSee below for an example:\n\ncr> SELECT acos(-1) AS acos;\n+-------------------+\n|              acos |\n+-------------------+\n| 3.141592653589793 |\n+-------------------+\nSELECT 1 row in set (... sec)\n\ntan(number)\n\nReturns the tangent of the argument.\n\nReturns: double precision\n\nSee below for an example:\n\ncr> SELECT tan(1) AS tan;\n+--------------------+\n|                tan |\n+--------------------+\n| 1.5574077246549023 |\n+--------------------+\nSELECT 1 row in set (... sec)\n\ncot(number)\n\nReturns the cotangent of the argument that represents the angle expressed in radians. The range of the argument is all real numbers. The cotangent of zero is undefined and returns Infinity.\n\nReturns: double precision\n\nSee below for an example:\n\ncr> select cot(1) AS cot;\n+--------------------+\n|                cot |\n+--------------------+\n| 0.6420926159343306 |\n+--------------------+\nSELECT 1 row in set (... sec)\n\natan(number)\n\nReturns the arctangent of the argument.\n\nReturns: double precision\n\nSee below for an example:\n\ncr> SELECT atan(1) AS atan;\n+--------------------+\n|               atan |\n+--------------------+\n| 0.7853981633974483 |\n+--------------------+\nSELECT 1 row in set (... sec)\n\natan2(y: number, x: number)\n\nReturns the arctangent of y/x.\n\nReturns: double precision\n\ncr> SELECT atan2(2, 1) AS atan2;\n+--------------------+\n|              atan2 |\n+--------------------+\n| 1.1071487177940904 |\n+--------------------+\nSELECT 1 row in set (... sec)\n\npi()\n\nReturns the π constant.\n\nReturns: double precision\n\ncr> SELECT pi() AS pi;\n+-------------------+\n|                pi |\n+-------------------+\n| 3.141592653589793 |\n+-------------------+\nSELECT 1 row in set (... sec)\n\nRegular expression functions\n\nThe regular expression functions in CrateDB use Java Regular Expressions.\n\nSee the API documentation for more details.\n\nNote\n\nBe aware that, in contrast to the functions, the regular expression operator uses Lucene Regular Expressions.\n\nregexp_replace(source, pattern, replacement [, flags])\n\nregexp_replace can be used to replace every (or only the first) occurrence of a subsequence matching pattern in the source string with the replacement string. If no subsequence in source matches the regular expression pattern, source is returned unchanged.\n\nReturns: text\n\npattern is a Java regular expression. For details on the regexp syntax, see Java Regular Expressions.\n\nThe replacement string may contain expressions like $N where N is a digit between 0 and 9. It references the nth matched group of pattern and the matching subsequence of that group will be inserted in the returned string. The expression $0 will insert the whole matching source.\n\nBy default, only the first occurrence of a subsequence matching pattern will be replaced. If all occurrences shall be replaced use the g flag.\n\nFlags\n\nregexp_replace supports a number of flags as optional parameters. These flags are given as a string containing any of the characters listed below. Order does not matter.\n\nFlag\n\n\t\n\nDescription\n\n\n\n\ni\n\n\t\n\nenable case insensitive matching\n\n\n\n\nu\n\n\t\n\nenable unicode case folding when used together with i\n\n\n\n\nU\n\n\t\n\nenable unicode support for character classes like \\W\n\n\n\n\ns\n\n\t\n\nmake . match line terminators, too\n\n\n\n\nm\n\n\t\n\nmake ^ and $ match on the beginning or end of a line too.\n\n\n\n\nx\n\n\t\n\npermit whitespace and line comments starting with #\n\n\n\n\nd\n\n\t\n\nonly \\n is considered a line-terminator when using ^, $ and .\n\n\n\n\ng\n\n\t\n\nreplace all occurrences of a subsequence matching pattern, not only the first\n\nExamples\ncr> select\n...     name,\n...     regexp_replace(\n...         name, '(\\w+)\\s(\\w+)+', '$1 - $2'\n...      ) as replaced\n... from locations\n... order by name limit 5;\n +---------------------+-----------------------+\n | name                | replaced              |\n +---------------------+-----------------------+\n |                     |                       |\n | Aldebaran           | Aldebaran             |\n | Algol               | Algol                 |\n | Allosimanius Syneca | Allosimanius - Syneca |\n | Alpha Centauri      | Alpha - Centauri      |\n +---------------------+-----------------------+\n SELECT 5 rows in set (... sec)\n\ncr> select\n...     regexp_replace(\n...         'alcatraz', '(foo)(bar)+', '$1baz'\n...     ) as replaced;\n +----------+\n | replaced |\n +----------+\n | alcatraz |\n +----------+\n SELECT 1 row in set (... sec)\n\ncr> select\n...     name,\n...     regexp_replace(\n...         name, '([A-Z]\\w+) .+', '$1', 'ig'\n...     ) as replaced\n... from locations\n... order by name limit 5;\n +---------------------+--------------+\n | name                | replaced     |\n +---------------------+--------------+\n |                     |              |\n | Aldebaran           | Aldebaran    |\n | Algol               | Algol        |\n | Allosimanius Syneca | Allosimanius |\n | Alpha Centauri      | Alpha        |\n +---------------------+--------------+\n SELECT 5 rows in set (... sec)\n\nArray functions\narray_append(anyarray, value)\n\nThe array_append function adds the value at the end of the array\n\nReturns: array\n\ncr> select\n...     array_append([1,2,3], 4) AS array_append;\n+--------------+\n| array_append |\n+--------------+\n| [1, 2, 3, 4] |\n+--------------+\nSELECT 1 row in set (... sec)\n\narray_cat(first_array, second_array)\n\nThe array_cat function concatenates two arrays into one array\n\nReturns: array\n\ncr> select\n...     array_cat([1,2,3],[3,4,5,6]) AS array_cat;\n+-----------------------+\n| array_cat             |\n+-----------------------+\n| [1, 2, 3, 3, 4, 5, 6] |\n+-----------------------+\nSELECT 1 row in set (... sec)\n\n\nYou can also use the concat operator || with arrays:\n\ncr> select\n...     [1,2,3] || [4,5,6] || [7,8,9] AS arr;\n+-----------------------------+\n| arr                         |\n+-----------------------------+\n| [1, 2, 3, 4, 5, 6, 7, 8, 9] |\n+-----------------------------+\nSELECT 1 row in set (... sec)\n\narray_unique(first_array, [ second_array])\n\nThe array_unique function merges two arrays into one array with unique elements\n\nReturns: array\n\ncr> select\n...     array_unique(\n...         [1, 2, 3],\n...         [3, 4, 4]\n...     ) AS arr;\n+--------------+\n| arr          |\n+--------------+\n| [1, 2, 3, 4] |\n+--------------+\nSELECT 1 row in set (... sec)\n\n\nIf the arrays have different types all elements will be cast to a common type based on the type precedence.\n\ncr> select\n...      array_unique(\n...          [10, 20],\n...          [10.0, 20.3]\n...      ) AS arr;\n+--------------------+\n| arr                |\n+--------------------+\n| [10.0, 20.0, 20.3] |\n+--------------------+\nSELECT 1 row in set (... sec)\n\narray_difference(first_array, second_array)\n\nThe array_difference function removes elements from the first array that are contained in the second array.\n\nReturns: array\n\ncr> select\n...     array_difference(\n...         [1,2,3,4,5,6,7,8,9,10],\n...         [2,3,6,9,15]\n...     ) AS arr;\n+---------------------+\n| arr                 |\n+---------------------+\n| [1, 4, 5, 7, 8, 10] |\n+---------------------+\nSELECT 1 row in set (... sec)\n\narray(subquery)\n\nThe array(subquery) expression is an array constructor function which operates on the result of the subquery.\n\nReturns: array\n\nSee Also\n\nArray construction with subquery\n\narray_upper(anyarray, dimension)\n\nThe array_upper function returns the number of elements in the requested array dimension (the upper bound of the dimension). CrateDB allows mixing arrays with different sizes on the same dimension. Returns NULL if array argument is NULL or if dimension <= 0 or if dimension is NULL.\n\nReturns: integer\n\ncr> select array_upper([[1, 4], [3]], 1) AS size;\n+------+\n| size |\n+------+\n|    2 |\n+------+\nSELECT 1 row in set (... sec)\n\n\nAn empty array has no dimension and returns NULL instead of 0.\n\ncr> select array_upper(ARRAY[]::int[], 1) AS size;\n+------+\n| size |\n+------+\n| NULL |\n+------+\nSELECT 1 row in set (... sec)\n\narray_length(anyarray, dimension)\n\nAn alias for array_upper(anyarray, dimension).\n\ncr> select array_length([[1, 4], [3]], 1) AS len;\n+-----+\n| len |\n+-----+\n|   2 |\n+-----+\nSELECT 1 row in set (... sec)\n\narray_lower(anyarray, dimension)\n\nThe array_lower function returns the lower bound of the requested array dimension (which is 1 if the dimension is valid and has at least one element). Returns NULL if array argument is NULL or if dimension <= 0 or if dimension is NULL.\n\nReturns: integer\n\ncr> select array_lower([[1, 4], [3]], 1) AS size;\n+------+\n| size |\n+------+\n|    1 |\n+------+\nSELECT 1 row in set (... sec)\n\n\nIf there is at least one empty array or NULL on the requested dimension return value is NULL. Example:\n\ncr> select array_lower([[1, 4], [3], []], 2) AS size;\n+------+\n| size |\n+------+\n| NULL |\n+------+\nSELECT 1 row in set (... sec)\n\narray_set(array, index, value)\n\nThe array_set function returns the array with the element at index set to value.\n\nGaps are filled with null.\n\nReturns: array\n\ncr> select array_set(['_', 'b'], 1, 'a') AS arr;\n+------------+\n| arr        |\n+------------+\n| [\"a\", \"b\"] |\n+------------+\nSELECT 1 row in set (... sec)\n\narray_set(source_array, indexes_array, values_array)\n\nSecond overload for array_set that updates many indices with many values at once. Depending on the indexes provided, array_set updates or appends the values and also fills any gaps with nulls.\n\nReturns: array\n\ncr> select array_set(['_', 'b'], [1, 4], ['a', 'd']) AS arr;\n+-----------------------+\n| arr                   |\n+-----------------------+\n| [\"a\", \"b\", null, \"d\"] |\n+-----------------------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nUpdating indexes less than or equal to 0 is not supported.\n\narray_slice(anyarray, from, to)\n\nThe array_slice function returns a slice of the given array using the given lower and upper bound.\n\nReturns: array\n\nSee Also\n\nAccessing arrays\n\ncr> select array_slice(['a', 'b', 'c', 'd'], 2, 3) AS arr;\n+------------+\n| arr        |\n+------------+\n| [\"b\", \"c\"] |\n+------------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nThe first index value is 1. The maximum array index is 2147483647. Both the from and to index values are inclusive. Using an index greater than the array size results in an empty array.\n\narray_to_string(anyarray, separator, [ null_string ])\n\nThe array_to_string function concatenates elements of the given array into a single string using the separator.\n\nReturns: text\n\ncr> select\n...     array_to_string(\n...         ['Arthur', 'Ford', 'Trillian'], ','\n...     ) AS str;\n+----------------------+\n| str                  |\n+----------------------+\n| Arthur,Ford,Trillian |\n+----------------------+\nSELECT 1 row in set (... sec)\n\n\nIf the separator argument is NULL, the result is NULL:\n\ncr> select\n...     array_to_string(\n...         ['Arthur', 'Ford', 'Trillian'], NULL\n...     ) AS str;\n+------+\n|  str |\n+------+\n| NULL |\n+------+\nSELECT 1 row in set (... sec)\n\n\nIf null_string is provided and is not NULL, then NULL elements of the array are replaced by that string, otherwise they are omitted:\n\ncr> select\n...     array_to_string(\n...         ['Arthur', NULL, 'Trillian'], ',', 'Ford'\n...     ) AS str;\n+----------------------+\n| str                  |\n+----------------------+\n| Arthur,Ford,Trillian |\n+----------------------+\nSELECT 1 row in set (... sec)\n\ncr> select\n...     array_to_string(\n...         ['Arthur', NULL, 'Trillian'], ','\n...     ) AS str;\n+-----------------+\n| str             |\n+-----------------+\n| Arthur,Trillian |\n+-----------------+\nSELECT 1 row in set (... sec)\n\ncr> select\n...     array_to_string(\n...         ['Arthur', NULL, 'Trillian'], ',', NULL\n...     ) AS str;\n+-----------------+\n| str             |\n+-----------------+\n| Arthur,Trillian |\n+-----------------+\nSELECT 1 row in set (... sec)\n\nstring_to_array(string, separator, [ null_string ])\n\nThe string_to_array splits a string into an array of text elements using a supplied separator and an optional null-string to set matching substring elements to NULL.\n\nReturns: array(text)\n\ncr> select string_to_array('Arthur,Ford,Trillian', ',') AS arr;\n+--------------------------------+\n| arr                            |\n+--------------------------------+\n| [\"Arthur\", \"Ford\", \"Trillian\"] |\n+--------------------------------+\nSELECT 1 row in set (... sec)\n\ncr> select string_to_array('Arthur,Ford,Trillian', ',', 'Ford') AS arr;\n+------------------------------+\n| arr                          |\n+------------------------------+\n| [\"Arthur\", null, \"Trillian\"] |\n+------------------------------+\nSELECT 1 row in set (... sec)\n\nseparator\n\nIf the separator argument is NULL, each character of the input string becomes a separate element in the resulting array.\n\ncr> select string_to_array('Ford', NULL) AS arr;\n+----------------------+\n| arr                  |\n+----------------------+\n| [\"F\", \"o\", \"r\", \"d\"] |\n+----------------------+\nSELECT 1 row in set (... sec)\n\n\nIf the separator is an empty string, then the entire input string is returned as a one-element array.\n\ncr> select string_to_array('Arthur,Ford', '') AS arr;\n+-----------------+\n| arr             |\n+-----------------+\n| [\"Arthur,Ford\"] |\n+-----------------+\nSELECT 1 row in set (... sec)\n\nnull_string\n\nIf the null_string argument is omitted or NULL, none of the substrings of the input will be replaced by NULL.\n\narray_min(array)\n\nThe array_min function returns the smallest element in array. If array is NULL or an empty array, the function returns NULL. This function supports arrays of any of the primitive types.\n\ncr> SELECT array_min([3, 2, 1]) AS min;\n+-----+\n| min |\n+-----+\n|   1 |\n+-----+\nSELECT 1 row in set (... sec)\n\narray_position(anycompatiblearray, anycompatible [, integer ] ) → integer\n\nThe array_position function returns the position of the first occurrence of the second argument in the array, or NULL if it’s not present. If the third argument is given, the search begins at that position. The third argument is ignored if it’s null. If not within the array range, NULL is returned. It is also possible to search for NULL values.\n\ncr> SELECT array_position([1,3,7,4], 7) as position;\n+----------+\n| position |\n+----------+\n|        3 |\n+----------+\nSELECT 1 row in set (... sec)\n\n\nBegin the search from given position (optional).\n\ncr> SELECT array_position([1,3,7,4], 7, 2) as position;\n+----------+\n| position |\n+----------+\n|        3 |\n+----------+\nSELECT 1 row in set (... sec)\n\n\nTip\n\nWhen searching for the existence of an array element, using the ANY operator inside the WHERE clause is much more efficient as it can utilize the index whereas array_position won’t even when used inside the WHERE clause.\n\narray_max(array)\n\nThe array_max function returns the largest element in array. If array is NULL or an empty array, the function returns NULL. This function supports arrays of any of the primitive types.\n\ncr> SELECT array_max([1,2,3]) AS max;\n+-----+\n| max |\n+-----+\n|   3 |\n+-----+\nSELECT 1 row in set (... sec)\n\narray_sum(array)\n\nReturns the sum of array elements that are not NULL. If array is NULL or an empty array, the function returns NULL. This function supports arrays of any numeric types.\n\nFor real and double precison arguments, the return type is equal to the argument type. For char, smallint, integer, and bigint arguments, the return type changes to bigint.\n\nIf any bigint value exceeds range limits (-2^64 to 2^64-1), an ArithmeticException will be raised.\n\ncr> SELECT array_sum([1,2,3]) AS sum;\n+-----+\n| sum |\n+-----+\n|   6 |\n+-----+\nSELECT 1 row in set (... sec)\n\n\nThe sum on the bigint array will result in an overflow in the following query:\n\ncr> SELECT\n...     array_sum(\n...         [9223372036854775807, 9223372036854775807]\n...     ) as sum;\nArithmeticException[long overflow]\n\n\nTo address the overflow of the sum of the given array elements, we cast the array to the numeric data type:\n\ncr>  SELECT\n...     array_sum(\n...         [9223372036854775807, 9223372036854775807]::numeric[]\n...     ) as sum;\n+----------------------+\n|                  sum |\n+----------------------+\n| 18446744073709551614 |\n+----------------------+\nSELECT 1 row in set (... sec)\n\narray_avg(array)\n\nReturns the average of all values in array that are not NULL If array is NULL or an empty array, the function returns NULL. This function supports arrays of any numeric types.\n\nFor real and double precison arguments, the return type is equal to the argument type. For char, smallint, integer, and bigint arguments, the return type is numeric.\n\ncr> SELECT array_avg([1,2,3]) AS avg;\n+-----+\n| avg |\n+-----+\n|   2 |\n+-----+\nSELECT 1 row in set (... sec)\n\narray_unnest(nested_array)\n\nTakes a nested array and returns a flattened array. Only flattens one level at a time.\n\nReturns NULL if the argument is NULL. NULL array elements are skipped and NULL leaf elements within arrays are preserved.\n\ncr> SELECT array_unnest([[1, 2], [3, 4, 5]]) AS result;\n+-----------------+\n| result          |\n+-----------------+\n| [1, 2, 3, 4, 5] |\n+-----------------+\nSELECT 1 row in set (... sec)\n\n\ncr> SELECT array_unnest([[1, null, 2], null, [3, 4, 5]]) AS result;\n+-----------------------+\n| result                |\n+-----------------------+\n| [1, null, 2, 3, 4, 5] |\n+-----------------------+\nSELECT 1 row in set (... sec)\n\n\nSee Also\n\nUNNEST table function\n\nObject functions\nobject_keys(object)\n\nThe object_keys function returns the set of first level keys of an object.\n\nReturns: array(text)\n\ncr> SELECT\n...     object_keys({a = 1, b = {c = 2}}) AS object_keys;\n+-------------+\n| object_keys |\n+-------------+\n| [\"a\", \"b\"]  |\n+-------------+\nSELECT 1 row in set (... sec)\n\nconcat(object, object)\n\nThe concat(object, object) function combines two objects into a new object containing the union of their first level properties, taking the second object’s values for duplicate properties. If one of the objects is NULL, the function returns the non-NULL object. If both objects are NULL, the function returns NULL.\n\nReturns: object\n\ncr> SELECT\n...     concat({a = 1}, {a = 2, b = {c = 2}}) AS object_concat;\n+-------------------------+\n| object_concat           |\n+-------------------------+\n| {\"a\": 2, \"b\": {\"c\": 2}} |\n+-------------------------+\nSELECT 1 row in set (... sec)\n\n\nYou can also use the concat operator || with objects:\n\ncr> SELECT\n...     {a = 1} || {b = 2} || {c = 3} AS object_concat;\n+--------------------------+\n| object_concat            |\n+--------------------------+\n| {\"a\": 1, \"b\": 2, \"c\": 3} |\n+--------------------------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nconcat(object, object) does not operate recursively: only the top-level object structure is merged:\n\ncr> SELECT\n...     concat({a = {b = 4}}, {a = {c = 2}}) as object_concat;\n+-----------------+\n| object_concat   |\n+-----------------+\n| {\"a\": {\"c\": 2}} |\n+-----------------+\nSELECT 1 row in set (... sec)\n\nnull_or_empty(object)\n\nThe null_or_empty(object) function returns a boolean indicating if an object is NULL or empty ({}).\n\nThis can serve as a faster alternative to IS NULL if matching on empty objects is acceptable. It makes better use of indices.\n\ncr> SELECT null_or_empty({}) x, null_or_empty(NULL) y, null_or_empty({x=10}) z;\n+------+------+-------+\n| x    | y    | z     |\n+------+------+-------+\n| TRUE | TRUE | FALSE |\n+------+------+-------+\nSELECT 1 row in set (... sec)\n\nConditional functions and expressions\nCASE WHEN ... THEN ... END\n\nThe case expression is a generic conditional expression similar to if/else statements in other programming languages and can be used wherever an expression is valid.\n\nCASE WHEN condition THEN result\n     [WHEN ...]\n     [ELSE result]\nEND\n\n\nEach condition expression must result in a boolean value. If the condition’s result is true, the value of the result expression that follows the condition will be the final result of the case expression and the subsequent when branches will not be processed. If the condition’s result is not true, any subsequent when clauses are examined in the same manner. If no when condition yields true, the value of the case expression is the result of the else clause. If the else clause is omitted and no condition is true, the result is null.\n\nExample:\n\ncr> select id,\n...   case when id = 0 then 'zero'\n...        when id % 2 = 0 then 'even'\n...        else 'odd'\n...   end as parity\n... from case_example order by id;\n+----+--------+\n| id | parity |\n+----+--------+\n|  0 | zero   |\n|  1 | odd    |\n|  2 | even   |\n|  3 | odd    |\n+----+--------+\nSELECT 4 rows in set (... sec)\n\n\nAs a variant, a case expression can be written using the simple form:\n\nCASE expression\n     WHEN value THEN result\n     [WHEN ...]\n     [ELSE result]\nEND\n\n\nExample:\n\ncr> select id,\n...   case id when 0 then 'zero'\n...           when 1 then 'one'\n...           else 'other'\n...   end as description\n... from case_example order by id;\n+----+-------------+\n| id | description |\n+----+-------------+\n|  0 | zero        |\n|  1 | one         |\n|  2 | other       |\n|  3 | other       |\n+----+-------------+\nSELECT 4 rows in set (... sec)\n\n\nNote\n\nAll result expressions must be convertible to a single data type.\n\nif(condition, result [, default])\n\nThe if function is a conditional function comparing to if statements of most other programming languages. If the given condition expression evaluates to true, the result expression is evaluated and its value is returned. If the condition evaluates to false, the result expression is not evaluated and the optional given default expression is evaluated instead and its value will be returned. If the default argument is omitted, NULL will be returned instead.\n\ncr> select\n...     id,\n...     if(id = 0, 'zero', 'other') as description\n... from if_example\n... order by id;\n+----+-------------+\n| id | description |\n+----+-------------+\n|  0 | zero        |\n|  1 | other       |\n|  2 | other       |\n|  3 | other       |\n+----+-------------+\nSELECT 4 rows in set (... sec)\n\ncoalesce('first_arg', second_arg [, ... ])\n\nThe coalesce function takes one or more arguments of the same type and returns the first non-null value of these. The result will be NULL only if all the arguments evaluate to NULL.\n\nReturns: same type as arguments\n\ncr> select coalesce(clustered_by, 'nothing') AS clustered_by\n...   from information_schema.tables\n...   where table_name='nodes';\n+--------------+\n| clustered_by |\n+--------------+\n| nothing      |\n+--------------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nIf the data types of the arguments are not of the same type, coalesce will try to cast them to a common type, and if it fails to do so, an error is thrown.\n\ngreatest('first_arg', second_arg[ , ... ])\n\nThe greatest function takes one or more arguments of the same type and will return the largest value of these. NULL values in the arguments list are ignored. The result will be NULL only if all the arguments evaluate to NULL.\n\nReturns: same type as arguments\n\ncr> select greatest(1, 2) AS greatest;\n+----------+\n| greatest |\n+----------+\n|        2 |\n+----------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nIf the data types of the arguments are not of the same type, greatest will try to cast them to a common type, and if it fails to do so, an error is thrown.\n\nleast('first_arg', second_arg[ , ... ])\n\nThe least function takes one or more arguments of the same type and will return the smallest value of these. NULL values in the arguments list are ignored. The result will be NULL only if all the arguments evaluate to NULL.\n\nReturns: same type as arguments\n\ncr> select least(1, 2) AS least;\n+-------+\n| least |\n+-------+\n|     1 |\n+-------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nIf the data types of the arguments are not of the same type, least will try to cast them to a common type, and if it fails to do so, an error is thrown.\n\nnullif('first_arg', second_arg)\n\nThe nullif function compares two arguments of the same type and, if they have the same value, returns NULL; otherwise returns the first argument.\n\nReturns: same type as arguments\n\ncr> select nullif(table_schema, 'sys') AS nullif\n...   from information_schema.tables\n...   where table_name='nodes';\n+--------+\n| nullif |\n+--------+\n|   NULL |\n+--------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nIf the data types of the arguments are not of the same type, nullif will try to cast them to a common type, and if it fails to do so, an error is thrown.\n\nSystem information functions\nCURRENT_SCHEMA\n\nThe CURRENT_SCHEMA system information function returns the name of the current schema of the session. If no current schema is set, this function will return the default schema, which is doc.\n\nReturns: text\n\nThe default schema can be set when using the JDBC client and HTTP clients such as CrateDB PDO.\n\nNote\n\nThe CURRENT_SCHEMA function has a special SQL syntax, meaning that it must be called without trailing parenthesis (()). However, CrateDB also supports the optional parenthesis.\n\nSynopsis:\n\nCURRENT_SCHEMA [ ( ) ]\n\n\nExample:\n\ncr> SELECT CURRENT_SCHEMA;\n+----------------+\n| current_schema |\n+----------------+\n|            doc |\n+----------------+\nSELECT 1 row in set (... sec)\n\nCURRENT_SCHEMAS(boolean)\n\nThe CURRENT_SCHEMAS() system information function returns the current stored schemas inside the search_path session state, optionally including implicit schemas (e.g. pg_catalog). If no custom search_path is set, this function will return the default search_path schemas.\n\nReturns: array(text)\n\nSynopsis:\n\nCURRENT_SCHEMAS ( boolean )\n\n\nExample:\n\ncr> SELECT CURRENT_SCHEMAS(true) AS schemas;\n+-----------------------+\n| schemas               |\n+-----------------------+\n| [\"pg_catalog\", \"doc\"] |\n+-----------------------+\nSELECT 1 row in set (... sec)\n\nCURRENT_USER\n\nThe CURRENT_USER system information function returns the name of the current connected user or crate if the user management module is disabled.\n\nReturns: text\n\nSynopsis:\n\nCURRENT_USER\n\n\nExample:\n\ncr> select current_user AS name;\n+-------+\n| name  |\n+-------+\n| crate |\n+-------+\nSELECT 1 row in set (... sec)\n\nUSER\n\nEquivalent to CURRENT_USER.\n\nReturns: text\n\nSynopsis:\n\nUSER\n\n\nExample:\n\ncr> select user AS name;\n+-------+\n| name  |\n+-------+\n| crate |\n+-------+\nSELECT 1 row in set (... sec)\n\nSESSION_USER\n\nThe SESSION_USER system information function returns the name of the current connected user or crate if the user management module is disabled.\n\nReturns: text\n\nSynopsis:\n\nSESSION_USER\n\n\nExample:\n\ncr> select session_user AS name;\n+-------+\n| name  |\n+-------+\n| crate |\n+-------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nCrateDB doesn’t currently support the switching of execution context. This makes SESSION_USER functionally equivalent to CURRENT_USER. We provide it as it’s part of the SQL standard.\n\nAdditionally, the CURRENT_USER, SESSION_USER and USER functions have a special SQL syntax, meaning that they must be called without trailing parenthesis (()).\n\nhas_database_privilege([user,] database, privilege text)\n\nReturns boolean or NULL if at least one argument is NULL.\n\nFirst argument is TEXT user name or INTEGER user OID. If user is not specified current user is used as an argument.\n\nSecond argument is TEXT database name or INTEGER database OID.\n\nNote\n\nOnly crate is valid for database name and only 0 is valid for database OID.\n\nThird argument is privilege(s) to check. Multiple privileges can be provided as a comma separated list, in which case the result will be true if any of the listed privileges is held. Allowed privilege types are CONNECT, CREATE and TEMP or TEMPORARY. Privilege string is case insensitive and extra whitespace is allowed between privilege names. Duplicate entries in privilege string are allowed.\n\nCONNECT\n\nis true for all defined users in the database\n\nCREATE\n\nis true if the user has any DDL privilege on CLUSTER or on any SCHEMA\n\nTEMP\n\nis false for all users\n\nExample:\n\ncr> select has_database_privilege('crate', ' Connect ,  CREATe ')\n... as has_priv;\n+----------+\n| has_priv |\n+----------+\n| TRUE     |\n+----------+\nSELECT 1 row in set (... sec)\n\nhas_schema_privilege([user,] schema, privilege text)\n\nReturns boolean or NULL if at least one argument is NULL.\n\nFirst argument is TEXT user name or INTEGER user OID. If user is not specified current user is used as an argument.\n\nSecond argument is TEXT schema name or INTEGER schema OID.\n\nThird argument is privilege(s) to check. Multiple privileges can be provided as a comma separated list, in which case the result will be true if any of the listed privileges is held. Allowed privilege types are CREATE and USAGE which corresponds to CrateDB’s DDL and DQL. Privilege string is case insensitive and extra whitespace is allowed between privilege names. Duplicate entries in privilege string are allowed.\n\nExample:\n\ncr> select has_schema_privilege('pg_catalog', ' Create , UsaGe , CREATe ')\n... as has_priv;\n+----------+\n| has_priv |\n+----------+\n| TRUE     |\n+----------+\nSELECT 1 row in set (... sec)\n\npg_backend_pid()\n\nThe pg_backend_pid() system information function is implemented for enhanced compatibility with PostgreSQL. CrateDB will always return -1 as there isn’t a single process attached to one query. This is different to PostgreSQL, where this represents the process ID of the server process attached to the current session.\n\nReturns: integer\n\nSynopsis:\n\npg_backend_pid()\n\n\nExample:\n\ncr> select pg_backend_pid() AS pid;\n+-----+\n| pid |\n+-----+\n|  -1 |\n+-----+\nSELECT 1 row in set (... sec)\n\npg_postmaster_start_time()\n\nReturns the server start time as timestamp with time zone.\n\ncurrent_database()\n\nThe current_database function returns the name of the current database, which in CrateDB will always be crate:\n\ncr> select current_database() AS db;\n+-------+\n| db    |\n+-------+\n| crate |\n+-------+\nSELECT 1 row in set (... sec)\n\ncurrent_setting(text [,boolean])\n\nThe current_setting function returns the current value of a session setting.\n\nReturns: text\n\nSynopsis:\n\ncurrent_setting(setting_name [, missing_ok])\n\n\nIf no setting exists for setting_name, current_setting throws an error, unless missing_ok argument is provided and is true.\n\nExamples:\n\ncr> select current_setting('search_path') AS search_path;\n+-------------+\n| search_path |\n+-------------+\n| doc         |\n+-------------+\nSELECT 1 row in set (... sec)\n\ncr> select current_setting('foo');\nSQLParseException[Unrecognised Setting: foo]\n\ncr> select current_setting('foo', true) AS foo;\n+------+\n|  foo |\n+------+\n| NULL |\n+------+\nSELECT 1 row in set (... sec)\n\npg_get_expr()\n\nThe function pg_get_expr is implemented to improve compatibility with clients that use the PostgreSQL wire protocol. The function always returns null.\n\nSynopsis:\n\npg_get_expr(expr text, relation_oid int [, pretty boolean])\n\n\nExample:\n\ncr> select pg_get_expr('literal', 1) AS col;\n+------+\n|  col |\n+------+\n| NULL |\n+------+\nSELECT 1 row in set (... sec)\n\npg_get_partkeydef()\n\nThe function pg_get_partkeydef is implemented to improve compatibility with clients that use the PostgreSQL wire protocol. Partitioning in CrateDB is different from PostgreSQL, therefore this function always returns null.\n\nSynopsis:\n\npg_get_partkeydef(relation_oid int)\n\n\nExample:\n\ncr> select pg_get_partkeydef(1) AS col;\n+------+\n|  col |\n+------+\n| NULL |\n+------+\nSELECT 1 row in set (... sec)\n\npg_get_serial_sequence()\n\nThe function pg_get_serial_sequence is implemented to improve compatibility with clients that use the PostgreSQL wire protocol. The function always returns null. Existence of tables or columns is not validated.\n\nSynopsis:\n\npg_get_serial_sequence(table_name text, column_name text)\n\n\nExample:\n\ncr> select pg_get_serial_sequence('t1', 'c1') AS col;\n+------+\n|  col |\n+------+\n| NULL |\n+------+\nSELECT 1 row in set (... sec)\n\npg_encoding_to_char()\n\nThe function pg_encoding_to_char converts an PostgreSQL encoding’s internal identifier to a human-readable name.\n\nReturns: text\n\nSynopsis:\n\npg_encoding_to_char(encoding int)\n\n\nExample:\n\ncr> select pg_encoding_to_char(6) AS encoding;\n+----------+\n| encoding |\n+----------+\n| UTF8     |\n+----------+\nSELECT 1 row in set (... sec)\n\npg_get_userbyid()\n\nThe function pg_get_userbyid is implemented to improve compatibility with clients that use the PostgreSQL wire protocol. The function always returns the default CrateDB user for non-null arguments, otherwise, null is returned.\n\nReturns: text\n\nSynopsis:\n\npg_get_userbyid(id integer)\n\n\nExample:\n\ncr> select pg_get_userbyid(1) AS name;\n+-------+\n| name  |\n+-------+\n| crate |\n+-------+\nSELECT 1 row in set (... sec)\n\npg_typeof()\n\nThe function pg_typeof returns the text representation of the value’s data type passed to it.\n\nReturns: text\n\nSynopsis:\n\npg_typeof(expression)\n\n\nExample:\n\ncr> select pg_typeof([1, 2, 3]) as typeof;\n+---------------+\n| typeof        |\n+---------------+\n| integer_array |\n+---------------+\nSELECT 1 row in set (... sec)\n\npg_function_is_visible()\n\nThe function pg_function_is_visible returns true for OIDs that refer to a system or a user defined function.\n\nReturns: boolean\n\nSynopsis:\n\npg_function_is_visible(OID)\n\n\nExample:\n\ncr> select pg_function_is_visible(-919555782) as pg_function_is_visible;\n+------------------------+\n| pg_function_is_visible |\n+------------------------+\n| TRUE                   |\n+------------------------+\nSELECT 1 row in set (... sec)\n\npg_get_function_result()\n\nThe function pg_get_function_result returns the text representation of the return value’s data type of the function referred by the OID.\n\nReturns: text\n\nSynopsis:\n\npg_get_function_result(OID)\n\n\nExample:\n\ncr> select pg_get_function_result(-919555782) as _pg_get_function_result;\n+-------------------------+\n| _pg_get_function_result |\n+-------------------------+\n| time with time zone     |\n+-------------------------+\nSELECT 1 row in set (... sec)\n\nversion()\n\nReturns the CrateDB version information.\n\nReturns: text\n\nSynopsis:\n\nversion()\n\n\nExample:\n\ncr> select version() AS version;\n+---------...-+\n| version     |\n+---------...-+\n| CrateDB ... |\n+---------...-+\nSELECT 1 row in set (... sec)\n\ncol_description(integer, integer)\n\nThis function exists mainly for compatibility with PostgreSQL. In PostgreSQL, the function returns the comment for a table column. CrateDB doesn’t support user defined comments for table columns, so it always returns null.\n\nReturns: text\n\nExample:\n\ncr> SELECT pg_catalog.col_description(1, 1) AS comment;\n+---------+\n| comment |\n+---------+\n|    NULL |\n+---------+\nSELECT 1 row in set (... sec)\n\nobj_description(integer, text)\n\nThis function exists mainly for compatibility with PostgreSQL. In PostgreSQL, the function returns the comment for a database object. CrateDB doesn’t support user defined comments for database objects, so it always returns null.\n\nReturns: text\n\nExample:\n\ncr> SELECT pg_catalog.obj_description(1, 'pg_type') AS comment;\n+---------+\n| comment |\n+---------+\n|    NULL |\n+---------+\nSELECT 1 row in set (... sec)\n\nformat_type(integer, integer)\n\nReturns the type name of a type. The first argument is the OID of the type. The second argument is the type modifier. This function exits for PostgreSQL compatibility and the type modifier is always ignored.\n\nReturns: text\n\nExample:\n\ncr> SELECT pg_catalog.format_type(25, null) AS name;\n+------+\n| name |\n+------+\n| text |\n+------+\nSELECT 1 row in set (... sec)\n\n\nIf the given OID is not know, ??? is returned:\n\ncr> SELECT pg_catalog.format_type(3, null) AS name;\n+------+\n| name |\n+------+\n|  ??? |\n+------+\nSELECT 1 row in set (... sec)\n\nSpecial functions\nknn_match(float_vector, float_vector, int)\n\nThe knn_match function uses a k-nearest neighbour (kNN) search algorithm to find vectors that are similar to a query vector.\n\nThe first argument is the column to search. The second argument is the query vector. The third argument is the number of nearest neighbours to search in the index. Searching a larger number of nearest neighbours is more expensive. There is one index per shard, and on each shard the function will match at most k records. To limit the total query result, add a LIMIT clause to the query.\n\nknn_match(search_vector, target, k)\n\nThis function must be used within a WHERE clause targeting a table to use it as a predicate that searches the whole dataset of a table. Using it outside of a WHERE clause, or in a WHERE clause targeting a virtual table instead of a physical table, results in an error.\n\nSimilar to the MATCH predicate, this function affects the _score value.\n\nAn example:\n\ncr> CREATE TABLE IF NOT EXISTS doc.vectors (\n...    xs float_vector(2)\n...  );\nCREATE OK, 1 row affected (... sec)\n\ncr> INSERT INTO doc.vectors (xs)\n...   VALUES\n...   ([3.14, 8.17]),\n...   ([14.3, 19.4]);\nINSERT OK, 2 rows affected (... sec)\n\ncr> SELECT xs, _score FROM doc.vectors\n... WHERE knn_match(xs, [3.14, 8], 2)\n... ORDER BY _score DESC;\n+--------------+--------------+\n| xs           |       _score |\n+--------------+--------------+\n| [3.14, 8.17] | 0.9719117    |\n| [14.3, 19.4] | 0.0039138086 |\n+--------------+--------------+\nSELECT 2 rows in set (... sec)\n\nignore3vl(boolean)\n\nThe ignore3vl function operates on a boolean argument and eliminates the 3-valued logic on the whole tree of operators beneath it. More specifically, FALSE is evaluated to FALSE, TRUE to TRUE and NULL to FALSE.\n\nReturns: boolean\n\nNote\n\nThe main usage of the ignore3vl function is in the WHERE clause when a NOT operator is involved. Such filtering, with 3-valued logic, cannot be translated to an optimized query in the internal storage engine, and therefore can degrade performance. E.g.:\n\nSELECT * FROM t\nWHERE NOT 5 = ANY(t.int_array_col);\n\n\nIf we can ignore the 3-valued logic, we can write the query as:\n\nSELECT * FROM t\nWHERE NOT IGNORE3VL(5 = ANY(t.int_array_col));\n\n\nwhich will yield better performance (in execution time) than before.\n\nCaution\n\nIf there are NULL values in the long_array_col, in the case that 5 = ANY(t.long_array_col) evaluates to NULL, without the ignore3vl, it would be evaluated as NOT NULL => NULL, resulting to zero matched rows. With the IGNORE3VL in place it will be evaluated as NOT FALSE => TRUE resulting to all rows matching the filter. E.g:\n\ncr> SELECT * FROM t\n... WHERE NOT 5 = ANY(t.int_array_col);\n+---------------+\n| int_array_col |\n+---------------+\n+---------------+\nSELECT 0 rows in set (... sec)\n\ncr> SELECT * FROM t\n... WHERE NOT IGNORE3VL(5 = ANY(t.int_array_col));\n+-----------------+\n| int_array_col   |\n+-----------------+\n| [1, 2, 3, null] |\n+-----------------+\nSELECT 1 row in set (... sec)\n\n\nSynopsis:\n\nignore3vl(boolean)\n\n\nExample:\n\ncr> SELECT\n...     ignore3vl(true) as v1,\n...     ignore3vl(false) as v2,\n...     ignore3vl(null) as v3;\n+------+-------+-------+\n| v1   | v2    | v3    |\n+------+-------+-------+\n| TRUE | FALSE | FALSE |\n+------+-------+-------+\nSELECT 1 row in set (... sec)\n"
  },
  {
    "title": "Geo search — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/dql/geo.html",
    "html": "5.6\nGeo search\n\nTable of contents\n\nIntroduction\n\nMATCH predicate\n\nExact queries\n\nIntroduction\n\nCrateDB can be used to store and query geographical information of many kinds using the Geometric points and Geometric shapes types. With these it is possible to store geographical locations, ways, shapes, areas and other entities. These can be queried for distance, containment, intersection and so on, making it possible to create apps and services with rich geographical features.\n\nGeographic shapes are stored using special indices. Geographic points are represented by their coordinates. They are represented as columns of the respective data types.\n\nGeographic indices for geo_shape columns are used in order to speed up geographic searches even on complex shapes. This indexing process results in a representation that is not exact (See geo_shape for details). CrateDB does not operate on vector shapes but on a kind of a grid with the given precision as resolution.\n\nCreating tables containing geographic information is straightforward:\n\ncr> CREATE TABLE country (\n...   name text,\n...   country_code text primary key,\n...   shape geo_shape INDEX USING \"geohash\" WITH (precision='100m'),\n...   capital text,\n...   capital_location geo_point\n... ) WITH (number_of_replicas=0);\nCREATE OK, 1 row affected  (... sec)\n\n\nThis table will contain the shape of a country and the location of its capital alongside with other metadata. The shape is indexed with a maximum precision of 100 meters using a geohash index (For more information, see Geo shape index structure).\n\nLet’s insert Austria:\n\ncr> INSERT INTO country (name, country_code, shape, capital, capital_location)\n... VALUES (\n...  'Austria',\n...  'at',\n...  {type='Polygon', coordinates=[\n...        [[16.979667, 48.123497], [16.903754, 47.714866],\n...        [16.340584, 47.712902], [16.534268, 47.496171],\n...        [16.202298, 46.852386], [16.011664, 46.683611],\n...        [15.137092, 46.658703], [14.632472, 46.431817],\n...        [13.806475, 46.509306], [12.376485, 46.767559],\n...        [12.153088, 47.115393], [11.164828, 46.941579],\n...        [11.048556, 46.751359], [10.442701, 46.893546],\n...        [9.932448, 46.920728], [9.47997, 47.10281],\n...        [9.632932, 47.347601], [9.594226, 47.525058],\n...        [9.896068, 47.580197], [10.402084, 47.302488],\n...        [10.544504, 47.566399], [11.426414, 47.523766],\n...        [12.141357, 47.703083], [12.62076, 47.672388],\n...        [12.932627, 47.467646], [13.025851, 47.637584],\n...        [12.884103, 48.289146], [13.243357, 48.416115],\n...        [13.595946, 48.877172], [14.338898, 48.555305],\n...        [14.901447, 48.964402], [15.253416, 49.039074],\n...        [16.029647, 48.733899], [16.499283, 48.785808],\n...        [16.960288, 48.596982], [16.879983, 48.470013],\n...        [16.979667, 48.123497]]\n...  ]},\n...  'Vienna',\n...  [16.372778, 48.209206]\n... );\nINSERT OK, 1 row affected (... sec)\n\n\nCaution\n\nGeoshapes has to be fully valid by ISO 19107. If you have problems importing geo data, they may not be fully valid. In most cases they could be repaired using this tool: https://github.com/tudelft3d/prepair\n\nNote\n\nWhen using a polygon shape that resembles a rectangle, and that rectangle is wider than 180 degrees, CrateDB will convert it into a multipolygon consisting of 2 rectangular shapes covering the narrower area between the 4 original points split by the dateline (+/- 180deg).\n\nThis is due to CrateDB operating in the geospatial context of the earth.\n\nGeographic points can be inserted as a double precision array with longitude and latitude values as seen above or by using a WKT string.\n\nGeographic shapes can be inserted as GeoJSON object literal or parameter as seen above and as WKT string.\n\nWhen it comes to get some meaningful insights into your geographical data CrateDB supports different kinds of geographic queries.\n\nFast queries that leverage the geographic index are done using the MATCH predicate:\n\nMATCH predicate\n\nThe MATCH predicate can be used to perform multiple kinds of searches on indices or indexed columns. While it can be used to perform fulltext searches on analyzed indices of type TEXT, it is also handy for operating on geographic indices, querying for relations between geographical shapes and points.\n\nMATCH (column_ident, query_term) [ using match_type ]\n\n\nThe MATCH predicate for geographical search supports a single column_ident of a geo_shape indexed column as first argument.\n\nThe second argument, the query_term is taken to match against the indexed geo_shape.\n\nThe matching operation is determined by the match_type which determines the spatial relation we want to match. Available match_types are:\n\nintersects\n\n(Default) If the two shapes share some points and/or area, they are intersecting and considered matching using this match_type. This also precludes containment or complete equality.\n\ndisjoint\n\nIf the two shapes share no single point or area, they are disjoint. This is the opposite of intersects.\n\nwithin\n\nIf the indexed column_ident shape is completely inside the query_term shape, they are considered matching using this match_type.\n\nNote\n\nThe MATCH predicate can only be used in the WHERE clause and on user-created tables. Using the MATCH predicate on system tables is not supported.\n\nOne MATCH predicate cannot combine columns of both relations of a join.\n\nAdditionally, MATCH predicates cannot be used on columns of both relations of a join if they cannot be logically applied to each of them separately. For example:\n\nThis is allowed:\n\n FROM t1, t2\nWHERE match(t1.shape, 'POINT(1.1 2.2)')\n  AND match(t2.shape, 'POINT(3.3 4.4)')\n\n\nBut this is not:\n\n FROM t1, t2\nWHERE match(t1.shape, 'POINT(1.1 2.2)')\n   OR match(t2.shape, 'POINT(3.3 4.4)')``\n\n\nCaution\n\nThe within match type does not support LineString and MultiLineString geo shapes provided as query_term for columns indexed using bkdtree.\n\nHaving a table countries with a GEO_SHAPE column geo, indexed using geohash, you can query that column using the MATCH predicate with different match types as described above:\n\ncr> SELECT name from countries\n... WHERE match(\"geo\",\n...   'LINESTRING (13.3813 52.5229, 11.1840 51.5497, 8.6132 50.0782, 8.3715 47.9457, 8.5034 47.3685)'\n... );\n+---------+\n| name    |\n+---------+\n| Germany |\n+---------+\nSELECT 1 row in set (... sec)\n\ncr> SELECT name from countries\n... WHERE match(\"geo\",\n...   'LINESTRING (13.3813 52.5229, 11.1840 51.5497, 8.6132 50.0782, 8.3715 47.9457, 8.5034 47.3685)'\n... ) USING disjoint\n... ORDER BY name;\n+--------------+\n| name         |\n+--------------+\n| Austria      |\n| France       |\n| South Africa |\n| Turkey       |\n+--------------+\nSELECT 4 rows in set (... sec)\n\nExact queries\n\nExact queries are done using the following scalar functions:\n\nintersects(geo_shape, geo_shape)\n\nwithin(shape1, shape2)\n\ndistance(geo_point1, geo_point2)\n\nThey are exact, but this comes at the price of performance.\n\nThey do not make use of the index but work on the GeoJSON that was inserted to compute the shape vector. This access is quite expensive and may significantly slow down your queries.\n\nFor fast querying, use the MATCH predicate.\n\nBut executed on a limited result set, they will help you get precise insights into your geographic data:\n\ncr> SELECT within(capital_location, shape) AS capital_in_country\n... FROM country;\n+--------------------+\n| capital_in_country |\n+--------------------+\n| TRUE               |\n+--------------------+\nSELECT 1 row in set (... sec)\n\ncr> SELECT distance(capital_location, 'POINT(0.0 90.0)') as from_northpole\n... FROM country ORDER BY country_code;\n+-------------------+\n|    from_northpole |\n+-------------------+\n| 4646930.675034644 |\n+-------------------+\nSELECT 1 row in set (... sec)\n\ncr> SELECT intersects(\n...   {type='LineString', coordinates=[[13.3813, 52.5229],\n...                                    [11.1840, 51.5497],\n...                                    [8.6132,  50.0782],\n...                                    [8.3715,  47.9457],\n...                                    [8.5034,  47.3685]]},\n...   shape) as berlin_zurich_intersects\n... FROM country ORDER BY country_code;\n+--------------------------+\n| berlin_zurich_intersects |\n+--------------------------+\n| FALSE                    |\n+--------------------------+\nSELECT 1 row in set (... sec)\n\n\nNonetheless these scalars can be used everywhere in a SQL query where scalar functions are allowed."
  },
  {
    "title": "Fulltext search — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/dql/fulltext.html",
    "html": "5.6\nFulltext search\n\nIn order to use fulltext search on one or more columns, a fulltext index with an analyzer has to be defined while creating the column: either with CREATE TABLE or ALTER TABLE ADD COLUMN. For more information see Fulltext indices.\n\nTable of contents\n\nMATCH Predicate\n\nSynopsis\n\nArguments\n\nMatch Types\n\nOptions\n\nUsage\n\nSearching On Multiple Columns\n\nNegative Search\n\nFilter By _score\n\nMATCH Predicate\nSynopsis\nMATCH (\n     {  column_or_idx_ident | ( column_or_idx_ident [boost]  [, ...] ) }\n , query_term\n)  [ using match_type [ with ( match_parameter [= value] [, ... ] ) ] ]\n\n\nThe MATCH predicate performs a fulltext search on one or more indexed columns or indices and supports different matching techniques. It can also be used to perform geographical searches on Geometric shapes indices.\n\nThe actual applicability of the MATCH predicate depends on the index’s type. In fact, the availability of certain match_types and match_parameters depend on the index. This section however, only covers the usage of the MATCH predicate on fulltext indices on text columns. To use MATCH on Geometric shapes indices, see Geo search.\n\nIn order to use fulltext searches on a column, a fulltext index with an analyzer must be created for this column. See Fulltext indices for details. There are different types of Fulltext indices with different goals, however it’s not possible to query multiple index columns with different index types within the same MATCH predicate.\n\nTo get the relevance of a matching row, a specific system column _score can be selected. It contains a numeric score relative to the other rows: The higher, the more relevant the row:\n\ncr> select name, _score from locations\n... where match(name_description_ft, 'time') order by _score desc;\n+-----------+------------+\n| name      |     _score |\n+-----------+------------+\n| Bartledan | 0.75782394 |\n| Altair    | 0.63013375 |\n+-----------+------------+\nSELECT 2 rows in set (... sec)\n\n\nThe MATCH predicate in its simplest form performs a fulltext search against a single column. It takes the query_term and, if no analyzer was provided, analyzes the term with the analyzer configured on column_or_idx_ident. The resulting tokens are then matched against the index at column_or_idx_ident and if one of them matches, MATCH returns TRUE.\n\nThe MATCH predicate can be also used to perform a fulltext search on multiple columns with a single query_term and to add weight to specific columns it’s possible to add a boost argument to each column_or_idx_ident. Matches on columns with a higher boost result in a higher _score value for that document.\n\nThe match_type argument determines how a single query_term is applied and how the resulting _score is computed. For more information see Match Types.\n\nResults are ordered by _score by default, but can be overridden by adding an ORDER BY clause.\n\nArguments\ncolumn_or_idx_ident\n\nA reference to a column or an index.\n\nIf the column has an implicit index (e.g. created with something like TEXT column_a INDEX USING FULLTEXT) this should be the name of the column.\n\nIf the column has an explicit index (e.g. created with something like INDEX \"column_a_idx\" USING FULLTEXT (\"column_a\") WITH (...)) this should be the name of the index.\n\nBy default every column is indexed but only the raw data is stored, so matching against a text column without a fulltext index is equivalent to using the = operator. To perform real fulltext searches use a fulltext index.\n\nboost\n\nA column ident can have a boost attached. That is a weight factor that increases the relevance of a column in respect to the other columns. The default boost is 1.\n\nquery_term\n\nThis string is analyzed (using the explicitly given analyzer or the analyzer of the columns to perform the search on) and the resulting tokens are compared to the index. The tokens used for search are combined using the boolean OR operator unless stated otherwise using the operator option.\n\nmatch_type\n\nOptional. Defaults to best_fields for fulltext indices. For details see Match Types.\n\nNote\n\nThe MATCH predicate can only be used in the WHERE clause and on user-created tables. Using the MATCH predicate on system tables is not supported.\n\nOne MATCH predicate cannot combine columns of both relations of a join.\n\nAdditionally, MATCH predicates cannot be used on columns of both relations of a join if they cannot be logically applied to each of them separately. For example:\n\nThis is allowed:\n\nFROM t1, t2 WHERE match(t1.txt, 'foo') AND match(t2.txt, 'bar');``\n\n\nBut this is not:\n\nFROM t1, t2 WHERE match(t1.txt, 'foo') OR match(t2.txt, 'bar');\n\nMatch Types\n\nThe match type determines how the query_term is applied and the _score is created, thereby influencing which documents are considered more relevant. The default match_type for fulltext indices is best_fields.\n\nbest_fields\n\nUse the _score of the column that matched best. For example if a column contains all the tokens of the query_term it’s considered more relevant than other columns containing only one.\n\nThis type is the default, if omitted.\n\nmost_fields\n\nThis match type takes the _score of every matching column and averages their scores.\n\ncross_fields\n\nThis match type analyzes the query_term into tokens and searches all tokens in all given columns at once as if they were one big column (given they have the same analyzer). All tokens have to be present in at least one column, so querying for foo bar should have the tokens foo in one column and bar in the same or any other.\n\nphrase\n\nThis match type differs from best_fields in that it constructs a phrase query from the query_term. A phrase query will only match if the tokens in the columns are exactly in the same order as the analyzed columns from the query_term. So, querying for foo bar (analyzed tokens: foo and bar) will only match if one of the columns contains those two token in that order - without any other tokens in between.\n\nphrase_prefix\n\nThis match type is roughly the same than phrase but it allows to match by prefix on the last token of the query_term. For example if your query for foo ba, one of the columns has to contain foo and a token that starts with ba in that order. So a column containing foo baz would match and foo bar too.\n\nOptions\n\nThe match options further distinguish the way the matching process using a certain match type works. Not all options are applicable to all match types. See the options below for details.\n\nanalyzer\n\nThe analyzer used to convert the query_term into tokens.\n\nboost\n\nThis numeric value is multiplied with the resulting _score of this match call.\n\nIf this match call is used with other conditions in the where clause a value above 1.0 will increase its influence on the overall _score of the whole query, a value below 1.0 will decrease it.\n\ncutoff_frequency\n\nThe token frequency is the number of occurrences of a token in a column.\n\nThis option specifies a minimum token frequency that excludes matching tokens with a higher frequency from the overall _score. Their _score is only included if another token with a lower frequency also matches. This can be used to suppress results where only high frequency terms like the would cause a match.\n\nfuzziness\n\nCan be used to perform fuzzy full text search.\n\nOn numeric columns use a numeric, on timestamp columns a long indicating milliseconds, on strings use a number indicating the maximum allowed Levenshtein Edit Distance. Use prefix_length, fuzzy_rewrite and max_expansions to fine tune the fuzzy matching process.\n\nfuzzy_rewrite\n\nThe same than rewrite but only applies to queries using fuzziness.\n\nmax_expansions\n\nWhen using fuzziness or phrase_prefix this options controls to how many different possible tokens a search token will be expanded. The fuzziness controls how big the distance or difference between the original token and the set of tokens it is expanded to can be. This option controls how big this set can get.\n\nminimum_should_match\n\nThe number of tokens from the query_term to match when or is used. Defaults to 1.\n\noperator\n\nCan be or or and. The default operator is or. It is used to combine the tokens of the query_term. If and is used, every token from the query_term has to match. If or is used only the number of minimum_should_match have to match.\n\nprefix_length\n\nWhen used with fuzziness option or with phrase_prefix this options controls how long the common prefix of the tokens that are considered as similar (same prefix or fuzziness distance/difference)has to be.\n\nrewrite\n\nWhen using phrase_prefix the prefix query is constructed using all possible terms and rewriting them into another kind of query to compute the score. Possible values are constant_score_auto, constant_score_boolean, constant_score_filter, scoring_boolean,``top_terms_N``, top_terms_boost_N. The constant_... values can be used together with the boost option to set a constant _score for rows with a matching prefix or fuzzy match.\n\nslop\n\nWhen matching for phrases this option controls how exact the phrase match should be (proximity search). If set to 0 (the default), the terms must be in the exact order. If two transposed terms should match, a minimum slop of 2 has to be set. Only applicable to phrase and phrase_prefix queries. As an example with slop 2, querying for foo bar will not only match foo bar but also foo what a bar.\n\ntie_breaker\n\nWhen using best_fields, phrase or phrase_prefix the _score of every other column will be multiplied with this value and added to the _score of the best matching column.\n\nDefaults to 0.0.\n\nNot applicable to match type most_fields as this type is executed as if it had a tie_breaker of 1.0.\n\nzero_terms_query\n\nIf no tokens are generated analyzing the query_term then no documents are matched. If all is given here, all documents are matched.\n\nUsage\n\nA fulltext search is done using the MATCH Predicate predicate:\n\ncr> select name from locations where match(name_description_ft, 'time') order by _score desc;\n+-----------+\n| name      |\n+-----------+\n| Bartledan |\n| Altair    |\n+-----------+\nSELECT 2 rows in set (... sec)\n\n\nIt returns TRUE for rows which match the search string. To get more detailed information about the quality of a match, the relevance of the row, the _score can be selected:\n\ncr> select name, _score\n... from locations where match(name_description_ft, 'time') order by _score desc;\n+-----------+------------+\n| name      |     _score |\n+-----------+------------+\n| Bartledan | 0.75782394 |\n| Altair    | 0.63013375 |\n+-----------+------------+\nSELECT 2 rows in set (... sec)\n\n\nNote\n\nThe _score is not an absolute value. It just sets a row in relation to the other ones.\n\nSearching On Multiple Columns\n\nThere are two possibilities if a search should span the contents of multiple columns:\n\nuse a composite index column on your table. See Defining a composite index.\n\nuse the MATCH Predicate predicate on multiple columns.\n\nWhen querying multiple columns, there are many ways how the relevance a.k.a. _score can be computed. These different techniques are called Match Types.\n\nTo increase the relevance of rows where one column matches extremely well, use best_fields (the default).\n\nIf rows with good matches spread over all included columns should be more relevant, use most_fields. If searching multiple columns as if they were one, use cross_fields.\n\nFor searching of matching phrases (tokens are in the exact same order) use phrase or phrase_prefix:\n\ncr> select name, _score from locations\n... where match(\n...     (name_description_ft, inhabitants['name'] 1.5, kind 0.75),\n...     'end of the galaxy'\n... ) order by _score desc;\n+-------------------+------------+\n| name              |     _score |\n+-------------------+------------+\n| NULL              | 1.5614427  |\n| Altair            | 0.63013375 |\n| Aldebaran         | 0.55650693 |\n| Outer Eastern Rim | 0.38915473 |\n| North West Ripple | 0.37936807 |\n+-------------------+------------+\nSELECT 5 rows in set (... sec)\n\ncr> select name, description, _score from locations\n... where match(\n...     (name_description_ft), 'end of the galaxy'\n... ) using phrase with (analyzer='english', slop=4);\n+------+-------------------------+-----------+\n| name | description             |    _score |\n+------+-------------------------+-----------+\n| NULL | The end of the Galaxy.% | 1.5614427 |\n+------+-------------------------+-----------+\nSELECT 1 row in set (... sec)\n\n\nA vast amount of options exist to fine-tune your fulltext search. A detailed reference can be found here MATCH Predicate.\n\nNegative Search\n\nA negative fulltext search can be done using a NOT clause:\n\ncr> select name, _score from locations\n... where not match(name_description_ft, 'time')\n... order by _score, name asc;\n+------------------------------------+--------+\n| name                               | _score |\n+------------------------------------+--------+\n|                                    |    1.0 |\n| Aldebaran                          |    1.0 |\n| Algol                              |    1.0 |\n| Allosimanius Syneca                |    1.0 |\n| Alpha Centauri                     |    1.0 |\n| Argabuthon                         |    1.0 |\n| Arkintoofle Minor                  |    1.0 |\n| Galactic Sector QQ7 Active J Gamma |    1.0 |\n| North West Ripple                  |    1.0 |\n| Outer Eastern Rim                  |    1.0 |\n| NULL                               |    1.0 |\n+------------------------------------+--------+\nSELECT 11 rows in set (... sec)\n\nFilter By _score\n\nIt is possible to filter results by the _score column but as its value is a computed value relative to the highest score of all results and consequently never absolute or comparable across searches the usefulness outside of sorting is very limited.\n\nAlthough possible, filtering by the greater-than-or-equals operator (>=) on the _score column would not make much sense and can lead to unpredictable result sets.\n\nAnyway let’s do it here for demonstration purpose:\n\ncr> select name, _score\n... from locations where match(name_description_ft, 'time')\n... and _score >= 0.8 order by _score;\n+-----------+-----------+\n| name      |    _score |\n+-----------+-----------+\n| Altair    | 1.6301337 |\n| Bartledan | 1.757824  |\n+-----------+-----------+\nSELECT 2 rows in set (... sec)\n\n\nAs you might have noticed, the _score value has changed for the same query text and document because it’s a ratio relative to all results, and by filtering on _score, ‘all results’ has already changed.\n\nCaution\n\nAs noted above _score is a relative number and not comparable across searches. Filtering is therefore greatly discouraged."
  },
  {
    "title": "Union — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/dql/union.html",
    "html": "5.6\nUnion\n\nUNION can be used to combine results from multiple SELECT statements.\n\nSee Also\n\nUnion (SQL)\n\nTable of contents\n\nUnion All\n\nUnion Distinct\n\nUnion of object types\n\nUnion of different types\n\nUnion All\n\nIf duplicates are allowed, use UNION ALL.\n\ncr> select name from photos\n... union all\n... select name from countries\n... union all\n... select name from photos\n... order by name;\n+--------------+\n| name         |\n+--------------+\n| Austria      |\n| Berlin Wall  |\n| Berlin Wall  |\n| Eiffel Tower |\n| Eiffel Tower |\n| France       |\n| Germany      |\n| South Africa |\n| Turkey       |\n+--------------+\nSELECT 9 rows in set (... sec)\n\nUnion Distinct\n\nTo remove duplicates, use UNION DISTINCT or simply UNION.\n\ncr> select name from photos\n... union distinct\n... select name from countries\n... union\n... select name from photos\n... order by name;\n+--------------+\n| name         |\n+--------------+\n| Austria      |\n| Berlin Wall  |\n| Eiffel Tower |\n| France       |\n| Germany      |\n| South Africa |\n| Turkey       |\n+--------------+\nSELECT 7 rows in set (... sec)\n\nUnion of object types\n\nWhen object types are unioned, the resulting object will contain the merged sub-columns from both of the input objects.\n\ncr> SET error_on_unknown_object_key = FALSE;\nSET OK, 0 rows affected  (... sec)\ncr> SELECT o, o['a'], o['b'] FROM (SELECT {a=1} AS o UNION SELECT {b=1} AS o) AS t;\n+----------+--------+--------+\n| o        | o['a'] | o['b'] |\n+----------+--------+--------+\n| {\"a\": 1} |      1 |   NULL |\n| {\"b\": 1} |   NULL |      1 |\n+----------+--------+--------+\nSELECT 2 rows in set (... sec)\n\nUnion of different types\n\nIf types do not match, CrateDB will choose one of the type with higher precedence and try to implicitly cast the values with lower precedence.\n\ncr> SELECT pg_typeof(c) FROM (SELECT 1 AS c UNION SELECT '1' AS c UNION SELECT 1 AS c) AS t;\n+-------------------------+\n| pg_catalog.pg_typeof(c) |\n+-------------------------+\n| integer                 |\n+-------------------------+\nSELECT 1 row in set (... sec)\n"
  },
  {
    "title": "Joins — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/dql/joins.html",
    "html": "5.6\nJoins\n\nWhen selecting data from CrateDB, you can join one or more relations (e.g., tables) to combine columns into one result set.\n\nSee Also\n\nJoin (SQL)\n\nTable of contents\n\nCross joins\n\nInner joins\n\nOuter joins\n\nLeft outer joins\n\nRight outer joins\n\nFull outer joins\n\nJoin conditions\n\nAvailable join algorithms\n\nNested loop join algorithm\n\nBlock hash join algorithm\n\nLimitations\n\nCross joins\n\nReferencing two tables results in a CROSS JOIN.\n\nThe result is computed by creating every possible combination (i.e., a cartesian product) of their rows (t1 * t2 * t3 * tn) and then applying the given query operation on it (e.g., WHERE clause, SELECT list, ORDER BY clause, and so on):\n\ncr> select articles.name as article, colors.name as color, price\n... from articles cross join colors\n... where price > 5000.0\n... order by price, color, article;\n+------------------------------+---------------+----------+\n| article                      | color         |    price |\n+------------------------------+---------------+----------+\n| Infinite Improbability Drive | Antique White | 19999.99 |\n| Infinite Improbability Drive | Gold          | 19999.99 |\n| Infinite Improbability Drive | Midnight Blue | 19999.99 |\n| Infinite Improbability Drive | Olive Drab    | 19999.99 |\n| Starship Titanic             | Antique White | 50000.0  |\n| Starship Titanic             | Gold          | 50000.0  |\n| Starship Titanic             | Midnight Blue | 50000.0  |\n| Starship Titanic             | Olive Drab    | 50000.0  |\n+------------------------------+---------------+----------+\nSELECT 8 rows in set (... sec)\n\n\nCross joins can be done explicitly using the CROSS JOIN statement as shown in the example above, or implicitly by just specifying two or more tables in the FROM list:\n\ncr> select articles.name as article, colors.name as color, price\n... from articles, colors\n... where price > 5000.0\n... order by price, color, article;\n+------------------------------+---------------+----------+\n| article                      | color         |    price |\n+------------------------------+---------------+----------+\n| Infinite Improbability Drive | Antique White | 19999.99 |\n| Infinite Improbability Drive | Gold          | 19999.99 |\n| Infinite Improbability Drive | Midnight Blue | 19999.99 |\n| Infinite Improbability Drive | Olive Drab    | 19999.99 |\n| Starship Titanic             | Antique White | 50000.0  |\n| Starship Titanic             | Gold          | 50000.0  |\n| Starship Titanic             | Midnight Blue | 50000.0  |\n| Starship Titanic             | Olive Drab    | 50000.0  |\n+------------------------------+---------------+----------+\nSELECT 8 rows in set (... sec)\n\nInner joins\n\nInner joins require each record of one table to have matching records on the other table:\n\ncr> select s.id, s.table_name, t.number_of_shards\n... from sys.shards s, information_schema.tables t\n... where s.table_name = t.table_name\n... and s.table_name = 'employees'\n... order by s.id;\n+----+------------+------------------+\n| id | table_name | number_of_shards |\n+----+------------+------------------+\n|  0 | employees  |                4 |\n|  1 | employees  |                4 |\n|  2 | employees  |                4 |\n|  3 | employees  |                4 |\n+----+------------+------------------+\nSELECT 4 rows in set (... sec)\n\nOuter joins\nLeft outer joins\n\nLeft outer join returns tuples for all matching records of the left-hand and right-hand relation like inner joins. Additionally it returns tuples for all other records from left-hand that don’t match any record on the right-hand by using NULL values for the columns of the right-hand relation:\n\ncr> select e.name || ' ' || e.surname as employee, coalesce(d.name, '') as manager_of_department\n... from employees e left join departments d\n... on e.id = d.manager_id\n... order by e.id;\n+--------------------+-----------------------+\n| employee           | manager_of_department |\n+--------------------+-----------------------+\n| John Doe           | Administration        |\n| John Smith         | IT                    |\n| Sean Lee           |                       |\n| Rebecca Sean       |                       |\n| Tim Ducan          |                       |\n| Robert Duval       |                       |\n| Clint Johnson      |                       |\n| Sarrah Mcmillan    |                       |\n| David Limb         |                       |\n| David Bowe         |                       |\n| Smith Clark        | Marketing             |\n| Ted Kennedy        |                       |\n| Ronald Reagan      |                       |\n| Franklin Rossevelt |                       |\n| Sam Malone         |                       |\n| Marry Georgia      |                       |\n| Tim Doe            | Human Resources       |\n| Tim Malone         | Purchasing            |\n+--------------------+-----------------------+\nSELECT 18 rows in set (... sec)\n\nRight outer joins\n\nRight outer join returns tuples for all matching records of the right-hand and left-hand relation like inner joins. Additionally it returns tuples for all other records from right-hand that don’t match any record on the left-hand by using NULL values for the columns of the left-hand relation:\n\ncr> select e.name || ' ' || e.surname as employee, d.name as manager_of_department\n... from employees e right join departments d\n... on e.id = d.manager_id\n... order by d.id;\n+-------------+-----------------------+\n| employee    | manager_of_department |\n+-------------+-----------------------+\n| John Doe    | Administration        |\n| Smith Clark | Marketing             |\n| Tim Malone  | Purchasing            |\n| Tim Doe     | Human Resources       |\n|             | Shipping              |\n| John Smith  | IT                    |\n+-------------+-----------------------+\nSELECT 6 rows in set (... sec)\n\nFull outer joins\n\nFull outer join returns tuples for all matching records of the left-hand and right-hand relation like inner joins. Additionally it returns tuples for all other records from left-hand that don’t match any record on the right-hand by using NULL values for the columns of the right-hand relation. Additionally it returns tuples for all other records from right-hand that don’t match any record on the left-hand by using NULL values for the columns of the left-hand relation:\n\ncr> select e.name || ' ' || e.surname as employee, coalesce(d.name, '') as manager_of_department\n... from employees e full join departments d\n... on e.id = d.manager_id\n... order by e.id;\n+--------------------+-----------------------+\n| employee           | manager_of_department |\n+--------------------+-----------------------+\n| John Doe           | Administration        |\n| John Smith         | IT                    |\n| Sean Lee           |                       |\n| Rebecca Sean       |                       |\n| Tim Ducan          |                       |\n| Robert Duval       |                       |\n| Clint Johnson      |                       |\n| Sarrah Mcmillan    |                       |\n| David Limb         |                       |\n| David Bowe         |                       |\n| Smith Clark        | Marketing             |\n| Ted Kennedy        |                       |\n| Ronald Reagan      |                       |\n| Franklin Rossevelt |                       |\n| Sam Malone         |                       |\n| Marry Georgia      |                       |\n| Tim Doe            | Human Resources       |\n| Tim Malone         | Purchasing            |\n|                    | Shipping              |\n+--------------------+-----------------------+\nSELECT 19 rows in set (... sec)\n\nJoin conditions\n\nCrateDB supports all operators and scalar functions as join conditions in the WHERE clause.\n\nExample with within scalar function:\n\ncr> select photos.name, countries.name\n... from countries, photos\n... where within(location, geo)\n... order by countries.name, photos.name;\n+--------------+---------+\n| name         | name    |\n+--------------+---------+\n| Eiffel Tower | France  |\n| Berlin Wall  | Germany |\n+--------------+---------+\nSELECT 2 rows in set (... sec)\n\nAvailable join algorithms\nNested loop join algorithm\n\nThe nested loop algorithm evaluates the join conditions on every record of the left-hand table with every record of the right-hand table in a distributed manner (for each shard of the used tables). The right-hand table is scanned once for every row in the left-hand table.\n\nThis is the default algorithm used for all types of joins.\n\nBlock hash join algorithm\n\nThe performance of equi-joins is substantially improved by using the hash join algorithm. At first, one relation is scanned and loaded into a hash table using the attributes of the join conditions as hash keys. Once the hash table is built, the second relation is scanned and the join condition values of every row are hashed and matched against the hash table.\n\nIn order to built a hash table even if the first relation wouldn’t fit into the available memory, only a certain block size of a relation is loaded at once. The whole operation will be repeated with the next block of the first relation once scanning the second relation has finished.\n\nThis optimisation cannot be applied unless the join is an INNER join and the join condition satisfies the following rules:\n\nContains at least one EQUAL operator\n\nContains no OR operator\n\nEvery argument of a EQUAL operator can only references fields from one relation\n\nThe hash join algorithm is faster but has a bigger memory footprint. As such it can explicitly be disabled on demand when memory is scarce using the session setting enable_hashjoin:\n\nSET enable_hashjoin=false\n\nLimitations\n\nJoining more than 2 tables can result in poor execution plans.\n\nInternally the relations are joined in pairs. So for example in a 3 table join, we’d join (r1 ⋈ r2) ⋈ r3 (r1 with r2 first, then with r3). The poor execution plan could happen as there is no optimization in place which improves the join ordering. Ideally we’d join those relations first which narrow the intermediate result set to a large degree, so that later joins have less work to do. In the example before, joining r1 ⋈ (r2 ⋈ r3) might be the better order."
  },
  {
    "title": "Refresh — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/dql/refresh.html",
    "html": "5.6\nRefresh\n\nTable of contents\n\nIntroduction\n\nMultiple Table Refresh\n\nPartition Refresh\n\nIntroduction\n\nCrateDB is eventually consistent. Data written with a former statement is not guaranteed to be fetched with the next following select statement for the affected rows.\n\nIf required one or more tables can be refreshed explicitly in order to ensure that the latest state of the table gets fetched.\n\ncr> refresh table locations;\nREFRESH OK, 1 row affected (... sec)\n\n\nA table is refreshed periodically with a specified refresh interval. By default, the refresh interval is set to 1000 milliseconds. The refresh interval of a table can be changed with the table parameter refresh_interval (see refresh_interval).\n\nSee Also\n\nOptimistic Concurrency Control\n\nMultiple Table Refresh\n\nIf needed, multiple tables can be defined comma-separated in a single SQL request. This ensures that they all get refreshed and so their datasets get consistent. The result message is printed if the request on every given table is completed.\n\ncr> REFRESH TABLE locations, parted_table;\nREFRESH OK, 2 rows affected (... sec)\n\n\nNote\n\nIf one or more tables or partitions do not exist, none of the given tables/partitions are refreshed and an error is returned. The error returns only the first non-existent table/partition.\n\nPartition Refresh\n\nAdditionally it is possible to define a specific PARTITION of a partitioned table which should be refreshed (see Partitioned tables).\n\nBy using the PARTITION clause in the refresh statement a separate request for a given partition can be performed. That means that only specific partitions of a partitioned table are refreshed. For further details on how to create a refresh request on partitioned tables see the SQL syntax and its synopsis (see REFRESH).\n\ncr> REFRESH TABLE parted_table PARTITION (day='2014-04-08');\nREFRESH OK, 1 row affected (... sec)\n\n\nIn case the PARTITION clause is omitted all partitions will be refreshed. If a table has many partitions this should be avoided due to performance reasons."
  },
  {
    "title": "Column policy — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/ddl/column-policy.html",
    "html": "5.6\nColumn policy\n\nThe Column Policy defines if a table enforces its defined schema or if it’s allowed to store additional columns which are a not defined in the table schema.\n\nIf the column policy is not defined within the with clause, strict will be used.\n\nstrict\n\nThe column policy can be configured to be strict, rejecting any column on insert/update/copy_to which is not defined in the schema.\n\nExample:\n\ncr> create table my_table (\n...   title text,\n...   author text\n... ) with (column_policy = 'strict');\nCREATE OK, 1 row affected (... sec)\n\n\nIf you try to insert using a column not specified in the table schema, CrateDB will raise an error.\n\ncr> insert into my_table (new_col) values(1);\nColumnUnknownException[Column new_col unknown]\n\ndynamic\n\nThe other option is dynamic. dynamic means that new columns can be added using insert, update or copy from.\n\nNote that adding new columns to a table with a dynamic policy will affect the schema of the table. Once a column is added, it shows up in the information_schema.columns table and its type and attributes are fixed. It will have the type that was guessed by its inserted/updated value and they will be analyzed as plain with the plain analyzer, which means as-is.\n\nNote\n\nThe data type of the new column is guessed from the data type of the value provided in the insert statement that creates the column.\n\nIf a new column a was added with type boolean, adding strings to this column will result in an error, except the string can be implicit casted to a boolean value.\n\nHere’s an example:\n\ncr> create table my_table (\n...   title text,\n...   author text\n... ) with (column_policy = 'dynamic');\nCREATE OK, 1 row affected (... sec)\n\n\nNow, you can do inserts that add columns:\n\ncr> insert into my_table (new_col) values(1);\nINSERT OK, 1 row affected  (... sec)\n\n\nWhich will update the table schema:\n\ncr> show create table my_table;\n+-----------------------------------------------------+\n| SHOW CREATE TABLE doc.my_table                      |\n+-----------------------------------------------------+\n| CREATE TABLE IF NOT EXISTS \"doc\".\"my_table\" (       |\n|    \"title\" TEXT,                                    |\n|    \"author\" TEXT,                                   |\n|    \"new_col\" BIGINT                                 |\n| )                                                   |\n| CLUSTERED INTO 4 SHARDS                             |\n...\n\n\nNew columns added to dynamic tables are identical to regular columns. You can retrieve them, sort by them and use them in where clauses.\n\nWarning\n\nThe mapping update is processed asynchronously on multiple nodes. If a new field gets added to the local mapping of two shards, these shards are sending their mapping to the master. If this mapping update gets delivered later than the next query on the previously added column, it will result in a ColumnUnknownException."
  },
  {
    "title": "Altering tables — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/ddl/alter-table.html",
    "html": "5.6\nAltering tables\n\nTable of contents\n\nUpdating parameters\n\nChanging the number of shards\n\nDecreasing the number of shards\n\nIncrease the number of shards\n\nAdding columns\n\nRenaming columns\n\nClosing and opening tables\n\nRenaming tables\n\nReroute shards\n\nNote\n\nALTER COLUMN action is not currently supported. See SQL compatibility.\n\nUpdating parameters\n\nThe parameters of a table can be modified using the ALTER TABLE clause:\n\ncr> alter table my_table set (number_of_replicas = '0-all');\nALTER OK, -1 rows affected (... sec)\n\n\nIn order to set a parameter to its default value use reset:\n\ncr> alter table my_table reset (number_of_replicas);\nALTER OK, -1 rows affected (... sec)\n\n\nNote\n\nChanging compression using the codec parameter only takes effect for newly written segments. To enforce a rewrite of already existing segments, run OPTIMIZE with the parameter max_num_segments = 1.\n\nChanging the number of shards\n\nChanging the number of shards in general works in the following steps.\n\nA new target table is created but with more/less number of primary shards.\n\nThe segments from the source table (the underling Lucene index to be precise) are hard-linked into the target table at file system level.\n\nThe source table is dropped while the new table is renamed into the source and then recovered in the cluster.\n\nNote\n\nSegment hard-linking makes this operation relevantly cheap as it involves no data copying. If the file system, however, does not support hard-linking, then all segments will be copied into the new table, resulting in much more time and resource consuming operation.\n\nTo change the number of primary shards of a table, it is necessary to first satisfy certain conditions.\n\nDecreasing the number of shards\n\nTo decrease the number of shards, it is necessary to ensure the following two conditions:\n\nFirst, a (primary or replica) copy of every shard of the table must be present on the same node. The user can choose the most suitable node for this operation and then restrict table shard allocation on that node using the shard allocation filtering.\n\nThe second condition for decreasing a table’s number of shards is to block write operations to the table:\n\ncr> alter table my_table set (\"blocks.write\" = true);\nALTER OK, -1 rows affected (... sec)\n\n\nAfterwards the number of shards can be decreased:\n\ncr> alter table my_table set (number_of_shards = 1);\nALTER OK, 0 rows affected (... sec)\n\n\nThe user should then revert the restrictions applied on the table, for instance\n\ncr> alter table my_table reset (\"routing.allocation.require._name\", \"blocks.write\");\nALTER OK, -1 rows affected (... sec)\n\n\nIt is necessary to use a factor of the current number of primary shards as the target number of shards. For example, a table with 8 shards can be shrunk into 4, 2 or 1 primary shards.\n\nIncrease the number of shards\n\nIncreasing the number of shards is limited to tables which have been created with a number_of_routing_shards setting. For such tables the shards can be increased by a factor that depends on this setting. For example, a table with 5 shards, with number_of_routing_shards set to 20 can be changed to have either 10 or 20 shards. (5 x 2 (x 2)) = 20 or (5 x 4) = 20.\n\nThe only condition required for increasing the number of shards is to block operations to the table:\n\ncr> alter table my_table set (\"blocks.write\" = true);\nALTER OK, -1 rows affected (... sec)\n\n\nAfterwards, the table shards can be increased:\n\ncr> alter table my_table set (number_of_shards = 2);\nALTER OK, 0 rows affected (... sec)\n\n\nSimilarly, the user should revert the restrictions applied on the table, for instance:\n\ncr> alter table my_table set (\"blocks.write\" = false);\nALTER OK, -1 rows affected (... sec)\n\n\nRead Alter Partitioned Tables to see how to alter parameters of partitioned tables.\n\nAdding columns\n\nIn order to add a column to an existing table use ALTER TABLE with the ADD COLUMN clause:\n\ncr> alter table my_table add column new_column_name text;\nALTER OK, -1 rows affected (... sec)\n\n\nThe inner schema of object columns can also be extended, as shown in the following example.\n\nFirst a column of type object is added:\n\ncr> alter table my_table add column obj_column object as (age int);\nALTER OK, -1 rows affected (... sec)\n\n\nAnd now a nested column named name is added to the obj_column:\n\ncr> alter table my_table add column obj_column['name'] text;\nALTER OK, -1 rows affected (... sec)\n\ncr> select column_name, data_type from information_schema.columns\n... where table_name = 'my_table' and column_name like 'obj_%';\n+--------------------+-----------+\n| column_name        | data_type |\n+--------------------+-----------+\n| obj_column         | object    |\n| obj_column['age']  | integer   |\n| obj_column['name'] | text      |\n+--------------------+-----------+\nSELECT 3 rows in set (... sec)\n\nRenaming columns\n\nTo rename a column of an existing table, use ALTER TABLE with the RENAME COLUMN clause:\n\ncr> alter table my_table rename new_column_name to renamed_column;\nALTER OK, -1 rows affected (... sec)\n\n\nThis also works on object columns:\n\ncr> alter table my_table rename column obj_column to renamed_obj_column;\nALTER OK, -1 rows affected (... sec)\n\n\nTo rename a sub-column of an object column, you can use subscript expressions:\n\ncr> alter table my_table rename column renamed_obj_column['age'] to\n...  renamed_obj_column['renamed_age'];\nALTER OK, -1 rows affected (... sec)\n\n\ncr> select column_name, data_type from information_schema.columns\n... where table_name = 'my_table' and column_name like 'renamed_obj_%';\n+-----------------------------------+-----------+\n| column_name                       | data_type |\n+-----------------------------------+-----------+\n| renamed_obj_column                | object    |\n| renamed_obj_column['renamed_age'] | integer   |\n| renamed_obj_column['name']        | text      |\n+-----------------------------------+-----------+\nSELECT 3 rows in set (... sec)\n\nClosing and opening tables\n\nA table can be closed by using ALTER TABLE with the CLOSE clause:\n\ncr> alter table my_table close;\nALTER OK, -1 rows affected (... sec)\n\n\nClosing a table will cause all operations beside ALTER TABLE ... OPEN to fail.\n\nA table can be reopened again by using ALTER TABLE with the OPEN clause:\n\ncr> alter table my_table open;\nALTER OK, -1 rows affected (... sec)\n\n\nNote\n\nThis setting is not the same as blocks.read_only. Closing and opening a table will preserve these settings if they are already set.\n\nRenaming tables\n\nYou can rename a table or view using ALTER TABLE with the RENAME TO clause:\n\ncr> ALTER TABLE my_table RENAME TO my_new_table;\nALTER OK, -1 rows affected (... sec)\n\n\nIf renaming a table, the shards of the table become temporarily unavailable.\n\nReroute shards\n\nWith the REROUTE command it is possible to control the allocations of shards. This gives you the ability to re-balance the cluster state manually. The supported reroute options are listed in the reference documentation of ALTER TABLE REROUTE.\n\nShard rerouting can help solve several problems:\n\nUnassigned shards: Due to cause of lack of space, shard awareness or any other failure that happens during the automatic shard allocation it is possible to gain unassigned shards in the cluster.\n\n“Hot Shards”: Most of your queries affect certain shards only. These shards lie on a node that has insufficient resources.\n\nThis command takes these Routing Allocation Settings into account. Once an allocation occurs CrateDB tries (by default) to re-balance shards to an even state. CrateDB can be set to disable shard re-balancing with the setting cluster.routing.rebalance.enable=None to perform only the explicit triggered allocations. .\n\nNote\n\nThe command only triggers the allocation and reports back if the process has been acknowledged or rejected. Moving or allocating large shards takes more time to complete.\n\nIn those two cases it may be necessary to move shards manually to another node or force the retry of the allocation process."
  },
  {
    "title": "Selecting data — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/dql/selects.html",
    "html": "5.6\nSelecting data\n\nSelecting (i.e., retrieving) data from CrateDB can be done by using an SQL SELECT statement. The response to a SELECT query includes the column names of the result, the result rows as a two-dimensional array of values, the row count, and the execution time.\n\nTable of contents\n\nIntroduction\n\nFROM clause\n\nJoins\n\nDISTINCT clause\n\nWHERE clause\n\nComparison operators\n\nRegular expressions\n\nLIKE (ILIKE)\n\nNOT\n\nIS NULL\n\nIS NOT NULL\n\nArray comparisons\n\nIN\n\nANY (array)\n\nNegating ANY\n\nEXISTS\n\nContainer data types\n\nArrays\n\nObjects\n\nNested structures\n\nArrays within objects\n\nLimitations\n\nObjects within arrays\n\nAggregation\n\nWindow functions\n\nGROUP BY\n\nHAVING\n\nWITH Queries (Common Table Expressions)\n\nNested WITH clauses\n\nIntroduction\n\nA simple select:\n\ncr> select id, name from locations order by id limit 2;\n+----+-------------------+\n| id | name              |\n+----+-------------------+\n|  1 | North West Ripple |\n|  2 | Outer Eastern Rim |\n+----+-------------------+\nSELECT 2 rows in set (... sec)\n\n\nIf the * operator is used, all columns defined in the schema are returned for each row:\n\ncr> select * from locations order by id limit 2;\n+----+-------------------+--------------+--------+----------+-------------...-+-----------+\n| id | name              |         date | kind   | position | description ... | landmarks |\n+----+-------------------+--------------+--------+----------+-------------...-+-----------+\n|  1 | North West Ripple | 308534400000 | Galaxy |        1 | Relative to ... |      NULL |\n|  2 | Outer Eastern Rim | 308534400000 | Galaxy |        2 | The Outer Ea... |      NULL |\n+----+-------------------+--------------+--------+----------+-------------...-+-----------+\nSELECT 2 rows in set (... sec)\n\n\nAliases can be used to change the output name of the columns:\n\ncr> select name as n\n... from locations\n... where name = 'North West Ripple';\n+-------------------+\n| n                 |\n+-------------------+\n| North West Ripple |\n+-------------------+\nSELECT 1 row in set (... sec)\n\nFROM clause\n\nThe FROM clause is used to reference the relation this select query is based upon. Can be a single table, many tables, a view, a JOIN or another SELECT statement.\n\nTables and views are referenced by schema and table name and can optionally be aliased. If the relation t is only referenced by name, CrateDB assumes the relation doc.t was meant. Schemas that were newly created using CREATE TABLE must be referenced explicitly.\n\nThe two following queries are equivalent:\n\ncr> select name, position from locations\n... order by name desc nulls last limit 2;\n+-------------------+----------+\n| name              | position |\n+-------------------+----------+\n| Outer Eastern Rim |        2 |\n| North West Ripple |        1 |\n+-------------------+----------+\nSELECT 2 rows in set (... sec)\n\ncr> select doc.locations.name as n, position from doc.locations\n... order by name desc nulls last limit 2;\n+-------------------+----------+\n| n                 | position |\n+-------------------+----------+\n| Outer Eastern Rim |        2 |\n| North West Ripple |        1 |\n+-------------------+----------+\nSELECT 2 rows in set (... sec)\n\n\nA table can be aliased for the sake of brevity too:\n\ncr> select name from doc.locations as l\n... where l.name = 'Outer Eastern Rim';\n+-------------------+\n| name              |\n+-------------------+\n| Outer Eastern Rim |\n+-------------------+\nSELECT 1 row in set (... sec)\n\nJoins\n\nNote\n\nCrateDB currently supports only a limited set of JOINs.\n\nSee the Joins for current state.\n\nDISTINCT clause\n\nIf DISTINCT is specified, one unique row is kept. All other duplicate rows are removed from the result set:\n\ncr> select distinct date from locations order by date;\n+---------------+\n| date          |\n+---------------+\n| 308534400000  |\n| 1367366400000 |\n| 1373932800000 |\n+---------------+\nSELECT 3 rows in set (... sec)\n\n\nNote\n\nUsing DISTINCT is only supported on Primitive types.\n\nWHERE clause\n\nHere is a simple WHERE clause using an equality operator:\n\ncr> select description from locations where id = '1';\n+---------------------------------------...--------------------------------------+\n| description                                                                    |\n+---------------------------------------...--------------------------------------+\n| Relative to life on NowWhat, living on... a factor of about seventeen million. |\n+---------------------------------------...--------------------------------------+\nSELECT 1 row in set (... sec)\n\nComparison operators\n\nCrateDB supports a variety of comparison operators (including basic operators such as =, <, >, and so on).\n\nRegular expressions\n\nComparison operators for matching using regular expressions:\n\nOperator\n\n\t\n\nDescription\n\n\t\n\nExample\n\n\n\n\n~\n\n\t\n\nMatches regular expression, case sensitive\n\n\t\n'foo' ~ '.*foo.*'\n\n\n\n\n~*\n\n\t\n\nMatches regular expression, case insensitive\n\n\t\n'Foo' ~* '.*foo.*'\n\n\n\n\n!~\n\n\t\n\nDoes not match regular expression, case sensitive\n\n\t\n'Foo' !~ '.*foo.*'\n\n\n\n\n!~*\n\n\t\n\nDoes not match regular expression, case insensitive\n\n\t\n'foo' !~* '.*bar.*'\n\n\nThe ~ operator can be used to match a string against a regular expression. It returns true if the string matches the pattern, false if not, and NULL if string is NULL.\n\nTo negate the matching, use the optional ! prefix. The operator returns true if the string does not match the pattern, false otherwise.\n\nTo make the search case insensitive use ~* or !~* for negative search.\n\nNote\n\nThese operators are not intended to be used against TEXT INDEX USING FULLTEXT fields. To use both regex lookups and full-text search on the same field, create the field as a normal TEXT field and add a FULLTEXT index with a separate name to be used with the MATCH predicate:\n\ncr> CREATE TABLE tbl1(\n...   a TEXT,\n...   INDEX a_ft USING FULLTEXT(a)\n... );\nCREATE OK, 1 row affected (... sec)\n\n\nThe regular expression pattern is implicitly anchored, meaning the whole string must match, not a single subsequence. All unicode characters are allowed.\n\nIf using PCRE features in the regular expression pattern, the operator uses the regular expression engine of the Java standard library java.util.regex.\n\nIf not using PCRE features in the regular expression pattern, the operator uses Lucene Regular Expressions, which are optimized for fast regular expression matching on Lucene terms.\n\nLucene Regular Expressions are basically POSIX Extended Regular Expressions without the character classes and with some extensions, like a meta character # for the empty string or ~ for negation and others. By default all Lucene extensions are enabled. See the Lucene documentation for more details.\n\nNote\n\nSince case-insensitive matching using ~* or !~* implicitly uses the regular expression engine of the Java standard library, features of Lucene Regular Expressions do not work there.\n\nExamples:\n\ncr> select name from locations where name ~ '([A-Z][a-z0-9]+)+'\n... order by name;\n+------------+\n| name       |\n+------------+\n| Aldebaran  |\n| Algol      |\n| Altair     |\n| Argabuthon |\n| Bartledan  |\n+------------+\nSELECT 5 rows in set (... sec)\n\ncr> select 'matches' from sys.cluster where\n... 'gcc --std=c99 -Wall source.c' ~ '[A-Za-z0-9]+( (-|--)[A-Za-z0-9]+)*( [^ ]+)*';\n+-----------+\n| 'matches' |\n+-----------+\n| matches   |\n+-----------+\nSELECT 1 row in set (... sec)\n\ncr> select 'no_match' from sys.cluster where 'foobaz' !~ '(foo)?(bar)$';\n+------------+\n| 'no_match' |\n+------------+\n| no_match   |\n+------------+\nSELECT 1 row in set (... sec)\n\nLIKE (ILIKE)\n\nCrateDB supports the LIKE and ILIKE operators. These operators can be used to query for rows where only part of a columns value should match something. The only difference is that, in the case of ILIKE, the matching is case insensitive.\n\nBoth LIKE and ILIKE support optional ESCAPE character. When no value is provided, backslash character \\ is used as the escape character. Providing an empty value disables escaping.\n\nExample of query with custom ESCAPE:\n\ncr> SELECT 'test' LIKE 'te%' escape 'e' as res;\n+-------+\n| res   |\n+-------+\n| FALSE |\n+-------+\nSELECT 1 row in set (... sec)\n\n\nFor example to get all locations where the name starts with Ar the following queries can be used:\n\ncr> select name from locations where name like 'Ar%' order by name asc;\n+-------------------+\n| name              |\n+-------------------+\n| Argabuthon        |\n| Arkintoofle Minor |\n+-------------------+\nSELECT 2 rows in set (... sec)\n\ncr> select name from locations where name ilike 'ar%' order by name asc;\n+-------------------+\n| name              |\n+-------------------+\n| Argabuthon        |\n| Arkintoofle Minor |\n+-------------------+\nSELECT 2 rows in set (... sec)\n\n\nThe following wildcard operators are available:\n\n%\n\n\t\n\nA substitute for zero or more characters\n\n\n\n\n_\n\n\t\n\nA substitute for a single character\n\nThe wildcard operators may be used at any point in the string literal. For example a more complicated like clause could look like this:\n\ncr> select name from locations where name like '_r%a%' order by name asc;\n+------------+\n| name       |\n+------------+\n| Argabuthon |\n+------------+\nSELECT 1 row in set (... sec)\n\n\nIn order so search for the wildcard characters themselves it is possible to escape them using a backslash:\n\ncr> select description from locations\n... where description like '%\\%' order by description asc;\n+-------------------------+\n| description             |\n+-------------------------+\n| The end of the Galaxy.% |\n+-------------------------+\nSELECT 1 row in set (... sec)\n\n\nCaution\n\nLIKE and ILIKE clauses can slow a query down, especially when used in combination with wildcard characters. This is because CrateDB has to iterate over all rows for the comparison and cannot utilize the index.\n\nFor better performance, consider using fulltext search.\n\nNOT\n\nNOT negates a boolean expression:\n\n[ NOT ] boolean_expression\n\n\nThe result type is boolean.\n\nexpression\n\n\t\n\nresult\n\n\n\n\ntrue\n\n\t\n\nfalse\n\n\n\n\nfalse\n\n\t\n\ntrue\n\n\n\n\nnull\n\n\t\n\nnull\n\nIS NULL\n\nReturns TRUE if the expression evaluates to NULL. Given a column reference, it returns TRUE if the field contains NULL or is missing.\n\nUse this predicate to check for NULL values as SQL’s three-valued logic does always return NULL when comparing NULL.\n\nexpr\n\nExpression of one of the supported data types supported by CrateDB.\n\ncr> select name from locations where inhabitants is null order by name;\n+------------------------------------+\n| name                               |\n+------------------------------------+\n|                                    |\n| Aldebaran                          |\n| Algol                              |\n| Allosimanius Syneca                |\n| Alpha Centauri                     |\n| Altair                             |\n| Galactic Sector QQ7 Active J Gamma |\n| North West Ripple                  |\n| Outer Eastern Rim                  |\n| NULL                               |\n+------------------------------------+\nSELECT 10 rows in set (... sec)\n\ncr> select count(*) from locations where name is null;\n+----------+\n| count(*) |\n+----------+\n|        1 |\n+----------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nOn object columns using IS NULL can be slow because objects themselves don’t have indices. They only exist for their child columns.\n\nYou can either query on inner columns or try the null_or_empty scalar for improved performance.\n\nIS NOT NULL\n\nReturns TRUE if expr does not evaluate to NULL. Additionally, for column references it returns FALSE if the column does not exist.\n\nUse this predicate to check for non-NULL values as SQL’s three-valued logic does always return NULL when comparing NULL.\n\nexpr\n\nExpression of one of the supported data types supported by CrateDB.\n\ncr> select name from locations where inhabitants['interests'] is not null;\n+-------------------+\n| name              |\n+-------------------+\n| Arkintoofle Minor |\n| Bartledan         |\n| Argabuthon        |\n+-------------------+\nSELECT 3 rows in set (... sec)\n\ncr> select count(*) from locations where name is not null;\n+----------+\n| count(*) |\n+----------+\n|       12 |\n+----------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nOn object columns using IS NOT NULL can be slow because objects themselves don’t have indices. They only exist for their child columns.\n\nYou can either query on inner columns or try the null_or_empty scalar for improved performance.\n\nArray comparisons\n\nCrateDB supports a variety of array comparisons.\n\nIN\n\nCrateDB supports the operator IN which allows you to verify the membership of the left-hand operator operand in a right-hand set of expressions. Returns true if any evaluated expression value from a right-hand set equals left-hand operand. Returns false otherwise:\n\ncr> select name, kind from locations\n... where (kind in ('Star System', 'Planet'))  order by name asc;\n +---------------------+-------------+\n | name                | kind        |\n +---------------------+-------------+\n |                     | Planet      |\n | Aldebaran           | Star System |\n | Algol               | Star System |\n | Allosimanius Syneca | Planet      |\n | Alpha Centauri      | Star System |\n | Altair              | Star System |\n | Argabuthon          | Planet      |\n | Arkintoofle Minor   | Planet      |\n | Bartledan           | Planet      |\n +---------------------+-------------+\n SELECT 9 rows in set (... sec)\n\n\nThe IN construct can be used in subquery expressions or array comparisons.\n\nANY (array)\n\nThe ANY (or SOME) operator allows you to query elements within arrays.\n\nFor example, this query returns any row where the array inhabitants['interests'] contains a netball element:\n\ncr> select inhabitants['name'], inhabitants['interests'] from locations\n... where 'netball' = ANY(inhabitants['interests']);\n+---------------------+------------------------------+\n| inhabitants['name'] | inhabitants['interests']     |\n+---------------------+------------------------------+\n| Minories            | [\"netball\", \"short stories\"] |\n| Bartledannians      | [\"netball\"]                  |\n+---------------------+------------------------------+\nSELECT 2 rows in set (... sec)\n\n\nThis query combines the ANY operator with the LIKE operator:\n\ncr> select inhabitants['name'], inhabitants['interests'] from locations\n... where '%stories%' LIKE ANY(inhabitants['interests']);\n+---------------------+------------------------------+\n| inhabitants['name'] | inhabitants['interests']     |\n+---------------------+------------------------------+\n| Minories            | [\"netball\", \"short stories\"] |\n+---------------------+------------------------------+\nSELECT 1 row in set (... sec)\n\n\nFor LIKE ANY operators, the patterns can be provided on either sides:\n\ncr> select name from locations where name ILIKE ANY(['al%', 'ar%']) order\n... by name asc;\n+---------------------+\n| name                |\n+---------------------+\n| Aldebaran           |\n| Algol               |\n| Allosimanius Syneca |\n| Alpha Centauri      |\n| Altair              |\n| Argabuthon          |\n| Arkintoofle Minor   |\n+---------------------+\nSELECT 7 rows in set (... sec)\n\n\nThis query passes a literal array value to the ANY operator:\n\ncr> select name, inhabitants['interests'] from locations\n... where name = ANY(ARRAY['Bartledan', 'Algol'])\n... order by name asc;\n+-----------+--------------------------+\n| name      | inhabitants['interests'] |\n+-----------+--------------------------+\n| Algol     | NULL                     |\n| Bartledan | [\"netball\"]              |\n+-----------+--------------------------+\nSELECT 2 rows in set (... sec)\n\n\nThis query selects any locations with at least one (i.e., ANY) population figure above 100:\n\ncr> select name, information['population'] from locations\n... where 100 < ANY (information['population'])\n... order by name;\n+-------------------+---------------------------+\n| name              | information['population'] |\n+-------------------+---------------------------+\n| North West Ripple | [12, 163]                 |\n| Outer Eastern Rim | [5673745846]              |\n+-------------------+---------------------------+\nSELECT 2 rows in set (... sec)\n\n\nANY automatically unnests the array argument to the number of dimensions required:\n\ncr> SELECT 1 = ANY([[1, 2], [3, 4]]);\n+------+\n| true |\n+------+\n| TRUE |\n+------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nIt is possible to use ANY to compare values directly against the properties of object arrays, as above. However, this usage is discouraged as it cannot utilize the table index and requires the equivalent of a table scan.\n\nThe ANY operator can be used in subquery expressions and array comparisons.\n\nNegating ANY\n\nNegating the ANY operator does not behave like other comparison operators.\n\nThe following query negates ANY using != to return all rows where inhabitants['interests'] has at least one array element that is not netball:\n\ncr> select inhabitants['name'], inhabitants['interests'] from locations\n... where 'netball' != ANY(inhabitants['interests']);\n+---------------------+------------------------------+\n| inhabitants['name'] | inhabitants['interests']     |\n+---------------------+------------------------------+\n| Minories            | [\"netball\", \"short stories\"] |\n| Argabuthonians      | [\"science\", \"reason\"]        |\n+---------------------+------------------------------+\nSELECT 2 rows in set (... sec)\n\n\nNote\n\nWhen != ANY(<array_col>)) causes TooManyClauses errors you could consider increasing indices.query.bool.max_clause_count setting as appropriate on each node.\n\nNegating the same query with a preceding not returns all rows where inhabitants['interests'] has no netball element:\n\ncr> select inhabitants['name'], inhabitants['interests'] from locations\n... where not 'netball' = ANY(inhabitants['interests']);\n+---------------------+--------------------------+\n| inhabitants['name'] | inhabitants['interests'] |\n+---------------------+--------------------------+\n| Argabuthonians      | [\"science\", \"reason\"]    |\n+---------------------+--------------------------+\nSELECT 1 row in set (... sec)\n\n\nThis behaviour applies to:\n\nLIKE and NOT LIKE\n\nAll other comparison operators (excluding IS NULL and IS NOT NULL)\n\nNote\n\nWhen using the NOT with ANY, the performance of the query may be poor because special handling is required to implement the 3-valued logic. For better performance, consider using the ignore3vl function.\n\nAdditionally, When NOT with LIKE ANY or NOT LIKE ANY on arrays causes TooManyClauses errors, you could consider increasing indices.query.bool.max_clause_count setting as appropriate on each node.\n\nEXISTS\n\nEXISTS takes a SELECT statement, or subquery as argument. It evaluates to TRUE if the subquery returns at least one row.\n\nThe result only depends on the amount of rows, not on the actual values. Because of that it is common practice to use SELECT 1 [...] within the subquery.\n\nAn example:\n\ncr> SELECT mountain FROM sys.summits t\n...   WHERE EXISTS (SELECT 1 FROM sys.summits WHERE mountain = t.mountain)\n...   ORDER BY height DESC\n...   LIMIT 2;\n+------------+\n| mountain   |\n+------------+\n| Mont Blanc |\n| Monte Rosa |\n+------------+\nSELECT 2 rows in set (... sec)\n\nContainer data types\nArrays\n\nCrateDB supports arrays. It is possible to select and query array elements.\n\nFor example, you might insert an array like so:\n\ncr> insert into locations (id, name, position, kind, landmarks)\n... values (14, 'Frogstar', 4, 'Star System',\n...     ['Total Perspective Vortex', 'Milliways']\n... );\nINSERT OK, 1 row affected (... sec)\n\n\nThe result:\n\ncr> select name, landmarks from locations\n... where name = 'Frogstar';\n+----------+-------------------------------------------+\n| name     | landmarks                                 |\n+----------+-------------------------------------------+\n| Frogstar | [\"Total Perspective Vortex\", \"Milliways\"] |\n+----------+-------------------------------------------+\nSELECT 1 row in set (... sec)\n\n\nThe individual array elements can be selected from the landmarks column with landmarks[n], where n is the integer array index, like so:\n\ncr> select name, landmarks[1] from locations\n... where name = 'Frogstar';\n+----------+--------------------------+\n| name     | landmarks[1]             |\n+----------+--------------------------+\n| Frogstar | Total Perspective Vortex |\n+----------+--------------------------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nThe minimum index value is -2147483648. The maximum array index is 2147483647. Using an index out of the range results in an exception.\n\nIndividual array elements can also be addressed in the where clause, like so:\n\ncr> select name, landmarks from locations\n... where landmarks[2] = 'Milliways';\n+----------+-------------------------------------------+\n| name     | landmarks                                 |\n+----------+-------------------------------------------+\n| Frogstar | [\"Total Perspective Vortex\", \"Milliways\"] |\n+----------+-------------------------------------------+\nSELECT 1 row in set (... sec)\n\n\nWhen using the = operator, as above, the value of the array element at index n is compared. To compare against any array element, see ANY (array).\n\nThe slice of array elements can be selected from the landmarks column with landmarks[from:to], where from and to are the integer array indices, like so:\n\ncr> select name, landmarks[1:2] from locations\n... where name = 'Frogstar';\n+----------+-------------------------------------------+\n| name     | array_slice(landmarks, 1, 2)              |\n+----------+-------------------------------------------+\n| Frogstar | [\"Total Perspective Vortex\", \"Milliways\"] |\n+----------+-------------------------------------------+\nSELECT 1 row in set (... sec)\n\n\nWhen the from index is omitted, then the slice starts from the first element:\n\ncr> select name, landmarks[:2] from locations\n... where name = 'Frogstar';\n+----------+-------------------------------------------+\n| name     | array_slice(landmarks, NULL, 2)           |\n+----------+-------------------------------------------+\n| Frogstar | [\"Total Perspective Vortex\", \"Milliways\"] |\n+----------+-------------------------------------------+\nSELECT 1 row in set (... sec)\n\n\nWhen the to index is omitted, then the slice uses the size of the array as an upper-bound:\n\ncr> select name, landmarks[1:] from locations\n... where name = 'Frogstar';\n+----------+-------------------------------------------+\n| name     | array_slice(landmarks, 1, NULL)           |\n+----------+-------------------------------------------+\n| Frogstar | [\"Total Perspective Vortex\", \"Milliways\"] |\n+----------+-------------------------------------------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nThe first index value is 1. The maximum array index is 2147483647. Both the from and to index values are inclusive. Using an index greater than the array size results in an empty array.\n\nObjects\n\nCrateDB supports objects. It is possible to select and query object properties.\n\nFor example, you might insert an object like so:\n\ncr> insert into locations (id, name, position, kind, inhabitants)\n... values (15, 'Betelgeuse', 2, 'Star System',\n...     {name = 'Betelgeuseans',\n...      description = 'Humanoids with two heads'}\n... );\nINSERT OK, 1 row affected (... sec)\n\n\nThe result:\n\ncr> select name, inhabitants from locations\n... where name = 'Betelgeuse';\n+------------+----------------------------------------------------------------------+\n| name       | inhabitants                                                          |\n+------------+----------------------------------------------------------------------+\n| Betelgeuse | {\"description\": \"Humanoids with two heads\", \"name\": \"Betelgeuseans\"} |\n+------------+----------------------------------------------------------------------+\nSELECT 1 row in set (... sec)\n\n\nThe object properties can be selected from the inhabitants column with inhabitants['property'], where property is the property name, like so:\n\ncr> select name, inhabitants['name'] from locations\n... where name = 'Betelgeuse';\n+------------+---------------------+\n| name       | inhabitants['name'] |\n+------------+---------------------+\n| Betelgeuse | Betelgeuseans       |\n+------------+---------------------+\nSELECT 1 row in set (... sec)\n\n\nObject property can also be addressed in the where clause, like so:\n\ncr> select name, inhabitants from locations\n... where inhabitants['name'] = 'Betelgeuseans';\n+------------+----------------------------------------------------------------------+\n| name       | inhabitants                                                          |\n+------------+----------------------------------------------------------------------+\n| Betelgeuse | {\"description\": \"Humanoids with two heads\", \"name\": \"Betelgeuseans\"} |\n+------------+----------------------------------------------------------------------+\nSELECT 1 row in set (... sec)\n\nNested structures\n\nObjects may contain arrays and arrays may contain objects. These nested structures can be selected and queried.\n\nFor example, you might insert something like this:\n\ncr> insert into locations (id, name, position, kind, inhabitants, information)\n... values (16, 'Folfanga', 4, 'Star System',\n...     {name = 'A-Rth-Urp-Hil-Ipdenu',\n...      description = 'A species of small slug',\n...      interests = ['lettuce', 'slime']},\n...     [{evolution_level=42, population=1},\n...     {evolution_level=6, population=3600001}]\n... );\nINSERT OK, 1 row affected (... sec)\n\n\nThe query above includes:\n\nAn array nested within an object. Specifically, the inhabitants column contains an parent object with an interests property set to a child array of strings (e.g., lettuce).\n\nObjects nested within an array. Specifically, the information column contains a parent array with two child objects (e.g., {evolution_level=42, population=1}).\n\nArrays within objects\n\nThe child array (above) can be selected as a property of the parent object:\n\ncr> select name, inhabitants['interests'] from locations\n... where name = 'Folfanga';\n+----------+--------------------------+\n| name     | inhabitants['interests'] |\n+----------+--------------------------+\n| Folfanga | [\"lettuce\", \"slime\"]     |\n+----------+--------------------------+\nSELECT 1 row in set (... sec)\n\n\nIndividual elements of the child array can be selected by combining the array index syntax with the object property name syntax, like so:\n\ncr> select name, inhabitants[1]['interests'] from locations\n... where name = 'Folfanga';\n+----------+-----------------------------+\n| name     | inhabitants[1]['interests'] |\n+----------+-----------------------------+\n| Folfanga | lettuce                     |\n+----------+-----------------------------+\nSELECT 1 row in set (... sec)\n\n\nCaution\n\nThe example above might surprise you because the child array index comes before the parent object property name, which doesn’t follow the usual left-to-right convention for addressing the contents of a nested structure.\n\nDue to an implementation quirk in early versions of CrateDB, the array index always comes first (see the next subsection for more information). Support for a more traditional left-to-right syntax may be added in the future.\n\nLimitations\n\nThere are two limitations to be aware of:\n\nYou cannot directly nest an array within an array (i.e., array(array(...) is not a valid column definition). You can, however, nest multiple arrays as long as an object comes between them (e.g., array(object as (array(...))) is a valid).\n\nUsing the standard syntax, you can only address the elements of one array in a single expression. If you do address the elements of an array, the array index must appear before any object property names (see the previous admonition for more information).\n\nTip\n\nIf you want to address the elements of more than one array in a single expression, you can use the following non-standard syntax:\n\nselect foo[n1]['bar']::text[][n2] from my_table;\n\n\nHere, n1 is the index of the first array (column foo) and n2 is the index of the second array (object property bar).\n\nThis works by:\n\nType casting the second array (i.e., foo[n1]['bar']) to a string using the <expression>::text syntax, which is equivalent to cast(<expression> as text)\n\nCreating a temporary array (in-memory and addressable) from that string using the <expression>[] syntax, which is equivalent to array(expression)\n\nNote: Because this syntax effectively circumvents the index, it may considerably degrade query performance.\n\nObjects within arrays\n\nAn individual child object (above) can be selected from a parent array as an array element using the array index syntax:\n\ncr> select name, information[1] from locations\n... where name = 'Outer Eastern Rim';\n+-------------------+--------------------------------------------------+\n| name              | information[1]                                   |\n+-------------------+--------------------------------------------------+\n| Outer Eastern Rim | {\"evolution_level\": 2, \"population\": 5673745846} |\n+-------------------+--------------------------------------------------+\nSELECT 1 row in set (... sec)\n\n\nProperties of individual child objects can be selected by combining the array index syntax with the object property name syntax, like so:\n\ncr> select name, information[1]['population'] from locations\n... where name = 'Outer Eastern Rim';\n+-------------------+------------------------------+\n| name              | information[1]['population'] |\n+-------------------+------------------------------+\n| Outer Eastern Rim |                   5673745846 |\n+-------------------+------------------------------+\nSELECT 1 row in set (... sec)\n\n\nAdditionally, consider this data:\n\ncr> select name, information from locations\n... where information['population'] is not null;\n+-------------------+-------------------------------------------------------------------------------------------+\n| name              | information                                                                               |\n+-------------------+-------------------------------------------------------------------------------------------+\n| North West Ripple | [{\"evolution_level\": 4, \"population\": 12}, {\"evolution_level\": 42, \"population\": 163}]    |\n| Outer Eastern Rim | [{\"evolution_level\": 2, \"population\": 5673745846}]                                        |\n| Folfanga          | [{\"evolution_level\": 42, \"population\": 1}, {\"evolution_level\": 6, \"population\": 3600001}] |\n+-------------------+-------------------------------------------------------------------------------------------+\nSELECT 3 rows in set (... sec)\n\n\nIf you’re only interested in one property of each object (e.g., population), you can select a virtual array containing all of the values for that property, like so:\n\ncr> select name, information['population'] from locations\n... where information['population'] is not null;\n+-------------------+---------------------------+\n| name              | information['population'] |\n+-------------------+---------------------------+\n| North West Ripple | [12, 163]                 |\n| Outer Eastern Rim | [5673745846]              |\n| Folfanga          | [1, 3600001]              |\n+-------------------+---------------------------+\nSELECT 3 rows in set (... sec)\n\nAggregation\n\nCrateDB provides built-in aggregation functions that allow you to calculate a single summary value for one or more columns:\n\ncr> select count(*) from locations;\n+----------+\n| count(*) |\n+----------+\n|       16 |\n+----------+\nSELECT 1 row in set (... sec)\n\nWindow functions\n\nCrateDB supports the OVER clause to enable the execution of window functions:\n\ncr> select sum(position) OVER() AS pos_sum, name from locations order by name;\n+---------+------------------------------------+\n| pos_sum | name                               |\n+---------+------------------------------------+\n|      48 |                                    |\n|      48 | Aldebaran                          |\n|      48 | Algol                              |\n|      48 | Allosimanius Syneca                |\n|      48 | Alpha Centauri                     |\n|      48 | Altair                             |\n|      48 | Argabuthon                         |\n|      48 | Arkintoofle Minor                  |\n|      48 | Bartledan                          |\n|      48 | Betelgeuse                         |\n|      48 | Folfanga                           |\n|      48 | Frogstar                           |\n|      48 | Galactic Sector QQ7 Active J Gamma |\n|      48 | North West Ripple                  |\n|      48 | Outer Eastern Rim                  |\n|      48 | NULL                               |\n+---------+------------------------------------+\nSELECT 16 rows in set (... sec)\n\nGROUP BY\n\nCrateDB supports the GROUP BY clause. This clause can be used to group the resulting rows by the value(s) of one or more columns. That means that rows that contain duplicate values will be merged.\n\nThis is useful if used in conjunction with aggregation functions:\n\ncr> select count(*), kind from locations\n... group by kind order by count(*) desc, kind asc;\n+----------+-------------+\n| count(*) | kind        |\n+----------+-------------+\n|        7 | Star System |\n|        5 | Planet      |\n|        4 | Galaxy      |\n+----------+-------------+\nSELECT 3 rows in set (... sec)\n\n\nNote\n\nAll columns that are used either as result column or in the order by clause have to be used within the group by clause. Otherwise the statement won’t execute.\n\nGrouping will be executed against the real table column when aliases that shadow the table columns are used.\n\nGrouping on array columns is supported. Arrays are compared per element.\n\nHAVING\n\nThe HAVING clause is the equivalent to the WHERE clause for the resulting rows of a GROUP BY clause.\n\nA simple HAVING clause example using an equality operator:\n\ncr> select count(*), kind from locations\n... group by kind having count(*) = 4 order by kind;\n+----------+--------+\n| count(*) | kind   |\n+----------+--------+\n|        4 | Galaxy |\n+----------+--------+\nSELECT 1 row in set (... sec)\n\n\nThe condition of the HAVING clause can refer to the resulting columns of the GROUP BY clause.\n\nIt is also possible to use aggregate functions in the HAVING clause, like in the result columns:\n\ncr> select count(*), kind from locations\n... group by kind having min(name) = 'Aldebaran';\n+----------+-------------+\n| count(*) | kind        |\n+----------+-------------+\n|        7 | Star System |\n+----------+-------------+\nSELECT 1 row in set (... sec)\n\ncr> select count(*), kind from locations\n... group by kind having count(*) = 4 and kind like 'Gal%';\n+----------+--------+\n| count(*) | kind   |\n+----------+--------+\n|        4 | Galaxy |\n+----------+--------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nAliases are not supported in the HAVING clause.\n\nWITH Queries (Common Table Expressions)\n\nWITH queries, also referred to as common table expressions (CTE), provides a way to reference subqueries by a name within the primary query. The subqueries effectively act as temporary tables or views for the duration of the primary query.\n\nThis can improve the readability of SQL code as it break down complicated queries into smaller parts. An example is:\n\ncr> WITH\n... message_count_per_device AS (\n...  SELECT COUNT(*) AS cnt, device_id\n...  FROM UNNEST([1, 1, 1, 2, 2]) AS u(device_id)\n...  GROUP BY device_id\n... )\n... SELECT AVG(cnt) AS average_message_count\n... FROM message_count_per_device;\n+-----------------------+\n| average_message_count |\n+-----------------------+\n|                   2.5 |\n+-----------------------+\nWITH 1 row in set (... sec)\n\n\nwhich defines a temporary relation message_count_per_device inside the WITH clause which can be used as a relation name in the subsequent SELECT clause.\n\nThe same query could have been written using subqueries only, but possibly harder to read:\n\ncr> SELECT AVG(cnt) AS average_message_count\n... FROM (\n...  SELECT COUNT(*) AS cnt, device_id\n...  FROM UNNEST([1, 1, 1, 2, 2]) AS u(device_id)\n...  GROUP BY device_id\n... ) as message_count_per_device;\n+-----------------------+\n| average_message_count |\n+-----------------------+\n|                   2.5 |\n+-----------------------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nCTEs can be used in combination with SELECT clauses only.\n\nRecursive CTEs are not supported.\n\nCTEs are never materialized.\n\nNested WITH clauses\n\nIt is possible to use WITH clauses within a subquery or another WITH clause. Nested clauses can use the CTE’s defined within the parent’s scope, but not the other way around.\n\nIn this example, the inner WITH clause uses the outer CTE a:\n\ncr> WITH\n...  a(id) AS (SELECT * FROM unnest([1])),\n...  b AS (WITH c AS (SELECT * FROM a) SELECT * FROM c)\n... SELECT * FROM b;\n+----+\n| id |\n+----+\n|  1 |\n+----+\nWITH 1 row in set (... sec)\n"
  },
  {
    "title": "Views — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/ddl/views.html",
    "html": "5.6\nViews\n\nTable of contents\n\nCreating views\n\nQuerying views\n\nPrivileges\n\nDropping views\n\nCreating views\n\nViews are stored named queries which can be used in place of table names. They’re resolved at runtime and can be used to simplify common queries.\n\nViews are created using the CREATE VIEW statement\n\nFor example, a common use case is to create a view which queries a table with a pre-defined filter:\n\ncr> CREATE VIEW big_mountains AS\n... SELECT * FROM sys.summits WHERE height > 2000;\nCREATE OK, 1 row affected (... sec)\n\nQuerying views\n\nOnce created, views can be used instead of a table in a statement:\n\ncr> SELECT mountain, height FROM big_mountains ORDER BY 1 LIMIT 3;\n+--------------+--------+\n| mountain     | height |\n+--------------+--------+\n| Acherkogel   |   3008 |\n| Ackerlspitze |   2329 |\n| Adamello     |   3539 |\n+--------------+--------+\nSELECT 3 rows in set (... sec)\n\nPrivileges\n\nIn order to be able to query data from a view, a user needs to have DQL privileges on a view. DQL privileges can be granted on a cluster level, on the schema in which the view is contained, or the view itself. Privileges on relations accessed by the view are not necessary.\n\nHowever, it is required, at all times, that the owner (the user who created the view), has DQL privileges on all relations occurring within the view’s query definition.\n\nA common use case for this is to give users access to a subset of a table without exposing the table itself as well. If the owner DQL permissions on the underlying relations, a user who has access to the view will no longer be able to query it.\n\nSee Also\n\nAdministration: Privileges\n\nDropping views\n\nViews can be dropped using the DROP VIEW statement:\n\ncr> DROP VIEW big_mountains;\nDROP OK, 1 row affected (... sec)\n"
  },
  {
    "title": "Show Create Table — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/ddl/show-create-table.html",
    "html": "5.6\nShow Create Table\n\nThe SHOW CREATE TABLE statement prints the CREATE TABLE statement of an existing user-created doc table in the cluster:\n\ncr> show create table my_table;\n+-----------------------------------------------------+\n| SHOW CREATE TABLE doc.my_table                      |\n+-----------------------------------------------------+\n| CREATE TABLE IF NOT EXISTS \"doc\".\"my_table\" (       |\n|    \"first_column\" INTEGER NOT NULL,                          |\n|    \"second_column\" TEXT,                            |\n|    \"third_column\" TIMESTAMP WITH TIME ZONE,         |\n|    \"fourth_column\" OBJECT(STRICT) AS (              |\n|       \"key\" TEXT,                                   |\n|       \"value\" TEXT                                  |\n|    ),                                               |\n|    PRIMARY KEY (\"first_column\")                     |\n| )                                                   |\n| CLUSTERED BY (\"first_column\") INTO 5 SHARDS         |\n| WITH (                                              |\n|    \"allocation.max_retries\" = 5,                    |\n|    \"blocks.metadata\" = false,                       |\n|    \"blocks.read\" = false,                           |\n|    \"blocks.read_only\" = false,                      |\n|    \"blocks.read_only_allow_delete\" = false,         |\n|    \"blocks.write\" = false,                          |\n|    codec = 'default',                               |\n|    column_policy = 'strict',                        |\n|    \"mapping.total_fields.limit\" = 1000,             |\n|    max_ngram_diff = 1,                              |\n|    max_shingle_diff = 3,                            |\n|    \"merge.scheduler.max_thread_count\" = 1,          |\n|    number_of_replicas = '0-1',                      |\n|    \"routing.allocation.enable\" = 'all',             |\n|    \"routing.allocation.total_shards_per_node\" = -1, |\n|    \"store.type\" = 'fs',                             |\n|    \"translog.durability\" = 'REQUEST',               |\n|    \"translog.flush_threshold_size\" = 536870912,     |\n|    \"translog.sync_interval\" = 5000,                 |\n|    \"unassigned.node_left.delayed_timeout\" = 60000,  |\n|    \"write.wait_for_active_shards\" = '1'             |\n| )                                                   |\n+-----------------------------------------------------+\nSHOW 1 row in set (... sec)\n\n\nThe table settings returned within the WITH clause of the result are all available table settings showing their respective values at the time of the execution of the SHOW statement.\n\nDifferent versions of CrateDB may have different default table settings. This means that if you re-create the table using the resulting CREATE TABLE statement the settings of the ‘old’ table may differ from the settings of the ‘new’ table. This is because the table settings are set explicitly on creation time."
  },
  {
    "title": "Fulltext analyzers — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/ddl/analyzers.html",
    "html": "5.6\nFulltext analyzers\n\nTable of contents\n\nOverview\n\nBuilt-in analyzers\n\nstandard\n\ndefault\n\nsimple\n\nplain\n\nwhitespace\n\nstop\n\nkeyword\n\npattern\n\nlanguage\n\nsnowball\n\nfingerprint\n\nBuilt-in tokenizers\n\nStandard tokenizer\n\nClassic tokenizer\n\nThai tokenizer\n\nLetter tokenizer\n\nLowercase tokenizer\n\nWhitespace tokenizer\n\nUAX URL email tokenizer\n\nN-gram tokenizer\n\nEdge n-gram tokenizer\n\nKeyword tokenizer\n\nPattern tokenizer\n\nSimple pattern tokenizer\n\nSimple pattern split tokenizer\n\nPath hierarchy tokenizer\n\nChar group tokenizer\n\nBuilt-in token filters\n\nstandard\n\nclassic\n\napostrophe\n\nasciifolding\n\nlength\n\nlowercase\n\nngram\n\nedge_ngram\n\nporter_stem\n\nshingle\n\nstop\n\nword_delimiter\n\nstemmer\n\nkeyword_marker\n\nkstem\n\nsnowball\n\nsynonym\n\n*_decompounder\n\nreverse\n\nelision\n\ntruncate\n\nunique\n\npattern_capture\n\npattern_replace\n\ntrim\n\nlimit\n\nhunspell\n\ncommon_grams\n\n*_normalization\n\nscandinavian_folding\n\ndelimited_payload\n\nkeep\n\nstemmer_override\n\ncjk_bigram\n\ncjk_width\n\n*_stem\n\ndecimal_digit\n\nremove_duplicates\n\nphonetic\n\ndouble_metaphone\n\nbeider_morse\n\nBuilt-in char filter\n\nmapping\n\nhtml_strip\n\npattern_replace\n\nkeep_types\n\nmin_hash\n\nfingerprint\n\nOverview\n\nAnalyzers are used for creating fulltext-indexes. They take the content of a field and split it into tokens, which are then searched. Analyzers filter, reorder and/or transform the content of a field before it becomes the final stream of tokens.\n\nAn analyzer consists of one tokenizer, zero or more token-filters, and zero or more char-filters.\n\nWhen a field-content is analyzed to become a stream of tokens, the char-filter is applied at first. It is used to filter some special chars from the stream of characters that make up the content.\n\nTokenizers take a possibly filtered stream of characters and split it into a stream of tokens.\n\nToken-filters can add tokens, delete tokens or transform them.\n\nWith these elements in place, analyzer provide fine-grained control over building a token stream used for fulltext search. For example you can use language specific analyzers, tokenizers and token-filters to get proper search results for data provided in a certain language.\n\nBelow the builtin analyzers, tokenizers, token-filters and char-filters are listed. They can be used as is or can be extended.\n\nSee Also\n\nFulltext indices for examples showing how to create tables which make use of analyzers.\n\nCreating a custom analyzer for an example showing how to create a custom analyzer.\n\nCREATE ANALYZER for the syntax reference.\n\nBuilt-in analyzers\nstandard\n\ntype='standard'\n\nAn analyzer of type standard is built using the Standard tokenizer tokenizer with the standard Token Filter, lowercase Token Filter, and stop Token Filter.\n\nLowercase all Tokens, uses NO stopwords and excludes tokens longer than 255 characters. This analyzer uses unicode text segmentation, which is defined by UAX#29.\n\nFor example, the standard analyzer converts the sentence\n\nThe quick brown fox jumps Over the lAzY DOG.\n\n\ninto the following tokens\n\nquick, brown, fox, jumps, lazy, dog\n\n\nParameters\n\nstopwords\n\nA list of stopwords to initialize the stop filter with. Defaults to the english stop words.\n\nmax_token_length\n\nThe maximum token length. If a token exceeds this length it is split in max_token_length chunks. Defaults to 255.\n\ndefault\n\ntype='default'\n\nThis is the same as the standard-analyzer analyzer.\n\nsimple\n\ntype='simple'\n\nUses the Lowercase tokenizer tokenizer.\n\nplain\n\ntype='plain'\n\nThe plain analyzer is an alias for the keyword analyzer and cannot be extended. You must extend the keyword analyzer instead.\n\nwhitespace\n\ntype='whitespace'\n\nUses a Whitespace tokenizer tokenizer\n\nstop\n\ntype='stop'\n\nUses a Lowercase tokenizer tokenizer, with stop Token Filter.\n\nParameters\n\nstopwords\n\nA list of stopwords to initialize the :ref:’stop-tokenfilter` filter with. Defaults to the english stop words.\n\nstopwords_path\n\nA path (either relative to configuration location, or absolute) to a stopwords file configuration.\n\nkeyword\n\ntype='keyword'\n\nCreates one single token from the field-contents.\n\npattern\n\ntype='pattern'\n\nAn analyzer of type pattern that can flexibly separate text into terms via a regular expression.\n\nParameters\n\nlowercase\n\nShould terms be lowercased or not. Defaults to true.\n\npattern\n\nThe regular expression pattern, defaults to W+.\n\nflags\n\nThe regular expression flags.\n\nNote\n\nThe regular expression should match the token separators, not the tokens themselves.\n\nFlags should be pipe-separated, e.g. CASE_INSENSITIVE|COMMENTS. Check Java Pattern API for more details about flags options.\n\nlanguage\n\ntype='<language-name>'\n\nThe following types are supported:\n\narabic, armenian, basque, brazilian, bengali, bulgarian, catalan, chinese, cjk, czech, danish, dutch, english, finnish, french, galician, german, greek, hindi, hungarian, indonesian, italian, latvian, lithuanian, norwegian, persian, portuguese, romanian, russian, sorani, spanish, swedish, turkish, thai.\n\nParameters\n\nstopwords\n\nA list of stopwords to initialize the stop filter with. Defaults to the english stop words.\n\nstopwords_path\n\nA path (either relative to configuration location, or absolute) to a stopwords file configuration.\n\nstem_exclusion\n\nThe stem_exclusion parameter allows you to specify an array of lowercase words that should not be stemmed. The following analyzers support setting stem_exclusion: arabic, armenian, basque, brazilian, bengali, bulgarian, catalan, czech, danish, dutch, english, finnish, french, galician, german, hindi, hungarian, indonesian, italian, latvian, lithuanian, norwegian, portuguese, romanian, russian, spanish, swedish, turkish.\n\nsnowball\n\ntype='snowball'\n\nUses the Standard tokenizer tokenizer, with standard filter, lowercase filter, stop filter, and snowball filter.\n\nParameters\n\nstopwords\n\nA list of stopwords to initialize the stop filter with. Defaults to the english stop words.\n\nlanguage\n\nSee the language-parameter of snowball.\n\nfingerprint\n\ntype='fingerprint'\n\nThe fingerprint analyzer implements a fingerprinting algorithm which is used by the OpenRefine project to assist in clustering. Input text is lowercased, normalized to remove extended characters, sorted, de-duplicated and concatenated into a single token. If a stopword list is configured, stop words will also be removed. It uses the Standard tokenizer tokenizer and the following filters: lowercase, asciifolding, fingerprint and ref:stop-tokenfilter.\n\nParameters\n\nseparator\n\nThe character to use to concatenate the terms. Defaults to a space.\n\nmax_output_size\n\nThe maximum token size to emit, tokens larger than this size will be discarded. Defaults to 255.\n\nstopwords\n\nA pre-defined stop words list like _english_ or an array containing a list of stop words. Defaults to \\_none_.\n\nstopwords_path\n\nThe path to a file containing stop words.\n\nBuilt-in tokenizers\nStandard tokenizer\n\ntype='standard'\n\nThe tokenizer of type standard is providing a grammar based tokenizer, which is a good tokenizer for most European language documents. The tokenizer implements the Unicode Text Segmentation algorithm, as specified in Unicode Standard Annex #29.\n\nParameters\n\nmax_token_length\n\nThe maximum token length. If a token exceeds this length it is split in max_token_length chunks. Defaults to 255.\n\nClassic tokenizer\n\ntype='classic'\n\nThe classic tokenizer is a grammar based tokenizer that is good for English language documents. This tokenizer has heuristics for special treatment of acronyms, company names, email addresses, and internet host names. However, these rules don’t always work, and the tokenizer doesn’t work well for most languages other than English.\n\nParameters\n\nmax_token_length\n\nThe maximum token length. If a token exceeds this length it is split in max_token_length chunks. Defaults to 255.\n\nThai tokenizer\n\ntype='thai'\n\nThe thai tokenizer splits Thai text correctly, treats all other languages like the standard-tokenizer does.\n\nLetter tokenizer\n\ntype='letter'\n\nThe letter tokenizer splits text at non-letters.\n\nLowercase tokenizer\n\ntype='lowercase'\n\nThe lowercase tokenizer performs the function of Letter tokenizer and lowercase together. It divides text at non-letters and converts them to lower case.\n\nWhitespace tokenizer\n\ntype='whitespace'\n\nThe whitespace tokenizer splits text at whitespace.\n\nParameters\n\nmax_token_length\n\nThe maximum token length. If a token exceeds this length it is split in max_token_length chunks. Defaults to 255.\n\nUAX URL email tokenizer\n\ntype='uax_url_email'\n\nThe uax_url_email tokenizer behaves like the Standard tokenizer, but tokenizes emails and URLs as single tokens.\n\nParameters\n\nmax_token_length\n\nThe maximum token length. If a token exceeds this length it is split in max_token_length chunks. Defaults to 255.\n\nN-gram tokenizer\n\ntype='ngram'\n\nParameters\n\nmin_gram\n\nMinimum length of characters in a gram. default: 1.\n\nmax_gram\n\nMaximum length of characters in a gram. default: 2.\n\ntoken_chars\n\nCharacters classes to keep in the tokens, will split on characters that don’t belong to any of these classes. default: [] (Keep all characters).\n\nClasses: letter, digit, whitespace, punctuation, symbol\n\nEdge n-gram tokenizer\n\ntype='edge_ngram'\n\nThe edge_ngram tokenizer is very similar to N-gram tokenizer but only keeps n-grams which start at the beginning of a token.\n\nParameters\n\nmin_gram\n\nMinimum length of characters in a gram. default: 1\n\nmax_gram\n\nMaximum length of characters in a gram. default: 2\n\ntoken_chars\n\nCharacters classes to keep in the tokens, will split on characters that don’t belong to any of these classes. default: [] (Keep all characters).\n\nClasses: letter, digit, whitespace, punctuation, symbol\n\nKeyword tokenizer\n\ntype='keyword'\n\nThe keyworkd tokenizer emits the entire input as a single token.\n\nParameters\n\nbuffer_size\n\nThe term buffer size. Defaults to 256.\n\nPattern tokenizer\n\ntype='pattern'\n\nThe pattern tokenizer separates text into terms via a regular expression.\n\nParameters\n\npattern\n\nThe regular expression pattern, defaults to \\W+.\n\nflags\n\nThe regular expression flags.\n\ngroup\n\nWhich group to extract into tokens. Defaults to -1 (split).\n\nNote\n\nThe regular expression should match the token separators, not the tokens themselves.\n\nFlags should be pipe-separated, e.g. CASE_INSENSITIVE|COMMENTS. Check Java Pattern API for more details about flags options.\n\nSimple pattern tokenizer\n\ntype='simple_pattern'\n\nSimilar to the pattern tokenizer, this tokenizer uses a regular expression to split matching text into terms, however with a limited, more restrictive subset of expressions. This is in general faster than the normal pattern tokenizer, but does not support splitting on pattern.\n\nParameters\n\npattern\n\nA Lucene regular expression, defaults to empty string.\n\nSimple pattern split tokenizer\n\ntype='simple_patten_split'\n\nThe simple_pattern_split tokenizer operates with the same restricted subset of regular expressions as the simple_pattern tokenizer, but it splits the input on the pattern, rather than the matching pattern.\n\nParameters\n\npattern\n\nA Lucene regular expression, defaults to empty string.\n\nPath hierarchy tokenizer\n\ntype='path_hierarchy'\n\nTakes something like this:\n\n/something/something/else\n\n\nAnd produces tokens:\n\n/something\n/something/something\n/something/something/else\n\n\nParameters\n\ndelimiter\n\nThe character delimiter to use, defaults to /.\n\nreplacement\n\nAn optional replacement character to use. Defaults to the delimiter.\n\nbuffer_size\n\nThe buffer size to use, defaults to 1024.\n\nreverse\n\nGenerates tokens in reverse order, defaults to false.\n\nskip\n\nControls initial tokens to skip, defaults to 0.\n\nChar group tokenizer\n\ntype=char_group\n\nBreaks text into terms whenever it encounters a character that is part of a predefined set.\n\nParameters\n\ntokenize_on_chars\n\nA list containing characters to tokenize on.\n\nBuilt-in token filters\nstandard\n\ntype='standard'\n\nNormalizes tokens extracted with the Standard tokenizer tokenizer.\n\nclassic\n\ntype='classic'\n\nDoes optional post-processing of terms that are generated by the classic tokenizer. It removes the english possessive from the end of words, and it removes dots from acronyms.\n\napostrophe\n\ntype='apostrophe'\n\nStrips all characters after an apostrophe, and the apostrophe itself.\n\nasciifolding\n\ntype='asciifolding'\n\nConverts alphabetic, numeric, and symbolic Unicode characters which are not in the first 127 ASCII characters (the “Basic Latin” Unicode block) into their ASCII equivalents, if one exists.\n\nlength\n\ntype='length'\n\nRemoves words that are too long or too short for the stream.\n\nParameters\n\nmin\n\nThe minimum number. Defaults to 0.\n\nmax\n\nThe maximum number. Defaults to Integer.MAX_VALUE.\n\nlowercase\n\ntype='lowercase'\n\nNormalizes token text to lower case.\n\nParameters\n\nlanguage\n\nFor options, see language analyzer.\n\nngram\n\ntype='ngram'\n\nParameters\n\nmin_gram\n\nDefaults to 1.\n\nmax_gram\n\nDefaults to 2.\n\nedge_ngram\n\ntype='edge_ngram'\n\nParameters\n\nmin_gram\n\nDefaults to 1.\n\nmax_gram\n\nDefaults to 2.\n\nside\n\nEither front or back. Defaults to front.\n\nporter_stem\n\ntype='porter_stem'\n\nTransforms the token stream as per the Porter stemming algorithm.\n\nNote\n\nThe input to the stemming filter must already be in lower case, so you will need to use Lower Case Token Filter or Lower Case tokenizer farther down the tokenizer chain in order for this to work properly! For example, when using custom analyzer, make sure the lowercase filter comes before the porterStem filter in the list of filters.\n\nshingle\n\ntype='shingle'\n\nConstructs shingles (token n-grams), combinations of tokens as a single token, from a token stream.\n\nParameters\n\nmax_shingle_size\n\nThe maximum shingle size. Defaults to 2.\n\nmin_shingle_sizes\n\nThe minimum shingle size. Defaults to 2.\n\noutput_unigrams\n\nIf true the output will contain the input tokens (unigrams) as well as the shingles. Defaults to true.\n\noutput_unigrams_if_no_shingles\n\nIf output_unigrams is false the output will contain the input tokens (unigrams) if no shingles are available. Note if output_unigrams is set to true this setting has no effect. Defaults to false.\n\ntoken_separator\n\nThe string to use when joining adjacent tokens to form a shingle. Defaults to ” “.\n\nstop\n\ntype='stop'\n\nRemoves stop words from token streams.\n\nParameters\n\nstopwords\n\nA list of stop words to use. Defaults to english stop words.\n\nstopwords_path\n\nA path (either relative to configuration location, or absolute) to a stopwords file configuration. Each stop word should be in its own “line” (separated by a line break). The file must be UTF-8 encoded.\n\nignore_case\n\nSet to true to lower case all words first. Defaults to false.\n\nremove_trailing\n\nSet to false in order to not ignore the last term of a search if it is a stop word. Defaults to true\n\nword_delimiter\n\ntype='word_delimiter'\n\nSplits words into subwords and performs optional transformations on subword groups.\n\nParameters\n\ngenerate_word_parts\n\nIf true causes parts of words to be generated: “PowerShot” ⇒ “Power” “Shot”. Defaults to true.\n\ngenerate_number_parts\n\nIf true causes number subwords to be generated: “500-42” ⇒ “500” “42”. Defaults to true.\n\ncatenate_words\n\nIf true causes maximum runs of word parts to be catenated: wi-fi ⇒ wifi. Defaults to false.\n\ncatenate_numbers\n\nIf true causes maximum runs of number parts to be catenated: “500-42” ⇒ “50042”. Defaults to false.\n\ncatenate_all\n\nIf true causes all subword parts to be catenated: “wi-fi-4000” ⇒ “wifi4000”. Defaults to false.\n\nsplit_on_case_change\n\nIf true causes “PowerShot” to be two tokens; (“Power-Shot” remains two parts regards). Defaults to true.\n\npreserve_original\n\nIf true includes original words in subwords: “500-42” ⇒ “500-42” “500” “42”. Defaults to false.\n\nsplit_on_numerics\n\nIf true causes j2se to be three tokens; j 2 se. Defaults to true.\n\nstem_english_possessive\n\nIf true causes trailing “‘s” to be removed for each subword: “O’Neil’s” ⇒ “O”, “Neil”. Defaults to true.\n\nprotected_words\n\nA list of words protected from being delimiter.\n\nprotected_words_path\n\nA relative or absolute path to a file configured with protected words (one on each line). If relative, automatically resolves to config/ based location if exists.\n\ntype_table\n\nA custom type mapping table\n\nstemmer\n\ntype='stemmer'\n\nA filter that stems words (similar to snowball, but with more options).\n\nParameters\n\nlanguage/name\n\narabic, armenian, basque, brazilian, bulgarian, catalan, czech, danish, dutch, english, finnish, french, german, german2, greek, hungarian, italian, kp, kstem, lovins, latvian, norwegian, minimal_norwegian, porter, portuguese, romanian, russian, spanish, swedish, turkish, minimal_english, possessive_english, light_finnish, light_french, minimal_french, light_german, minimal_german, hindi, light_hungarian, indonesian, light_italian, light_portuguese, minimal_portuguese, portuguese, light_russian, light_spanish, light_swedish.\n\nkeyword_marker\n\ntype='keyword_marker'\n\nProtects words from being modified by stemmers. Must be placed before any stemming filters.\n\nParameters\n\nkeywords\n\nA list of words to use.\n\nkeywords_path\n\nA path (either relative to configuration location, or absolute) to a list of words.\n\nignore_case\n\nSet to true to lower case all words first. Defaults to false.\n\nkstem\n\ntype='kstem'\n\nHigh performance filter for english.\n\nAll terms must already be lowercased (use lowercase filter) for this filter to work correctly.\n\nsnowball\n\ntype='snowball'\n\nA filter that stems words using a Snowball-generated stemmer.\n\nParameters\n\nlanguage\n\nPossible values: Armenian, Basque, Catalan, Danish, Dutch, English, Finnish, French, German, German2, Hungarian, Italian, Kp, Lovins, Norwegian, Porter, Portuguese, Romanian, Russian, Spanish, Swedish, Turkish.\n\nsynonym\n\ntype='synonym'\n\nAllows to easily handle synonyms during the analysis process. Synonyms are configured using a file in the Solr/WordNet synonym format.\n\nParameters\n\nsynonyms_path\n\nPath to synonyms configuration file, relative to the configuration directory.\n\nignore_case\n\nDefaults to false\n\nexpand\n\nDefaults to true\n\n*_decompounder\n\ntype='dictionary_decompounder' or type='hyphenation_decompounder'\n\nDecomposes compound words.\n\nParameters\n\nword_list\n\nA list of words to use.\n\nword_list_path\n\nA path (either relative to configuration location, or absolute) to a list of words.\n\nmin_word_size\n\nMinimum word size(Integer). Defaults to 5.\n\nmin_subword_size\n\nMinimum subword size(Integer). Defaults to 2.\n\nmax_subword_size\n\nMaximum subword size(Integer). Defaults to 15.\n\nonly_longest_match\n\nOnly matching the longest(Boolean). Defaults to false\n\nreverse\n\ntype='reverse'\n\nReverses each token.\n\nelision\n\ntype='elision'\n\nRemoves elisions.\n\nParameters\n\narticles\n\nA set of stop words articles, for example ['j', 'l'] for content like J'aime l'odeur.\n\ntruncate\n\ntype='truncate'\n\nTruncates tokens to a specific length.\n\nParameters\n\nlength\n\nNumber of characters to truncate to. default 10\n\nunique\n\ntype='unique'\n\nUsed to only index unique tokens during analysis. By default it is applied on all the token stream.\n\nParameters\n\nonly_on_same_position\n\nIf set to true, it will only remove duplicate tokens on the same position.\n\npattern_capture\n\ntype='pattern_capture'\n\nEmits a token for every capture group in the regular expression.\n\nParameters\n\npreserve_original\n\nIf set to true (the default) then it would also emit the original token\n\npattern_replace\n\ntype='pattern_replace'\n\nHandle string replacements based on a regular expression.\n\nParameters\n\npattern\n\nRegular expression whose matches will be replaced.\n\nreplacement\n\nThe replacement, can reference the original text with $1-like (the first matched group) references.\n\ntrim\n\ntype='trim'\n\nTrims the whitespace surrounding a token.\n\nlimit\n\ntype='limit'\n\nLimits the number of tokens that are indexed per document and field.\n\nParameters\n\nmax_token_count\n\nThe maximum number of tokens that should be indexed per document and field. The default is 1\n\nconsume_all_tokens\n\nIf set to true the filter exhaust the stream even if max_token_count tokens have been consumed already. The default is false.\n\nhunspell\n\ntype='hunspell'\n\nBasic support for Hunspell stemming. Hunspell dictionaries will be picked up from the dedicated directory <path.conf>/hunspell. Each dictionary is expected to have its own directory named after its associated locale (language). This dictionary directory is expected to hold both the *.aff and *.dic files (all of which will automatically be picked up).\n\nParameters\n\nignore_case\n\nIf true, dictionary matching will be case insensitive (defaults to false)\n\nstrict_affix_parsing\n\nDetermines whether errors while reading a affix rules file will cause exception or simply be ignored (defaults to true)\n\nlocale\n\nA locale for this filter. If this is unset, the lang or language are used instead - so one of these has to be set.\n\ndictionary\n\nThe name of a dictionary contained in <path.conf>/hunspell.\n\ndedup\n\nIf only unique terms should be returned, this needs to be set to true. Defaults to true.\n\nrecursion_level\n\nConfigures the recursion level a stemmer can go into. Defaults to 2. Some languages (for example czech) give better results when set to 1 or 0, so you should test it out.\n\ncommon_grams\n\ntype='common_grams'\n\nGenerates bigrams for frequently occurring terms. Single terms are still indexed. It can be used as an alternative to the stop Token filter when we don’t want to completely ignore common terms.\n\nParameters\n\ncommon_words\n\nA list of common words to use.\n\ncommon_words_path\n\nA path (either relative to configuration location, or absolute) to a list of common words. Each word should be in its own “line” (separated by a line break). The file must be UTF-8 encoded.\n\nignore_case\n\nIf true, common words matching will be case insensitive (defaults to false).\n\nquery_mode\n\nGenerates bigrams then removes common words and single terms followed by a common word (defaults to false).\n\nNote\n\nEither common_words or common_words_path must be given.\n\n*_normalization\n\ntype='<language>_normalization'\n\nNormalizes special characters of several languages.\n\nAvailable languages:\n\narabic\n\nbengali\n\ngerman\n\nhindi\n\nindic\n\npersian\n\nscandinavian\n\nserbian\n\nsorani\n\nscandinavian_folding\n\ntype='scandinavian_folding'\n\nFolds Scandinavian characters like ø to o or å to a.\n\nThough this might result in different words, it is easier to match different Scandinavian languages using this folding algorithm.\n\ndelimited_payload\n\ntype='delimited_payload'\n\nSplit tokens up by delimiter (default |) into the real token being indexed and the payload stored additionally into the index. For example Trillian|65535 will be indexed as Trillian with 65535 as payload.\n\nParameters\n\nencoding\n\nHow the payload should be interpreted, possible values are real for float values, integer for integer values and identity for keeping the payload as byte array (string).\n\ndelimiter\n\nThe string used to separate the token and its payload.\n\nkeep\n\ntype='keep'\n\nOnly keep tokens defined within the settings of this filter keep_words and variations.\n\nAll other tokens will be filtered. This filter works like an inverse stop-tokenfilter filter.\n\nParameters\n\nkeep_words\n\nA list of words to keep and index as tokens.\n\nkeep_words_path\n\nA path (either relative to configuration location, or absolute) to a list of words to keep and index.\n\nEach word should be in its own “line” (separated by a line break). The file must be UTF-8 encoded.\n\nstemmer_override\n\ntype='stemmer_override'\n\nOverride any previous stemmer that recognizes keywords with a custom mapping, defined by rules or rules_path. One of these settings has to be set.\n\nParameters\n\nrules\n\nA list of rules for overriding, in the form of [<source>=><replacement>] e.g. \"foo=>bar\"\n\nrules_path\n\nA path to a file with one rule per line, like above.\n\ncjk_bigram\n\ntype='cjk_bigram'\n\nHandle Chinese, Japanese and Korean (CJK) bigrams.\n\nParameters\n\noutput_bigrams\n\nBoolean flag to enable a combined unigram+bigram approach.\n\nDefault is false, so single CJK characters that do not form a bigram are passed as unigrams.\n\nAll non CJK characters are output unmodified.\n\nignored_scripts\n\nScripts to ignore. possible values: han, hiragana, katakana, hangul\n\ncjk_width\n\ntype='cjk_width'\n\nA filter that normalizes CJK.\n\n*_stem\ntype='arabic_stem' or\ntype='brazilian_stem' or\ntype='czech_stem' or\ntype='dutch_stem' or\ntype='french_stem' or\ntype='german_stem' or\ntype='russian_stem'\n\nA group of filters that applies language specific stemmers to the token stream. To prevent terms from being stemmed put a keywordmarker-tokenfilter before this filter into the token_filter chain.\n\ndecimal_digit\n\nA token filter that folds unicode digits to 0-9\n\nremove_duplicates\n\nA token filter that drops identical tokens at the same position.\n\nphonetic\n\nA token filter which converts tokens to their phonetic representation using Soundex, Metaphone, and a variety of other algorithms.\n\nParameters\n\nencoder\n\nWhich phonetic encoder to use. Accepts metaphone (default), double_metaphone, soundex, refined_soundex, caverphone1, caverphone2, cologne, nysiis, koelnerphonetik, haasephonetik, beider_morse, daitch_mokotoff.\n\nreplace\n\nWhether or not the original token should be replaced by the phonetic token. Accepts true (default) and false. Not supported by beider_morse encoding.\n\nNote\n\nBe aware that replace: false can lead to unexpected behavior since the original and the phonetically analyzed version are both kept at the same token position. Some queries handle these stacked tokens in special ways. For example, the fuzzy match query does not apply fuzziness to stacked synonym tokens. This can lead to issues that are difficult to diagnose and reason about. For this reason, it is often beneficial to use separate fields for analysis with and without phonetic filtering. That way searches can be run against both fields with differing boosts and trade-offs (e.g. only run a fuzzy match query on the original text field, but not on the phonetic version).\n\ndouble_metaphone\n\nIf the double_metaphone encoder is used, then this additional parameter is supported:\n\nParameters\n\nmax_code_len\n\nThe maximum length of the emitted metaphone token. Defaults to 4.\n\nbeider_morse\n\nIf the beider_morse encoder is used, then these additional parameters are supported:\n\nParameters\n\nrule_type\n\nWhether matching should be exact or approx (default).\n\nname_type\n\nWhether names are ashkenazi, sephardic, or generic (default).\n\nlanguageset\n\nAn array of languages to check. If not specified, then the language will be guessed. Accepts: any, common, cyrillic, english, french, german, hebrew, hungarian, polish, romanian, russian, spanish.\n\nBuilt-in char filter\nmapping\n\ntype='mapping'\n\nParameters\n\nmappings\n\nA list of mappings as strings of the form [<source>=><replacement>], e.g. \"ph=>f\".\n\nmappings_path\n\nA path to a file with one mapping per line, like above.\n\nhtml_strip\n\ntype='html_strip'\n\nStrips out HTML elements from an analyzed text.\n\npattern_replace\n\ntype='pattern_replace'\n\nManipulates the characters in a string before analysis with a regex.\n\nParameters\n\npattern\n\nRegex whose matches will be replaced\n\nreplacement\n\nReplacement string, can reference replaced text by $1 like references (first matched element)\n\nkeep_types\n\ntype='keep_types'\n\nKeeps only the tokens with a token type contained in a predefined set.\n\nParameters\n\ntypes\n\nA list of token types to keep.\n\nmin_hash\n\ntype='min_hash'\n\nHashes each token of the token stream and divides the resulting hashes into buckets, keeping the lowest-valued hashes per bucket. It then returns these hashes as tokens.\n\nParameters\n\nhash_count\n\nThe number of hashes to hash the token stream with. Defaults to 1.\n\nbucket_count\n\nThe number of buckets to divide the min hashes into. Defaults to 512.\n\nhash_set_size\n\nThe number of min hashes to keep per bucket. Defaults to 1.\n\nwith_rotation\n\nWhether or not to fill empty buckets with the value of the first non-empty bucket to its circular right. Only takes effect if hash_set_size is equal to one. Defaults to true if bucket_count is greater than 1, else false.\n\nfingerprint\n\ntype='fingerprint'\n\nEmits a single token which is useful for fingerprinting a body of text, and/or providing a token that can be clustered on. It does this by sorting the tokens, de-duplicating and then concatenating them back into a single token.\n\nParameters\n\nseparator\n\nSeparator which is used for concatenating the tokens. Defaults to a space.\n\nmax_output_size\n\nIf the concatenated fingerprint grows larger than max_output_size, the token filter will exit and will not emit a token. Defaults to 255."
  },
  {
    "title": "Shard allocation filtering — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/ddl/shard-allocation.html",
    "html": "5.6\nShard allocation filtering\n\nShard allocation filters allows to configure shard and replicas allocation per table across generic attributes associated with nodes.\n\nNote\n\nThe per-table shard allocation filtering works in conjunction with Cluster Level Allocation.\n\nIt is possible to assign certain attributes to a node, see Custom attributes.\n\nThese attributes can be used with routing.allocation.* settings to allocate a table to a particular group of nodes.\n\nSettings\n\nThe following settings are dynamic, allowing tables to be allocated (when defined on table creation) or moved (when defined by altering a table) from one set of nodes to another:\n\nrouting.allocation.include.{attribute}\n\nAssign the table to a node whose {attribute} has at least one of the comma-separated values.\n\nrouting.allocation.require.{attribute}\n\nAssign the table to a node whose {attribute} has all of the comma-separated values.\n\nrouting.allocation.exclude.{attribute}\n\nAssign the table to a node whose {attribute} has none of the comma-separated values.\n\nNote\n\nThese settings are not mutually exclusive. You can for instance have a table with \"routing.allocation.require.storage\" = 'ssd' and \"routing.allocation.exclude.datacenterzone\" = 'zoneA'.\n\nSpecial attributes\n\nFollowing special attributes are supported:\n\n_name\n\nMatch nodes by node name.\n\n_host_ip\n\nMatch nodes by host IP address (IP associated with hostname).\n\n_publish_ip\n\nMatch nodes by publish IP address.\n\n_ip\n\nMatch either _host_ip or _publish_ip.\n\n_host\n\nMatch nodes by hostname."
  },
  {
    "title": "Fulltext indices — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/ddl/fulltext-indices.html",
    "html": "5.6\nFulltext indices\n\nFulltext indices take the contents of one or more fields and split it up into tokens that are used for fulltext-search. The transformation from a text to separate tokens is done by an analyzer. In order to create fulltext search queries a fulltext index with an analyzer must be defined for the related columns.\n\nTable of contents\n\nIndex definition\n\nDisable indexing\n\nPlain index (default)\n\nFulltext index with analyzer\n\nDefining a named index column definition\n\nDefining a composite index\n\nCreating a custom analyzer\n\nExtending a built-in analyzer\n\nIndex definition\n\nIn CrateDB, every column’s data is indexed using the plain index method by default. Currently there are three choices related to index definition:\n\nDisable indexing\n\nPlain index (Default)\n\nFulltext index with analyzer\n\nWarning\n\nCreating an index after a table was already created is currently not supported, so think carefully while designing your table definition.\n\nDisable indexing\n\nIndexing can be turned off by using the INDEX OFF column definition.\n\ncr> create table table_a (\n...   first_column text INDEX OFF\n... );\nCREATE OK, 1 row affected (... sec)\n\n\nNote\n\nDisabling indexing will reduce disk consumption but cause poorer query performance.\n\nNote\n\nINDEX OFF cannot be used with partition columns, as those are not stored as normal columns of a table.\n\nPlain index (default)\n\nAn index of type plain is indexing the input data as-is without analyzing. Using the plain index method is the default behaviour but can also be declared explicitly:\n\ncr> create table table_b1 (\n...   first_column text INDEX using plain\n... );\nCREATE OK, 1 row affected (... sec)\n\n\nThis results in the same behaviour than without any index declaration:\n\ncr> create table table_b2 (\n...   first_column text\n... );\nCREATE OK, 1 row affected (... sec)\n\nFulltext index with analyzer\n\nBy defining an index on a column, it’s analyzed data is indexed instead of the raw data. Thus, depending on the used analyzer, querying for the exact data may not work anymore. See Built-in analyzers for details about available builtin analyzer or Creating a custom analyzer.\n\nIf no analyzer is specified when using a fulltext index, the standard analyzer is used:\n\ncr> create table table_c (\n...   first_column text INDEX using fulltext\n... );\nCREATE OK, 1 row affected (... sec)\n\n\nDefining the usage of a concrete analyzer is straight forward by defining the analyzer as a parameter using the WITH statement:\n\ncr> create table table_d (\n...   first_column text INDEX using fulltext with (analyzer = 'english')\n... );\nCREATE OK, 1 row affected (... sec)\n\nDefining a named index column definition\n\nIt’s also possible to define an index column which treat the data of a given column as input. This is especially useful if you want to search for both, the exact and analyzed data:\n\ncr> create table table_e (\n...   first_column text,\n...   INDEX first_column_ft using fulltext (first_column)\n... );\nCREATE OK, 1 row affected (... sec)\n\n\nOf course defining a custom analyzer is possible here too:\n\ncr> create table table_f (\n...   first_column text,\n...   INDEX first_column_ft\n...     using fulltext(first_column) with (analyzer = 'english')\n... );\nCREATE OK, 1 row affected (... sec)\n\nDefining a composite index\n\nDefining a composite (or combined) index is done using the same syntax as above despite multiple columns are given to the fulltext index method:\n\ncr> create table documents_a (\n...   title text,\n...   body text,\n...   INDEX title_body_ft\n...     using fulltext(title, body) with (analyzer = 'english')\n... );\nCREATE OK, 1 row affected (... sec)\n\n\nComposite indices can include nested columns within object columns as well:\n\ncr> create table documents_b (\n...   title text,\n...   author object(dynamic) as (\n...     name text,\n...     birthday timestamp with time zone\n...   ),\n...   INDEX author_title_ft using fulltext(title, author['name'])\n... );\nCREATE OK, 1 row affected (... sec)\n\n\nNote\n\nIf plain index method is used, this internally translates to fulltext with (analyzer = 'keyword').\n\nCreating a custom analyzer\n\nAn analyzer consists of one tokenizer, zero or more token-filters, and zero or more char-filters.\n\nWhen a field-content is analyzed to become a stream of tokens, the char-filter is applied at first. It is used to filter some special chars from the stream of characters that make up the content.\n\nTokenizers split the possibly filtered stream of characters into tokens.\n\nToken-filters can add tokens, delete tokens or transform them to finally produce the desired stream of tokens.\n\nWith these elements in place, analyzers provide fine grained control over building a token stream used for fulltext search. For example you can use language specific analyzers, tokenizers and token-filters to get proper search results for data provided in a certain language.\n\nHere is a simple Example:\n\ncr> CREATE ANALYZER myanalyzer (\n...   TOKENIZER whitespace,\n...   TOKEN_FILTERS (\n...     lowercase,\n...     kstem\n...   ),\n...   CHAR_FILTERS (\n...     html_strip\n...   )\n... );\nCREATE OK, 1 row affected (... sec)\n\n\nThis creates a custom analyzer called myanalyzer. It uses the built-in Whitespace tokenizer tokenizer and two built-in token filters. lowercase and kstem, as well as a mapping char-filter. : It is possible to further customize the built-in token filters, char-filters or tokenizers:\n\ncr> create ANALYZER myanalyzer_customized (\n...   TOKENIZER whitespace,\n...   TOKEN_FILTERS (\n...     lowercase,\n...     kstem\n...   ),\n...   CHAR_FILTERS (\n...     mymapping WITH (\n...       type='mapping',\n...       mappings = ['ph=>f', 'qu=>q', 'foo=>bar']\n...     )\n...   )\n... );\nCREATE OK, 1 row affected (... sec)\n\n\nThis example creates another analyzer. This time called myanalyzer_customized. It uses the same tokenizer and token filters as in the previous example, but specifies custom options to the mapping char-filter. : The name (mymapping) is a custom name which may not conflict with built-in char-filters or other custom char-filters.\n\nThe provided type property is required as it specifies which built-in char-filter should be customized. The other option mappings is specific to the used type/char-filter.\n\nTokenizer and token-filters can be customized in the same way.\n\nNote\n\nAltering analyzers is not supported yet.\n\nSee Also\n\nCREATE ANALYZER for the syntax reference.\n\nBuilt-in tokenizers for a list of built-in tokenizer.\n\nBuilt-in token filters for a list of built-in token-filter.\n\nBuilt-in char filter for a list of built-in char-filter.\n\nExtending a built-in analyzer\n\nExisting Analyzers can be used to create custom Analyzers by means of extending them.\n\nYou can extend and parameterize Built-in analyzers like this:\n\ncr> create ANALYZER \"german_snowball\" extends snowball WITH (\n...   language = 'german'\n... );\nCREATE OK, 1 row affected (... sec)\n\n\nIf you extend Built-in analyzers, tokenizer, char-filter or token-filter cannot be defined. In this case use the parameters available for the extended Built-in analyzers.\n\nIf you extend custom-analyzers, every part of the analyzer that is omitted will be taken from the extended one. Example:\n\ncr> create ANALYZER e2 EXTENDS myanalyzer (\n...     TOKENIZER mypattern WITH (\n...       type = 'pattern',\n...       pattern = '.*'\n...     )\n... );\nCREATE OK, 1 row affected (... sec)\n\n\nThis analyzer will use the char-filters and token-filters from myanalyzer and will override the tokenizer with mypattern.\n\nSee Also\n\nSee the reference documentation of the Built-in analyzers to get detailed information on the available analyzers."
  },
  {
    "title": "Sharding — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/ddl/sharding.html",
    "html": "5.6\nSharding\n\nTable of contents\n\nIntroduction\n\nNumber of shards\n\nRouting\n\nIntroduction\n\nEvery table partition is split into a configured number of shards. Shards are then distributed across the cluster. As nodes are added to the cluster, CrateDB will move shards around to achieve maximum possible distribution.\n\nTip\n\nNon-partitioned tables function as a single partition, so non-partitioned tables are still split into the configured number of shards.\n\nShards are transparent at the table-level. You do not need to know about or think about shards when querying a table.\n\nRead requests are broken down and executed in parallel across multiple shards on multiple nodes, massively improving read performance.\n\nNumber of shards\n\nThe number of shards can be defined by using the CLUSTERED INTO <number> SHARDS statement upon the table creation.\n\nExample:\n\ncr> create table my_table5 (\n...   first_column integer\n... ) clustered into 10 shards;\nCREATE OK, 1 row affected (... sec)\n\n\nIf the number of shards is not defined explicitly, the sensible default value is applied.\n\nSee Also\n\nCREATE TABLE: CLUSTERED\n\nNote\n\nThe number of shards can be changed after table creation, providing the value is a multiple of number_of_routing_shards (set at table-creation time). Altering the number of shards will put the table into a read-only state until the operation has completed.\n\nCaution\n\nWell tuned shard allocation is vital. Read the Sharding Guide to make sure you’re getting the best performance out of CrateDB.\n\nRouting\n\nGiven a fixed number of primary shards, individual rows can be routed to a fixed shard number with a simple formula:\n\nshard number = hash(routing column) % total primary shards\n\nWhen hash values are distributed evenly (which will be approximately true in most cases), rows will be distributed evenly amongst the fixed amount of available shards.\n\nThe routing column can be specified with the CLUSTERED clause when creating the table. All rows that have the same routing column row value are stored in the same shard. If a primary key has been defined, it will be used as the default routing column, otherwise the internal document ID is used.\n\nExample:\n\ncr> create table my_table6 (\n...   first_column integer,\n...   second_column text\n... ) clustered by (first_column);\nCREATE OK, 1 row affected (... sec)\n\n\nIf primary key constraints are defined, the routing column definition can be omitted as primary key columns are always used for routing by default.\n\nIf the routing column is defined explicitly, it must match a primary key column:\n\ncr> create table my_table8 (\n...   first_column integer primary key,\n...   second_column text primary key,\n...   third_column text\n... ) clustered by (first_column);\nCREATE OK, 1 row affected (... sec)\n\n\nExample for combining custom routing and shard definition:\n\ncr> create table my_table9 (\n...   first_column integer primary key,\n...   second_column text primary key,\n...   third_column text\n... ) clustered by (first_column) into 10 shards;\nCREATE OK, 1 row affected (... sec)\n"
  },
  {
    "title": "Replication — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/ddl/replication.html",
    "html": "5.6\nReplication\n\nYou can configure CrateDB to replicate tables. When you configure replication, CrateDB will try to ensure that every table shard has one or more copies available at all times.\n\nWhen there are multiple copies of the same shard, CrateDB will mark one as the primary shard and treat the rest as replica shards. Write operations always go to the primary shard, whereas read operations can go to any shard. CrateDB continually synchronizes data from the primary shard to all replica shards (through a process known as shard recovery).\n\nWhen a primary shard is lost (e.g., due to node failure), CrateDB will promote a replica shard to a primary. Hence, more table replicas mean a smaller chance of permanent data loss (through increased data redundancy) in exchange for more disk space utilization and intra-cluster network traffic.\n\nReplication can also improve read performance because any increase in the number of shards distributed across a cluster also increases the opportunities for CrateDB to parallelize query execution across multiple nodes.\n\nTable of contents\n\nTable configuration\n\nShard recovery\n\nUnderreplication\n\nTable configuration\n\nYou can configure the number of per-shard replicas WITH the number_of_replicas table setting.\n\nFor example:\n\ncr> CREATE TABLE my_table (\n...   first_column integer,\n...   second_column text\n... ) WITH (number_of_replicas = 0);\nCREATE OK, 1 row affected (... sec)\n\n\nAs well as being able to configure a fixed number of replicas, you can configure a range of values by using a string to specify a minimum and a maximum (dependent on the number of nodes in the cluster).\n\nHere are some examples of replica ranges:\n\nRange\n\n\t\n\nExplanation\n\n\n\n\n0-1\n\n\t\n\nIf you only have one node, CrateDB will not create any replicas. If you have more than one node, CrateDB will create one replica per shard.\n\nThis range is the default value.\n\n\n\n\n2-4\n\n\t\n\nEach table will require at least two replicas for CrateDB to consider it fully replicated (i.e., a green replication health status).\n\nIf the cluster has five nodes, CrateDB will create four replicas and allocate each one to a node that does not hold the corresponding primary.\n\nSuppose a cluster has four nodes or fewer. In that case, CrateDB will be unable to allocate every replica to a node that does not hold the corresponding primary, putting the table into underreplication. As a result, CrateDB will give the table a yellow replication health status.\n\n\n\n\n0-all\n\n\t\n\nCrateDB will create one replica shard for every node that is available in addition to the node that holds the primary shard.\n\nIf you do not specify a number_of_replicas, CrateDB will create one or zero replicas, depending on the number of available nodes at the cluster (e.g., on a single-node cluster, number_of_replicas will be set to zero to allow fast write operations with the default setting of write.wait_for_active_shards).\n\nYou can change the number_of_replicas setting at any time.\n\nSee Also\n\nCREATE TABLE: WITH clause\n\nShard recovery\n\nCrateDB allocates each primary and replica shard to a specific node. You can control this behavior by configuring the allocation settings.\n\nIf one or more nodes become unavailable (e.g., due to hardware failure or network issues), CrateDB will try to recover a replicated table by doing the following:\n\nFor every lost primary shard, locate a replica and promote it to a primary.\n\nWhen CrateDB promotes a replica to primary, it can no longer function as a replica, and so the total number of replicas decreases by one. Because each primary requires a fixed number_of_replicas, a new replica has to be created (see next item).\n\nFor every primary with too few replicas (due to node loss or replica promotion), use the primary shard to recover the required number of replicas.\n\nShard recovery is one of the features that allows CrateDB to provide continuous availability and partition tolerance in exchange for some consistency trade-offs.\n\nSee Also\n\nWikipedia: CAP theorem\n\nUnderreplication\n\nHaving more replicas per primary and distributing shards as thinly as possible (i.e., fewer shards per node) can both increase chances of a successful recovery in the event of node loss.\n\nA single node can hold multiple shards belonging to the same table. For example, suppose a table has more shards (primaries and replicas) than nodes available in the cluster. In that case, CrateDB will determine the best way to allocate shards to the nodes available.\n\nHowever, there is never a benefit to allocating multiple copies of the same shard to a single node (e.g., the primary and a replica of the same shard or two replicas of the same shard).\n\nFor example:\n\nSuppose a single node held the primary and a replica of the same shard. If that node were lost, CrateDB would be unable to use either copy of the shard for recovery (because both were lost), effectively making the replica useless.\n\nSuppose a single node held two replicas of the same shard. If the primary shard were lost (on a different node), CrateDB would only need one of the replica shards on this node to promote a new primary, effectively making the second replica useless.\n\nIn both cases, the second copy of the shard serves no purpose.\n\nFor this reason, CrateDB will never allocate multiple copies of the same shard to a single node.\n\nThe above rule means that for one primary shard and n replicas, a cluster must have at least n + 1 available nodes for CrateDB to fully replicate all shards. When CrateDB cannot fully replicate all shards, the table enters a state known as underreplication.\n\nCrateDB gives underreplicated tables a yellow health status.\n\nTip\n\nThe CrateDB Admin UI provides visual indicators of cluster health that take replication status into account.\n\nAlternatively, you can query health information directly from the sys.health table and replication information from the sys.shards and sys.allocations tables."
  },
  {
    "title": "Partitioned tables — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/ddl/partitioned-tables.html",
    "html": "5.6\nPartitioned tables\n\nTable of contents\n\nIntroduction\n\nCreation\n\nInformation schema\n\nInsert\n\nUpdate\n\nDelete\n\nQuerying\n\nPartitioning by generated columns\n\nAlter\n\nChanging the number of shards\n\nAltering a single partition\n\nAlter table parameters\n\nAlter table ONLY\n\nClosing and opening a partition\n\nLimitations\n\nConsistency notes related to concurrent DML statement\n\nIntroduction\n\nA partitioned table is a virtual table consisting of zero or more partitions. A partition is similar to a regular single table and consists of one or more shards.\n\npartitioned_table\n  |\n  +-- partition 1\n  |     |\n  |     +- shard 0\n  |     |\n  |     +- shard 1\n  |\n  +-- partition 2\n        |\n        +- shard 0\n        |\n        +- shard 1\n\n\nA table becomes a partitioned table by defining partition columns. When a record with a new distinct combination of values for the configured partition columns is inserted, a new partition is created and the document will be inserted into this partition.\n\nA partitioned table can be queried like a regular table.\n\nPartitioned tables have the following advantages:\n\nThe number of shards can be changed on the partitioned table, which will then change how many shards will be used for the next partition creation. This enables one to start out with few shards per partition initially, and scale up the number of shards for later partitions once traffic and ingest rates increase with the lifetime of an application.\n\nPartitions can be backed up and restored individually.\n\nQueries which contain filters in the WHERE clause which identify a single partition or a subset of partitions is less expensive than querying all partitions because the shards of the excluded partitions won’t have to be accessed.\n\nDeleting data from a partitioned table is cheap if full partitions are dropped. Full partitions are dropped with DELETE statements where the optimizer can infer from the WHERE clause and partition columns that all records of a partition match without having to evaluate against the records.\n\nPartitioned tables have the following disadvantages:\n\nIf the partition columns are badly chosen you can end up with too many shards in the cluster, affecting the overall stability and performance negatively.\n\nYou may end up with empty, stale partitions if delete operations couldn’t be optimized to drop full partitions. You may have to watch out for this and invoke DELETE statements to target single partitions to clean them up.\n\nSome optimizations don’t apply to partitioned tables. An example for this is a GROUP BY query where the grouping keys match the CLUSTERED BY columns of a table. This kind of query can be optimized on regular tables, but cannot be optimized on a partitioned table.\n\nNote\n\nKeep in mind that the values of the partition columns are internally base32 encoded into the partition name (which is a separate table).\n\nSo, for every partition, the partition table name includes:\n\nThe table schema (optional)\n\nThe table name\n\nThe base32 encoded partition column value(s)\n\nAn internal overhead of 14 bytes\n\nAltogether, the table name length must not exceed the 255 bytes length limitation.\n\nCaution\n\nEvery table partition is clustered into as many shards as you configure for the table. Because of this, a good partition configuration depends on good shard allocation.\n\nWell tuned shard allocation is vital. Read the sharding guide to make sure you’re getting the best performance out of CrateDB.\n\nCreation\n\nIt can be created using the CREATE TABLE statement using the PARTITIONED BY:\n\ncr> CREATE TABLE parted_table (\n...   id bigint,\n...   title text,\n...   content text,\n...   width double precision,\n...   day timestamp with time zone\n... ) CLUSTERED BY (title) INTO 4 SHARDS PARTITIONED BY (day);\nCREATE OK, 1 row affected (... sec)\n\n\nThis creates an empty partitioned table which is not yet backed by real partitions. Nonetheless does it behave like a normal table.\n\nWhen the value to partition by references one or more Base Columns, their values must be supplied upon INSERT or COPY FROM. Often these values are computed on client side. If this is not possible, a generated column can be used to create a suitable partition value from the given values on database-side:\n\ncr> CREATE TABLE computed_parted_table (\n...   id bigint,\n...   data double precision,\n...   created_at timestamp with time zone,\n...   month timestamp with time zone GENERATED ALWAYS AS date_trunc('month', created_at)\n... ) PARTITIONED BY (month);\nCREATE OK, 1 row affected (... sec)\n\nInformation schema\n\nThis table shows up in the information_schema.tables table, recognizable as partitioned table by a non null partitioned_by column (aliased as p_b here):\n\ncr> SELECT table_schema as schema,\n...   table_name,\n...   number_of_shards as num_shards,\n...   number_of_replicas as num_reps,\n...   clustered_by as c_b,\n...   partitioned_by as p_b,\n...   blobs_path\n... FROM information_schema.tables\n... WHERE table_name='parted_table';\n+--------+--------------+------------+----------+-------+---------+------------+\n| schema | table_name   | num_shards | num_reps | c_b   | p_b     | blobs_path |\n+--------+--------------+------------+----------+-------+---------+------------+\n| doc    | parted_table |          4 |      0-1 | title | [\"day\"] | NULL       |\n+--------+--------------+------------+----------+-------+---------+------------+\nSELECT 1 row in set (... sec)\n\ncr> SELECT table_schema as schema, table_name, column_name, data_type\n... FROM information_schema.columns\n... WHERE table_schema = 'doc' AND table_name = 'parted_table'\n... ORDER BY table_schema, table_name, column_name;\n+--------+--------------+-------------+--------------------------+\n| schema | table_name   | column_name | data_type                |\n+--------+--------------+-------------+--------------------------+\n| doc    | parted_table | content     | text                     |\n| doc    | parted_table | day         | timestamp with time zone |\n| doc    | parted_table | id          | bigint                   |\n| doc    | parted_table | title       | text                     |\n| doc    | parted_table | width       | double precision         |\n+--------+--------------+-------------+--------------------------+\nSELECT 5 rows in set (... sec)\n\n\nAnd so on.\n\nYou can get information about the partitions of a partitioned table by querying the information_schema.table_partitions table:\n\ncr> SELECT count(*) as partition_count\n... FROM information_schema.table_partitions\n... WHERE table_schema = 'doc' AND table_name = 'parted_table';\n+-----------------+\n| partition_count |\n+-----------------+\n| 0               |\n+-----------------+\nSELECT 1 row in set (... sec)\n\n\nAs this table is still empty, no partitions have been created.\n\nInsert\ncr> INSERT INTO parted_table (id, title, width, day)\n... VALUES (1, 'Don''t Panic', 19.5, '2014-04-08');\nINSERT OK, 1 row affected (... sec)\n\ncr> SELECT partition_ident, \"values\", number_of_shards\n... FROM information_schema.table_partitions\n... WHERE table_schema = 'doc' AND table_name = 'parted_table'\n... ORDER BY partition_ident;\n+--------------------------+------------------------+------------------+\n| partition_ident          | values                 | number_of_shards |\n+--------------------------+------------------------+------------------+\n| 04732cpp6osj2d9i60o30c1g | {\"day\": 1396915200000} |                4 |\n+--------------------------+------------------------+------------------+\nSELECT 1 row in set (... sec)\n\n\nOn subsequent inserts with the same partition column values, no additional partition is created:\n\ncr> INSERT INTO parted_table (id, title, width, day)\n... VALUES (2, 'Time is an illusion, lunchtime doubly so', 0.7, '2014-04-08');\nINSERT OK, 1 row affected (... sec)\n\ncr> REFRESH TABLE parted_table;\nREFRESH OK, 1 row affected (... sec)\n\ncr> SELECT partition_ident, \"values\", number_of_shards\n... FROM information_schema.table_partitions\n... WHERE table_schema = 'doc' AND table_name = 'parted_table'\n... ORDER BY partition_ident;\n+--------------------------+------------------------+------------------+\n| partition_ident          | values                 | number_of_shards |\n+--------------------------+------------------------+------------------+\n| 04732cpp6osj2d9i60o30c1g | {\"day\": 1396915200000} |                4 |\n+--------------------------+------------------------+------------------+\nSELECT 1 row in set (... sec)\n\nUpdate\n\nPartition columns cannot be changed, because this would necessitate moving all affected documents. Such an operation would not be atomic and could lead to inconsistent state:\n\ncr> UPDATE parted_table set content = 'now panic!', day = '2014-04-07'\n... WHERE id = 1;\nColumnValidationException[Validation failed for day: Updating a partitioned-by column is not supported]\n\n\nWhen using a generated column as partition column, all the columns referenced in its generation expression cannot be updated either:\n\ncr> UPDATE computed_parted_table set created_at='1970-01-01'\n... WHERE id = 1;\nColumnValidationException[Validation failed for created_at: Updating a column which is referenced in a partitioned by generated column expression is not supported]\n\ncr> UPDATE parted_table set content = 'now panic!'\n... WHERE id = 2;\nUPDATE OK, 1 row affected (... sec)\n\ncr> REFRESH TABLE parted_table;\nREFRESH OK, 1 row affected (... sec)\n\ncr> SELECT * from parted_table WHERE id = 2;\n+----+------------------------------------------+------------+-------+---------------+\n| id | title                                    | content    | width |           day |\n+----+------------------------------------------+------------+-------+---------------+\n|  2 | Time is an illusion, lunchtime doubly so | now panic! |   0.7 | 1396915200000 |\n+----+------------------------------------------+------------+-------+---------------+\nSELECT 1 row in set (... sec)\n\nDelete\n\nDeleting with a WHERE clause matching all rows of a partition will drop the whole partition instead of deleting every matching document, which is a lot faster:\n\ncr> delete from parted_table where day = 1396915200000;\nDELETE OK, -1 rows affected (... sec)\n\ncr> SELECT count(*) as partition_count\n... FROM information_schema.table_partitions\n... WHERE table_schema = 'doc' AND table_name = 'parted_table';\n+-----------------+\n| partition_count |\n+-----------------+\n| 0               |\n+-----------------+\nSELECT 1 row in set (... sec)\n\nQuerying\n\nUPDATE, DELETE and SELECT queries are all optimized to only affect as few partitions as possible based on the partitions referenced in the WHERE clause.\n\nThe WHERE clause is analyzed for partition use by checking the WHERE conditions against the values of the partition columns.\n\nFor example, the following query will only operate on the partition for day=1396915200000:\n\ncr> SELECT count(*) FROM parted_table\n... WHERE day='1970-01-01'\n... ORDER by 1;\n+----------+\n| count(*) |\n+----------+\n| 2        |\n+----------+\nSELECT 1 row in set (... sec)\n\n\nAny combination of conditions that can be evaluated to a partition before actually executing the query is supported:\n\ncr> SELECT id, title FROM parted_table\n... WHERE date_trunc('year', day) > '1970-01-01'\n... OR extract(day_of_week from day) = 1\n... ORDER BY id DESC;\n+----+--------------------+\n| id | title              |\n+----+--------------------+\n|  4 | Spice Pork And haM |\n|  1 | The incredible foo |\n+----+--------------------+\nSELECT 2 rows in set (... sec)\n\n\nInternally the WHERE clause is evaluated against the existing partitions and their partition values. These partitions are then filtered to obtain the list of partitions that need to be accessed.\n\nPartitioning by generated columns\n\nQuerying on tables partitioned by generated columns is optimized to infer a minimum list of partitions from the partition columns referenced in the WHERE clause:\n\ncr> SELECT id, date_format('%Y-%m', month) as m FROM computed_parted_table\n... WHERE created_at = '2015-11-16T13:27:00.000Z'\n... ORDER BY id;\n+----+---------+\n| id | m       |\n+----+---------+\n| 1  | 2015-11 |\n+----+---------+\nSELECT 1 row in set (... sec)\n\nAlter\n\nParameters of partitioned tables can be changed as usual (see Altering tables for more information on how to alter regular tables) with the ALTER TABLE statement. Common ALTER TABLE parameters affect both existing partitions and partitions that will be created in the future.\n\ncr> ALTER TABLE parted_table SET (number_of_replicas = '0-all')\nALTER OK, -1 rows affected (... sec)\n\n\nAltering schema information (such as the column policy or adding columns) can only be done on the table (not on single partitions) and will take effect on both existing and new partitions of the table.\n\ncr> ALTER TABLE parted_table ADD COLUMN new_col text\nALTER OK, -1 rows affected (... sec)\n\nChanging the number of shards\n\nIt is possible at any time to change the number of shards of a partitioned table.\n\ncr> ALTER TABLE parted_table SET (number_of_shards = 10)\nALTER OK, -1 rows affected (... sec)\n\n\nNote\n\nThis will not change the number of shards of existing partitions, but the new number of shards will be taken into account when new partitions are created.\n\ncr> INSERT INTO parted_table (id, title, width, day)\n... VALUES (2, 'All Good', 3.1415, '2014-04-08');\nINSERT OK, 1 row affected (... sec)\n\ncr> SELECT count(*) as num_shards, sum(num_docs) as num_docs\n... FROM sys.shards\n... WHERE schema_name = 'doc' AND table_name = 'parted_table';\n+------------+----------+\n| num_shards | num_docs |\n+------------+----------+\n|         10 |      1   |\n+------------+----------+\nSELECT 1 row in set (... sec)\n\ncr> SELECT partition_ident, \"values\", number_of_shards\n... FROM information_schema.table_partitions\n... WHERE table_schema = 'doc' AND table_name = 'parted_table'\n... ORDER BY partition_ident;\n+--------------------------+------------------------+------------------+\n| partition_ident          | values                 | number_of_shards |\n+--------------------------+------------------------+------------------+\n| 04732cpp6osj2d9i60o30c1g | {\"day\": 1396915200000} |               10 |\n+--------------------------+------------------------+------------------+\nSELECT 1 row in set (... sec)\n\nAltering a single partition\n\nWe also provide the option to change the number of shards that are already allocated for an existing partition. This option operates on a partition basis, thus a specific partition needs to be specified:\n\ncr> ALTER TABLE parted_table PARTITION (day=1396915200000) SET (\"blocks.write\" = true)\nALTER OK, -1 rows affected (... sec)\n\ncr> ALTER TABLE parted_table PARTITION (day=1396915200000) SET (number_of_shards = 5)\nALTER OK, 0 rows affected (... sec)\n\ncr> ALTER TABLE parted_table PARTITION (day=1396915200000) SET (\"blocks.write\" = false)\nALTER OK, -1 rows affected (... sec)\n\ncr> SELECT partition_ident, \"values\", number_of_shards\n... FROM information_schema.table_partitions\n... WHERE table_schema = 'doc' AND table_name = 'parted_table'\n... ORDER BY partition_ident;\n+--------------------------+------------------------+------------------+\n| partition_ident          | values                 | number_of_shards |\n+--------------------------+------------------------+------------------+\n| 04732cpp6osj2d9i60o30c1g | {\"day\": 1396915200000} |                5 |\n+--------------------------+------------------------+------------------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nThe same prerequisites and restrictions as with normal tables apply. See Changing the number of shards.\n\nAlter table parameters\n\nIt is also possible to alter parameters of single partitions of a partitioned table. However, unlike with partitioned tables, it is not possible to alter the schema information of single partitions.\n\nTo change table parameters such as number_of_replicas or other table settings use the PARTITION.\n\ncr> ALTER TABLE parted_table PARTITION (day=1396915200000) RESET (number_of_replicas)\nALTER OK, -1 rows affected (... sec)\n\nAlter table ONLY\n\nSometimes one wants to alter a partitioned table, but the changes should only affect new partitions and not existing ones. This can be done by using the ONLY keyword.\n\ncr> ALTER TABLE ONLY parted_table SET (number_of_replicas = 1);\nALTER OK, -1 rows affected (... sec)\n\nClosing and opening a partition\n\nA single partition within a partitioned table can be opened and closed in the same way a normal table can.\n\ncr> ALTER TABLE parted_table PARTITION (day=1396915200000) CLOSE;\nALTER OK, -1 rows affected (... sec)\n\n\nThis will all operations beside ALTER TABLE ... OPEN to fail on this partition. The partition will also not be included in any query on the partitioned table.\n\nLimitations\n\nWHERE clauses cannot contain queries like partitioned_by_column='x' OR normal_column=x\n\nConsistency notes related to concurrent DML statement\n\nIf a partition is deleted during an active insert or update bulk operation this partition won’t be re-created.\n\nThe number of affected rows will always reflect the real number of inserted/updated documents."
  },
  {
    "title": "Data types — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/ddl/data-types.html",
    "html": "5.6\nData types\n\nData can be stored in different formats. CrateDB has different types that can be specified if a table is created using the CREATE TABLE statement.\n\nData types play a central role as they limit what kind of data can be inserted and how it is stored. They also influence the behaviour when the records are queried.\n\nData type names are reserved words and need to be escaped when used as column names.\n\nTable of contents\n\nOverview\n\nSupported types\n\nRanges and widths\n\nPrimitive types\n\nNull values\n\nBoolean values\n\nCharacter data\n\nNumeric data\n\nDates and times\n\nBit strings\n\nIP addresses\n\nContainer types\n\nObjects\n\nArrays\n\nFLOAT_VECTOR\n\nGeographic types\n\nGeometric points\n\nGeometric shapes\n\nType casting\n\nCast expressions\n\nCast functions\n\nCast from string literals\n\nPostgreSQL compatibility\n\nType aliases\n\nInternal-use types\n\nOverview\nSupported types\n\nCrateDB supports the following data types. Scroll down for more details.\n\nType\n\n\t\n\nDescription\n\n\t\n\nExample\n\n\n\n\nBOOLEAN\n\n\t\n\nA boolean value\n\n\t\n\ntrue or false\n\n\n\n\nVARCHAR(n) and TEXT\n\n\t\n\nA string of Unicode characters\n\n\t\n\n'foobar'\n\n\n\n\nCHARACTER(n) and CHAR(n)\n\n\t\n\nA fixed-length, blank padded string of Unicode characters\n\n\t\n\n'foobar'\n\n\n\n\nSMALLINT, INTEGER and BIGINT\n\n\t\n\nA signed integer value\n\n\t\n\n12345 or -12345\n\n\n\n\nREAL\n\n\t\n\nAn inexact single-precision floating-point value.\n\n\t\n\n3.4028235e+38\n\n\n\n\nDOUBLE PRECISION\n\n\t\n\nAn inexact double-precision floating-point value.\n\n\t\n\n1.7976931348623157e+308\n\n\n\n\nNUMERIC(precision, scale)\n\n\t\n\nAn exact fixed-point fractional number with an arbitrary, user-specified precision.\n\n\t\n\n123.45\n\n\n\n\nTIMESTAMP WITH TIME ZONE\n\n\t\n\nTime and date with time zone\n\n\t\n\n'1970-01-02T00:00:00+01:00'\n\n\n\n\nTIMESTAMP WITHOUT TIME ZONE\n\n\t\n\nTime and date without time zone\n\n\t\n\n'1970-01-02T00:00:00'\n\n\n\n\nDATE\n\n\t\n\nA specific year, month and a day in UTC.\n\n\t\n\n'2021-03-09'\n\n\n\n\nTIME\n\n\t\n\nA specific time as the number of milliseconds since midnight along with an optional time zone offset\n\n\t\n\n'13:00:00' or '13:00:00+01:00'\n\n\n\n\nBIT(n)\n\n\t\n\nA bit sequence\n\n\t\n\nB'00010010'\n\n\n\n\nIP\n\n\t\n\nAn IP address (IPv4 or IPv6)\n\n\t\n\n'127.0.0.1' or '0:0:0:0:0:ffff:c0a8:64'\n\n\n\n\nOBJECT\n\n\t\n\nExpress an object\n\n\t\n{\n    \"foo\" = 'bar',\n    \"baz\" = 'qux'\n}\n\n\n\n\nARRAY\n\n\t\n\nExpress an array\n\n\t\n[\n    {\"name\" = 'Alice', \"age\" = 33},\n    {\"name\" = 'Bob', \"age\" = 45}\n]\n\n\n\n\nGEO_POINT\n\n\t\n\nA geographic data type comprised of a pair of coordinates (latitude and longitude)\n\n\t\n\n[13.46738, 52.50463] or POINT( 13.46738 52.50463 )\n\n\n\n\nGEO_SHAPE\n\n\t\n\nExpress arbitrary GeoJSON geometry objects\n\n\t\n\n[13.46738, 52.50463] or POINT( 13.46738 52.50463 )\n\n{\n    type = 'Polygon',\n    coordinates = [\n        [\n            [100.0, 0.0],\n            [101.0, 0.0],\n            [101.0, 1.0],\n            [100.0, 1.0],\n            [100.0, 0.0]\n        ]\n    ]\n}\n\n\nor:\n\n'POLYGON ((5 5, 10 5, 10 10, 5 10, 5 5))'\n\n\n\n\nfloat_vector(n)\n\n\t\n\nA fixed length vector of floating point numbers\n\n\t\n\n[3.14, 42.21]\n\nRanges and widths\n\nThis section lists all data types supported by CrateDB at a glance in tabular form, including some facts about their byte widths, value ranges and properties.\n\nPlease note that the byte widths do not equal the total storage sizes, which are likely to be larger due to additional metadata.\n\nType\n\n\t\n\nWidth\n\n\t\n\nRange\n\n\t\n\nDescription\n\n\n\n\nBOOLEAN\n\n\t\n\n1 byte\n\n\t\n\ntrue or false\n\n\t\n\nBoolean type\n\n\n\n\nVARCHAR(n)\n\n\t\n\nvariable\n\n\t\n\nMinimum length: 1. Maximum length: 2^31-1 (upper integer range). 1\n\n\t\n\nStrings of variable length. All Unicode characters are allowed.\n\n\n\n\nTEXT\n\n\t\n\nvariable\n\n\t\n\nMinimum length: 1. Maximum length: 2^31-1 (upper integer range). 1\n\n\t\n\nStrings of variable length. All Unicode characters are allowed.\n\n\n\n\nCHARACTER(n), CHAR(n)\n\n\t\n\nvariable\n\n\t\n\nMinimum length: 1. Maximum length: 2^31-1 (upper integer range). 1\n\n\t\n\nStrings of fixed length, blank padded. All Unicode characters are allowed.\n\n\n\n\nSMALLINT\n\n\t\n\n2 bytes\n\n\t\n\n-32,768 to 32,767\n\n\t\n\nSmall-range integer\n\n\n\n\nINTEGER\n\n\t\n\n4 bytes\n\n\t\n\n-2^31 to 2^31-1\n\n\t\n\nTypical choice for integer\n\n\n\n\nBIGINT\n\n\t\n\n8 bytes\n\n\t\n\n-2^63 to 2^63-1\n\n\t\n\nLarge-range integer\n\n\n\n\nNUMERIC\n\n\t\n\nvariable\n\n\t\n\nUp to 131072 digits before, and up to 16383 digits after the decimal point\n\n\t\n\nuser-specified precision, exact\n\n\n\n\nREAL\n\n\t\n\n4 bytes\n\n\t\n\n6 decimal digits precision\n\n\t\n\nInexact, variable-precision\n\n\n\n\nDOUBLE PRECISION\n\n\t\n\n8 bytes\n\n\t\n\n15 decimal digits precision\n\n\t\n\nInexact, variable-precision\n\n\n\n\nTIMESTAMP WITH TIME ZONE\n\n\t\n\n8 bytes\n\n\t\n\n292275054BC to 292278993AD\n\n\t\n\nTime and date with time zone\n\n\n\n\nTIMESTAMP WITHOUT TIME ZONE\n\n\t\n\n8 bytes\n\n\t\n\n292275054BC to 292278993AD\n\n\t\n\nTime and date without time zone\n\n\n\n\nDATE\n\n\t\n\n8 bytes\n\n\t\n\n292275054BC to 292278993AD\n\n\t\n\nDate in UTC. Internally stored as BIGINT.\n\n\n\n\nTIME WITH TIME ZONE\n\n\t\n\n12 bytes\n\n\t\n\n292275054BC to 292278993AD\n\n\t\n\n00:00:00.000000 to 23:59:59.999999 zone: -18:00 to 18:00\n\n\n\n\nBIT(n)\n\n\t\n\nvariable\n\n\t\n\nA sequence of 0 or 1 digits. Minimum length: 1. Maximum length: 2^31-1 (upper integer range).\n\n\t\n\nA string representation of a bit sequence.\n\n\n\n\nIP\n\n\t\n\n8 bytes\n\n\t\n\nIP addresses are stored as BIGINT values.\n\n\t\n\nA string representation of an IP address (IPv4 or IPv6).\n\n\n\n\nOBJECT\n\n\t\n\nvariable\n\n\t\n\nThe theoretical maximum length (number of key/value pairs) is slightly below Java’s Integer.MAX_VALUE.\n\n\t\n\nAn object is structured as a collection of key-values, containing any other type, including further child objects.\n\n\n\n\nARRAY\n\n\t\n\nvariable\n\n\t\n\nThe theoretical maximum length (number of elements) is slightly below Java’s Integer.MAX_VALUE.\n\n\t\n\nAn array is structured as a sequence of any other type.\n\n\n\n\nGEO_POINT\n\n\t\n\n16 bytes\n\n\t\n\nEach coordinate is stored as a DOUBLE PRECISION type.\n\n\t\n\nA GEO_POINT is a geographic data type used to store latitude and longitude coordinates.\n\n\n\n\nGEO_SHAPE\n\n\t\n\nvariable\n\n\t\n\nEach coordinate is stored as a DOUBLE PRECISION type.\n\n\t\n\nA GEO_SHAPE column can store different kinds of GeoJSON geometry objects.\n\n\n\n\nFLOAT_VECTOR(n)\n\n\t\n\nn\n\n\t\n\nVector Minimum length: 1. Maximum length: 2048.\n\n\t\n\nA vector of floating point numbers.\n\nFootnotes\n\n1(1,2,3)\n\nUsing the Column Store limits the values of text columns to a maximum length of 32766 bytes. You can relax that limitation by either defining a column to not use the column store or by turning off indexing.\n\nPrimitive types\n\nPrimitive types are types with scalar values:\n\nNull values\n\nNULL\n\nBoolean values\n\nBOOLEAN\n\nCharacter data\n\nVARCHAR(n)\n\nCHARACTER(n)\n\nTEXT\n\njson\n\nNumeric data\n\nSMALLINT\n\nINTEGER\n\nBIGINT\n\nNUMERIC(precision, scale)\n\nREAL\n\nDOUBLE PRECISION\n\nDates and times\n\nTIMESTAMP\n\nTIME\n\nDATE\n\nINTERVAL\n\nBit strings\n\nBIT(n)\n\nIP addresses\n\nIP\n\nNull values\nNULL\n\nA NULL represents a missing value.\n\nNote\n\nNULL values are not the same as 0, an empty string (''), an empty object ({}), an empty array ([]), or any other kind of empty or zeroed data type.\n\nYou can use NULL values when inserting records to indicate the absence of a data point when the value for a specific column is not known.\n\nSimilarly, CrateDB will produce NULL values when, for example, data is missing from an outer left-join operation. This happens when a row from one relation has no corresponding row in the joined relation.\n\nIf you insert a record without specifying the value for a particular column, CrateDB will insert a NULL value for that column.\n\nFor example:\n\ncr> CREATE TABLE users (\n...     first_name TEXT,\n...     surname TEXT\n... );\nCREATE OK, 1 row affected (... sec)\n\n\nInsert a record without specifying surname:\n\ncr> INSERT INTO users (\n...     first_name\n... ) VALUES (\n...     'Alice'\n... );\nINSERT OK, 1 row affected (... sec)\n\n\nThe resulting row will have a NULL value for surname:\n\ncr> SELECT\n...     first_name,\n...     surname\n... FROM users\n... WHERE first_name = 'Alice';\n+------------+---------+\n| first_name | surname |\n+------------+---------+\n| Alice      | NULL    |\n+------------+---------+\nSELECT 1 row in set (... sec)\n\n\nYou can prevent NULL values being inserted altogether with a NOT NULL constraint, like so:\n\ncr> CREATE TABLE users_with_surnames (\n...     first_name TEXT,\n...     surname TEXT NOT NULL\n... );\nCREATE OK, 1 row affected (... sec)\n\n\nNow, when you try to insert a user without a surname, it will produce an error:\n\ncr> INSERT INTO users_with_surnames (\n...     first_name\n... ) VALUES (\n...     'Alice'\n... );\nSQLParseException[\"surname\" must not be null]\n\nBoolean values\nBOOLEAN\n\nA basic boolean type accepting true and false as values.\n\nExample:\n\ncr> CREATE TABLE my_table (\n...     first_column BOOLEAN\n... );\nCREATE OK, 1 row affected (... sec)\n\ncr> INSERT INTO my_table (\n...     first_column\n... ) VALUES (\n...     true\n... );\nINSERT OK, 1 row affected (... sec)\n\ncr> SELECT * FROM my_table;\n+--------------+\n| first_column |\n+--------------+\n| TRUE         |\n+--------------+\nSELECT 1 row in set (... sec)\n\nCharacter data\n\nCharacter types are general purpose strings of character data.\n\nCrateDB supports the following character types:\n\nVARCHAR(n)\n\nCHARACTER(n)\n\nTEXT\n\njson\n\nNote\n\nOnly character data types without specified length can be analyzed for full text search.\n\nBy default, the plain analyzer is used.\n\nVARCHAR(n)\n\nThe VARCHAR(n) (or CHARACTER VARYING(n)) type represents variable length strings. All Unicode characters are allowed.\n\nThe optional length specification n is a positive integer that defines the maximum length, in characters, of the values that have to be stored or cast. The minimum length is 1. The maximum length is defined by the upper integer range.\n\nAn attempt to store a string literal that exceeds the specified length of the character data type results in an error.\n\ncr> CREATE TABLE users (\n...     id VARCHAR,\n...     name VARCHAR(3)\n... );\nCREATE OK, 1 row affected (... sec)\n\ncr> INSERT INTO users (\n...     id,\n...     name\n... ) VALUES (\n...     '1',\n...     'Alice Smith'\n... );\nSQLParseException['Alice Smith' is too long for the text type of length: 3]\n\n\nIf the excess characters are all spaces, the string literal will be truncated to the specified length.\n\ncr> INSERT INTO users (\n...     id,\n...     name\n... ) VALUES (\n...     '1',\n...     'Bob     '\n... );\nINSERT OK, 1 row affected (... sec)\n\ncr> SELECT\n...    id,\n...    name,\n...    char_length(name) AS name_length\n... FROM users;\n+----+------+-------------+\n| id | name | name_length |\n+----+------+-------------+\n| 1  | Bob  |           3 |\n+----+------+-------------+\nSELECT 1 row in set (... sec)\n\n\nIf a value is explicitly cast to VARCHAR(n), then an over-length value will be truncated to n characters without raising an error.\n\ncr> SELECT 'Alice Smith'::VARCHAR(5) AS name;\n+-------+\n| name  |\n+-------+\n| Alice |\n+-------+\nSELECT 1 row in set (... sec)\n\n\nCHARACTER VARYING and VARCHAR without the length specifier are aliases for the text data type, see also type aliases.\n\nCHARACTER(n)\n\nThe CHARACTER(n) (or CHAR(n)) type represents fixed-length, blank padded strings. All Unicode characters are allowed.\n\nThe optional length specification n is a positive integer that defines the maximum length, in characters, of the values that have to be stored or cast. The minimum length is 1. The maximum length is defined by the upper integer range. If the type is used without the length parameter, a length of 1 is used.\n\nAn attempt to store a string literal that exceeds the specified length of the character data type results in an error.\n\ncr> CREATE TABLE users (\n...     id CHARACTER,\n...     name CHAR(3)\n... );\nCREATE OK, 1 row affected (... sec)\n\ncr> INSERT INTO users (\n...     id,\n...     name\n... ) VALUES (\n...     '1',\n...     'Alice Smith'\n... );\nSQLParseException['Alice Smith' is too long for the character type of length: 3]\n\n\nIf the excess characters are all spaces, the string literal will be truncated to the specified length.\n\ncr> INSERT INTO users (\n...     id,\n...     name\n... ) VALUES (\n...     '1',\n...     'Bob     '\n... );\nINSERT OK, 1 row affected (... sec)\n\ncr> SELECT\n...    id,\n...    name,\n...    char_length(name) AS name_length\n... FROM users;\n+----+------+-------------+\n| id | name | name_length |\n+----+------+-------------+\n| 1  | Bob  |           3 |\n+----+------+-------------+\nSELECT 1 row in set (... sec)\n\ncr> INSERT INTO users (\n...     id,\n...     name\n... ) VALUES (\n...     '1',\n...     'Bob     '\n... );\nINSERT OK, 1 row affected (... sec)\n\n\nIf a value is inserted with a length lower than the defined one, the value will be right padded with whitespaces.\n\ncr> INSERT INTO users (\n...     id,\n...     name\n... ) VALUES (\n...     '1',\n...     'Bo'\n... );\nINSERT OK, 1 row affected (... sec)\n\ncr> SELECT\n...    id,\n...    name,\n...    char_length(name) AS name_length\n... FROM users;\n+----+------+-------------+\n| id | name | name_length |\n+----+------+-------------+\n| 1  | Bo   |           3 |\n+----+------+-------------+\nSELECT 1 row in set (... sec)\n\n\nIf a value is explicitly cast to CHARACTER(n), then an over-length value will be truncated to n characters without raising an error.\n\ncr> SELECT 'Alice Smith'::CHARACTER(5) AS name;\n+-------+\n| name  |\n+-------+\n| Alice |\n+-------+\nSELECT 1 row in set (... sec)\n\nTEXT\n\nA text-based basic type containing one or more characters. All Unicode characters are allowed.\n\nCreate table:\n\ncr> CREATE TABLE users (\n...     name TEXT\n... );\nCREATE OK, 1 row affected (... sec)\n\n\nInsert data:\n\ncr> INSERT INTO users (\n...     name\n... ) VALUES (\n...     '🌻 Alice 🌻'\n... );\nINSERT OK, 1 row affected (... sec)\n\n\nQuery data:\n\ncr> SELECT * FROM users;\n+-------------+\n| name        |\n+-------------+\n| 🌻 Alice 🌻 |\n+-------------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nThe maximum indexed string length is restricted to 32766 bytes when encoded with UTF-8 unless the string is analyzed using full text or indexing and the usage of the Column store is disabled.\n\nThere is no difference in storage costs among all character data types.\n\njson\n\nA type representing a JSON string.\n\nThis type only exists for compatibility and interoperability with PostgreSQL. It cannot to be used in data definition statements and it is not possible to use it to store data. To store JSON data use the existing OBJECT type. It is a more powerful alternative that offers more flexibility but delivers the same benefits.\n\nThe primary use of the JSON type is in type casting for interoperability with PostgreSQL clients which may use the JSON type. The following type casts are example of supported usage of the JSON data type:\n\nCasting from STRING to JSON:\n\ncr> SELECT '{\"x\": 10}'::json;\n+-------------+\n| '{\"x\": 10}' |\n+-------------+\n| {\"x\": 10}   |\n+-------------+\nSELECT 1 row in set (... sec)\n\n\nCasting from JSON to OBJECT:\n\ncr> SELECT ('{\"x\": 10}'::json)::object;\n+-----------+\n| {\"x\"=10}  |\n+-----------+\n| {\"x\": 10} |\n+-----------+\nSELECT 1 row in set (... sec)\n\n\nCasting from OBJECT to JSON:\n\ncr> SELECT {x=10}::json;\n+------------+\n| '{\"x\":10}' |\n+------------+\n| {\"x\":10}   |\n+------------+\nSELECT 1 row in set (... sec)\n\nNumeric data\n\nCrateDB supports the following numeric types:\n\nSMALLINT\n\nINTEGER\n\nBIGINT\n\nNUMERIC(precision, scale)\n\nREAL\n\nDOUBLE PRECISION\n\nNote\n\nThe REAL and DOUBLE PRECISION data types are inexact, variable-precision floating-point types, meaning that these types are stored as an approximation.\n\nAccordingly, storage, calculation, and retrieval of the value will not always result in an exact representation of the actual floating-point value. For instance, the result of applying SUM or AVG aggregate functions may slightly vary between query executions or comparing floating-point values for equality might not always match.\n\nCrateDB conforms to the IEEE 754 standard concerning special values for floating-point data types, meaning that NaN, Infinity, -Infinity (negative infinity), and -0 (signed zero) are all supported:\n\ncr> SELECT\n...     0.0 / 0.0 AS a,\n...     1.0 / 0.0 AS B,\n...     1.0 / -0.0 AS c;\n+-----+----------+-----------+\n| a   | b        | c         |\n+-----+----------+-----------+\n| NaN | Infinity | -Infinity |\n+-----+----------+-----------+\nSELECT 1 row in set (... sec)\n\n\nThese special numeric values can also be inserted into a column of type REAL or DOUBLE PRECISION using a TEXT literal.\n\nFor instance:\n\ncr> CREATE TABLE my_table (\n...     column_1 INTEGER,\n...     column_2 BIGINT,\n...     column_3 SMALLINT,\n...     column_4 DOUBLE PRECISION,\n...     column_5 REAL,\n...     column_6 \"CHAR\"\n... );\nCREATE OK, 1 row affected (... sec)\n\ncr> INSERT INTO my_table (\n...     column_4,\n...     column_5\n... ) VALUES (\n...     'NaN',\n...     'Infinity'\n... );\nINSERT OK, 1 row affected (... sec)\n\ncr> SELECT\n...     column_4,\n...     column_5\n... FROM my_table;\n+----------+----------+\n| column_4 | column_5 |\n+----------+----------+\n| NaN      | Infinity |\n+----------+----------+\nSELECT 1 row in set (... sec)\n\nSMALLINT\n\nA small integer.\n\nLimited to two bytes, with a range from -32,768 to 32,767.\n\nExample:\n\ncr> CREATE TABLE my_table (\n...     number SMALLINT\n... );\nCREATE OK, 1 row affected (... sec)\n\ncr> INSERT INTO my_table (\n...     number\n... ) VALUES (\n...     32767\n... );\nINSERT OK, 1 row affected (... sec)\n\ncr> SELECT number FROM my_table;\n+--------+\n| number |\n+--------+\n| 32767  |\n+--------+\nSELECT 1 row in set (... sec)\n\nINTEGER\n\nAn integer.\n\nLimited to four bytes, with a range from -2^31 to 2^31-1.\n\nExample:\n\ncr> CREATE TABLE my_table (\n...     number INTEGER\n... );\nCREATE OK, 1 row affected (... sec)\n\ncr> INSERT INTO my_table (\n...     number\n... ) VALUES (\n...     2147483647\n... );\nINSERT OK, 1 row affected (... sec)\n\ncr> SELECT number FROM my_table;\n+------------+\n| number     |\n+------------+\n| 2147483647 |\n+------------+\nSELECT 1 row in set (... sec)\n\nBIGINT\n\nA large integer.\n\nLimited to eight bytes, with a range from -2^63 to 2^63-1.\n\nExample:\n\ncr> CREATE TABLE my_table (\n...     number BIGINT\n... );\nCREATE OK, 1 row affected (... sec)\n\ncr> INSERT INTO my_table (\n...     number\n... ) VALUES (\n...     9223372036854775807\n... );\nINSERT OK, 1 row affected (... sec)\n\ncr> SELECT number FROM my_table;\n+---------------------+\n| number              |\n+---------------------+\n| 9223372036854775807 |\n+---------------------+\nSELECT 1 row in set (... sec)\n\nNUMERIC(precision, scale)\n\nAn exact fixed-point fractional number with an arbitrary, user-specified precision.\n\nVariable size, with up to 131072 digits before the decimal point and up to 16383 digits after the decimal point.\n\nFor example, using a cast from a string literal:\n\ncr> SELECT NUMERIC(5, 2) '123.45' AS number;\n+--------+\n| number |\n+--------+\n| 123.45 |\n+--------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nThe NUMERIC type is only supported as a type literal (i.e., for use in SQL expressions, like a type cast, as above).\n\nYou cannot create table columns of type NUMERIC.\n\nThis type is usually used when it is important to preserve exact precision or handle values that exceed the range of the numeric types of the fixed length. The aggregations and arithmetic operations on numeric values are much slower compared to operations on the integer or floating-point types.\n\nThe NUMERIC type can be configured with the precision and scale. The precision value of a numeric is the total count of significant digits in the unscaled numeric value. The scale value of a numeric is the count of decimal digits in the fractional part, to the right of the decimal point. For example, the number 123.45 has a precision of 5 and a scale of 2. Integers have a scale of zero.\n\nTo declare the NUMERIC type with the precision and scale, use the syntax:\n\nNUMERIC(precision, scale)\n\n\nAlternatively, only the precision can be specified, the scale will be zero or positive integer in this case:\n\nNUMERIC(precision)\n\n\nWithout configuring the precision and scale the NUMERIC type value will be represented by an unscaled value of the unlimited precision:\n\nNUMERIC\n\n\nThe NUMERIC type is internally backed by the Java BigDecimal class. For more detailed information about its behaviour, see BigDecimal documentation.\n\nREAL\n\nAn inexact single-precision floating-point value.\n\nLimited to four bytes, six decimal digits precision.\n\nExample:\n\ncr> CREATE TABLE my_table (\n...     number REAL\n... );\nCREATE OK, 1 row affected (... sec)\n\ncr> INSERT INTO my_table (\n...     number\n... ) VALUES (\n...     3.4028235e+38\n... );\nINSERT OK, 1 row affected (... sec)\n\n\nTip\n\n3.4028235+38 represents the value 3.4028235 × 1038\n\ncr> SELECT number FROM my_table;\n+---------------+\n| number        |\n+---------------+\n| 3.4028235e+38 |\n+---------------+\nSELECT 1 row in set (... sec)\n\n\nYou can insert values which exceed the maximum precision, like so:\n\ncr> INSERT INTO my_table (\n...     number\n... ) VALUES (\n...     3.4028234664e+38\n... );\nINSERT OK, 1 row affected (... sec)\n\n\nHowever, the recorded value will be an approximation of the original (i.e., the additional precision is lost):\n\ncr> SELECT number FROM my_table;\n+---------------+\n| number        |\n+---------------+\n| 3.4028235e+38 |\n+---------------+\nSELECT 1 row in set (... sec)\n\n\nSee Also\n\nCrateDB floating-point values\n\nDOUBLE PRECISION\n\nAn inexact number with variable precision supporting double-precision floating-point values.\n\nLimited to eight bytes, with 15 decimal digits precision.\n\nExample:\n\ncr> CREATE TABLE my_table (\n...     number DOUBLE PRECISION\n... );\nCREATE OK, 1 row affected (... sec)\n\ncr> INSERT INTO my_table (\n...     number\n... ) VALUES (\n...     1.7976931348623157e+308\n... );\nINSERT OK, 1 row affected (... sec)\n\n\nTip\n\n1.7976931348623157e+308 represents the value 1.7976931348623157 × 10308\n\ncr> SELECT number FROM my_table;\n+-------------------------+\n| number                  |\n+-------------------------+\n| 1.7976931348623157e+308 |\n+-------------------------+\nSELECT 1 row in set (... sec)\n\n\nYou can insert values which exceed the maximum precision, like so:\n\ncr> INSERT INTO my_table (\n...     number\n... ) VALUES (\n...     1.79769313486231572014e+308\n... );\nINSERT OK, 1 row affected (... sec)\n\n\nHowever, the recorded value will be an approximation of the original (i.e., the additional precision is lost):\n\ncr> SELECT number FROM my_table;\n+-------------------------+\n| number                  |\n+-------------------------+\n| 1.7976931348623157e+308 |\n+-------------------------+\nSELECT 1 row in set (... sec)\n\n\nSee Also\n\nCrateDB floating-point values\n\nDates and times\n\nCrateDB supports the following types for dates and times:\n\nTIMESTAMP\n\nWITH TIME ZONE\n\nWITHOUT TIME ZONE\n\nAT TIME ZONE\n\nTIME\n\nDATE\n\nINTERVAL\n\nWith a few exceptions (noted below), the + and - operators can be used to create arithmetic expressions with temporal operands:\n\nOperand\n\n\t\n\nOperator\n\n\t\n\nOperand\n\n\n\n\nTIMESTAMP\n\n\t\n\n-\n\n\t\n\nTIMESTAMP\n\n\n\n\nINTERVAL\n\n\t\n\n+\n\n\t\n\nTIMESTAMP\n\n\n\n\nTIMESTAMP\n\n\t\n\n+ or -\n\n\t\n\nINTERVAL\n\n\n\n\nINTERVAL\n\n\t\n\n+ or -\n\n\t\n\nINTERVAL\n\nNote\n\nIf an object column is dynamically created, the type detection will not recognize date and time types, meaning that date and time type columns must always be declared beforehand.\n\nTIMESTAMP\n\nA timestamp expresses a specific date and time as the number of milliseconds since the Unix epoch (i.e., 1970-01-01T00:00:00Z).\n\nTIMESTAMP has two variants:\nTIMESTAMP WITHOUT TIME ZONE which\n\npresents all values in UTC.\n\nTIMESTAMP WITH TIME ZONE which presents all values in UTC in respect to the TIME ZONE related offset.\n\nBy default a TIMESTAMP is an alias for TIMESTAMP WITHOUT TIME ZONE.\n\nTimestamps can be expressed as string literals (e.g., '1970-01-02T00:00:00') with the following syntax:\n\ndate-element [time-separator [time-element [offset]]]\n\ndate-element:   yyyy-MM-dd\ntime-separator: 'T' | ' '\ntime-element:   HH:mm:ss [fraction]\nfraction:       '.' digit+\noffset:         {+ | -} HH [:mm] | 'Z'\n\n\nSee Also\n\nFor more information about date and time formatting, see Java 15: Patterns for Formatting and Parsing.\n\nTime zone syntax as defined by ISO 8601 time zone designators.\n\nInternally, CrateDB stores timestamps as BIGINT values, which are limited to eight bytes.\n\nIf you cast a BIGINT to a TIMEZONE, the integer value will be interpreted as the number of milliseconds since the Unix epoch.\n\nUsing the date_format() function, for readability:\n\ncr> SELECT\n...     date_format(0::TIMESTAMP) AS ts_0,\n...     date_format(1000::TIMESTAMP) AS ts_1;\n+-----------------------------+-----------------------------+\n| ts_0                        | ts_1                        |\n+-----------------------------+-----------------------------+\n| 1970-01-01T00:00:00.000000Z | 1970-01-01T00:00:01.000000Z |\n+-----------------------------+-----------------------------+\nSELECT 1 row in set (... sec)\n\n\nIf you cast a REAL or a DOUBLE PRECISION to a TIMESTAMP, the numeric value will be interpreted as the number of seconds since the Unix epoch, with fractional values approximated to the nearest millisecond:\n\ncr> SELECT\n...     date_format(0::TIMESTAMP) AS ts_0,\n...     date_format(1.5::TIMESTAMP) AS ts_1;\n+-----------------------------+-----------------------------+\n| ts_0                        | ts_1                        |\n+-----------------------------+-----------------------------+\n| 1970-01-01T00:00:00.000000Z | 1970-01-01T00:00:01.500000Z |\n+-----------------------------+-----------------------------+\nSELECT 1 row in set (... sec)\n\n\nIf you cast a literal to a TIMESTAMP, years outside the range 0000 to 9999 must be prefixed by the plus or minus symbol. See also Year.parse Javadoc:\n\ncr> SELECT '+292278993-12-31T23:59:59.999Z'::TIMESTAMP as tmstp;\n+---------------------+\n|               tmstp |\n+---------------------+\n| 9223372017129599999 |\n+---------------------+\nSELECT 1 row in set (... sec)\n\n\nCaution\n\nDue to internal date parsing, the full BIGINT range is not supported for timestamp values. The valid range of dates is from 292275054BC to 292278993AD.\n\nWhen inserting timestamps smaller than -999999999999999 (equal to -29719-04-05T22:13:20.001Z) or bigger than 999999999999999 (equal to 33658-09-27T01:46:39.999Z) rounding issues may occur.\n\nA TIMESTAMP can be further defined as:\n\nWITH TIME ZONE\n\nWITHOUT TIME ZONE\n\nAT TIME ZONE\n\nWITH TIME ZONE\n\nIf you define a timestamp as TIMESTAMP WITH TIME ZONE, CrateDB will convert string literals to Coordinated Universal Time (UTC) using the offset value (e.g., +01:00 for plus one hour or Z for UTC).\n\nExample:\n\ncr> CREATE TABLE my_table (\n...     ts_tz_1 TIMESTAMP WITH TIME ZONE,\n...     ts_tz_2 TIMESTAMP WITH TIME ZONE\n... );\nCREATE OK, 1 row affected (... sec)\n\ncr> INSERT INTO my_table (\n...     ts_tz_1,\n...     ts_tz_2\n... ) VALUES (\n...     '1970-01-02T00:00:00',\n...     '1970-01-02T00:00:00+01:00'\n... );\nINSERT OK, 1 row affected (... sec)\n\ncr> SELECT\n...     ts_tz_1,\n...     ts_tz_2\n... FROM my_table;\n+----------+----------+\n|  ts_tz_1 |  ts_tz_2 |\n+----------+----------+\n| 86400000 | 82800000 |\n+----------+----------+\nSELECT 1 row in set (... sec)\n\n\nYou can use date_format() to make the output easier to read:\n\ncr> SELECT\n...     date_format('%Y-%m-%dT%H:%i', ts_tz_1) AS ts_tz_1,\n...     date_format('%Y-%m-%dT%H:%i', ts_tz_2) AS ts_tz_2\n... FROM my_table;\n+------------------+------------------+\n| ts_tz_1          | ts_tz_2          |\n+------------------+------------------+\n| 1970-01-02T00:00 | 1970-01-01T23:00 |\n+------------------+------------------+\nSELECT 1 row in set (... sec)\n\n\nNotice that ts_tz_2 is smaller than ts_tz_1 by one hour. CrateDB used the +01:00 offset (i.e., ahead of UTC by one hour) to convert the second timestamp into UTC prior to insertion. Contrast this with the behavior of WITHOUT TIME ZONE.\n\nNote\n\nTIMESTAMPTZ is an alias for TIMESTAMP WITH TIME ZONE.\n\nWITHOUT TIME ZONE\n\nIf you define a timestamp as TIMESTAMP WITHOUT TIME ZONE, CrateDB will convert string literals to Coordinated Universal Time (UTC) without using the offset value (i.e., any time zone information present is stripped prior to insertion).\n\nExample:\n\ncr> CREATE TABLE my_table (\n...     ts_1 TIMESTAMP WITHOUT TIME ZONE,\n...     ts_2 TIMESTAMP WITHOUT TIME ZONE\n... );\nCREATE OK, 1 row affected (... sec)\n\ncr> INSERT INTO my_table (\n...     ts_1,\n...     ts_2\n... ) VALUES (\n...     '1970-01-02T00:00:00',\n...     '1970-01-02T00:00:00+01:00'\n... );\nINSERT OK, 1 row affected (... sec)\n\n\nUsing the date_format() function, for readability:\n\ncr> SELECT\n...     date_format('%Y-%m-%dT%H:%i', ts_1) AS ts_1,\n...     date_format('%Y-%m-%dT%H:%i', ts_2) AS ts_2\n... FROM my_table;\n+------------------+------------------+\n| ts_1             | ts_2             |\n+------------------+------------------+\n| 1970-01-02T00:00 | 1970-01-02T00:00 |\n+------------------+------------------+\nSELECT 1 row in set (... sec)\n\n\nNotice that ts_1 and ts_2 are identical. CrateDB ignored the +01:00 offset (i.e., ahead of UTC by one hour) when processing the second string literal. Contrast this with the behavior of WITH TIME ZONE.\n\nAT TIME ZONE\n\nYou can use the AT TIME ZONE clause to modify a timestamp in two different ways. It converts a timestamp without time zone to a timestamp with time zone and vice versa.\n\nConvert a timestamp time zone\n\nAdd a timestamp time zone\n\nNote\n\nThe AT TIME ZONE type is only supported as a type literal (i.e., for use in SQL expressions, like a type cast, as below).\n\nYou cannot create table columns of type AT TIME ZONE.\n\nConvert a timestamp time zone\n\nIf you use AT TIME ZONE tz with a TIMESTAMP WITH TIME ZONE, CrateDB will convert the timestamp to time zone tz and cast the return value as a TIMESTAMP WITHOUT TIME ZONE (which discards the time zone information). This process effectively allows you to correct the offset used to calculate UTC.\n\nExample:\n\ncr> CREATE TABLE my_table (\n...     ts_tz TIMESTAMP WITH TIME ZONE\n... );\nCREATE OK, 1 row affected (... sec)\n\ncr> INSERT INTO my_table (\n...     ts_tz\n... ) VALUES (\n...     '1970-01-02T00:00:00'\n... );\nINSERT OK, 1 row affected (... sec)\n\n\nUsing the date_format() function, for readability:\n\ncr> SELECT date_format(\n...     '%Y-%m-%dT%H:%i', ts_tz AT TIME ZONE '+01:00'\n... ) AS ts\n... FROM my_table;\n+------------------+\n| ts               |\n+------------------+\n| 1970-01-02T01:00 |\n+------------------+\nSELECT 1 row in set (... sec)\n\n\nTip\n\nThe AT TIME ZONE clause does the same as the timezone() function:\n\ncr> SELECT\n...     date_format('%Y-%m-%dT%H:%i', ts_tz AT TIME ZONE '+01:00') AS ts_1,\n...     date_format('%Y-%m-%dT%H:%i', timezone('+01:00', ts_tz)) AS ts_2\n... FROM my_table;\n+------------------+------------------+\n| ts_1             | ts_2             |\n+------------------+------------------+\n| 1970-01-02T01:00 | 1970-01-02T01:00 |\n+------------------+------------------+\nSELECT 1 row in set (... sec)\n\nAdd a timestamp time zone\n\nIf you use AT TIME ZONE with a TIMESTAMP WITHOUT TIME ZONE, CrateDB will add the missing time zone information, recalculate the timestamp in UTC, and cast the return value as a TIMESTAMP WITH TIME ZONE.\n\nExample:\n\ncr> CREATE TABLE my_table (\n...     ts TIMESTAMP WITHOUT TIME ZONE\n... );\nCREATE OK, 1 row affected (... sec)\n\ncr> INSERT INTO my_table (\n...     ts\n... ) VALUES (\n...     '1970-01-02T00:00:00'\n... );\nINSERT OK, 1 row affected (... sec)\n\n\nUsing the date_format() function, for readability:\n\ncr> SELECT date_format(\n...     '%Y-%m-%dT%H:%i', ts AT TIME ZONE '+01:00'\n... ) AS ts_tz\n... FROM my_table;\n+------------------+\n| ts_tz            |\n+------------------+\n| 1970-01-01T23:00 |\n+------------------+\nSELECT 1 row in set (... sec)\n\n\nTip\n\nThe AT TIME ZONE clause does the same as the timezone() function:\n\ncr> SELECT date_format(\n...     '%Y-%m-%dT%H:%i', timezone('+01:00', ts)\n... ) AS ts_tz\n... FROM my_table;\n+------------------+\n| ts_tz            |\n+------------------+\n| 1970-01-01T23:00 |\n+------------------+\nSELECT 1 row in set (... sec)\n\nTIME\n\nA TIME expresses a specific time as the number of milliseconds since midnight along with a time zone offset.\n\nLimited to 12 bytes, with a time range from 00:00:00.000000 to 23:59:59.999999 and a time zone range from -18:00 to 18:00.\n\nCaution\n\nCrateDB does not support TIME by itself or TIME WITHOUT TIME ZONE. You must always specify TIME WITH TIME ZONE or its alias TIMETZ.\n\nThis behaviour does not comply with standard SQL and is incompatible with PostgreSQL. This behavior may change in a future version of CrateDB (see tracking issue #11491).\n\nNote\n\nThe TIME type is only supported as a type literal (i.e., for use in SQL expressions, like a type cast, as below).\n\nYou cannot create table columns of type TIME.\n\nTimes can be expressed as string literals (e.g., '13:00:00') with the following syntax:\n\ntime-element [offset]\n\ntime-element: time-only [fraction]\ntime-only:    HH[[:][mm[:]ss]]\nfraction:     '.' digit+\noffset:       {+ | -} time-only | geo-region\ngeo-region:   As defined by ISO 8601.\n\n\nAbove, fraction accepts up to six digits, with a precision in microseconds.\n\nSee Also\n\nFor more information about time formatting, see Java 15: Patterns for Formatting and Parsing.\n\nTime zone syntax as defined by ISO 8601 time zone designators.\n\nFor example:\n\ncr> SELECT '13:00:00'::TIMETZ AS t_tz;\n+------------------+\n| t_tz             |\n+------------------+\n| [46800000000, 0] |\n+------------------+\nSELECT 1 row in set (... sec)\n\n\nThe value of first element is the number of milliseconds since midnight. The value of the second element is the number of seconds corresponding to the time zone offset (zero in this instance, as no time zone was specified).\n\nFor example, with a +01:00 time zone:\n\ncr> SELECT '13:00:00+01:00'::TIMETZ AS t_tz;\n+---------------------+\n| t_tz                |\n+---------------------+\n| [46800000000, 3600] |\n+---------------------+\nSELECT 1 row in set (... sec)\n\n\nThe time zone offset is calculated as 3600 seconds, which is equivalent to an hour.\n\nNegative time zone offsets will return negative seconds:\n\ncr> SELECT '13:00:00-01:00'::TIMETZ AS t_tz;\n+----------------------+\n| t_tz                 |\n+----------------------+\n| [46800000000, -3600] |\n+----------------------+\nSELECT 1 row in set (... sec)\n\n\nHere’s an example that uses fractional seconds:\n\ncr> SELECT '13:59:59.999999'::TIMETZ as t_tz;\n+------------------+\n| t_tz             |\n+------------------+\n| [50399999999, 0] |\n+------------------+\nSELECT 1 row in set (... sec)\n\n\nCaution\n\nThe current implementation of the TIME type has the following limitations:\n\nTIME types cannot be cast to any other types (including TEXT)\n\nTIME types cannot be used in arithmetic expressions (e.g., with TIME, DATE, and INTERVAL types)\n\nTIME types cannot be used with time and date scalar functions (e.g., date_format() and extract())\n\nThis behaviour does not comply with standard SQL and is incompatible with PostgreSQL. This behavior may change in a future version of CrateDB (see tracking issue #11528).\n\nDATE\n\nA DATE expresses a specific year, month and a day in UTC.\n\nInternally, CrateDB stores dates as BIGINT values, which are limited to eight bytes.\n\nIf you cast a BIGINT to a DATE, the integer value will be interpreted as the number of milliseconds since the Unix epoch. If you cast a REAL or a DOUBLE PRECISION to a DATE, the numeric value will be interpreted as the number of seconds since the Unix epoch.\n\nIf you cast a literal to a DATE, years outside the range 0000 to 9999 must be prefixed by the plus or minus symbol. See also Year.parse Javadoc:\n\ncr> SELECT '+10000-03-09'::DATE as date;\n+-----------------+\n|            date |\n+-----------------+\n| 253408176000000 |\n+-----------------+\nSELECT 1 row in set (... sec)\n\n\nCaution\n\nDue to internal date parsing, the full BIGINT range is not supported for timestamp values. The valid range of dates is from 292275054BC to 292278993AD.\n\nWhen inserting dates smaller than -999999999999999 (equal to -29719-04-05) or bigger than 999999999999999 (equal to 33658-09-27) rounding issues may occur.\n\nWarning\n\nThe DATE type was not designed to allow time-of-day information (i.e., it is supposed to have a resolution of one day).\n\nHowever, CrateDB allows you violate that constraint by casting any number of milliseconds within limits to a DATE type. The result is then returned as a TIMESTAMP. When used in conjunction with arithmetic expressions, these TIMESTAMP values may produce unexpected results.\n\nThis behaviour does not comply with standard SQL and is incompatible with PostgreSQL. This behavior may change in a future version of CrateDB (see tracking issue #11528).\n\nCaution\n\nThe current implementation of the DATE type has the following limitations:\n\nDATE types cannot be added or subtracted to or from other DATE types as expected (i.e., to calculate the difference between the two in a number of days).\n\nDoing so will convert both DATE values into TIMESTAMP values before performing the operation, resulting in a TIMESTAMP value corresponding to a full date and time (see WARNING above).\n\nNumeric data types cannot be added to or subtracted from DATE types as expected (e.g., to increase the date by n days).\n\nDoing so will, for example, convert the DATE into a TIMESTAMP and increase the value by n milliseconds (see WARNING above).\n\nTIME types cannot be added to or subtracted from DATE types.\n\nINTERVAL types cannot be added to or subtracted from DATE types.\n\nThis behaviour does not comply with standard SQL and is incompatible with PostgreSQL. This behavior may change in a future version of CrateDB (see tracking issue #11528).\n\nNote\n\nThe DATE type is only supported as a type literal (i.e., for use in SQL expressions, like a type cast, as below).\n\nYou cannot create table columns of type DATE.\n\nDates can be expressed as string literals (e.g., '2021-03-09') with the following syntax:\n\nyyyy-MM-dd\n\n\nSee Also\n\nFor more information about date and time formatting, see Java 15: Patterns for Formatting and Parsing.\n\nFor example, using the date_format() function, for readability:\n\ncr> SELECT\n...    date_format(\n...        '%Y-%m-%d',\n...        '2021-03-09'::DATE\n...    ) AS date;\n+------------+\n| date       |\n+------------+\n| 2021-03-09 |\n+------------+\nSELECT 1 row in set (... sec)\n\nINTERVAL\n\nAn INTERVAL represents a span of time.\n\nNote\n\nThe INTERVAL type is only supported as a type literal (i.e., for use in SQL expressions, like a type cast, as above).\n\nYou cannot create table columns of type INTERVAL.\n\nThe basic syntax is:\n\nINTERVAL <quantity> <unit>\n\n\nWhere unit can be any of the following:\n\nYEAR\n\nMONTH\n\nDAY\n\nHOUR\n\nMINUTE\n\nSECOND\n\nFor example:\n\ncr> SELECT INTERVAL '1' DAY AS result;\n+----------------+\n| result         |\n+----------------+\n| 1 day 00:00:00 |\n+----------------+\nSELECT 1 row in set (... sec)\n\n\nIntervals can be positive or negative:\n\ncr> SELECT INTERVAL -'1' DAY AS result;\n+------------------+\n| result           |\n+------------------+\n| -1 days 00:00:00 |\n+------------------+\nSELECT 1 row in set (... sec)\n\n\nWhen using SECOND, you can define fractions of a seconds (with a precision of zero to six digits):\n\ncr> SELECT INTERVAL '1.5' SECOND AS result;\n+--------------+\n| result       |\n+--------------+\n| 00:00:01.500 |\n+--------------+\nSELECT 1 row in set (... sec)\n\n\nCaution\n\nThe INTERVAL data type does not currently support the input units MILLENNIUM, CENTURY, DECADE, MILLISECOND, or MICROSECOND.\n\nThis behaviour does not comply with standard SQL and is incompatible with PostgreSQL. This behavior may change in a future version of CrateDB (see tracking issue #11490).\n\nYou can also use the following syntax to express an interval:\n\nINTERVAL <string>\n\n\nWhere string describes the interval using one of the recognized formats:\n\nDescription\n\n\t\n\nExample\n\n\t\n\nEquivalent\n\n\n\n\nStandard SQL format (year-month)\n\n\t\n\n1-2\n\n\t\n\n1 year 2 months\n\n\n\n\nStandard SQL format\n\n\t\n\n1-2 3 4:05:06\n\n\t\n\n1 year 2 months 3 days 4 hours 5 minutes 6 seconds\n\n\n\n\nStandard SQL format (day-time)\n\n\t\n\n3 4:05:06\n\n\t\n\n3 days 4 hours 5 minutes 6 seconds\n\n\n\n\nPostgreSQL interval format\n\n\t\n\n1 year 2 months 3 days 4 hours 5 minutes 6 seconds\n\n\t\n\n1 year 2 months 3 days 4 hours 5 minutes 6 seconds\n\n\n\n\nISO 8601 duration format\n\n\t\n\nP1Y2M3DT4H5M6S\n\n\t\n\n1 year 2 months 3 days 4 hours 5 minutes 6 seconds\n\nFor example:\n\ncr> SELECT INTERVAL '1-2 3 4:05:06' AS result;\n+-------------------------------+\n| result                        |\n+-------------------------------+\n| 1 year 2 mons 3 days 04:05:06 |\n+-------------------------------+\nSELECT 1 row in set (... sec)\n\n\nYou can limit the precision of an interval by specifying <unit> TO <unit> after the interval string.\n\nFor example, you can use YEAR TO MONTH to limit an interval to a day-month value:\n\ncr> SELECT INTERVAL '1-2 3 4:05:06' YEAR TO MONTH AS result;\n+------------------------+\n| result                 |\n+------------------------+\n| 1 year 2 mons 00:00:00 |\n+------------------------+\nSELECT 1 row in set (... sec)\n\n\nYou can use DAY TO HOUR, as another example, to limit a day-time interval to days and hours:\n\ncr> SELECT INTERVAL '3 4:05:06' DAY TO HOUR AS result;\n+-----------------+\n| result          |\n+-----------------+\n| 3 days 04:00:00 |\n+-----------------+\nSELECT 1 row in set (... sec)\n\n\nYou can multiply an interval by an integer:\n\ncr> SELECT 2 * INTERVAL '2 years 1 month 10 days' AS result;\n+---------------------------------+\n| result                          |\n+---------------------------------+\n| 4 years 2 mons 20 days 00:00:00 |\n+---------------------------------+\nSELECT 1 row in set (... sec)\n\n\nTip\n\nYou can use intervals in combination with CURRENT_TIMESTAMP to calculate values that are offset relative to the current date and time.\n\nFor example, to calculate a timestamp corresponding to exactly one day ago, use:\n\ncr> SELECT CURRENT_TIMESTAMP - INTERVAL '1' DAY AS result;\n+---------------+\n| result        |\n+---------------+\n| ...           |\n+---------------+\nSELECT 1 row in set (... sec)\n\nBit strings\nBIT(n)\n\nA string representation of a bit sequence, useful for visualizing a bit mask.\n\nValues of this type can be created using the bit string literal syntax. A bit string starts with the B prefix, followed by a sequence of 0 or 1 digits quoted within single quotes '.\n\nAn example:\n\nB'00010010'\n\n\nThe optional length specification n is a positive integer that defines the maximum length, in characters, of the values that have to be stored or cast. The minimum length is 1. The maximum length is defined by the upper integer range.\n\nFor example:\n\ncr> CREATE TABLE my_table (\n...     bit_mask BIT(4)\n... );\nCREATE OK, 1 row affected (... sec)\n\ncr> INSERT INTO my_table (\n...     bit_mask\n... ) VALUES (\n...     B'0110'\n... );\nINSERT OK, 1 row affected  (... sec)\n\ncr> SELECT bit_mask FROM my_table;\n+----------+\n| bit_mask |\n+----------+\n| B'0110'  |\n+----------+\nSELECT 1 row in set (... sec)\n\n\nInserting values that are either too short or too long results in an error:\n\ncr> INSERT INTO my_table (\n...     bit_mask\n... ) VALUES (\n...    B'00101'\n... );\nSQLParseException[bit string length 5 does not match type bit(4)]\n\nIP addresses\nIP\n\nAn IP is a string representation of an IP address (IPv4 or IPv6).\n\nInternally IP addresses are stored as BIGINT values, allowing expected sorting, filtering, and aggregation.\n\nFor example:\n\ncr> CREATE TABLE my_table (\n...     fqdn TEXT,\n...     ip_addr IP\n... );\nCREATE OK, 1 row affected (... sec)\n\ncr> INSERT INTO my_table (\n...     fqdn,\n...     ip_addr\n... ) VALUES (\n...     'localhost',\n...     '127.0.0.1'\n... ), (\n...     'router.local',\n...     '0:0:0:0:0:ffff:c0a8:64'\n... );\nINSERT OK, 2 rows affected (... sec)\n\ncr> SELECT fqdn, ip_addr FROM my_table ORDER BY fqdn;\n+--------------+------------------------+\n| fqdn         | ip_addr                |\n+--------------+------------------------+\n| localhost    | 127.0.0.1              |\n| router.local | 0:0:0:0:0:ffff:c0a8:64 |\n+--------------+------------------------+\nSELECT 2 rows in set (... sec)\n\n\nThe fqdn column (see Fully Qualified Domain Name) will accept any value because it was specified as TEXT. However, trying to insert fake.ip won’t work, because it is not a correctly formatted IP address:\n\ncr> INSERT INTO my_table (\n...     fqdn,\n...     ip_addr\n... ) VALUES (\n...     'localhost',\n...     'fake.ip'\n... );\nSQLParseException[Cannot cast `'fake.ip'` of type `text` to type `ip`]\n\n\nIP addresses support the << operator, which checks for subnet inclusion using CIDR notation. The left-hand operand must an IP type and the right-hand must be TEXT type (e.g., '192.168.1.5' << '192.168.1/24').\n\nContainer types\n\nContainer types are types with nonscalar values that may contain other values:\n\nObjects\n\nOBJECT\n\nObject column policy\n\nObject literals\n\nInserting objects as JSON\n\nArrays\n\nARRAY\n\nArray literals\n\nNested arrays\n\nObjects\nOBJECT\n\nAn object is structured as a collection of key-values.\n\nAn object can contain any other type, including further child objects. An OBJECT column can be schemaless or can have a defined (i.e., enforced) schema.\n\nObjects are not the same as JSON objects, although they share a lot of similarities. However, objects can be inserted as JSON strings.\n\nSyntax:\n\n<columnName> OBJECT\n    [ ({DYNAMIC|STRICT|IGNORED}) ]\n    [ AS ( <columnDefinition>* ) ]\n\n\nThe only required syntax is OBJECT.\n\nThe column policy (DYNAMIC, STRICT, or IGNORED) is optional and defaults to DYNAMIC.\n\nIf the optional list of subcolumns (columnDefinition) is omitted, the object will have no schema. CrateDB will create a schema for DYNAMIC objects upon first insert.\n\nExample:\n\ncr> CREATE TABLE my_table (\n...     title TEXT,\n...     quotation OBJECT,\n...     protagonist OBJECT(STRICT) AS (\n...         age INTEGER,\n...         first_name TEXT,\n...         details OBJECT AS (\n...             birthday TIMESTAMP WITH TIME ZONE\n...         )\n...     )\n... );\nCREATE OK, 1 row affected (... sec)\n\ncr> INSERT INTO my_table (\n...     title,\n...     quotation,\n...     protagonist\n... ) VALUES (\n...     'Alice in Wonderland',\n...     {\n...         \"words\" = 'Curiouser and curiouser!',\n...         \"length\" = 3\n...     },\n...     {\n...         \"age\" = '10',\n...         \"first_name\" = 'Alice',\n...         \"details\" = {\n...             \"birthday\" = '1852-05-04T00:00Z'::TIMESTAMPTZ\n...         }\n...     }\n... );\nINSERT OK, 1 row affected (... sec)\n\ncr> SELECT\n...     protagonist['first_name'] AS name,\n...     date_format(\n...         '%D %b %Y',\n...         'GMT',\n...         protagonist['details']['birthday']\n...      ) AS born,\n...     protagonist['age'] AS age\n... FROM my_table;\n+-------+--------------+-----+\n| name  | born         | age |\n+-------+--------------+-----+\n| Alice | 4th May 1852 |  10 |\n+-------+--------------+-----+\nSELECT 1 row in set (... sec)\n\n\nNew sub-columns can be added to the columnDefinition at any time. See Adding columns for details.\n\nObject column policy\nSTRICT\n\nIf the column policy is configured as STRICT, CrateDB will reject any subcolumn that is not defined upfront by columnDefinition.\n\nExample:\n\ncr> CREATE TABLE my_table (\n...     title TEXT,\n...     protagonist OBJECT(STRICT) AS (\n...         name TEXT\n...     )\n... );\nCREATE OK, 1 row affected (... sec)\n\ncr> INSERT INTO my_table (\n...     title,\n...     protagonist\n... ) VALUES (\n...     'Alice in Wonderland',\n...     {\n...         \"age\" = '10'\n...     }\n... );\nSQLParseException[Cannot add column `age` to strict object `protagonist`]\n\n\nThe insert above failed because the age sub-column is not defined.\n\nNote\n\nObjects with a STRICT column policy and no columnDefinition will have one unusable column that will always be NULL.\n\nDYNAMIC\n\nIf the column policy is configured as DYNAMIC (the default), inserts may dynamically add new subcolumns to the object definition.\n\nExample:\n\ncr> CREATE TABLE my_table (\n...     title TEXT,\n...     quotation OBJECT\n... );\nCREATE OK, 1 row affected (... sec)\n\n\nThe following statement is equivalent to the above:\n\ncr> CREATE TABLE my_table (\n...     title TEXT,\n...     quotation OBJECT(DYNAMIC)\n... );\nCREATE OK, 1 row affected (... sec)\n\n\nThe following statement is also equivalent to the above:\n\ncr> CREATE TABLE my_table (\n...     title TEXT,\n...     quotation OBJECT(DYNAMIC) AS (\n...         words TEXT,\n...         length SMALLINT\n...     )\n... );\nCREATE OK, 1 row affected (... sec)\n\n\nYou can insert using the existing columns:\n\ncr> INSERT INTO my_table (\n...     title,\n...     quotation\n... ) VALUES (\n...     'Alice in Wonderland',\n...     {\n...         \"words\" = 'Curiouser and curiouser!',\n...         \"length\" = 3\n...     }\n... );\nINSERT OK, 1 row affected (... sec)\n\n\nOr you can add new columns:\n\ncr> INSERT INTO my_table (\n...     title,\n...     quotation\n... ) VALUES (\n...     'Alice in Wonderland',\n...     {\n...         \"words\" = 'DRINK ME',\n...         \"length\" = 2,\n...         \"chapter\" = 1\n...     }\n... );\nINSERT OK, 1 row affected (... sec)\n\n\nAll rows have the same columns (including newly added columns), but missing records will be returned as NULL values:\n\ncr> SELECT\n...     quotation['chapter'] as chapter,\n...     quotation['words'] as quote\n... FROM my_table\n... ORDER BY chapter ASC;\n+---------+--------------------------+\n| chapter | quote                    |\n+---------+--------------------------+\n|       1 | DRINK ME                 |\n|    NULL | Curiouser and curiouser! |\n+---------+--------------------------+\nSELECT 2 rows in set (... sec)\n\n\nNew columns are usable like any other subcolumn. You can retrieve them, sort by them, and use them in where clauses.\n\nNote\n\nAdding new columns to an object with a DYNAMIC policy will affect the schema of the table.\n\nOnce a column is added, it shows up in the information_schema.columns table and its type and attributes are fixed. If a new column a was added with type INTEGER, adding strings to the column will result in an error.\n\nDynamically added columns will always be analyzed as-is with the plain analyzer, which means the column will be indexed but not tokenized in the case of TEXT columns.\n\nIGNORED\n\nIf the column policy is configured as IGNORED, inserts may dynamically add new subcolumns to the object definition. However, dynamically added subcolumns do not cause a schema update and the values contained will not be indexed.\n\nBecause dynamically created columns are not recorded in the schema, you can insert mixed types into them. For example, one row may insert an integer and the next row may insert an object. Objects with a STRICT or DYNAMIC column policy do not allow this.\n\nExample:\n\ncr> CREATE TABLE my_table (\n...     title TEXT,\n...     protagonist OBJECT(IGNORED) AS (\n...         name TEXT,\n...         chapter SMALLINT\n...     )\n... );\nCREATE OK, 1 row affected (... sec)\n\ncr> INSERT INTO my_table (\n...     title,\n...     protagonist\n... ) VALUES (\n...     'Alice in Wonderland',\n...     {\n...         \"name\" = 'Alice',\n...         \"chapter\" = 1,\n...         \"size\" = {\n...             \"value\" = 10,\n...             \"units\" = 'inches'\n...         }\n...     }\n... );\nINSERT OK, 1 row affected  (... sec)\n\ncr> INSERT INTO my_table (\n...     title,\n...     protagonist\n... ) VALUES (\n...     'Alice in Wonderland',\n...     {\n...         \"name\" = 'Alice',\n...         \"chapter\" = 2,\n...         \"size\" = 'As big as a room'\n...     }\n... );\nINSERT OK, 1 row affected  (... sec)\n\ncr> SELECT\n...     protagonist['name'] as name,\n...     protagonist['chapter'] as chapter,\n...     protagonist['size'] as size\n... FROM my_table\n... ORDER BY protagonist['chapter'] ASC;\n+-------+---------+----------------------------------+\n| name  | chapter | size                             |\n+-------+---------+----------------------------------+\n| Alice |       1 | {\"units\": \"inches\", \"value\": 10} |\n| Alice |       2 | As big as a room                 |\n+-------+---------+----------------------------------+\nSELECT 2 rows in set (... sec)\n\n\nReflecting the types of the columns:\n\ncr> SELECT\n...     pg_typeof(protagonist['name']) as name_type,\n...     pg_typeof(protagonist['chapter']) as chapter_type,\n...     pg_typeof(protagonist['size']) as size_type\n... FROM my_table\n... ORDER BY protagonist['chapter'] ASC;\n+-----------+--------------+-----------+\n| name_type | chapter_type | size_type |\n+-----------+--------------+-----------+\n| text      | smallint     | undefined |\n| text      | smallint     | undefined |\n+-----------+--------------+-----------+\nSELECT 2 rows in set (... sec)\n\n\nNote\n\nGiven that dynamically added sub-columns of an IGNORED object are not indexed, filter operations on these columns cannot utilize the index and instead a value lookup is performed for each matching row. This can be mitigated by combining a filter using the AND clause with other predicates on indexed columns.\n\nFurthermore, values for dynamically added sub-columns of an IGNORED objects aren’t stored in a column store, which means that ordering on these columns or using them with aggregates is also slower than using the same operations on regular columns. For some operations it may also be necessary to add an explicit type cast because there is no type information available in the schema.\n\nAn example:\n\ncr> SELECT\n...     protagonist['name'] as name,\n...     protagonist['chapter'] as chapter,\n...     protagonist['size'] as size\n... FROM my_table\n... ORDER BY protagonist['size']::TEXT ASC;\n+-------+---------+----------------------------------+\n| name  | chapter | size                             |\n+-------+---------+----------------------------------+\n| Alice |       2 | As big as a room                 |\n| Alice |       1 | {\"units\": \"inches\", \"value\": 10} |\n+-------+---------+----------------------------------+\nSELECT 2 rows in set (... sec)\n\n\nGiven that it is possible have values of different types within the same sub-column of an ignored objects, aggregations may fail at runtime:\n\ncr> SELECT protagonist['size']::BIGINT FROM my_table ORDER BY protagonist['chapter'] LIMIT 1;\nSQLParseException[Cannot cast value `{value=10, units=inches}` to type `bigint`]\n\nObject literals\n\nYou can insert objects using object literals. Object literals are delimited using curly brackets and key-value pairs are connected via =.\n\nSynopsis:\n\n{ [ ident = expr [ , ... ] ] }\n\n\nHere, ident is the key and expr is the value. The key must be a lowercase column identifier or a quoted mixed-case column identifier. The value must be a value literal (object literals are permitted and can be nested in this way).\n\nEmpty object literal:\n\n{}\n\n\nBoolean type:\n\n{ my_bool_column = true }\n\n\nText type:\n\n{ my_str_col = 'this is a text value' }\n\n\nNumber types:\n\n{ my_int_col = 1234, my_float_col = 5.6 }\n\n\nArray type:\n\n{ my_array_column = ['v', 'a', 'l', 'u', 'e'] }\n\n\nCamel case keys must be quoted:\n\n{ \"CamelCaseColumn\" = 'this is a text value' }\n\n\nNested object:\n\n{ nested_obj_colmn = { int_col = 1234, str_col = 'text value' } }\n\n\nYou can even specify a placeholder parameter for a value:\n\n{ my_other_column = ? }\n\n\nCombined:\n\n{ id = 1, name = 'foo', tags = ['apple'], size = 3.1415, valid = ? }\n\n\nNote\n\nEven though they look like JSON, object literals are not JSON. If you want to use JSON, skip to the next subsection.\n\nSee Also\n\nSelecting values from inner objects and nested objects\n\nInserting objects as JSON\n\nYou can insert objects using JSON strings. To do this, you must type cast the string to an object with an implicit cast (i.e., passing a string into an object column) or an explicit cast (i.e., using the ::OBJECT syntax).\n\nTip\n\nExplicit casts can improve query readability.\n\nBelow you will find examples from the previous subsection rewritten to use JSON strings with explicit casts.\n\nEmpty object literal:\n\n'{}'::object\n\n\nBoolean type:\n\n'{ \"my_bool_column\": true }'::object\n\n\nText type:\n\n'{ \"my_str_col\": \"this is a text value\" }'::object\n\n\nNumber types:\n\n'{ \"my_int_col\": 1234, \"my_float_col\": 5.6 }'::object\n\n\nArray type:\n\n'{ \"my_array_column\": [\"v\", \"a\", \"l\", \"u\", \"e\"] }'::object\n\n\nCamel case keys:\n\n'{ \"CamelCaseColumn\": \"this is a text value\" }'::object\n\n\nNested object:\n\n'{ \"nested_obj_col\": { \"int_col\": 1234, \"str_col\": \"foo\" } }'::object\n\n\nNote\n\nYou cannot use placeholder parameters inside a JSON string.\n\nArrays\nARRAY\n\nAn array is structured as a collection of other data types.\n\nArrays can contain the following:\n\nPrimitive types\n\nObjects\n\nGeographic types\n\nArray types are defined as follows:\n\ncr> CREATE TABLE my_table_arrays (\n...     tags ARRAY(TEXT),\n...     objects ARRAY(OBJECT AS (age INTEGER, name TEXT))\n... );\nCREATE OK, 1 row affected (... sec)\n\ncr> INSERT INTO my_table_arrays (\n...     tags,\n...     objects\n... ) VALUES (\n...     ['foo', 'bar'],\n...     [{\"name\" = 'Alice', \"age\" = 33}, {\"name\" = 'Bob', \"age\" = 45}]\n... );\nINSERT OK, 1 row affected (... sec)\n\ncr> SELECT * FROM my_table_arrays;\n+----------------+------------------------------------------------------------+\n| tags           | objects                                                    |\n+----------------+------------------------------------------------------------+\n| [\"foo\", \"bar\"] | [{\"age\": 33, \"name\": \"Alice\"}, {\"age\": 45, \"name\": \"Bob\"}] |\n+----------------+------------------------------------------------------------+\nSELECT 1 row in set (... sec)\n\n\nAn alternative is the following syntax to refer to arrays:\n\n<typeName>[]\n\n\nThis means TEXT[] is equivalent to ARRAY(text).\n\nArrays are always represented as zero or more literal elements inside square brackets ([]), for example:\n\n[1, 2, 3]\n['Zaphod', 'Ford', 'Arthur']\n\nArray literals\n\nArrays can be written using the array constructor ARRAY[] or short []. The array constructor is an expression that accepts both literals and expressions as its parameters. Parameters may contain zero or more elements.\n\nSynopsis:\n\n[ ARRAY ] '[' element [ , ... ] ']'\n\n\nAll array elements must have the same data type, which determines the inner type of the array. If an array contains no elements, its element type will be inferred by the context in which it occurs, if possible.\n\nSome valid arrays are:\n\n[]\n[null]\n[1, 2, 3, 4, 5, 6, 7, 8]\n['Zaphod', 'Ford', 'Arthur']\n[?]\nARRAY[true, false]\nARRAY[column_a, column_b]\nARRAY[ARRAY[1, 2, 1 + 2], ARRAY[3, 4, 3 + 4]]\n\n\nAn alternative way to define arrays is to use string literals and casts to arrays. This requires a string literal that contains the elements separated by comma and enclosed with curly braces:\n\n'{ val1, val2, val3 }'\n\ncr> SELECT '{ab, CD, \"CD\", null, \"null\"}'::ARRAY(TEXT) AS arr;\n+----------------------------------+\n| arr                              |\n+----------------------------------+\n| [\"ab\", \"CD\", \"CD\", null, \"null\"] |\n+----------------------------------+\nSELECT 1 row in set (... sec)\n\n\nnull elements are interpreted as null (none, absent), if you want the literal null string, it has to be enclosed in double quotes.\n\nThis variant primarily exists for compatibility with PostgreSQL. The array constructor syntax explained further above is the preferred way to define constant array values.\n\nNested arrays\n\nNested arrays cannot be used directly in column definitions (i.e. ARRAY(ARRAY(DOUBLE)) is not accepted), but multiple arrays can be nested as long as there are objects in-between:\n\nCREATE TABLE SensorData (sensorID char(10), readings ARRAY(OBJECT AS (innerarray ARRAY(DOUBLE))));\n\n\nNested arrays can still be used directly in input and output to UDFs:\n\nCREATE FUNCTION sort_nested_array(\"data\" ARRAY(ARRAY(DOUBLE)), sort_dimension SMALLINT)\nRETURNS ARRAY(ARRAY(DOUBLE))\nLANGUAGE JAVASCRIPT\nAS 'function sort_nested_array(data, sort_dimension) {\n    data = data.sort(function compareFn(a, b) {\n        if (a[sort_dimension] < b[sort_dimension]){return -1;}\n        if (a[sort_dimension] > b[sort_dimension]){return 1;}\n        return 0;\n    });\n    return data;\n}';\n\n\nNested arrays can be constructed using ARRAY_AGG and accessing them requires an intermediate cast:\n\nCREATE TABLE metrics (ts TIMESTAMP, reading DOUBLE);\nINSERT INTO metrics SELECT '2022-11-01',2;\nINSERT INTO metrics SELECT '2022-10-01',1;\n\nWITH sorteddata AS (\n    SELECT sort_nested_array(ARRAY_AGG([ts,reading]),0) AS nestedarray\n    FROM metrics\n)\nSELECT (nestedarray[generate_series]::ARRAY(DOUBLE))[2] AS \"ReadingsSortedByTimestamp\"\nFROM generate_series(1, 2), sorteddata;\n\n+---------------------------+\n| ReadingsSortedByTimestamp |\n+---------------------------+\n|                       1.0 |\n|                       2.0 |\n+---------------------------+\n\nFLOAT_VECTOR\n\nA float_vector type allows to store dense vectors of float values of fixed length.\n\nIt support KNN_MATCH for k-nearest neighbour search. This allows you to find vectors in a dataset which are similar to a query vector.\n\nThe type can’t be used as an element type of a regular array. float_vector values are defined like float arrays.\n\nAn example:\n\ncr> CREATE TABLE my_vectors (\n...     xs FLOAT_VECTOR(2)\n... );\nCREATE OK, 1 row affected (... sec)\n\ncr> INSERT INTO my_vectors (xs) VALUES ([3.14, 27.34]);\nINSERT OK, 1 row affected (... sec)\n\n\nInserting a value with a different dimension than declared in CREATE TABLE results in an error.\n\ncr> INSERT INTO my_vectors (xs) VALUES ([3.14, 27.34, 38.4]);\nSQLParseException[The number of vector dimensions does not match the field type]\n\ncr> SELECT * FROM my_vectors;\n+---------------+\n| xs            |\n+---------------+\n| [3.14, 27.34] |\n+---------------+\nSELECT 1 row in set (... sec)\n\nGeographic types\n\nGeographic types are types with nonscalar values representing points or shapes in a 2D world:\n\nGeometric points\n\nGEO_POINT\n\nGeometric shapes\n\nGEO_SHAPE\n\nGeo shape column definition\n\nGeo shape index structure\n\nGeo shape literals\n\nGeo shape GeoJSON examples\n\nGeometric points\nGEO_POINT\n\nA GEO_POINT is a geographic data type used to store latitude and longitude coordinates.\n\nTo define a GEO_POINT column, use:\n\n<columnName> GEO_POINT\n\n\nValues for columns with the GEO_POINT type are represented and inserted using an array of doubles in the following format:\n\n[<lon_value>, <lat_value>]\n\n\nAlternatively, a WKT string can also be used to declare geo points:\n\n'POINT ( <lon_value> <lat_value> )'\n\n\nNote\n\nEmpty geo points are not supported.\n\nAdditionally, if a column is dynamically created, the type detection won’t recognize neither WKT strings nor double arrays. That means columns of type GEO_POINT must always be declared beforehand.\n\nAn example:\n\ncr> CREATE TABLE my_table_geo (\n...   id INTEGER PRIMARY KEY,\n...   pin GEO_POINT\n... ) WITH (number_of_replicas = 0)\nCREATE OK, 1 row affected (... sec)\n\n\nInsert using ARRAY syntax:\n\ncr> INSERT INTO my_table_geo (\n...     id, pin\n... ) VALUES (\n...     1, [13.46738, 52.50463]\n... );\nINSERT OK, 1 row affected (... sec)\n\n\nInsert using WKT syntax:\n\ncr> INSERT INTO my_table_geo (\n...     id, pin\n... ) VALUES (\n...     2, 'POINT (9.7417 47.4108)'\n... );\nINSERT OK, 1 row affected (... sec)\n\n\nQuery data:\n\ncr> SELECT * FROM my_table_geo;\n+----+-----------------------------------------+\n| id | pin                                     |\n+----+-----------------------------------------+\n|  1 | [13.467379929497838, 52.50462996773422] |\n|  2 | [9.741699993610382, 47.410799972712994] |\n+----+-----------------------------------------+\nSELECT 2 rows in set (... sec)\n\nGeometric shapes\nGEO_SHAPE\n\nA geo_shape is a geographic data type used to store 2D shapes defined as GeoJSON geometry objects.\n\nA GEO_SHAPE column can store different kinds of GeoJSON geometry objects:\n\n“Point”\n\n“MultiPoint”\n\n“LineString”\n\n“MultiLineString”,\n\n“Polygon”\n\n“MultiPolygon”\n\n“GeometryCollection”\n\nCaution\n\n3D coordinates are not supported.\n\nEmpty Polygon and LineString geo shapes are not supported.\n\nAn example:\n\ncr> CREATE TABLE my_table_geo (\n...   id INTEGER PRIMARY KEY,\n...   area GEO_SHAPE\n... ) WITH (number_of_replicas = 0)\nCREATE OK, 1 row affected (... sec)\n\ncr> INSERT INTO my_table_geo (\n...     id, area\n... ) VALUES (\n...     1, 'POLYGON ((5 5, 10 5, 10 10, 5 10, 5 5))'\n... );\nINSERT OK, 1 row affected (... sec)\n\ncr> SELECT * FROM my_table_geo;\n+----+--------------------------------------------------------------------------------------------------------+\n| id | area                                                                                                   |\n+----+--------------------------------------------------------------------------------------------------------+\n|  1 | {\"coordinates\": [[[5.0, 5.0], [5.0, 10.0], [10.0, 10.0], [10.0, 5.0], [5.0, 5.0]]], \"type\": \"Polygon\"} |\n+----+--------------------------------------------------------------------------------------------------------+\nSELECT 1 row in set (... sec)\n\nGeo shape column definition\n\nTo define a GEO_SHAPE column, use:\n\n<columnName> GEO_SHAPE\n\n\nA geographical index with default parameters is created implicitly to allow for geographical queries. Its default parameters are:\n\n<columnName> GEO_SHAPE INDEX USING geohash\n    WITH (precision='50m', distance_error_pct=0.025)\n\n\nThere are three geographic index types: geohash (default), quadtree and bkdtree. These indices are only allowed on geo_shape columns. For more information, see Geo shape index structure.\n\nBoth geohash and quadtree index types accept the following parameters:\n\nprecision\n\n(Default: 50m) Define the maximum precision of the used index and thus for all indexed shapes. Given as string containing a number and an optional distance unit (defaults to m).\n\nSupported units are inch (in), yard (yd), miles (mi), kilometers (km), meters (m), centimeters (cm), millimeters (mm).\n\ndistance_error_pct\n\n(Default: 0.025 (2,5%)) The measure of acceptable error for shapes stored in this column expressed as a percentage value of the shape size The allowed maximum is 0.5 (50%).\n\nThe percentage will be taken from the diagonal distance from the center of the bounding box enclosing the shape to the closest corner of the enclosing box. In effect bigger shapes will be indexed with lower precision than smaller shapes. The ratio of precision loss is determined by this setting, that means the higher the distance_error_pct the smaller the indexing precision.\n\nThis will have the effect of increasing the indexed shape internally, so e.g. points that are not exactly inside this shape will end up inside it when it comes to querying as the shape has grown when indexed.\n\ntree_levels\n\nMaximum number of layers to be used by the PrefixTree defined by the index type (either geohash or quadtree. See Geo shape index structure).\n\nThis can be used to control the precision of the used index. Since this parameter requires a certain level of understanding of the underlying implementation, users may use the precision parameter instead. CrateDB uses the tree_levels parameter internally and this is what is returned via the SHOW CREATE TABLE statement even if you use the precision parameter. Defaults to the value which is 50m converted to precision depending on the index type.\n\nGeo shape index structure\n\nComputations on very complex polygons and geometry collections are exact but very expensive. To provide fast queries even on complex shapes, CrateDB uses a different approach to store, analyze and query geo shapes. The available geo shape indexing strategies are based on two primary data structures: Prefix and BKD trees, which are described below.\n\nPrefix Tree\n\nThe surface of the earth is represented as a number of grid layers each with higher precision. While the upper layer has one grid cell, the layer below contains many cells for the equivalent space.\n\nEach grid cell on each layer is addressed in 2d space either by a Geohash for geohash trees or by tightly packed coordinates in a Quadtree. Those addresses conveniently share the same address-prefix between lower layers and upper layers. So we are able to use a Trie to represent the grids, and Tries can be queried efficiently as their complexity is determined by the tree depth only.\n\nA geo shape is transformed into these grid cells. Think of this transformation process as dissecting a vector image into its pixelated counterpart, reasonably accurately. We end up with multiple images each with a better resolution, up to the configured precision.\n\nEvery grid cell that processed up to the configured precision is stored in an inverted index, creating a mapping from a grid cell to all shapes that touch it. This mapping is our geographic index.\n\nThe main difference is that the geohash supports higher precision than the quadtree tree. Both tree implementations support precision in order of fractions of millimeters.\n\nBKD-tree\n\nIn the BKD-tree-based (bkdtree) approach, a geo shape is decomposed into a collection of triangles. Each triangle is represented as a 7-dimensional point and stored in this format within a BKD-tree.\n\nTo improve the storage efficiency of triangles within an index, the initial four dimensions are used to represent the bounding box of each triangle. These bounding boxes are stored in the internal nodes of the BKD-tree, while the remaining three dimensions are stored in the leaves to enable the reconstruction of the original triangles.\n\nThe BKD-tree-based indexing strategy maintains the original shapes with an accuracy of 1 cm. Its primary advantage over the Prefix tree approach lies in its better performance in searching and indexing, coupled with a more efficient use of storage.\n\nGeo shape literals\n\nColumns with the GEO_SHAPE type are represented and inserted as an object containing a valid GeoJSON geometry object:\n\n{\n    type = 'Polygon',\n    coordinates = [\n        [\n            [100.0, 0.0],\n            [101.0, 0.0],\n            [101.0, 1.0],\n            [100.0, 1.0],\n            [100.0, 0.0]\n        ]\n    ]\n}\n\n\nAlternatively a WKT string can be used to represent a GEO_SHAPE as well:\n\n'POLYGON ((5 5, 10 5, 10 10, 5 10, 5 5))'\n\n\nNote\n\nIt is not possible to detect a GEO_SHAPE type for a dynamically created column. Like with GEO_POINT type, GEO_SHAPE columns need to be created explicitly using either CREATE TABLE or ALTER TABLE.\n\nGeo shape GeoJSON examples\n\nThose are examples showing how to insert all possible kinds of GeoJSON types using WKT syntax.\n\ncr> CREATE TABLE my_table_geo (\n...   id INTEGER PRIMARY KEY,\n...   area GEO_SHAPE\n... ) WITH (number_of_replicas = 0)\nCREATE OK, 1 row affected (... sec)\n\ncr> INSERT INTO my_table_geo (\n...     id, area\n... ) VALUES\n...     (1, 'POINT (9.7417 47.4108)'),\n...     (2, 'MULTIPOINT (47.4108 9.7417, 9.7483 47.4106)'),\n...     (3, 'LINESTRING (47.4108 9.7417, 9.7483 47.4106)'),\n...     (4, 'MULTILINESTRING ((47.4108 9.7417, 9.7483 47.4106), (52.50463 13.46738, 52.51000 13.47000))'),\n...     (5, 'POLYGON ((47.4108 9.7417, 9.7483 47.4106, 9.7426 47.4142, 47.4108 9.7417))'),\n...     (6, 'MULTIPOLYGON (((5 5, 10 5, 10 10, 5 5)), ((6 6, 10 5, 10 10, 6 6)))'),\n...     (7, 'GEOMETRYCOLLECTION (POINT (9.7417 47.4108), MULTIPOINT (47.4108 9.7417, 9.7483 47.4106))')\n... ;\nINSERT OK, 7 rows affected (... sec)\n\ncr> SELECT * FROM my_table_geo ORDER BY id;\n+----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| id | area                                                                                                                                                                               |\n+----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|  1 | {\"coordinates\": [9.7417, 47.4108], \"type\": \"Point\"}                                                                                                                                |\n|  2 | {\"coordinates\": [[47.4108, 9.7417], [9.7483, 47.4106]], \"type\": \"MultiPoint\"}                                                                                                      |\n|  3 | {\"coordinates\": [[47.4108, 9.7417], [9.7483, 47.4106]], \"type\": \"LineString\"}                                                                                                      |\n|  4 | {\"coordinates\": [[[47.4108, 9.7417], [9.7483, 47.4106]], [[52.50463, 13.46738], [52.51, 13.47]]], \"type\": \"MultiLineString\"}                                                       |\n|  5 | {\"coordinates\": [[[47.4108, 9.7417], [9.7483, 47.4106], [9.7426, 47.4142], [47.4108, 9.7417]]], \"type\": \"Polygon\"}                                                                 |\n|  6 | {\"coordinates\": [[[[5.0, 5.0], [10.0, 5.0], [10.0, 10.0], [5.0, 5.0]]], [[[6.0, 6.0], [10.0, 5.0], [10.0, 10.0], [6.0, 6.0]]]], \"type\": \"MultiPolygon\"}                            |\n|  7 | {\"geometries\": [{\"coordinates\": [9.7417, 47.4108], \"type\": \"Point\"}, {\"coordinates\": [[47.4108, 9.7417], [9.7483, 47.4106]], \"type\": \"MultiPoint\"}], \"type\": \"GeometryCollection\"} |\n+----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nSELECT 7 rows in set (... sec)\n\nType casting\n\nA type CAST specifies a conversion from one data type to another. It will only succeed if the value of the expression is convertible to the desired data type, otherwise an error is returned.\n\nCrateDB supports two equivalent syntaxes for type casts:\n\nCAST(expression AS TYPE)\nexpression::TYPE\n\n\nCast expressions\n\nCast functions\n\nCast from string literals\n\nCast expressions\nCAST(expression AS TYPE)\nexpression::TYPE\n\nCast functions\nCAST\n\nExample usages:\n\ncr> SELECT CAST(port['http'] AS BOOLEAN) AS col FROM sys.nodes LIMIT 1;\n+------+\n| col  |\n+------+\n| TRUE |\n+------+\nSELECT 1 row in set (... sec)\n\ncr> SELECT (2+10)/2::TEXT AS col;\n+-----+\n| col |\n+-----+\n|   6 |\n+-----+\nSELECT 1 row in set (... sec)\n\n\nIt is also possible to convert array structures to different data types, e.g. converting an array of integer values to a boolean array.\n\ncr> SELECT CAST([0,1,5] AS ARRAY(BOOLEAN)) AS active_threads ;\n+---------------------+\n| active_threads      |\n+---------------------+\n| [false, true, true] |\n+---------------------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nIt is not possible to cast to or from OBJECT, GEO_POINT, and GEO_SHAPE types.\n\nTRY_CAST\n\nWhile CAST throws an error for incompatible type casts, TRY_CAST returns null in this case. Otherwise the result is the same as with CAST.\n\nTRY_CAST(expression AS TYPE)\n\n\nExample usages:\n\ncr> SELECT TRY_CAST('true' AS BOOLEAN) AS col;\n+------+\n| col  |\n+------+\n| TRUE |\n+------+\nSELECT 1 row in set (... sec)\n\n\nTrying to cast a TEXT to INTEGER, will fail with CAST if TEXT is no valid integer but return null with TRY_CAST:\n\ncr> SELECT TRY_CAST(name AS INTEGER) AS name_as_int FROM sys.nodes LIMIT 1;\n+-------------+\n| name_as_int |\n+-------------+\n|        NULL |\n+-------------+\nSELECT 1 row in set (... sec)\n\nCast from string literals\n\nThis cast operation is applied to a string literal and it effectively initializes a constant of an arbitrary type.\n\nExample usages, initializing an INTEGER and a TIMESTAMP constant:\n\ncr> SELECT INTEGER '25' AS int;\n+-----+\n| int |\n+-----+\n|  25 |\n+-----+\nSELECT 1 row in set (... sec)\n\ncr> SELECT TIMESTAMP WITH TIME ZONE '2029-12-12T11:44:00.24446' AS ts;\n+---------------+\n| ts            |\n+---------------+\n| 1891770240244 |\n+---------------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nThis cast operation is limited to primitive data types only. For complex types such as ARRAY or OBJECT, use the Cast functions syntax.\n\nPostgreSQL compatibility\n\nType aliases\n\nInternal-use types\n\nType aliases\n\nFor compatibility with PostgreSQL we include some type aliases which can be used instead of the CrateDB specific type names.\n\nFor example, in a type cast:\n\ncr> SELECT 10::INT2 AS INT2;\n+------+\n| int2 |\n+------+\n|   10 |\n+------+\nSELECT 1 row in set (... sec)\n\n\nSee the table below for a full list of aliases:\n\nAlias\n\n\t\n\nCrateDB Type\n\n\n\n\nSHORT\n\n\t\n\nSMALLINT\n\n\n\n\nINT\n\n\t\n\nINTEGER\n\n\n\n\nINT2\n\n\t\n\nSMALLINT\n\n\n\n\nINT4\n\n\t\n\nINTEGER\n\n\n\n\nINT8\n\n\t\n\nBIGINT\n\n\n\n\nLONG\n\n\t\n\nBIGINT\n\n\n\n\nSTRING\n\n\t\n\nTEXT\n\n\n\n\nVARCHAR\n\n\t\n\nTEXT\n\n\n\n\nCHARACTER VARYING\n\n\t\n\nTEXT\n\n\n\n\nNAME\n\n\t\n\nTEXT\n\n\n\n\nREGPROC\n\n\t\n\nTEXT\n\n\n\n\n\"CHAR\"\n\n\t\n\nBYTE\n\n\n\n\nFLOAT\n\n\t\n\nREAL\n\n\n\n\nFLOAT4\n\n\t\n\nREAL\n\n\n\n\nFLOAT8\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\nDOUBLE\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\nDECIMAL\n\n\t\n\nNUMERIC\n\n\n\n\nTIMESTAMP\n\n\t\n\nTIMESTAMP WITHOUT TIME ZONE\n\n\n\n\nTIMESTAMPTZ\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\nNote\n\nThe PG_TYPEOF system function can be used to resolve the data type of any expression.\n\nInternal-use types\n\"CHAR\"\n\nA one-byte character used internally for enumeration items in the PostgreSQL system catalogs.\n\nSpecified as a signed integer in the range -128 to 127.\n\nOID\n\nAn Object Identifier (OID). OIDS are used internally as primary keys in the PostgreSQL system catalogs.\n\nThe OID type is mapped to the integer data type.\n\nREGPROC\n\nAn alias for the oid type.\n\nThe REGPROC type is used by tables in the pg_catalog schema to reference functions in the pg_proc table.\n\nCasting a REGPROC type to a TEXT or integer type will result in the corresponding function name or oid value, respectively.\n\nREGCLASS\n\nAn alias for the oid type.\n\nThe REGCLASS type is used by tables in the pg_catalog schema to reference relations in the pg_class table.\n\nCasting a REGCLASS type to a TEXT or integer type will result in the corresponding relation name or oid value, respectively.\n\nNote\n\nString values casted to the REGCLASS type must match a valid relation name identifier, see also identifier naming restrictions. The given relation name won’t be validated against existing relations.\n\nOIDVECTOR\n\nThe OIDVECTOR type is used to represent one or more oid values.\n\nThis type is similar to an array of integers. However, you cannot use it with any scalar functions or expressions.\n\nSee Also\n\nPostgreSQL: Object Identifier (OID) types"
  },
  {
    "title": "Constraints — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/ddl/constraints.html",
    "html": "5.6\nConstraints\n\nColumns can be constrained in three ways:\n\nPRIMARY KEY\n\nNULL\n\nNOT NULL\n\nCHECK\n\nThe values of a constrained column must comply with the constraint.\n\nPRIMARY KEY\n\nThe primary key constraint combines a unique constraint and a not-null constraint. It also defines the default routing value used for sharding.\n\nExample:\n\ncr> create table my_table1 (\n...   first_column integer primary key,\n...   second_column text\n... );\nCREATE OK, 1 row affected (... sec)\n\n\nCurrently primary keys cannot be auto generated and have to be specified if data is inserted, otherwise an error is returned.\n\nDefining multiple columns with a primary key constraint is also supported:\n\ncr> create table my_table1pk (\n...   first_column integer primary key,\n...   second_column text primary key,\n...   third_column text\n... );\nCREATE OK, 1 row affected (... sec)\n\n\nOr using a alternate syntax:\n\ncr> create table my_table1pk1 (\n...   first_column integer,\n...   second_column text,\n...   third_column text,\n...   primary key (first_column, second_column)\n... );\nCREATE OK, 1 row affected (... sec)\n\n\nNote\n\nNot all column types can be used as PRIMARY KEY.\n\nFor further details see PRIMARY KEY.\n\nNULL\n\nExplicitly states that the column can contain NULL values. This is the default.\n\nExample:\n\ncr> create table my_table2 (\n...   first_column integer primary key,\n...   second_column text null\n... );\nCREATE OK, 1 row affected (... sec)\n\n\nSee Also\n\nNULL\n\nNOT NULL\n\nThe not null constraint can be used on any table column and it prevents null values from being inserted.\n\nExample:\n\ncr> create table my_table3 (\n...   first_column integer primary key,\n...   second_column text not null\n... );\nCREATE OK, 1 row affected (... sec)\n\n\nSee Also\n\nNOT NULL\n\nCHECK\n\nA check constraint allows you to specify that the values in a certain column must satisfy a boolean expression. This can be used to ensure data integrity. For example, if you have a table to store metrics from sensors and you want to ensure that negative values are rejected:\n\ncr> create table metrics (\n...   id TEXT PRIMARY KEY,\n...   weight double CHECK (weight >= 0)\n... );\nCREATE OK, 1 row affected  (... sec)\n\n\nNote\n\nFor further details see CHECK."
  },
  {
    "title": "Storage — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/ddl/storage.html",
    "html": "5.6\nStorage\n\nData storage options can be tuned for each column similar to how indexing is defined.\n\nColumn store\n\nBeside of storing the row data as-is (and indexing each value by default), each value term is stored into a Column Store by default. The usage of a Column Store is greatly improving global aggregations and groupings and enables ordering possibility as the data for one column is packed at one place. Using the Column Store limits the values of TEXT columns to a maximal length of 32766 bytes.\n\nTurning off the Column Store in conjunction of turning off indexing will remove the length limitation.\n\nExample:\n\ncr> CREATE TABLE t1 (\n...   id INTEGER,\n...   url TEXT INDEX OFF STORAGE WITH (columnstore = false)\n... );\nCREATE OK, 1 row affected  (... sec)\n\n\nDoing so will enable support for inserting strings longer than 32766 bytes into the url column, but the performance for global aggregations, groupings and sorting using this url column will decrease.\n\nNote\n\nINDEX OFF and therefore columnstore = false cannot be used with partition columns, as those are not stored as normal columns of a table.\n\nSupported data types\n\nControlling if values are stored into a Column Store is only supported on following data types:\n\nTEXT\n\nNumeric data\n\nTIMESTAMP\n\nWITH TIME ZONE\n\nFor all other Primitive types and Geometric points it is enabled by default and cannot be disabled. Container types and Geometric shapes do not support storing values into a Column Store at all."
  },
  {
    "title": "Generated columns — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/ddl/generated-columns.html",
    "html": "5.6\nGenerated columns\n\nIt is possible to define columns whose value is computed by applying a generation expression in the context of the current row. The generation expression can reference the values of other columns.\n\nTable of contents\n\nGeneration expressions\n\nLast modified dates\n\nPartitioning\n\nGeneration expressions\n\nGenerated columns are defined by providing a generation expression. Providing a data type is optional. It is inferred by the return type of the supplied expression if omitted:\n\ncr> CREATE TABLE computed (\n...   dividend double precision,\n...   divisor double precision,\n...   quotient GENERATED ALWAYS AS (dividend / divisor)\n... );\nCREATE OK, 1 row affected (... sec)\n\n\nSee Also\n\nFor a full syntax description, see CREATE TABLE.\n\nGenerated columns are read-only. Their values are computed as needed for every INSERT and UPDATE operation.\n\nFor example:\n\ncr> INSERT INTO computed (dividend, divisor)\n... VALUES (1.7, 1.5), (0.0, 10.0);\nINSERT OK, 2 rows affected (... sec)\n\n\nThe generated column is now filled with the computed value:\n\ncr> SELECT dividend, divisor, quotient\n... FROM computed\n... ORDER BY quotient;\n+----------+---------+--------------------+\n| dividend | divisor |           quotient |\n+----------+---------+--------------------+\n|      0.0 |    10.0 | 0.0                |\n|      1.7 |     1.5 | 1.1333333333333333 |\n+----------+---------+--------------------+\nSELECT 2 rows in set (... sec)\n\n\nThe generation expression is evaluated in the context of the current row. This means that you can compute a generated value from the values of base columns in the same row. However, it is not possible to reference other generated columns from within a generation expression.\n\nNote\n\nIf the generated expression is deterministic, its value will not be recomputed unless the value of a referenced column has changed.\n\nIf values are supplied for generated columns, these values are validated against the result of applying the generation expression:\n\ncr> INSERT INTO computed (dividend, divisor, quotient)\n... VALUES (100.0, 2.0, 12.0);\nSQLParseException[Given value 12.0 for generated column quotient does not match calculation (dividend / divisor) = 50.0]\n\nLast modified dates\n\nBecause CURRENT_TIMESTAMP is non-deterministic, you can use this expression to record a last modified date that is set when the row is first inserted, and subsequently updated every time the row is updated:\n\ncr> CREATE TABLE computed_non_deterministic (\n...   id LONG,\n...   last_modified TIMESTAMP WITH TIME ZONE GENERATED ALWAYS AS CURRENT_TIMESTAMP\n... );\nCREATE OK, 1 row affected (... sec)\n\nPartitioning\n\nGenerated columns can be used with the PARTITIONED BY clause to compute the partition column value from existing columns in the table:\n\ncr> CREATE TABLE computed_and_partitioned (\n...   huge_cardinality bigint,\n...   big_data text,\n...   partition_value GENERATED ALWAYS AS (huge_cardinality % 10)\n... ) PARTITIONED BY (partition_value);\nCREATE OK, 1 row affected (... sec)\n\n\nSee Also\n\nPartitioned tables: Generated columns"
  },
  {
    "title": "System columns — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/ddl/system-columns.html",
    "html": "5.6\nSystem columns\n\nOn every user table CrateDB implements several implicitly defined system columns. Their names are reserved and cannot be used as user-defined column names. All system columns are prefixed with an underscore, consist of lowercase letters and might contain underscores in between.\n\n_version\n\nCrateDB uses an internal versioning for every row, the version number is increased on every write.\n\nNote\n\nUsing the _version column for Optimistic Concurrency Control has been deprecated in favour of using the _seq_no and _primary_term. See Optimistic Concurrency Control for usage details.\n\n_seq_no\n\nThe CrateDB primary shards will increment a sequence number for every insert, update and delete operation executed against a row. The current sequence number of a row is exposed under this column. This column can be used in conjunction with the _primary_term column for Optimistic Concurrency Control, see Optimistic Concurrency Control for usage details.\n\n_primary_term\n\nThe sequence numbers give us an order of operations that happen at a primary shard, but they don’t help us distinguish between old and new primaries. For example, if a primary is isolated in a minority partition, a possible up to date replica shard on the majority partition will be promoted to be the new primary shard and continue to process write operations, subject to the write.wait_for_active_shards setting. When this partition heals we need a reliable way to know that the operations that come from the other shard are from an old primary and, equally, the operations that we send to the shard re-joining the cluster are from the newer primary. The cluster needs to have a consensus on which shards are the current serving primaries. In order to achieve this we use the primary terms which are generational counters that are incremented when a primary is promoted. Used in conjunction with _seq_no we can obtain a total order of operations across shards and Optimistic Concurrency Control.\n\n_score\n\nThis internal system column is available on all documents retrieved by a SELECT query. It represents the score of the document as returned by the query filter, and makes most sense for full text queries, giving an indication of how well the query expression matches the document. Documents with higher scores match the query more closely, allowing the score to be used for ranking.\n\nThe score is calculated using Lucene’s BM25Similarity implementation, based on the widely-used Okapi BM25 algorithm. This uses a combination of partition-level term statistics and the frequencies of particular terms within a matching row. Scores produced by different queries are not directly comparable. Note that, because term statistics are calculated per-partition, identical rows in different partitions may produce slightly different scores for the same query expression.\n\nIf the query does not include a fulltext search the value is 1.0f in most cases.\n\n_id\n\n_id is an internal system column that is available on each indexed document and can be retrieved by a SELECT query from doc schema tables.\n\nThe value is a unique identifier for each row in a table and is a compound string representation of all primary key values of that row. If no primary keys are defined the id is randomly generated. If no dedicated routing column is defined the _id value is used for distributing the records on the shards.\n\n_docid\n\n_docid exposes the internal id a document has within a Lucene segment. Although the id is unique within a segment, it is not unique across segments or shards and can change the value in case segments are merged."
  },
  {
    "title": "General Use — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/general/index.html",
    "html": "3.3\nGeneral Use\n\nThis section of the documentation covers any feature primarily of interest to a general user.\n\nTable of Contents\n\nData Definition\nCreating Tables\nData Types\nSystem Columns\nGenerated Columns\nConstraints\nStorage\nPartitioned Tables\nSharding\nReplication\nShard Allocation Filtering\nColumn Policy\nFulltext Indices\nFulltext Analyzers\nShow Create Table\nViews\nAltering Tables\nData Manipulation\nInserting Data\nUpdating Data\nDeleting Data\nImport and Export\nQuerying Crate\nSelecting Data\nJoins\nUnion All\nRefresh\nFulltext Search\nGeo Search\nBuilt-in Functions and Operators\nScalar Functions\nAggregation\nArithmetic Operators\nTable Functions\nComparison Operators\nArray Comparisons\nSubquery Expressions\nWindow Functions\nUser-Defined Functions\nCREATE OR REPLACE\nSupported Types\nOverloading\nDeterminism\nDROP FUNCTION\nSupported Languages\nBlobs\nCreating a Table for Blobs\nCustom Location for Storing Blob Data\nList\nAltering a Blob Table\nDeleting a Blob Table\nUsing Blob Tables\nOptimistic Concurrency Control\nIntroduction\nOptimistic Update\nOptimistic Delete\nKnown Limitations\nInformation Schema\nAccess\nVirtual Tables"
  },
  {
    "title": "General use — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/general/index.html",
    "html": "5.5\nGeneral use\n\nThis section of the documentation covers any feature primarily of interest to a general user.\n\nTable of contents\n\nData definition\nCreating tables\nData types\nSystem columns\nGenerated columns\nConstraints\nStorage\nPartitioned tables\nSharding\nReplication\nShard allocation filtering\nColumn policy\nFulltext indices\nFulltext analyzers\nShow Create Table\nViews\nAltering tables\nData manipulation\nInserting data\nUpdating data\nDeleting data\nImport and export\nQuerying\nSelecting data\nJoins\nUnion\nRefresh\nFulltext search\nGeo search\nBuilt-in functions and operators\nScalar functions\nAggregation\nArithmetic operators\nBit operators\nTable functions\nComparison operators\nArray comparisons\nSubquery expressions\nWindow functions\nUser-defined functions\nCREATE OR REPLACE\nSupported types\nOverloading\nDeterminism\nDROP FUNCTION\nSupported languages\nBlobs\nCreating a table for blobs\nCustom location for storing blob data\nList\nAltering a blob table\nDeleting a blob table\nUsing blob tables\nOptimistic Concurrency Control\nIntroduction\nOptimistic update\nOptimistic delete\nKnown limitations\nInformation schema\nAccess\nVirtual tables"
  },
  {
    "title": "Creating tables — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/ddl/create-table.html",
    "html": "5.6\nCreating tables\n\nTables are the basic building blocks of a relational database. A table can hold multiple rows (i.e., records), with each row having multiple columns and each column holding a single data element (i.e., value). You can query tables to insert data, select (i.e., retrieve) data, and delete data.\n\nTable of contents\n\nTable definition\n\nSchemas\n\nNaming restrictions\n\nTable configuration\n\nTable definition\n\nTo create a table, use the CREATE TABLE statement.\n\nAt a minimum, you must specify a table name and one or more column definitions. A column definition must specify a column name and a corresponding data type.\n\nHere’s an example statement:\n\ncr> CREATE TABLE my_table (\n...   first_column integer,\n...   second_column text\n... );\nCREATE OK, 1 row affected (... sec)\n\n\nThis statement creates a table named my_table with two columns named first_column and second_column with types integer and text.\n\nA table can be dropped (i.e., deleted) by using the DROP TABLE statement:\n\ncr> DROP TABLE my_table;\nDROP OK, 1 row affected (... sec)\n\n\nIf the my_table table did not exist, the DROP TABLE statement above would return an error message. If you specify the IF EXISTS clause, the instruction is conditional on the table’s existence and would not return an error message:\n\ncr> DROP TABLE IF EXISTS my_table;\nDROP OK, 0 rows affected (... sec)\n\n\nTip\n\nBy default, CrateDB will enforce the column definitions you specified with the CREATE TABLE statement (what’s known as a strict column policy).\n\nHowever, you can configure the column_policy table parameter so that the INSERT, UPDATE, and COPY FROM statements can arbitrarily create new columns as needed (what’s known as a dynamic column policy).\n\nSchemas\n\nTables can be created in different schemas. These are created implicitly on table creation and cannot be created explicitly. If a schema did not exist yet, it will be created.\n\nYou can create a table called my_table in a schema called my_schema schema like so:\n\ncr> create table my_schema.my_table (\n...   pk int primary key,\n...   label text,\n...   position geo_point\n... );\nCREATE OK, 1 row affected (... sec)\n\n\nWe can confirm this by looking up this table in the information_schema.tables table:\n\ncr> select table_schema, table_name from information_schema.tables\n... where table_name='my_table';\n+--------------+------------+\n| table_schema | table_name |\n+--------------+------------+\n| my_schema    | my_table   |\n+--------------+------------+\nSELECT 1 row in set (... sec)\n\n\nThe following schema names are reserved and may not be used:\n\nblob\n\ninformation_schema\n\nsys\n\nTip\n\nSchemas are primarily namespaces for tables. You can use privileges to control access to schemas.\n\nA user-created schema exists as long as there are tables with the same schema name. If the last table with that schema is dropped, the schema is gone (except for the blob and doc schema):\n\ncr> drop table my_schema.my_table ;\nDROP OK, 1 row affected (... sec)\n\n\nEvery table that is created without an explicit schema name, will be created in the doc schema:\n\ncr> create table my_doc_table (\n...   a_column int,\n...   another_one geo_point\n... );\nCREATE OK, 1 row affected (... sec)\n\ncr> select table_schema, table_name from information_schema.tables\n... where table_name='my_doc_table';\n+--------------+--------------+\n| table_schema | table_name   |\n+--------------+--------------+\n| doc          | my_doc_table |\n+--------------+--------------+\nSELECT 1 row in set (... sec)\n\nNaming restrictions\n\nTable, schema and column identifiers cannot have the same names as reserved key words. Please refer to the Lexical structure section for more information about naming.\n\nAdditionally, table and schema names are restricted in terms of characters and length. They:\n\nmay not contain one of the following characters: \\ / * ? \" < > | <whitespace> , # .\n\nshould not exceed 255 bytes when encoded with utf-8 (this limit applies on the optionally schema-qualified table name)\n\nColumn names are restricted in terms of patterns:\n\nColumns are not allowed to contain a dot (.), since this conflicts with internal path definitions.\n\nColumns that conflict with the naming scheme of virtual system columns are restricted.\n\nCharacter sequences that conform to the subscript notation (e.g. col['id']) are not allowed.\n\nTable configuration\n\nYou can configure tables in many different ways to take advantage of the range of functionality that CrateDB supports. For example:\n\nCrateDB transparently segments the underlying storage of table data into shards (four by default). You can configure the number of shards with the CLUSTERED BY clause. You control how CrateDB routes table rows to shards by specifying a routing column.\n\nYou can use cluster settings to configure how shards are balanced across a cluster and allocated to nodes (with attribute-based allocation, disk-based allocation, or both).\n\nSee Also\n\nHow-to guides: Tuning sharding performance\n\nYou can replicate shards WITH the number_of_replicas table setting. CrateDB will split replicated partitions into primary shards, with each primary shard having one or more replica shards.\n\nWhen you lose a primary shard (e.g., due to node failure), CrateDB will promote a replica shard to primary. More table replicas mean a smaller chance of permanent data loss (through increased data redundancy) in exchange for more disk space utilization and intra-cluster network traffic.\n\nReplication can also improve read performance because any increase in the number of shards distributed across a cluster also increases the opportunities for CrateDB to parallelize query execution across multiple nodes.\n\nYou can partition a table into one or more partitions with the PARTITIONED BY clause. You control how tables are partitioned by specifying one or more partition columns. Each unique combination of partition column values results in a new partition.\n\nBy partitioning a table, you can segment some SQL statements (e.g., those used for table optimization, import and export, and backup and restore) by constraining them to one or more partitions.\n\nSee Also\n\nHow-to guides: Tuning partitions for insert performance\n\nYou can fine-tune table operation by setting table parameters using the WITH clause. Available parameters include those used to configure replication, sharding, refresh interval, read and write operations, soft deletes, durability, column policy, and more."
  },
  {
    "title": "General use — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/general/index.html",
    "html": "4.8\nGeneral use\n\nThis section of the documentation covers any feature primarily of interest to a general user.\n\nTable of contents\n\nData definition\nCreating tables\nData types\nSystem columns\nGenerated columns\nConstraints\nStorage\nPartitioned tables\nSharding\nReplication\nShard allocation filtering\nColumn policy\nFulltext indices\nFulltext analyzers\nShow Create Table\nViews\nAltering tables\nData manipulation\nInserting data\nUpdating data\nDeleting data\nImport and export\nQuerying\nSelecting data\nJoins\nUnion\nRefresh\nFulltext search\nGeo search\nBuilt-in functions and operators\nScalar functions\nAggregation\nArithmetic operators\nTable functions\nComparison operators\nArray comparisons\nSubquery expressions\nWindow functions\nUser-defined functions\nCREATE OR REPLACE\nSupported types\nOverloading\nDeterminism\nDROP FUNCTION\nSupported languages\nBlobs\nCreating a table for blobs\nCustom location for storing blob data\nList\nAltering a blob table\nDeleting a blob table\nUsing blob tables\nOptimistic Concurrency Control\nIntroduction\nOptimistic update\nOptimistic delete\nKnown limitations\nInformation schema\nAccess\nVirtual tables"
  },
  {
    "title": "General use — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/index.html",
    "html": "5.6\nGeneral use\n\nThis section of the documentation covers any feature primarily of interest to a general user.\n\nTable of contents\n\nData definition\nCreating tables\nData types\nSystem columns\nGenerated columns\nConstraints\nStorage\nPartitioned tables\nSharding\nReplication\nShard allocation filtering\nColumn policy\nFulltext indices\nFulltext analyzers\nShow Create Table\nViews\nAltering tables\nData manipulation\nInserting data\nUpdating data\nDeleting data\nImport and export\nQuerying\nSelecting data\nJoins\nUnion\nRefresh\nFulltext search\nGeo search\nBuilt-in functions and operators\nScalar functions\nAggregation\nArithmetic operators\nBit operators\nTable functions\nComparison operators\nArray comparisons\nSubquery expressions\nWindow functions\nUser-defined functions\nCREATE OR REPLACE\nSupported types\nOverloading\nDeterminism\nDROP FUNCTION\nSupported languages\nBlobs\nCreating a table for blobs\nCustom location for storing blob data\nList\nAltering a blob table\nDeleting a blob table\nUsing blob tables\nOptimistic Concurrency Control\nIntroduction\nOptimistic update\nOptimistic delete\nKnown limitations\nInformation schema\nAccess\nVirtual tables"
  },
  {
    "title": "General use — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/general/index.html",
    "html": "master\nGeneral use\n\nThis section of the documentation covers any feature primarily of interest to a general user.\n\nTable of contents\n\nData definition\nCreating tables\nData types\nSystem columns\nGenerated columns\nConstraints\nStorage\nPartitioned tables\nSharding\nReplication\nShard allocation filtering\nColumn policy\nFulltext indices\nFulltext analyzers\nShow Create Table\nViews\nAltering tables\nData manipulation\nInserting data\nUpdating data\nDeleting data\nImport and export\nQuerying\nSelecting data\nJoins\nUnion\nRefresh\nFulltext search\nGeo search\nBuilt-in functions and operators\nScalar functions\nAggregation\nArithmetic operators\nBit operators\nTable functions\nComparison operators\nArray comparisons\nSubquery expressions\nWindow functions\nUser-defined functions\nCREATE OR REPLACE\nSupported types\nOverloading\nDeterminism\nDROP FUNCTION\nSupported languages\nBlobs\nCreating a table for blobs\nCustom location for storing blob data\nList\nAltering a blob table\nDeleting a blob table\nUsing blob tables\nOptimistic Concurrency Control\nIntroduction\nOptimistic update\nOptimistic delete\nKnown limitations\nInformation schema\nAccess\nVirtual tables"
  },
  {
    "title": "Configuration — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/config/index.html",
    "html": "5.6\nConfiguration\n\nCrateDB ships with sensible defaults, so configuration is typically not needed for basic, single node use.\n\nCrateDB can be configured via configuration files. These files are located in the config directory inside the CRATE_HOME directory.\n\nThe configuration directory can changed via the path.conf setting, like so:\n\nsh$ ./bin/crate -Cpath.conf=<CUSTOM_CONFIG_DIR>\n\n\nHere, replace <CUSTOM_CONFIG_DIR> with the path to your custom configuration directory.\n\nThe primary configuration file is named crate.yml. The default version of this file has a commented out listing of every available setting. (Some features, such as logging, use feature-specific files.)\n\nSettings can be configured via the configuration file or via the -C option at startup. So, for example, you can set the cluster name at startup, like so:\n\nsh$ ./bin/crate -Ccluster.name=cluster\n\n\nSettings passed at startup use the same name as the settings in the configuration file. So the equivalent setting in the configuration file would be:\n\ncluster.name = cluster\n\n\nSettings are applied in the following order:\n\nDefault values\n\nConfiguration file\n\nCommand-line options\n\nEach setting value overwrites any previous value. So, for example, command line settings will override configuration file settings.\n\nTip\n\nCluster settings can be changed at runtime.\n\nNote\n\nIf you’re just getting started with a particular part of CrateDB, we recommend you consult the appropriate top-level section of the documentation. The rest of this configuration documentation assumes a basic familiarity with the relevant parts of CrateDB.\n\nTable of contents\n\nNode-specific settings\nBasics\nNode types\nGeneral\nNetworking\nPaths\nPlug-ins\nCPU\nMemory\nGarbage collection\nAuthentication\nSecured communications (SSL/TLS)\nCross-origin resource sharing (CORS)\nBlobs\nRepositories\nQueries\nLegacy\nJavaScript language\nCustom attributes\nCluster-wide settings\nNon-runtime cluster-wide settings\nCollecting stats\nShard limits\nUsage data collector\nGraceful stop\nBulk operations\nDiscovery\nRouting allocation\nRecovery\nMemory management\nQuery circuit breaker\nRequest circuit breaker\nAccounting circuit breaker\nStats circuit breakers\nTotal circuit breaker\nThread pools\nOverload Protection\nMetadata\nLogical Replication\nSession settings\nUsage\nSupported session settings\nLogging\nApplication logging\nJVM logging\nEnvironment variables\nApplication variables\nJVM variables"
  },
  {
    "title": "ALTER CLUSTER — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/alter-cluster.html#alter-cluster-decommission",
    "html": "5.6\nALTER CLUSTER\n\nAlter the state of an existing cluster.\n\nTable of contents\n\nSynopsis\n\nDescription\n\nArguments\n\nREROUTE RETRY FAILED\n\nDECOMMISSION <nodeId | nodeName>\n\nSWAP TABLE\n\nOptions\n\nGC DANGLING ARTIFACTS\n\nSynopsis\nALTER CLUSTER\n  { REROUTE RETRY FAILED\n  | DECOMMISSION <nodeId | nodeName>\n  | SWAP TABLE source TO target [ WITH ( expr = expr [ , ... ] ) ]\n  | GC DANGLING ARTIFACTS\n  }\n\nDescription\n\nALTER CLUSTER applies a change to the cluster state.\n\nArguments\nREROUTE RETRY FAILED\n\nThe index setting allocation.max_retries indicates the maximum of attempts to allocate a shard on a node. If this limit is reached it leaves the shard unallocated.\n\nThis command allows the enforcement to retry the allocation of shards which failed to allocate. See Reroute shards to get convenient use-cases.\n\nThe row count defines the number of shards that will be allocated. A row count of -1 reflects an error or indicates that the statement did not get acknowledged.\n\nNote\n\nThis statement can only be invoked by superusers that already exist in the cluster.\n\nAdditionally, keep in mind that this statement only triggers the shard re-allocation and is therefore asynchronous. Unassigned shards with large size will take some time to allocate.\n\nDECOMMISSION <nodeId | nodeName>\n\nThis command triggers a graceful cluster node decommission. The node can be specified by either its Id or name. See Graceful stop for more information on decommissioning nodes gracefully.\n\nSWAP TABLE\nSWAP TABLE source TO target [ WITH ( expr = expr [ , ... ] ) ]\n\n\nThis command swaps two tables. source will be renamed to target and target will be renamed to source.\n\nAn example use case of this feature is some sort of schema migration using INSERT INTO ... query. You’d create a new table with an updated schema, copy over the data from the old table and then replace the old table with the new table.\n\nNote\n\nSwapping two tables causes the shards to be unavailable for a short period.\n\nOptions\ndrop_source\nDefault: false\n\nA boolean option that if set to true causes the command to remove the source table after the rename. This causes the command to replace source with target, instead of swapping the names.\n\nGC DANGLING ARTIFACTS\nGC DANGLING ARTIFACTS\n\n\nSome operations in CrateDB might temporarily create data to complete the operation. If during such an operation the cluster starts failing these temporary artifacts might not be cleaned up correctly.\n\nThe ALTER CLUSTER GC DANGLING ARTIFACTS command can be used to remove all artifacts created by such operations."
  },
  {
    "title": "Configuration — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/config/index.html",
    "html": "3.3\nConfiguration\n\nCrateDB ships with sensible defaults, so configuration is typically not needed for basic, single node use.\n\nCrateDB can be configured via configuration files. These files are located in the config directory inside the CRATE_HOME directory.\n\nThe configuration directory can changed via the path.conf setting, like so:\n\nsh$ ./bin/crate -Cpath.conf=<CUSTOM_CONFIG_DIR>\n\n\nHere, replace <CUSTOM_CONFIG_DIR> with the path to your custom configuration directory.\n\nThe primary configuration file is named crate.yml. The default version of this file has a commented out listing of every available setting. (Some features, such as logging, use feature-specific files.)\n\nSettings can be configured via the config file or via the -C option at startup. So, for example, you can set the cluster name at startup, like so:\n\nsh$ ./bin/crate -Ccluster.name=cluster\n\n\nSettings passed at startup use the same name as the settings in the configuration file. So the equivalent setting in the configuration file would be:\n\ncluster.name = cluster\n\n\nSettings are applied in the following order:\n\nDefault values\n\nConfiguration file\n\nCommand-line options\n\nEach setting value overwrites any previous value. So, for example, command line settings will override configuration file settings.\n\nTip\n\nCluster settings can be changed at runtime.\n\nNote\n\nIf you’re just getting started with a particular part of CrateDB, we recommend you consult the appropriate top-level section of the documentation. The rest of this configuration documentation assumes a basic familiarity with the relevant parts of CrateDB.\n\nTable of Contents\n\nNode Specific Settings\nBasics\nNode Types\nRead-only node\nHosts\nPorts\nPaths\nPlugins\nCPU\nMemory\nGarbage Collection\nAuthentication\nSecured Communications (SSL/TLS)\nElasticsearch HTTP REST API\nCross-Origin Resource Sharing (CORS)\nBlobs\nRepositories\nQueries\nJavascript Language\nCustom Attributes\nIngestion Framework\nEnterprise License\nCluster Wide Settings\nNon-Runtime Cluster Wide Settings\nCollecting Stats\nUsage Data Collector\nGraceful Stop\nBulk Operations\nDiscovery\nRouting Allocation\nRecovery\nQuery Circuit Breaker\nField Data Circuit Breaker\nRequest Circuit Breaker\nAccounting Circuit Breaker\nStats Circuit Breakers\nThread Pools\nMetadata\nSession Settings\nLogging\nApplication Logging\nJVM Logging\nEnvironment Variables\nApplication Variables\nJVM Variables"
  },
  {
    "title": "Configuration — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/config/index.html",
    "html": "5.5\nConfiguration\n\nCrateDB ships with sensible defaults, so configuration is typically not needed for basic, single node use.\n\nCrateDB can be configured via configuration files. These files are located in the config directory inside the CRATE_HOME directory.\n\nThe configuration directory can changed via the path.conf setting, like so:\n\nsh$ ./bin/crate -Cpath.conf=<CUSTOM_CONFIG_DIR>\n\n\nHere, replace <CUSTOM_CONFIG_DIR> with the path to your custom configuration directory.\n\nThe primary configuration file is named crate.yml. The default version of this file has a commented out listing of every available setting. (Some features, such as logging, use feature-specific files.)\n\nSettings can be configured via the configuration file or via the -C option at startup. So, for example, you can set the cluster name at startup, like so:\n\nsh$ ./bin/crate -Ccluster.name=cluster\n\n\nSettings passed at startup use the same name as the settings in the configuration file. So the equivalent setting in the configuration file would be:\n\ncluster.name = cluster\n\n\nSettings are applied in the following order:\n\nDefault values\n\nConfiguration file\n\nCommand-line options\n\nEach setting value overwrites any previous value. So, for example, command line settings will override configuration file settings.\n\nTip\n\nCluster settings can be changed at runtime.\n\nNote\n\nIf you’re just getting started with a particular part of CrateDB, we recommend you consult the appropriate top-level section of the documentation. The rest of this configuration documentation assumes a basic familiarity with the relevant parts of CrateDB.\n\nTable of contents\n\nNode-specific settings\nBasics\nNode types\nGeneral\nNetworking\nPaths\nPlug-ins\nCPU\nMemory\nGarbage collection\nAuthentication\nSecured communications (SSL/TLS)\nCross-origin resource sharing (CORS)\nBlobs\nRepositories\nQueries\nLegacy\nJavaScript language\nCustom attributes\nCluster-wide settings\nNon-runtime cluster-wide settings\nCollecting stats\nShard limits\nUsage data collector\nGraceful stop\nBulk operations\nDiscovery\nRouting allocation\nRecovery\nMemory management\nQuery circuit breaker\nRequest circuit breaker\nAccounting circuit breaker\nStats circuit breakers\nTotal circuit breaker\nThread pools\nOverload Protection\nMetadata\nLogical Replication\nSession settings\nUsage\nSupported session settings\nLogging\nApplication logging\nJVM logging\nEnvironment variables\nApplication variables\nJVM variables"
  },
  {
    "title": "CLI tools — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/cli-tools.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n4.8\nCLI tools\n\nCrateDB ships with command-line interface (CLI) tools (also referred to as executables) in the bin directory.\n\nIf your working directory is CRATE_HOME, you can run an executable like this:\n\nsh$ bin/crate\n\n\nOtherwise, you can run:\n\nsh$ <PATH_TO_CRATE_HOME>/bin/crate\n\n\nHere, replace <PATH_TO_CRATE_HOME> with a path to CRATE_HOME.\n\nAlternatively, if the CrateDB bin directory is on your PATH, you can run an executable directly:\n\nsh$ crate\n\n\nTable of contents\n\ncrate\n\nSynopsis\n\nOptions\n\nSignal handling\n\nExample\n\ncrate-node\n\nSynopsis\n\nCommands\n\nOptions\n\ncrate\n\nThe crate executable runs the CrateDB daemon.\n\nSee Also\n\nThis section is a low-level command reference. For help installing CrateDB for the first time, check out the CrateDB installation tutorial. Alternatively, consult the deployment guide for help running CrateDB in production.\n\nSynopsis\nsh$ bin/crate [-dhvCDX] [-p <PID_FILE>]\n\nOptions\n\nOption\n\n\t\n\nDescription\n\n\n\n\n-d\n\n\t\n\nStart the daemon in the background\n\n\n\n\n-h\n\n\t\n\nPrint usage information\n\n\n\n\n-p <PID_FILE>\n\n\t\n\nLog the PID to a file\n\n\n\n\n-v\n\n\t\n\nPrint version information\n\n\n\n\n-C\n\n\t\n\nSet a CrateDB configuration value (overrides configuration file)\n\n\n\n\n-D\n\n\t\n\nSet a Java system property value\n\n\n\n\n-X\n\n\t\n\nSet a nonstandard java option\n\nSignal handling\n\nThe CrateDB process can handle the following signals.\n\nSignal\n\n\t\n\nDescription\n\n\n\n\nTERM\n\n\t\n\nTerminates the process\n\n\n\n\nINT\n\n\t\n\nTerminates the process\n\nTip\n\nThe TERM signal stops CrateDB immediately. As a result, pending requests may fail. To ensure that CrateDB finishes handling pending requests before the node is stopped, you can, instead, perform a graceful stop with the DECOMMISSION statement.\n\nExample\n\nThe simplest way to start a CrateDB instance is to invoke crate without parameters:\n\nsh$ bin/crate\n\n\nThis command starts the process in the foreground.\n\nYou can also start CrateDB in the background with the -d option. When doing this, it’s helpful to write the process ID (PID) to a PID file with the -p option. So, in combination:\n\nsh$ bin/crate -d -p crate.pid\n\n\nTo stop the process, send a TERM signal using the PID file, like so:\n\nsh$ kill -TERM `cat crate.pid`\n\ncrate-node\n\nThe crate-node executable is a tool that can help you:\n\nRepurpose a node\n\nPerform an unsafe cluster bootstrap\n\nDetach a node from its cluster\n\nSee Also\n\nThis section is a low-level command reference. For help using crate-node, consult the troubleshooting guide.\n\nSynopsis\nsh$ bin/crate-node repurpose|unsafe-bootstrap|detach-cluster\n[--ordinal <INT>] [-E <KV_PAIR>]\n[-h, --help] ([-s, --silent] | [-v, --verbose])\n\nCommands\n\nCommand\n\n\t\n\nDescription\n\n\n\n\nrepurpose\n\n\t\n\nClean up any unnecessary data on disk after changing the role of a node.\n\n\n\n\nunsafe-bootstrap\n\n\t\n\nForce the election of a master and create a new cluster in the event of losing the majority of master-eligible nodes.\n\n\n\n\ndetach-cluster\n\n\t\n\nDetach a node from a cluster so that it can join a new one.\n\n\n\n\nremove-settings\n\n\t\n\nRemove persistent settings from the cluster state in case where it contains incompatible settings that prevent the cluster from forming.\n\n\n\n\noverride-version\n\n\t\n\nOverride the version number stored in the data path to be able to force a node to startup even when the node version is not compatible with the meta data.\n\nOptions\n\nOption\n\n\t\n\nDescription\n\n\n\n\n--ordinal <INT>\n\n\t\n\nSpecify which node to target if there is more than one node sharing a data path\n\n\n\n\n-E <KV_PAIR>\n\n\t\n\nConfigures a setting using a key-value (KV) pair\n\n\n\n\n-h, --help\n\n\t\n\nReturn all of the command parameters\n\n\n\n\n-s, --silent\n\n\t\n\nShow minimal output\n\n\n\n\n-v, --verbose\n\n\t\n\nShows verbose output"
  },
  {
    "title": "Configuration — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/config/index.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n4.8\nConfiguration\n\nCrateDB ships with sensible defaults, so configuration is typically not needed for basic, single node use.\n\nCrateDB can be configured via configuration files. These files are located in the config directory inside the CRATE_HOME directory.\n\nThe configuration directory can changed via the path.conf setting, like so:\n\nsh$ ./bin/crate -Cpath.conf=<CUSTOM_CONFIG_DIR>\n\n\nHere, replace <CUSTOM_CONFIG_DIR> with the path to your custom configuration directory.\n\nThe primary configuration file is named crate.yml. The default version of this file has a commented out listing of every available setting. (Some features, such as logging, use feature-specific files.)\n\nSettings can be configured via the configuration file or via the -C option at startup. So, for example, you can set the cluster name at startup, like so:\n\nsh$ ./bin/crate -Ccluster.name=cluster\n\n\nSettings passed at startup use the same name as the settings in the configuration file. So the equivalent setting in the configuration file would be:\n\ncluster.name = cluster\n\n\nSettings are applied in the following order:\n\nDefault values\n\nConfiguration file\n\nCommand-line options\n\nEach setting value overwrites any previous value. So, for example, command line settings will override configuration file settings.\n\nTip\n\nCluster settings can be changed at runtime.\n\nNote\n\nIf you’re just getting started with a particular part of CrateDB, we recommend you consult the appropriate top-level section of the documentation. The rest of this configuration documentation assumes a basic familiarity with the relevant parts of CrateDB.\n\nTable of contents\n\nNode-specific settings\nBasics\nNode types\nRead-only node\nHosts\nPorts\nPaths\nPlug-ins\nCPU\nMemory\nGarbage collection\nAuthentication\nSecured communications (SSL/TLS)\nCross-origin resource sharing (CORS)\nBlobs\nRepositories\nQueries\nJavaScript language\nCustom attributes\nCluster-wide settings\nNon-runtime cluster-wide settings\nCollecting stats\nShard limits\nUsage data collector\nGraceful stop\nBulk operations\nDiscovery\nRouting allocation\nRecovery\nMemory management\nQuery circuit breaker\nField data circuit breaker\nRequest circuit breaker\nAccounting circuit breaker\nStats circuit breakers\nTotal circuit breaker\nThread pools\nOverload Protection\nMetadata\nLogical Replication\nSession settings\nUsage\nSupported session settings\nLogging\nApplication logging\nJVM logging\nEnvironment variables\nApplication variables\nJVM variables"
  },
  {
    "title": "Configuration — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/config/index.html",
    "html": "master\nConfiguration\n\nCrateDB ships with sensible defaults, so configuration is typically not needed for basic, single node use.\n\nCrateDB can be configured via configuration files. These files are located in the config directory inside the CRATE_HOME directory.\n\nThe configuration directory can changed via the path.conf setting, like so:\n\nsh$ ./bin/crate -Cpath.conf=<CUSTOM_CONFIG_DIR>\n\n\nHere, replace <CUSTOM_CONFIG_DIR> with the path to your custom configuration directory.\n\nThe primary configuration file is named crate.yml. The default version of this file has a commented out listing of every available setting. (Some features, such as logging, use feature-specific files.)\n\nSettings can be configured via the configuration file or via the -C option at startup. So, for example, you can set the cluster name at startup, like so:\n\nsh$ ./bin/crate -Ccluster.name=cluster\n\n\nSettings passed at startup use the same name as the settings in the configuration file. So the equivalent setting in the configuration file would be:\n\ncluster.name = cluster\n\n\nSettings are applied in the following order:\n\nDefault values\n\nConfiguration file\n\nCommand-line options\n\nEach setting value overwrites any previous value. So, for example, command line settings will override configuration file settings.\n\nTip\n\nCluster settings can be changed at runtime.\n\nNote\n\nIf you’re just getting started with a particular part of CrateDB, we recommend you consult the appropriate top-level section of the documentation. The rest of this configuration documentation assumes a basic familiarity with the relevant parts of CrateDB.\n\nTable of contents\n\nNode-specific settings\nBasics\nNode types\nGeneral\nNetworking\nPaths\nPlug-ins\nCPU\nMemory\nGarbage collection\nAuthentication\nSecured communications (SSL/TLS)\nCross-origin resource sharing (CORS)\nBlobs\nRepositories\nQueries\nLegacy\nJavaScript language\nForeign Data Wrappers\nCustom attributes\nCluster-wide settings\nNon-runtime cluster-wide settings\nCollecting stats\nShard limits\nUsage data collector\nGraceful stop\nBulk operations\nDiscovery\nRouting allocation\nRecovery\nMemory management\nQuery circuit breaker\nRequest circuit breaker\nAccounting circuit breaker\nStats circuit breakers\nTotal circuit breaker\nThread pools\nOverload Protection\nMetadata\nLogical Replication\nSession settings\nUsage\nSupported session settings\nLogging\nApplication logging\nJVM logging\nEnvironment variables\nApplication variables\nJVM variables"
  },
  {
    "title": "CLI tools — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/cli-tools.html",
    "html": "5.5\nCLI tools\n\nCrateDB ships with command-line interface (CLI) tools (also referred to as executables) in the bin directory.\n\nIf your working directory is CRATE_HOME, you can run an executable like this:\n\nsh$ bin/crate\n\n\nOtherwise, you can run:\n\nsh$ <PATH_TO_CRATE_HOME>/bin/crate\n\n\nHere, replace <PATH_TO_CRATE_HOME> with a path to CRATE_HOME.\n\nAlternatively, if the CrateDB bin directory is on your PATH, you can run an executable directly:\n\nsh$ crate\n\n\nTable of contents\n\ncrate\n\nSynopsis\n\nOptions\n\nSignal handling\n\nExample\n\ncrate-node\n\nSynopsis\n\nCommands\n\nOptions\n\ncrate\n\nThe crate executable runs the CrateDB daemon.\n\nSee Also\n\nThis section is a low-level command reference. For help installing CrateDB for the first time, check out the CrateDB installation tutorial. Alternatively, consult the deployment guide for help running CrateDB in production.\n\nSynopsis\nsh$ bin/crate [-dhvCDX] [-p <PID_FILE>]\n\nOptions\n\nOption\n\n\t\n\nDescription\n\n\n\n\n-h\n\n\t\n\nPrint usage information\n\n\n\n\n-v\n\n\t\n\nPrint version information\n\n\n\n\n-C\n\n\t\n\nSet a CrateDB configuration value (overrides configuration file)\n\n\n\n\n-D\n\n\t\n\nSet a Java system property value\n\n\n\n\n-X\n\n\t\n\nSet a nonstandard java option\n\nSignal handling\n\nThe CrateDB process can handle the following signals.\n\nSignal\n\n\t\n\nDescription\n\n\n\n\nTERM\n\n\t\n\nTerminates the process\n\n\n\n\nINT\n\n\t\n\nTerminates the process\n\nTip\n\nThe TERM signal stops CrateDB immediately. As a result, pending requests may fail. To ensure that CrateDB finishes handling pending requests before the node is stopped, you can, instead, perform a graceful stop with the DECOMMISSION statement.\n\nExample\n\nThe simplest way to start a CrateDB instance is to invoke crate without parameters:\n\nsh$ bin/crate\n\n\nThis command starts the process in the foreground.\n\nIt’s helpful to write the process ID (PID) to a PID file with the use of echo $!. So you execute the following:\n\nsh$ bin/crate & echo $! > \"/tmp/crate.pid\"\n\n\nTo stop the process, send a TERM signal using the PID file, like so:\n\nsh$ kill -TERM `cat /tmp/crate.pid`\n\ncrate-node\n\nThe crate-node executable is a tool that can help you:\n\nRepurpose a node\n\nPerform an unsafe cluster bootstrap\n\nDetach a node from its cluster\n\nSee Also\n\nThis section is a low-level command reference. For help using crate-node, consult the troubleshooting guide.\n\nSynopsis\nsh$ bin/crate-node repurpose|unsafe-bootstrap|detach-cluster\n[--ordinal <INT>] [-C<key>=<value>]\n[-h, --help] ([-s, --silent] | [-v, --verbose])\n\nCommands\n\nCommand\n\n\t\n\nDescription\n\n\n\n\nrepurpose\n\n\t\n\nClean up any unnecessary data on disk after changing the role of a node.\n\n\n\n\nunsafe-bootstrap\n\n\t\n\nForce the election of a master and create a new cluster in the event of losing the majority of master-eligible nodes.\n\n\n\n\ndetach-cluster\n\n\t\n\nDetach a node from a cluster so that it can join a new one.\n\n\n\n\nremove-settings\n\n\t\n\nRemove persistent settings from the cluster state in case where it contains incompatible settings that prevent the cluster from forming.\n\n\n\n\noverride-version\n\n\t\n\nOverride the version number stored in the data path to be able to force a node to startup even when the node version is not compatible with the meta data.\n\n\n\n\nfix-metadata\n\n\t\n\nFix corrupted metadata after running table swap like: ALTER CLUSTER SWAP TABLE “schema”.”table” TO “schema.table”;\n\nOptions\n\nOption\n\n\t\n\nDescription\n\n\n\n\n--ordinal <INT>\n\n\t\n\nSpecify which node to target if there is more than one node sharing a data path\n\n\n\n\n-C\n\n\t\n\nSet a CrateDB configuration value (overrides configuration file)\n\n\n\n\n-h, --help\n\n\t\n\nReturn all of the command parameters\n\n\n\n\n-s, --silent\n\n\t\n\nShow minimal output\n\n\n\n\n-v, --verbose\n\n\t\n\nShows verbose output"
  },
  {
    "title": "CLI tools — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/cli-tools.html",
    "html": "5.6\nCLI tools\n\nCrateDB ships with command-line interface (CLI) tools (also referred to as executables) in the bin directory.\n\nIf your working directory is CRATE_HOME, you can run an executable like this:\n\nsh$ bin/crate\n\n\nOtherwise, you can run:\n\nsh$ <PATH_TO_CRATE_HOME>/bin/crate\n\n\nHere, replace <PATH_TO_CRATE_HOME> with a path to CRATE_HOME.\n\nAlternatively, if the CrateDB bin directory is on your PATH, you can run an executable directly:\n\nsh$ crate\n\n\nTable of contents\n\ncrate\n\nSynopsis\n\nOptions\n\nSignal handling\n\nExample\n\ncrate-node\n\nSynopsis\n\nCommands\n\nOptions\n\ncrate\n\nThe crate executable runs the CrateDB daemon.\n\nSee Also\n\nThis section is a low-level command reference. For help installing CrateDB for the first time, check out the CrateDB installation tutorial. Alternatively, consult the deployment guide for help running CrateDB in production.\n\nSynopsis\nsh$ bin/crate [-dhvCDX] [-p <PID_FILE>]\n\nOptions\n\nOption\n\n\t\n\nDescription\n\n\n\n\n-h\n\n\t\n\nPrint usage information\n\n\n\n\n-v\n\n\t\n\nPrint version information\n\n\n\n\n-C\n\n\t\n\nSet a CrateDB configuration value (overrides configuration file)\n\n\n\n\n-D\n\n\t\n\nSet a Java system property value\n\n\n\n\n-X\n\n\t\n\nSet a nonstandard java option\n\nSignal handling\n\nThe CrateDB process can handle the following signals.\n\nSignal\n\n\t\n\nDescription\n\n\n\n\nTERM\n\n\t\n\nTerminates the process\n\n\n\n\nINT\n\n\t\n\nTerminates the process\n\nTip\n\nThe TERM signal stops CrateDB immediately. As a result, pending requests may fail. To ensure that CrateDB finishes handling pending requests before the node is stopped, you can, instead, perform a graceful stop with the DECOMMISSION statement.\n\nExample\n\nThe simplest way to start a CrateDB instance is to invoke crate without parameters:\n\nsh$ bin/crate\n\n\nThis command starts the process in the foreground.\n\nIt’s helpful to write the process ID (PID) to a PID file with the use of echo $!. So you execute the following:\n\nsh$ bin/crate & echo $! > \"/tmp/crate.pid\"\n\n\nTo stop the process, send a TERM signal using the PID file, like so:\n\nsh$ kill -TERM `cat /tmp/crate.pid`\n\ncrate-node\n\nThe crate-node executable is a tool that can help you:\n\nRepurpose a node\n\nPerform an unsafe cluster bootstrap\n\nDetach a node from its cluster\n\nSee Also\n\nThis section is a low-level command reference. For help using crate-node, consult the troubleshooting guide.\n\nSynopsis\nsh$ bin/crate-node repurpose|unsafe-bootstrap|detach-cluster\n[--ordinal <INT>] [-C<key>=<value>]\n[-h, --help] ([-s, --silent] | [-v, --verbose])\n\nCommands\n\nCommand\n\n\t\n\nDescription\n\n\n\n\nrepurpose\n\n\t\n\nClean up any unnecessary data on disk after changing the role of a node.\n\n\n\n\nunsafe-bootstrap\n\n\t\n\nForce the election of a master and create a new cluster in the event of losing the majority of master-eligible nodes.\n\n\n\n\ndetach-cluster\n\n\t\n\nDetach a node from a cluster so that it can join a new one.\n\n\n\n\nremove-settings\n\n\t\n\nRemove persistent settings from the cluster state in case where it contains incompatible settings that prevent the cluster from forming.\n\n\n\n\noverride-version\n\n\t\n\nOverride the version number stored in the data path to be able to force a node to startup even when the node version is not compatible with the meta data.\n\n\n\n\nfix-metadata\n\n\t\n\nFix corrupted metadata after running table swap like: ALTER CLUSTER SWAP TABLE “schema”.”table” TO “schema.table”;\n\nOptions\n\nOption\n\n\t\n\nDescription\n\n\n\n\n--ordinal <INT>\n\n\t\n\nSpecify which node to target if there is more than one node sharing a data path\n\n\n\n\n-C\n\n\t\n\nSet a CrateDB configuration value (overrides configuration file)\n\n\n\n\n-h, --help\n\n\t\n\nReturn all of the command parameters\n\n\n\n\n-s, --silent\n\n\t\n\nShow minimal output\n\n\n\n\n-v, --verbose\n\n\t\n\nShows verbose output"
  },
  {
    "title": "CLI tools — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/cli-tools.html",
    "html": "master\nCLI tools\n\nCrateDB ships with command-line interface (CLI) tools (also referred to as executables) in the bin directory.\n\nIf your working directory is CRATE_HOME, you can run an executable like this:\n\nsh$ bin/crate\n\n\nOtherwise, you can run:\n\nsh$ <PATH_TO_CRATE_HOME>/bin/crate\n\n\nHere, replace <PATH_TO_CRATE_HOME> with a path to CRATE_HOME.\n\nAlternatively, if the CrateDB bin directory is on your PATH, you can run an executable directly:\n\nsh$ crate\n\n\nTable of contents\n\ncrate\n\nSynopsis\n\nOptions\n\nSignal handling\n\nExample\n\ncrate-node\n\nSynopsis\n\nCommands\n\nOptions\n\ncrate\n\nThe crate executable runs the CrateDB daemon.\n\nSee Also\n\nThis section is a low-level command reference. For help installing CrateDB for the first time, check out the CrateDB installation tutorial. Alternatively, consult the deployment guide for help running CrateDB in production.\n\nSynopsis\nsh$ bin/crate [-dhvCDX] [-p <PID_FILE>]\n\nOptions\n\nOption\n\n\t\n\nDescription\n\n\n\n\n-h\n\n\t\n\nPrint usage information\n\n\n\n\n-v\n\n\t\n\nPrint version information\n\n\n\n\n-C\n\n\t\n\nSet a CrateDB configuration value (overrides configuration file)\n\n\n\n\n-D\n\n\t\n\nSet a Java system property value\n\n\n\n\n-X\n\n\t\n\nSet a nonstandard java option\n\nSignal handling\n\nThe CrateDB process can handle the following signals.\n\nSignal\n\n\t\n\nDescription\n\n\n\n\nTERM\n\n\t\n\nTerminates the process\n\n\n\n\nINT\n\n\t\n\nTerminates the process\n\nTip\n\nThe TERM signal stops CrateDB immediately. As a result, pending requests may fail. To ensure that CrateDB finishes handling pending requests before the node is stopped, you can, instead, perform a graceful stop with the DECOMMISSION statement.\n\nExample\n\nThe simplest way to start a CrateDB instance is to invoke crate without parameters:\n\nsh$ bin/crate\n\n\nThis command starts the process in the foreground.\n\nIt’s helpful to write the process ID (PID) to a PID file with the use of echo $!. So you execute the following:\n\nsh$ bin/crate & echo $! > \"/tmp/crate.pid\"\n\n\nTo stop the process, send a TERM signal using the PID file, like so:\n\nsh$ kill -TERM `cat /tmp/crate.pid`\n\ncrate-node\n\nThe crate-node executable is a tool that can help you:\n\nRepurpose a node\n\nPerform an unsafe cluster bootstrap\n\nDetach a node from its cluster\n\nSee Also\n\nThis section is a low-level command reference. For help using crate-node, consult the troubleshooting guide.\n\nSynopsis\nsh$ bin/crate-node repurpose|unsafe-bootstrap|detach-cluster\n[--ordinal <INT>] [-C<key>=<value>]\n[-h, --help] ([-s, --silent] | [-v, --verbose])\n\nCommands\n\nCommand\n\n\t\n\nDescription\n\n\n\n\nrepurpose\n\n\t\n\nClean up any unnecessary data on disk after changing the role of a node.\n\n\n\n\nunsafe-bootstrap\n\n\t\n\nForce the election of a master and create a new cluster in the event of losing the majority of master-eligible nodes.\n\n\n\n\ndetach-cluster\n\n\t\n\nDetach a node from a cluster so that it can join a new one.\n\n\n\n\nremove-settings\n\n\t\n\nRemove persistent settings from the cluster state in case where it contains incompatible settings that prevent the cluster from forming.\n\n\n\n\noverride-version\n\n\t\n\nOverride the version number stored in the data path to be able to force a node to startup even when the node version is not compatible with the meta data.\n\n\n\n\nfix-metadata\n\n\t\n\nFix corrupted metadata after running table swap like: ALTER CLUSTER SWAP TABLE “schema”.”table” TO “schema.table”;\n\nOptions\n\nOption\n\n\t\n\nDescription\n\n\n\n\n--ordinal <INT>\n\n\t\n\nSpecify which node to target if there is more than one node sharing a data path\n\n\n\n\n-C\n\n\t\n\nSet a CrateDB configuration value (overrides configuration file)\n\n\n\n\n-h, --help\n\n\t\n\nReturn all of the command parameters\n\n\n\n\n-s, --silent\n\n\t\n\nShow minimal output\n\n\n\n\n-v, --verbose\n\n\t\n\nShows verbose output"
  },
  {
    "title": "Concepts — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/concepts/index.html",
    "html": "4.8\nConcepts\n\nThis section of the documentation covers important CrateDB concepts.\n\nTable of contents\n\nJoins\nClustering\nStorage and consistency\nResiliency"
  },
  {
    "title": "Concepts — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/concepts/index.html",
    "html": "5.5\nConcepts\n\nThis section of the documentation covers important CrateDB concepts.\n\nTable of contents\n\nJoins\nClustering\nStorage and consistency\nResiliency"
  },
  {
    "title": "Concepts — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/concepts/index.html",
    "html": "5.6\nConcepts\n\nThis section of the documentation covers important CrateDB concepts.\n\nTable of contents\n\nJoins\nClustering\nStorage and consistency\nResiliency"
  },
  {
    "title": "Software Testing — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/integrate/testing.html#testing",
    "html": "Software Testing\n\nJava and Python based test frameworks and libraries that support software integration testing with CrateDB.\n\nPython pytest\n\nThe popular pytest framework makes it easy to write small tests, but it also supports complex functional testing for applications and libraries. The pytest-crate package manages CrateDB instances for running integration tests against them.\n\nIt is based on cr8 for the heavy lifting, and additionally provides the crate, crate_execute, and crate_cursor pytest fixtures for developer convenience.\n\nUsing “pytest-crate” with CrateDB and pytest\n\nPython unittest\n\ncr8, a collection of tools for CrateDB developers, provides primitive elements to manage CrateDB single-node and multi-node instances through its run-crate subsystem, that can be used to create test layers for Python’s built-in unittest framework.\n\nUsing “cr8” test layers with CrateDB and unittest\n\nTestcontainers\n\nTestcontainers is an open source framework for providing throwaway, lightweight instances of databases, message brokers, web browsers, or just about anything that can run in a Docker container.\n\nCrateDB provides Testcontainers implementations for both Java and Python.\n\nUsing “Testcontainers for Java” with CrateDB\n\nUsing “Testcontainers for Python” with CrateDB and pytest\n\nUsing “Testcontainers for Python” with CrateDB and unittest"
  },
  {
    "title": "Data Visualization — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/integrate/visualize/index.html#visualization",
    "html": "Data Visualization\n\nGuidelines about data analysis and visualization with CrateDB.\n\nApache Superset / Preset\n\nProduct\n\nIntroduction to time series visualization in CrateDB and Apache Superset (Blog)\n\nUse CrateDB and Apache Superset for Open Source Data Warehousing and Visualization (Blog)\n\nIntroduction to time series visualization in CrateDB and Apache Superset (Webinar)\n\nIntroduction to Time-Series Visualization in CrateDB and Apache Superset (Preset.io)\n\nDevelopment\n\nSet up Apache Superset with CrateDB\n\nSet up an Apache Superset development sandbox with CrateDB\n\nVerify Apache Superset with CrateDB\n\nCluvio\n\nData Analysis with Cluvio and CrateDB\n\nExplo\n\nIntroduction to Time Series Visualization in CrateDB and Explo\n\nGrafana\n\nUsing Grafana with CrateDB Cloud\n\nhvPlot and Datashader\n\nThe cloud-datashader.ipynb notebook explores the HoloViews and Datashader frameworks and outlines how to use them to plot the venerable NYC Taxi dataset, after importing it into a CrateDB Cloud database cluster.\n\n🚧 Please note this notebook is a work in progress. 🚧\n\n \n\nMetabase\n\nUsing Metabase with CrateDB Cloud\n\nReal-time data analytics with Metabase and CrateDB\n\npandas\n\nFrom data storage to data analysis: Tutorial on CrateDB and pandas\n\nPlotly / Dash\n\nThe timeseries-queries-and-visualization.ipynb notebook explores how to access timeseries data from CrateDB via SQL, load it into pandas DataFrames, and visualize it using Plotly.\n\nIt includes advanced time series operations in SQL, like aggregations, window functions, interpolation of missing data, common table expressions, moving averages, JOINs, and the handling of JSON data.\n\n \n\nAlternatively, you are welcome to explore the canonical Dash Examples.\n\nR\nCrateDB with R"
  },
  {
    "title": "Business Intelligence — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/integrate/bi/index.html#bi",
    "html": "Business Intelligence\n\nIntegrations of CrateDB with other tools, specifically related to business analytics and intelligence software.\n\nPowerBI\nReports with CrateDB and Power BI Desktop\nReal Time Reports with CrateDB and Power BI"
  },
  {
    "title": "System Metrics — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/integrate/metrics/index.html#metrics",
    "html": "System Metrics\n\nCrateDB integrations with metrics collection agents, brokers, and stores.\n\nThis documentation section lists applications and daemons which can be used together with CrateDB, and outlines how to use them optimally.\n\nPrometheus\n\nCrateDB Prometheus Adapter\n\nGetting Started With Prometheus and CrateDB for Long-Term Storage\n\nStoring long-term metrics with Prometheus in CrateDB\n\nWebinar: Using Prometheus and Grafana with CrateDB Cloud\n\nTelegraf\n\nUse CrateDB With Telegraf, an Agent for Collecting & Reporting Metrics"
  },
  {
    "title": "Load and Export — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/integrate/etl/index.html#etl",
    "html": "Load and Export\n\nYou have a variety of options to connect and integrate with 3rd-party ETL applications, mostly using CrateDB’s PostgreSQL interface.\n\nThis documentation section lists corresponding ETL applications and frameworks which can be used together with CrateDB, and outlines how to use them optimally.\n\nApache Airflow / Astronomer\n\nA set of starter tutorials.\n\nAutomating the import of Parquet files with Apache Airflow\n\nUpdating stock market data automatically with CrateDB and Apache Airflow\n\nAutomating stock data collection and storage with CrateDB and Apache Airflow\n\nA set of elaborated tutorials, including blueprint implementations.\n\nAutomating export of CrateDB data to S3 using Apache Airflow\n\nImplementing a data retention policy in CrateDB using Apache Airflow\n\nCrateDB and Apache Airflow: Building a data ingestion pipeline\n\nBuilding a hot and cold storage data retention policy in CrateDB with Apache Airflow\n\nTutorials and resources about configuring the managed variants, Astro and CrateDB Cloud.\n\nETL with Astro and CrateDB Cloud in 30min - fully up in the cloud\n\nETL pipeline using Apache Airflow with CrateDB (Source)\n\nRun an ETL pipeline with CrateDB and data quality checks\n\nApache Flink\n\nData Ingestion using Kafka and Kafka Connect\n\nBuild a data ingestion pipeline using Kafka, Flink, and CrateDB\n\nCommunity Day: Stream processing with Apache Flink and CrateDB\n\nExecutable stack: Apache Kafka, Apache Flink, and CrateDB\n\nApache Kafka\n\nData Ingestion using Kafka and Kafka Connect\n\nExecutable stack: Apache Kafka, Apache Flink, and CrateDB\n\nTutorial: Replicating data to CrateDB with Debezium and Kafka\n\nAzure Functions\n\nData Enrichment using IoT Hubs, Azure Functions and CrateDB\n\ndbt\n\nUsing dbt with CrateDB\n\nDebezium\n\nTutorial: Replicating data to CrateDB with Debezium and Kafka\n\nWebinar: How to replicate data from other databases to CrateDB with Debezium and Kafka\n\nKestra\n\nSetting up data pipelines with CrateDB and Kestra\n\nMongoDB\n\nImport data from MongoDB\n\nMySQL\n\nImport data from MySQL\n\nNode-RED\n\nIngesting MQTT messages into CrateDB using Node-RED\n\nAutomating recurrent CrateDB queries using Node-RED\n\nSinger / Meltano\n\nmeltano-target-cratedb\n\nmeltano-tap-cratedb\n\nExamples about working with CrateDB and Meltano\n\n🚧 Please note these adapters are a work in progress. 🚧\n\nSQL Server Integration Services\n\nA demo project which uses SSIS and ODBC to read and write data from CrateDB:\n\nUsing SQL Server Integration Services with CrateDB\n\nStreamSets\n\nData Stream Pipelines with CrateDB and StreamSets Data Collector"
  },
  {
    "title": "Machine Learning — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/domain/ml/index.html#machine-learning",
    "html": "Machine Learning\n\nIntegrate CrateDB with machine learning frameworks and tools, for MLOps and Vector database operations.\n\nMachine Learning Operations\n\nTraining a machine learning model, running it in production, and maintaining it, requires a significant amount of data processing and bookkeeping operations.\n\nCrateDB, as a universal SQL database, supports this process through adapters to best-of-breed software components for MLOps procedures.\n\nMLOps is a paradigm that aims to deploy and maintain machine learning models in production reliably and efficiently, including experiment tracking, and in the spirit of continuous development and DevOps.\n\nVector Store\n\nCrateDB’s FLOAT_VECTOR data type implements a vector store and the k-nearest neighbour (kNN) search algorithm to find vectors that are similar to a query vector.\n\nThese feature vectors may be computed from raw data using machine learning methods such as feature extraction algorithms, word embeddings, or deep learning networks.\n\nVector databases can be used for similarity search, multi-modal search, recommendation engines, large language models (LLMs), retrieval-augmented generation (RAG), and other applications.\n\nAnomaly Detection and Forecasting\nMLflow\n\nTutorials and Notebooks about using MLflow together with CrateDB.\n\nBlog: Running Time Series Models in Production using CrateDB\n\nPart 1: Introduction to Time Series Modeling using Machine Learning\n\nThe article will introduce you to the concept of time series modeling, discussing the main obstacles running it in production. It will introduce you to CrateDB, highlighting its key features and benefits, why it stands out in managing time series data, and why it is an especially good fit for supporting machine learning models in production.\n\nFundamentals\nTime Series Modeling\n\nNotebook: Create a Time Series Anomaly Detection Model\n\nGuidelines and runnable code to get started with MLflow and CrateDB, exercising time series anomaly detection and time series forecasting / prediction using NumPy, Salesforce Merlion, and Matplotlib.\n\n  \n\nFundamentals\nTime Series\nAnomaly Detection\nPrediction / Forecasting\n\nPyCaret\n\nTutorials and Notebooks about using PyCaret together with CrateDB.\n\nNotebook: AutoML classification with PyCaret\n\nExplore the PyCaret framework and show how to use it to train different classification models.\n\n  \n\nFundamentals\nTime Series\nAnomaly Detection\nPrediction / Forecasting\n\nNotebook: Train time series forecasting models\n\nHow to train time series forecasting models using PyCaret and CrateDB.\n\n  \n\nFundamentals\nTime Series\nTraining\nClassification\nForecasting\n\nscikit-learn\n\nUse scikit-learn with CrateDB.\n\nRegression analysis with pandas and scikit-learn\n\nUse pandas and scikit-learn to run a regression analysis within a Jupyter Notebook.\n\nMachine Learning and CrateDB: An introduction\n\nMachine Learning and CrateDB: Getting Started With Jupyter\n\nMachine Learning and CrateDB: Experiment Design & Linear Regression\n\nFundamentals\nRegression Analysis\n\nTensorFlow\n\nUse TensorFlow with CrateDB.\n\nPredictive Maintenance\n\nBuild a machine learning model that will predict whether a machine will fail within a specified time window in the future.\n\nTensorFlow and CrateDB\n\nFundamentals\nPrediction\n\nLLMs / RAG\n\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific sources of information, using a technique known as Retrieval Augmented Generation, or RAG. RAG is a technique for augmenting LLM knowledge with additional data.\n\nLangChain\n\nTutorials and Notebooks about using LangChain together with CrateDB. LangChain has a number of components designed to help build Q&A applications, and RAG applications more generally. This feature uses CrateDB’s Vector Store implementation.\n\nWhat can you build with LangChain?\n\nLangChain: Retrieval augmented generation\n\nLangChain: Analyzing structured data\n\nLangChain: Chatbots\n\nTutorial: Set up LangChain with CrateDB\n\nLangChain is a framework for developing applications powered by language models. For this tutorial, we are going to use it to interact with CrateDB using only natural language without writing any SQL.\n\nTo achieve that, you will need a CrateDB instance running, an OpenAI API key, and some Python knowledge.\n\nFundamentals\nVector Store\nLLM\nRAG\n\nNotebook: Vector Similarity Search\n\nCrateDB’s FLOAT_VECTOR type and its KNN_MATCH function can be used for storing and retrieving embeddings, and for conducting similarity searches.\n\n   \n\nFundamentals\nVector Store\nLLM\nRAG\n\nNotebook: SQLAlchemy Document Loader\n\nDatabase tables in CrateDB can be used as a source provider for LangChain documents.\n\n   \n\nFundamentals\nVector Store\nData I/O\n\nNotebook: Conversational History\n\nCrateDB supports managing LangChain’s conversation history.\n\n   \n\nFundamentals\nVector Store\nHistory"
  },
  {
    "title": "Concepts — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/concepts/index.html",
    "html": "master\nConcepts\n\nThis section of the documentation covers important CrateDB concepts.\n\nTable of contents\n\nJoins\nClustering\nStorage and consistency\nResiliency"
  },
  {
    "title": "Time Series Data — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/domain/timeseries/index.html#timeseries",
    "html": "Time Series Data\n\nLearn how to optimally use CrateDB for time series use-cases.\n\nGenerate time series data\n\nNormalize time series data intervals\n\nFinancial data collection and processing using pandas\n\nTime Series: Analyzing Weather Data\n\nAnalyzing Device Readings with Metadata Integration\n\nTime-series data: From raw data to fast analysis in only three steps"
  },
  {
    "title": "Industrial Data — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/domain/industrial/index.html#industrial",
    "html": "Industrial Data\n\nLearn how to use CrateDB in industrial / IIoT / Industry 4.0 scenarios within engineering, manufacturing, and other operational domains.\n\nIn the realm of Industrial IoT, dealing with diverse data, ranging from slow-moving structured data, to high-frequency measurements, presents unique challenges.\n\nThe complexities of industrial big data are characterized by its high variety, unstructured features, different data sampling rates, and how these attributes influence data storage, retention, and integration.\n\nToday’s warehouses are complex systems with a very high degree of automation. The key to the successful operation of these warehouses lies in having a holistic view on the entire system based on data from various components like sensors, PLCs, embedded controllers and software systems.\n\nTGW Insights\n\nAfter trying multiple database systems, TGW Logistics moved to CrateDB for its ability to aggregate different data formats and ability to query this information without much hassle.\n\nIn the second presentation, you will learn how TGW leverages CrateDB to build digital twins of physical warehouses around the world.\n\nFixing data silos in a high-speed logistics environment\n\nChallenges of Storing and Analyzing Industrial Data\n\nWhat’s inside\n\nThe Complexity of IoT Data: An examination of the unique properties of industrial IoT data, including slow-moving structured information and high-frequency measurements.\n\nChallenges and Solutions: Discussion of the difficulties in data storage, retention, and integration posed by this complexity, and how CrateDB provides a targeted solution.\n\nReal-World Applications: Exploration of actual customer use cases to illustrate how CrateDB can be applied in various industrial scenarios."
  },
  {
    "title": "Full-Text Search — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/domain/search/index.html#fts",
    "html": "Full-Text Search\n\nLearn how to set up your database for full-text search, how to create the relevant indices, and how to query your text data efficiently. A must-read for anyone looking to make sense of large volumes of unstructured text data.\n\nFull-Text: Exploring the Netflix Catalog\n\nNote\n\nCrateDB is an exceptional choice for handling complex queries and large-scale data sets. One of its standout features are its full-text search capabilities, built on top of the powerful Lucene library. This makes it a great fit for organizing, searching, and analyzing extensive datasets."
  },
  {
    "title": "Installation — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/install/index.html",
    "html": "Installation\n\nThis section of the documentation covers the installation of CrateDB on different kinds of operating systems and environments, both suitable for on-premises and development sandbox operations.\n\nThe first step to using any software package is getting it properly installed. Please read this section carefully.\n\nDebian, Ubuntu\n\n \n\nDebian and Ubuntu Linux\nRed Hat, SUSE\n\n \n\nRPM Linux: Red Hat, SUSE\nmacOS\n\nmacOS\nWindows\n\nWindows\nTarball Archive\n\nInstallation from Tarball\nContainer Setup\n\nContainer Setup\nCloud Hosting\n\nCloud Hosting\nConfig Settings\n\nConfiguration Settings\n\nWe recommend to use the package-based installation methods for CrateDB on Debian, Ubuntu, and Derivates and CrateDB on Red Hat, SUSE, and Derivates, by subscribing to the corresponding package release channels.\n\nAlternatively, you can also download release archives and run CrateDB manually, by using the Ad Hoc method.\n\nNotes\n\nAfter the installation is finished, the CrateDB service should be up and running, and will run a HTTP server on localhost:4200. To access the Admin UI from your local machine, navigate to:\n\nhttp://localhost:4200/\n\n\nNote\n\nCrateDB requires a Java virtual machine to run.\n\nStarting with CrateDB 4.2, Java is bundled with CrateDB, and no extra installation is necessary.\n\nCrateDB versions before 4.2 required a separate Java installation. For CrateDB 3.0 to 4.1, Java 11 is the minimum requirement. CrateDB versions before 3.0 require Java 8. We recommend to use OpenJDK on Linux Systems."
  },
  {
    "title": "Document Store — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/domain/document/index.html#document",
    "html": "Document Store\n\nLearn how to efficiently store JSON or other structured data, also nested, and how to query this data with ease, based on CrateDB’s OBJECT data type.\n\nStoring documents in CrateDB provides the same development convenience like the document-oriented storage layer of Lotus Notes / Domino, CouchDB, MongoDB, and PostgreSQL’s JSON(B) types.\n\nObjects: Analyzing Marketing Data\n\nUnleashing the Power of Nested Data: Ingesting and Querying JSON Documents with SQL"
  },
  {
    "title": "Integrations — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/integrate/index.html",
    "html": "Integrations\n\nYou have a variety of options to connect and integrate 3rd-party applications, mostly using CrateDB’s PostgreSQL interface.\n\nThis documentation section lists applications, frameworks, and libraries, which can be used together with CrateDB, and outlines how to use them optimally.\n\nORM Libraries\nSQLAlchemy\nDataframe Libraries\nDask\npandas\nPolars\nLoad and Export\nApache Airflow / Astronomer\nApache Flink\nApache Kafka\nAzure Functions\ndbt\nDebezium\nKestra\nMongoDB\nMySQL\nNode-RED\nSinger / Meltano\nSQL Server Integration Services\nStreamSets\nSystem Metrics\nPrometheus\nTelegraf\nData Visualization\nApache Superset / Preset\nCluvio\nExplo\nGrafana\nhvPlot and Datashader\nMetabase\npandas\nPlotly / Dash\nR\nBusiness Intelligence\nPowerBI\nSoftware Testing\nPython pytest\nPython unittest\nTestcontainers\n\nTip\n\nPlease also visit the Overview of CrateDB integration tutorials."
  },
  {
    "title": "Reference Architectures — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/reference-architectures/index.html",
    "html": "Reference Architectures\n\nThis section of the documentation covers reference architectures involving CrateDB for various use-cases.\n\nAzure\n\nCrateDB on Azure IoT\nDistributed Machine Learning At The Edge"
  },
  {
    "title": "Application Domains — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/domain/index.html",
    "html": "Application Domains\n\nThis section of the documentation includes tutorials and guidelines about how to use CrateDB optimally, related to different topic domains.\n\nDocument Store\nFull-Text Search\nIndustrial Data\nTime Series Data\nMachine Learning"
  },
  {
    "title": "Getting Started — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/getting-started.html",
    "html": "Getting Started\nIntroduction\n\nOnce CrateDB is installed and running, you can start to interact with the database for the first time.\n\nThe Admin UI\n\nCrateDB ships with a browser-based administration interface called Admin UI. It is enabled on each CrateDB node, you can use it to inspect and interact with the whole CrateDB cluster in a number of ways.\n\nIf CrateDB is running on your workstation, access the Admin UI using http://localhost:4200/. Otherwise, replace localhost with the hostname CrateDB is running on.\n\nWhen using CrateDB Cloud, the URL will look like https://testdrive.aks1.westeurope.azure.cratedb.net:4200/.\n\n \n\nNote\n\nIf you are running CrateDB on a remote machine, you will have to create a dedicated user account for accessing the Admin UI. See Create User.\n\nThe CrateDB Shell\n\nThe CrateDB Shell, called crash, is an interactive command-line interface (CLI) program for working with CrateDB on your favorite terminal. To learn more about it, please refer to its documentation at The CrateDB Shell.\n\nConnect\n\nYou have a variety of options to connect to CrateDB, and integrate it with off-the-shelve, 3rd-party, open-source, and proprietary applications, mostly using CrateDB’s PostgreSQL interface.\n\nTo learn more, please refer to the documentation sections about supported client drivers, libraries, and frameworks, and corresponding tutorials.\n\nDrivers and Integrations\n\nDatabase Driver Code Examples\n\nIntegration Tutorials\n\nMore integration tutorials\n\nTip\n\nTo learn more about all the details of CrateDB features, operations, and its SQL dialect, please also visit the CrateDB Reference Manual."
  },
  {
    "title": "Performance Guides — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/performance/index.html",
    "html": "Performance Guides\n\nBest practices and tips for common scenarios around the topics of performance tuning and sharding.\n\nTable of contents\n\nSharding Guide\nOptimising for query performance\nOptimising for ingestion performance\nInsert Performance\nInsert Methods\nBulk Inserts\nParallel Inserts\nConfiguration Tuning for Inserts\nTesting Insert Performance\nSelect Performance\nAggregations and GROUP BY\nDownsampling with DATE_BIN\nDownsampling with LTTB\nRewrite JOINs as CTEs\nRetrieve individual records in bulk"
  },
  {
    "title": "Administration — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/admin/index.html",
    "html": "Administration\n\nCrateDB database administration practices.\n\nBootstrap checks\nCreate User\nGoing into production\nMemory Configuration\nTroubleshooting\n\nBest practices and tips for common scenarios around the topics of clustering, sharding, partitioning, and performance tuning.\n\nClustering\nSharding and Partitioning\nPerformance Guides\n\nGuidelines about upgrading CrateDB clusters.\n\nUpgrading"
  },
  {
    "title": "Configuration Settings — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/install/configure.html",
    "html": "Configuration Settings\n\nIn order to configure CrateDB, please take note of the configuration file locations and the available environment variables.\n\nConfiguration Files\n\nWhen using the package-based setup flavor for CrateDB on Debian, Ubuntu, and Derivates or CrateDB on Red Hat, SUSE, and Derivates, the main CrateDB configuration files are located within the /etc/crate directory.\n\nWhen using the Ad Hoc setup, or the Microsoft Windows setup, the configuration files are located within the config/ directory relative to the working directory.\n\nEnvironment Variables\n\nFor the vanilla package-based setup flavor, the CrateDB startup script reads Environment variables from the /etc/default/crate file as environment variables.\n\nNote\n\nRPM packages of CrateDB versions up to 5.2.11, 5.3.8, 5.4.7 and 5.5.2 are using the /etc/sysconfig/crate file instead.\n\nWhen using the Ad Hoc setup, or the Microsoft Windows setup, the environment variables will be defined by bin/crate{.sh,.bat} relative to the working directory.\n\nHere is an example:\n\n# Configure heap size (defaults to 256m min, 1g max).\nCRATE_HEAP_SIZE=2g\n\n# Maximum number of open files, defaults to 65535.\n# MAX_OPEN_FILES=65535\n\n# Maximum locked memory size. Set to \"unlimited\" if you use the\n# bootstrap.mlockall option in crate.yml. You must also set\n# CRATE_HEAP_SIZE.\nMAX_LOCKED_MEMORY=unlimited\n\n# Provide additional Java OPTS.\n# CRATE_JAVA_OPTS=\n\n# Force the JVM to use IPv4 only.\nCRATE_USE_IPV4=true\n"
  },
  {
    "title": "Cloud Hosting — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/install/cloud/index.html",
    "html": "Cloud Hosting\n\nCrateDB provides packages and executables that will work on any operating system capable of running Java.\n\nTable of contents\n\nAmazon AWS\nRunning CrateDB on Amazon EC2\nRunning CrateDB via Terraform\nUsing Amazon S3 as a snapshot repository\nMicrosoft Azure\nRunning CrateDB on Azure VMs\nRunning CrateDB via Terraform"
  },
  {
    "title": "Container Setup — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/install/container/index.html",
    "html": "Container Setup\n\nCrateDB is ideal for containerized environments, creating and scaling a cluster takes minutes and your valuable data is always in sync and available.\n\nQuickstart\n\nCrateDB and Docker are great matches thanks to CrateDB’s shared-nothing, horizontally scalable architecture that lends itself well to containerization.\n\nIn order to spin up a container using the most recent stable version of the official CrateDB Docker image, use:\n\ndocker run --publish=4200:4200 --publish=5432:5432 --env CRATE_HEAP_SIZE=1g --pull=always crate\n\n\nTip\n\nIf this command aborts with an error, please consult the Docker troubleshooting guide. You are also welcome to learn more about Resource constraints with respect to running CrateDB within containers.\n\nCaution\n\nThis type of invoking CrateDB will get you up and running quickly.\n\nPlease note, by default, the CrateDB Docker container is ephemeral, so data will not be stored in a persistent manner. When stopping the container, all data will be lost.\n\nWhen you are ready to start using CrateDB for data you care about, please consult the full guide to CrateDB and Docker in order to configure the Docker setup appropriately by using persistent disk volumes.\n\nAdvanced\n\nThis section demonstrates advanced container setup scenarios using Docker and Kubernetes.\n\nDocker\nKubernetes"
  },
  {
    "title": "Installation from Tarball Archive — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/install/tarball.html",
    "html": "Installation from Tarball Archive\n\nThis section of the documentation outlines how to use the release archives to install CrateDB. The walkthrough is suitable to install and run CrateDB on Unix-like systems, for example Linux and macOS.\n\nDownload the latest CrateDB release archive. Please make sure to select the right release archive matching your system.\n\nOnce downloaded, extract the archive either using your favorite terminal or command line shell or by using a GUI tool like 7-Zip:\n\n# Extract tarball on Unix-like systems\ntar -xzf crate-*.tar.gz\n\n\nOn the terminal, change into the extracted crate directory:\n\ncd crate-*\n\n\nRun a CrateDB single-node instance on the local network interface:\n\n./bin/crate\n\n\nIn order to stop CrateDB again, use ctrl-c.\n\nSee Also\n\nConsult the CLI tools documentation for further information about the ./bin/crate command.\n\nPost-install notes\n\nAfter successfully installing CrateDB, for example on your workstation, the web-based Admin UI can be visited at:\n\nhttp://localhost:4200/\n\n\nSee Also\n\nIf you are new to CrateDB, you may want to follow up by taking the guided tour.\n\nAlso, let us outline those information entrypoints as suggestions to explore next:\n\nRead more details about the Configuration.\n\nThe background about Bootstrap checks.\n\nMulti-node configuration within the section about Clustering and Going into production.\n\nWhen operating a CrateDB cluster in production, please also take performance tuning into consideration.\n\nNote\n\nThis kind of installation flavor will let you quickly set up and start a single-node cluster. When adding additional CrateDB nodes, in order to make it form a multi-node cluster, you will need to reset (remove) the cluster state after changing the configuration.\n\nCaution\n\nPlease make sure to read the General Upgrade Guidelines, and the guidelines about rolling upgrades and full restart upgrades, before upgrading a running cluster."
  },
  {
    "title": "Running CrateDB on Windows — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/install/windows.html",
    "html": "Running CrateDB on Windows\n\nHow to use the release archives to run CrateDB on Microsoft Windows.\n\nCaution\n\nWe do not yet officially support CrateDB on Windows for production use. If you would like to deploy CrateDB on Windows, please feel free to contact us so we can work with you on a solution.\n\nDownload the latest CrateDB release archive for Windows.\n\nOnce downloaded, extract the archive either using your favorite terminal or command-line shell or by using a GUI tool like 7-Zip. We recommend using PowerShell when using terminal:\n\n# Extract Zip archive\nunzip -o crate-*.zip\n\n\nOn the terminal, change into the extracted crate directory:\n\ncd crate-*\n\n\nRun a CrateDB single-node instance on the local network interface:\n\n./bin/crate\n\n\nYou will be notified by an INFO message similar to this, when your single-node cluster is started successfully:\n\n[2022-07-04T19:41:12,340][INFO ][o.e.n.Node] [Aiguille Verte] started\n\n\nIn order to stop CrateDB again, use ctrl-c. You will be asked to terminate the job. Input Y:\n\nTerminate batch job (Y/N)? Y\n\n\nSee Also\n\nConsult the CLI tools documentation for further information about the ./bin/crate command.\n\nNote\n\nIf you are installing CrateDB on a recent Windows Server edition, setting up the latest Microsoft Visual C++ 2019 Redistributable package is required. You can download it at msvcrt x86-64, msvcrt x86-32 or msvcrt ARM64.\n\nWithin the terminal, as a Windows user, the prompt after starting PowerShell will look like this.\n\nPS> ./bin/crate\n\nPost-install notes\n\nAfter successfully installing CrateDB, for example on your workstation, the web-based Admin UI can be visited at:\n\nhttp://localhost:4200/\n\n\nSee Also\n\nIf you are new to CrateDB, you may want to follow up by taking the guided tour.\n\nAlso, let us outline those information entrypoints as suggestions to explore next:\n\nRead more details about the Configuration.\n\nThe background about Bootstrap checks.\n\nMulti-node configuration within the section about Clustering and Going into production.\n\nWhen operating a CrateDB cluster in production, please also take performance tuning into consideration.\n\nNote\n\nThis kind of installation flavor will let you quickly set up and start a single-node cluster. When adding additional CrateDB nodes, in order to make it form a multi-node cluster, you will need to reset (remove) the cluster state after changing the configuration.\n\nCaution\n\nPlease make sure to read the General Upgrade Guidelines, and the guidelines about rolling upgrades and full restart upgrades, before upgrading a running cluster."
  },
  {
    "title": "Ad Hoc — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/install/adhoc.html",
    "html": "Ad Hoc\n\nTry CrateDB without installing\n\nIf you want to try out CrateDB on Linux or macOS but would prefer to avoid the hassle of manual installation or extracting release archives, you can get a fresh CrateDB node up and running in your current working directory with a single command:\n\nsh$ bash -c \"$(curl -L https://try.crate.io/)\"\n\n\nNote\n\nThis is a quick way to try out CrateDB. It is not the recommended method to install CrateDB in a durable way. The following sections will outline that method.\n\nPost-install notes\n\nAfter successfully installing CrateDB, for example on your workstation, the web-based Admin UI can be visited at:\n\nhttp://localhost:4200/\n\n\nSee Also\n\nIf you are new to CrateDB, you may want to follow up by taking the guided tour.\n\nAlso, let us outline those information entrypoints as suggestions to explore next:\n\nRead more details about the Configuration.\n\nThe background about Bootstrap checks.\n\nMulti-node configuration within the section about Clustering and Going into production.\n\nWhen operating a CrateDB cluster in production, please also take performance tuning into consideration.\n\nNote\n\nThis kind of installation flavor will let you quickly set up and start a single-node cluster. When adding additional CrateDB nodes, in order to make it form a multi-node cluster, you will need to reset (remove) the cluster state after changing the configuration.\n\nCaution\n\nPlease make sure to read the General Upgrade Guidelines, and the guidelines about rolling upgrades and full restart upgrades, before upgrading a running cluster."
  },
  {
    "title": "CrateDB Cloud Console — CrateDB Cloud",
    "url": "https://cratedb.com/docs/cloud/en/latest/reference/overview.html",
    "html": "CrateDB Cloud Console\n\nThe CrateDB Cloud Console is a hosted web administration interface for interacting with CrateDB Cloud.\n\nNote\n\nRefer to individual items in the current section of the documentation for more information on how to perform specific operations. You can also refer to the glossary for more information on CrateDB Cloud-related terminology.\n\nTable of contents\n\nBasics\n\nOrganization\n\nSettings\n\nOrganization billing\n\nOrganization payment methods\n\nOrganization audit logs\n\nOrganization regions\n\nCluster\n\nOverview\n\nAdmin UI\n\nNext Steps\n\nConnecting to your cluster\n\nQuery Console\n\nImport\n\nImport from URL\n\nImport from private S3 bucket\n\nImport from Azure Blob Storage Container\n\nImporting multiple files\n\nImport from file\n\nSchema evolution\n\nImport Limitations\n\nExport\n\nBackups\n\nCluster Cloning\n\nFailed cloning\n\nSQL Scheduler\n\nScale\n\nManage\n\nCommunity\n\nBasics\n\nThe CrateDB Cloud user interface permalink is the CrateDB Cloud Console. You can deploy a trial cluster on the CrateDB Cloud Console for free.\n\nHere is a list of all currently available regions for CrateDB Cloud:\n\nRegion\n\n\t\n\nUrl\n\n\n\n\nAWS West Europe\n\n\t\n\neks1.eu-west-1.aws.cratedb.cloud\n\n\n\n\nAzure East-US2\n\n\t\n\naks1.eastus2.azure.cratedb.cloud\n\n\n\n\nAzure West Europe\n\n\t\n\naks1.westeurope.azure.cratedb.cloud\n\nAzure East-US2 and Azure West-Europe are managed by Microsoft Azure. The AWS region is managed by AWS and is located in Ireland. Note that the AWS region does not serve the CrateDB Cloud Console directly.\n\nFrom the Cloud Console homepage, you can sign in using a Github, Google, or Microsoft Azure account or by creating a separate username and password.\n\nIf you don’t have a Cloud Console account yet, follow the steps in the signup tutorial. Select the authentication method you wish to use. From there, you will be given the option to sign up.\n\nOnce signed in, you will be presented with the Organization overview.\n\nOrganization\n\nThe organization is the highest structure in your CrateDB Cloud Console. Multiple clusters and users can exist in a organization at any moment. For first-time users, an organization called “My organization” is automatically created upon first login.\n\nTo see a list of all the organizations you have acesss to, go to\n\nthe My Account page in the dropdown menu in the top-right.\n\nThe Organization overview consists of six tabs: Clusters, Settings, Billing, Payment Methods, Audit Logs, and Regions. By default you are brought to the Clusters tab, which provides a quick overview of all your clusters.\n\nIf you are a member of multiple organizations, you can quickly change between them on every tab/page in the Cloud Console. Simply use the dropdown menu at the top-right of the current page/tab:\n\nThe CrateDB Cloud Console is structured on a per-organization basis: all pages and tabs in the Console will display values for the currently selected organization.\n\nSettings\n\nThe Settings tab shows you the name, notification settings, and ID of your currently selected organization.\n\nBy clicking the Edit button next to the organization, you can rename it. Here you can also set the email address for notifications and indicate whether you want to receive them or not.\n\nIt also shows a list of users in your organization. You can add new users by clicking the “Add user” button. You can also choose the role of a new user. To learn more about user roles and their meaning, see the documentation on user roles.\n\nOrganization Billing\n\nThe Billing tab shows all your existing subscriptions, along with which cluster is currently using the subscription. The current accumulated billing snapshot is also visible here, along with additional information:\n\nNote\n\nSubscriptions cannot be deleted in the billing tab. To delete a subscription, please contact support.\n\nOrganization payment methods\n\nThis tab shows all the information about your payment methods. If you have signed up with a credit card for your cluster (the recommended route), your card information overview will be shown here.\n\nIn case you use multiple cards, a default card can be set and cards can be deleted from the list by using the dots icon to the right of the card listing. Click the Add payment method button at the top right to add a new card.\n\nCloud subscription payment methods can also be added here.\n\nOrganization Audit Logs\n\nThis tab shows the Audit Logs of the current organization.\n\nIn the Audit Log, a user with the correct credentials (an organization admin) can see an overview of logged changes to the organization.\n\nOrganization Regions\n\nIn this tab, you will see the available regions for cluster deployment. It is possible to deploy clusters on this screen as well, by clicking the Deploy cluster button under each respective region field.\n\nFor those with access to CrateDB Cloud on Kubernetes, this tab also allows the deployment of CrateDB Cloud on Kubernetes clusters in a custom region. To do so, provide a name for the custom region and click the Create edge region button. Once created, the custom region will appear:\n\nThis field will show a script to set up the dependencies for cluster deployment in the custom region. Apply the script in your local CLI and follow the prompts to proceed. A --help parameter is available within the script for further information.\n\nCluster\n\nThe detailed view of Cluster provides a broad range of relevant data of the selected cluster. It also displays metrics for the cluster. It can be accessed by clicking “View” on the desired cluster in the Clusters tab.\n\nInformation visible on the Overview page includes:\n\nOverview\n\nStatus: Current status of your cluster:\n\nGREEN: Your cluster is healthy.\n\nYELLOW: Some of your tables have under-replicated shards. Please log in to your cluster’s Admin UI to check.\n\nRED: Some of your tables have missing shards. This can happen if you’ve recently restarted a node. The support team is already notified and investigating the issue.\n\nRegion: Name of the region where the cluster is deployed.\n\nPlan: This shows which subscription plan the cluster is running on.\n\nCPU metrics: Average CPU utilization on average per node. The sparkline shows the trend for the last hour.\n\nNumber of nodes: Number of nodes in the cluster.\n\nRAM metric: Percentage of ram used in each node on average. The sparkline shows the trend for the last hour.\n\nStorage metrics: Used and overall storage of the cluster. The sparkline shows the trend for the last hour.\n\nVersion: This indicates the version number of CrateDB the cluster is running.\n\nQuery metric: Queries per second.\n\nAdmin UI\n\nAccess cluster: The Open Admin UI button connects you to the CrateDB Admin UI for the cluster at its unique URL.\n\nNote\n\nThe Cluster URL points to a load balancer that distributes traffic internally to the whole CrateDB cluster. The load balancer closes idle connections after four minutes, therefore client applications that require stateful connections (e.g., JDBC) must be configured to send keep-alive heartbeat queries.\n\nNext Steps\n\nImport Data: Import some data into your cluster using the data import tool.\n\nSee my backups: The “see my backups” will take you to the Backups tab, where you can see all your backups. CrateDB Cloud clusters can now be cloned to a new cluster from any backup.\n\nAPI endpoint: CrateDB Cloud provides a Prometheus-compatible API endpoint for cluster metrics.\n\nFor more information on the CrateDB concepts used here, refer to the CrateDB architecture documentation or the glossary.\n\nConnecting to your cluster\n\nHere you can see a list of snippets for the available clients and libraries. These include: CLI, Python, Ruby, Java, JavaScript, PHP.\n\nQuery Console\n\nThe Query Console enables direct interaction with your CrateDB Cloud cluster and running queries directly from within the Cloud UI.\n\nAccessing and Using the Query Console\n\nThe Query Console can be found in the “Console” tab in the left-hand navigation menu.\n\nTo the left side of the editor panel, there is a tree view that displays all schemas and tables with corresponding columns. The Query Console is able to run multiple queries at once.\n\nOnce you execute a query, the result output can be formatted and exported as CSV or JSON. In order to support self-service query debugging, there is an option to show diagnostic output if an error occurs.\n\nHistory of used queries can be accessed via the “Show history” button on the bottom of the console. Here you can see all the executed queries and copy them or delete them from history.\n\nNote\n\nThe Query Console is only available to organization admins.\n\nImport\n\nThe first thing you see in the “Import” tab is the history of your imports. You can see whether you imported from a URL or from a file, file name, table into which you imported, date, and status. By clicking “Show details” you can display details of a particular import.\n\nClicking the “Import new data” button will bring up the Import page, where you can select the source of your data.\n\nIf you don’t have a dataset prepared, we also provide an example in the URL import section. It’s the New York City taxi trip dataset for July of 2019 (about 6.3M records).\n\nImport from URL\n\nTo import data, fill out the URL, name of the table which will be created and populated with your data, data format, and whether it is compressed.\n\nIf a table with the chosen name doesn’t exist, it will be automatically created.\n\nThe following data formats are supported:\n\nCSV (all variants)\n\nJSON (JSON-Lines, JSON Arrays and JSON Documents)\n\nParquet\n\nGzip compressed files are also supported.\n\nImport from private S3 bucket\n\nCrateDB Cloud allows convenient imports directly from S3-compatible storage. To import a file form bucket, provide the name of your bucket, and path to the file. The S3 Access Key ID, and S3 Secret Access Key are also needed. You can also specify the endpoint for non-AWS S3 buckets. Keep in mind that you may be charged for egress traffic, depending on your provider. There is also a volume limit of 10 GiB per file for S3 imports. The usual file formats are supported - CSV (all variants), JSON (JSON-Lines, JSON Arrays and JSON Documents), and Parquet.\n\nNote\n\nIt’s important to make sure that you have the right permissions to access objects in the specified bucket. For AWS S3, your user should have a policy that allows GetObject access, for example:\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n  {\n      \"Sid\": \"AllowGetObject\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n          \"AWS\": \"*\"\n      },\n      \"Action\": \"s3:GetObject\",\n      \"Resource\": \"arn:aws:s3:::EXAMPLE-BUCKET-NAME/*\"\n  }]\n  }\n\nImport from Azure Blob Storage Container\n\nImporting data from private Azure Blob Storage containers is possible using a stored secret, which includes a secret name and either an Azure Storage Connection string or an Azure SAS Token URL. An admin user at the organization level can add this secret.\n\nYou can specify a secret, a container, a table and a path in the form [/folder/my_file.parquet]\n\nAs with other imports Parquet, CSV, and JSON files are supported. File size limitation for imports is 10 GiB per file.\n\nImporting multiple files\n\nImporting multiple files, also known as import globbing is supported in any s3-complatible blob storage. The steps are the same as if importing from S3, i.e. bucket name, path to the file and S3 ID/Secret.\n\nImporting multiple files from Azure Container/Blob Storage is also supported: /folder/*.parquet\n\nFiles to be imported are specified by using the well-known wildcard notation, also known as “globbing”. In computer programming, glob patterns specify sets of filenames with wildcard characters. The following example would import all the files from the single specified day.\n\n/somepath/AWSLogs/123456678899/CloudTrail/us-east-1/2023/11/12/*.json.gz\n\n\nAs with other imports, the supported file types are CSV, JSON, and Parquet.\n\nImport from file\n\nUploading directly from your computer offers more control over your data. From the security point of view, you don’t have to share the data on the internet just to be able to import it to your cluster. You also have more control over who has access to your data. Your files are temporarily uploaded to a secure location managed by Crate (an S3 bucket in AWS) which is not publicly accessible. The files are automatically deleted after 3 days. You may re-import the same file into multiple tables without having to re-upload it within those 3 days. Up to 5 files may be uploaded at the same time, with the oldest ones being automatically deleted if you upload more.\n\nAs with other import, the supported file formats are:\n\nCSV (all variants)\n\nJSON (JSON-Lines, JSON Arrays and JSON Documents)\n\nParquet\n\nThere is also a limit to file size, currently 1GB.\n\nSchema evolution\n\nSchema Evolution, available for all import types, enables automatic addition of new columns to existing tables during data import, eliminating the need to pre-define table schemas. This feature is applicable to both pre-existing tables and those created during the import process. It can be toggled via the ‘Schema Evolution’ checkbox on the import page.\n\nNote that Schema Evolution is limited to adding new columns; it does not modify existing ones. For instance, if an existing table has an ‘OrderID’ column of type INTEGER, and an import is attempted with Schema Evolution enabled for data where ‘OrderID’ column is of type STRING, the import job will fail due to type mismatch.\n\nImport Limitations\n\nCSV files:\n\nComma, tab and pipe delimiters are supported.\n\nJSON files:\n\nThe following formats are supported for JSON:\n\nJSON Documents. Will insert as a single row in the table.\n\n{\n  \"id\":1,\n  \"text\": \"example\"\n}\n\n\nJSON Arrays. Will insert as a row per array item.\n\n[\n  {\n    \"id\":1,\n    \"text\": \"example\"\n  },\n  {\n    \"id\":2,\n    \"text\": \"example2\"\n  }\n]\n\n\nJSON-Lines. Each line will insert as a row.\n\n{\"id\":1, \"text\": \"example\"}\n{\"id\":2, \"text\": \"example2\"}\n\nExport\n\nThe export tab allows users to download specific tables/views. When you first visit the Export tab, you can specify the name of a table/view, format (CSV, JSON, or Parquet) and whether you’d like your data to be gzip compressed (recommended for CSV and JSON files).\n\nNote\n\nParquet is a highly compressed data format for very efficient storage of tabular data. Please note that for OBJECT and ARRAY columns in CrateDB, the exported data will be JSON encoded when saving to Parquet (effectively saving them as strings). This is due to the complexity of encoding structs and lists in the Parquet format, where determining the exact schema might not be possible. When re-importing such a Parquet file, make sure you pre-create the table with the correct schema.\n\nHistory of your exports is also visible in the Export tab.\n\nNote\n\nExport limitations:\n\nSize limit for exporting is 1 GiB\n\nExports are held for 3 days, then automatically deleted\n\nBackups\n\nYou can find the Backups page in the detailed view of your cluster and you can see and restore all existing backups here.\n\nBy default, a backup is made every hour. The backups are kept for 14 days. We also keep the last 14 backups indefinitely, no matter the state of your cluster.\n\nThe Backups tab provides a list of all your backups. By default, a backup is made every hour.\n\nYou can also control the schedule of your backups by clicking the Edit backup schedule button.\n\nHere you can create a custom schedule by selecting any number of hour slots. Backups will be created at selected times. At least one backup a day is mandatory.\n\nTo restore a particular backup, click the Restore button. A popup window with a SQL statement will appear. Input this statement to your Admin UI console eitheir by copy-pasting it, or clicking the Run query in Admin UI. The latter will bring you directly to the Admin UI console with the statement automatically pre-filled.\n\nYou have a choice between restoring the cluster fully, or only specific tables.\n\nCluster Cloning\n\nCluster cloning is a process of duplicating all the data from a specific snapshot into a different cluster. Creating the new cluster isn’t part of the cloning process, you need to create the target cluster yourself. You can clone a cluster from the Backups page.\n\nChoose a snapshot and click the Clone button. As with restoring a backup, you can choose between cloning the whole cluster, or only specific tables.\n\nNote\n\nKeep in mind that the full cluster clone will include users, views, privileges and everything else. Cloning also doesn’t distinguish between cluster plans, meaning you can clone from CR2 to CR1 or any other variation.\n\nFailed cloning\n\nThere are circumstances under which cloning can fail or behave unexpectedly. These are:\n\nIf you already have tables with the same names in the target cluster as in the source snapshot, the entire clone operation will fail.\n\nThere isn’t enough storage left on the target cluster to accommodate the tables you’re trying to clone. In this case, you might get an incomplete cloning as the cluster will run out of storage.\n\nYou’re trying to clone an invalid or no longer existing snapshot. This can happen if you’re cloning through Croud. In this case, the cloning will fail.\n\nYou’re trying to restore a table that is not included in the snapshot. This can happen if you’re restoring snapshots through Croud. In this case, the cloning will fail.\n\nWhen cloning fails, it is indicated by a banner in the cluster overview screen.\n\nSQL Scheduler\n\nThe SQL Scheduler is designed to automate routine database tasks by scheduling SQL queries to run at specific times, in UTC time. This feature supports creating job descriptions with valid cron patterns and SQL statements, enabling a wide range of tasks. Users can manage these jobs through the Cloud UI, adding, removing, editing, activating, and deactivating them as needed.\n\nUse Cases\n\nDeleting old/redundant data to maintain database efficiency.\n\nRegularly updating or aggregating table data.\n\nAutomating export and import of data.\n\nNote\n\nThe SQL Scheduler is automatically available for all newly deployed clusters.\n\nFor existing clusters, the feature can be enabled on demand. (Contact support for activation.)\n\nAccessing and Using the SQL Scheduler\n\nSQL Scheduler can be found in “SQL Scheduler” tab in the left-hand navigation menu. There are 2 tabs on the SQL Scheduler page:\n\nOverview\n\n\n\nOverview shows a list of your existing jobs. In the list you can activate/deactivate each job with a toggle in the “Active” column. You can also edit and delete jobs with buttons on the right side of the list.\n\nLogs\nExamples\nCleanup of old files\n\n\n\nCleanup tasks represent a common use case for these types of automated jobs. This example deletes records older than 30 days, from a specified table once a day:\n\nDELETE FROM \"sample_data\"\nWHERE\n  \"timestamp_column\" < NOW() - INTERVAL '30 days';\n\n\nHow often you run it of course depends on you, but once a day is common for clean up. This expression runs every day at 2:30 PM UTC:\n\nSchedule: 30 14 * * *\n\nCopying logs into persistent table\n\nNote\n\nLimitations and Known Issues:\n\nOnly one job can run at a time; subsequent jobs will be queued until the current one completes.\n\nLong-running jobs may block the execution of queued jobs, leading to potential delays.\n\nScale\n\nIn the Scale tab, current configuration of your cluster is shown. You can see your current plan, resources of a single node, and overall resources of the cluster.\n\nYou can scale your cluster by clicking the Edit cluster configuration button in the top-right:\n\nNow you can do three different things:\n\nChange the plan of your cluster\n\nIncrease storage on each node\n\nIcrease/decrease the number of nodes\n\nYou can do only one of those operations at a time, i.e. you can’t change plans and scale the number of nodes at the same time.\n\nThe difference in price of the cluster can be seen on the bottom right, when choosing different configurations.\n\nNote\n\nAny promotions or discounts applicable to your cluster will be applied for your organization as a whole at the end of the billing period. Due to technical limitations, they may not be directly visible in the cluster scale pricing shown here, but do not worry! This does not mean that your promotion or discount is not functioning.\n\nWarning\n\nStorage capacity increases for a given cluster are irreversible. To reduce cluster storage capacity, reduce the cluster nodes instead (up to a minimum of 2, although we recommend maintaining a minimum of 3 for production use).\n\nManage\n\nThe manage tab contains credentials settings, deletion protection, upgrades, IP allowlist, private links, suspend cluster, and delete cluster options.\n\nCredentials - These are the username and password used for accessing the Admin UI of your cluster. Username is always admin and the password can be changed.\n\nDeletion protection - While this is enabled, your cluster cannot be deleted.\n\nUpgrade CrateDB - Here you can enable the CrateDB version running on your cluster.\n\nIP Allowlist - By using the IP allowlisting feature, you can restrict access to your cluster to an indicated IP address or CIDR block. Click the blue Add Address button and you can fill out an IP address or range and give it a meaningful description. Click Save to store it or the bin icon to delete a range. Keep in mind that once IP allowlisting has been set, you cannot access the Admin UI for that cluster from any other address.\n\nIf no allowlist address or address range is set, the cluster is publicly accessible by default. (Of course, the normal authentication procedures are always required.) Only an org admin can change the allowlist.\n\nPrivate links - A private endpoint, or private link, is a mechanism that allows a secure, private connection to your cluster. Effectively, it allows you to bypass the public internet when accessing the environment where your cluster is deployed. Note that private endpoints don’t work accross providers, meaning that if you want to securely access your AWS cluster, you must do so from within the AWS environment.\n\nSuspend cluster Cluster suspension is a feature that enables you to temporarily pause your cluster while retaining all its data. An example situation might be that the project you’re working on has been put on hold. The cost of running a cluster is split into two parts: Compute and Storage. The benefit here is that while the cluster is suspended, you are only charged for the storage.\n\nDelete cluster All cluster data will be lost on deletion. This action cannot be undone.\n\nCommunity\n\nThe Community link goes to the CrateDB and CrateDB Cloud Community page. Here you can ask members of the community and Crate.io employees questions about uncertainties or problems you are having when using our products."
  },
  {
    "title": "CrateDB on Red Hat, SUSE, and Derivates — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/install/redhat.html",
    "html": "CrateDB on Red Hat, SUSE, and Derivates\n\nInstall CrateDB RPM packages using the YUM package manager.\n\nThis installation method is suitable for RedHat Enterprise Linux (RHEL) and compatible systems like Fedora, CentOS, Rocky Linux, AlmaLinux, AWS Linux, Oracle Linux, or Scientific Linux. Installation also works on openSUSE and SUSE Linux Enterprise Server (SLES) systems.\n\nConfigure package repository\n\nYou will need to configure your system to register with and trust packages from the CrateDB package repository:\n\n# Install prerequisites.\nyum install sudo\n\n# Import the public GPG key for verifying the package signatures.\nsudo rpm --import https://cdn.crate.io/downloads/yum/RPM-GPG-KEY-crate\n\n# Register with the CrateDB package repository.\nsudo rpm -Uvh https://cdn.crate.io/downloads/yum/7/x86_64/crate-release-7.0-1.x86_64.rpm\n\n\nThe command above will install the /etc/yum.repos.d/crate.repo package repository configuration file.\n\nNote\n\nCrateDB provides both stable release and testing release channels. You can read more about the release workflow.\n\nBy default, yum (Red Hat’s package manager) will use the stable repository. This is because the testing repository is disabled. If you would like to enable the testing repository, edit the crate.repo file and set enabled=1 within the [crate-testing] section.\n\nInstall CrateDB\n\nWith everything set up, you can install CrateDB:\n\nsudo yum install crate\n\nConfigure CrateDB\n\nPlease visit the Configuration Settings documentation section to learn about the location and meaning of CrateDB’s configuration files.\n\nControl CrateDB on Linux\n\nYou can control the crate service with the systemctl utility program:\n\nsudo systemctl COMMAND crate\n\n\nReplace COMMAND with start, stop, restart, status and so on.\n\nNotes\n\nAfter the installation is finished, the crate service should be installed, but may not be configured to start automatically. Use the following command to start CrateDB:\n\nsudo systemctl start crate\n\n\nIn order to make the service reboot-safe, invoke:\n\nsudo systemctl enable crate\n\nPost-install notes\n\nAfter successfully installing CrateDB, for example on your workstation, the web-based Admin UI can be visited at:\n\nhttp://localhost:4200/\n\n\nSee Also\n\nIf you are new to CrateDB, you may want to follow up by taking the guided tour.\n\nAlso, let us outline those information entrypoints as suggestions to explore next:\n\nRead more details about the Configuration.\n\nThe background about Bootstrap checks.\n\nMulti-node configuration within the section about Clustering and Going into production.\n\nWhen operating a CrateDB cluster in production, please also take performance tuning into consideration.\n\nNote\n\nThis kind of installation flavor will let you quickly set up and start a single-node cluster. When adding additional CrateDB nodes, in order to make it form a multi-node cluster, you will need to reset (remove) the cluster state after changing the configuration.\n\nCaution\n\nPlease make sure to read the General Upgrade Guidelines, and the guidelines about rolling upgrades and full restart upgrades, before upgrading a running cluster."
  },
  {
    "title": "CrateDB on Debian, Ubuntu, and Derivates — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/install/debian-ubuntu.html",
    "html": "CrateDB on Debian, Ubuntu, and Derivates\n\nInstall CrateDB deb packages using the apt package manager.\n\nThis installation method is suitable for Debian systems and derivates like Ubuntu.\n\nConfigure package repository\n\nYou will need to configure your system to register with and trust packages from the CrateDB package repository:\n\n# Install prerequisites.\nsudo apt update\nsudo apt install --yes apt-transport-https apt-utils curl gnupg lsb-release\n\n# Import the public GPG key for verifying the package signatures.\ncurl -sS https://cdn.crate.io/downloads/debian/DEB-GPG-KEY-crate | \\\n    sudo tee /etc/apt/trusted.gpg.d/cratedb.asc\n\n# Add CrateDB repository to Apt\necho \"deb https://cdn.crate.io/downloads/debian/stable/ default main\" | \\\n    sudo tee /etc/apt/sources.list.d/crate-stable.list\n\n\nNote\n\nCrateDB provides two repositories. A stable and a testing repository. To use the testing repository, replace stable with testing in the command above. You can read more about our release workflow.\n\nNow, update the package sources:\n\nsh$ sudo apt update\n\n\nYou should see a success message. This indicates that the CrateDB package repository is correctly registered.\n\nInstall CrateDB\n\nWith everything set up, you can install CrateDB:\n\nsh$ sudo apt install crate\n\n\nAfter the installation is finished, you can start the crate service:\n\nsh$ sudo systemctl start crate\n\n\nOnce the service is up and running, you can access CrateDB by visiting:\n\nhttp://localhost:4200/\n\nConfigure CrateDB\n\nPlease visit the Configuration Settings documentation section to learn about the location and meaning of CrateDB’s configuration files.\n\nControl CrateDB on Linux\n\nYou can control the crate service with the systemctl utility program:\n\nsudo systemctl COMMAND crate\n\n\nReplace COMMAND with start, stop, restart, status and so on.\n\nNotes\n\nAfter the installation is finished, the crate service should be installed, but may not be configured to start automatically. Use the following command to start CrateDB:\n\nsudo systemctl start crate\n\n\nIn order to make the service reboot-safe, invoke:\n\nsudo systemctl enable crate\n\nPost-install notes\n\nAfter successfully installing CrateDB, for example on your workstation, the web-based Admin UI can be visited at:\n\nhttp://localhost:4200/\n\n\nSee Also\n\nIf you are new to CrateDB, you may want to follow up by taking the guided tour.\n\nAlso, let us outline those information entrypoints as suggestions to explore next:\n\nRead more details about the Configuration.\n\nThe background about Bootstrap checks.\n\nMulti-node configuration within the section about Clustering and Going into production.\n\nWhen operating a CrateDB cluster in production, please also take performance tuning into consideration.\n\nNote\n\nThis kind of installation flavor will let you quickly set up and start a single-node cluster. When adding additional CrateDB nodes, in order to make it form a multi-node cluster, you will need to reset (remove) the cluster state after changing the configuration.\n\nCaution\n\nPlease make sure to read the General Upgrade Guidelines, and the guidelines about rolling upgrades and full restart upgrades, before upgrading a running cluster."
  },
  {
    "title": "The CrateDB Guide — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/index.html",
    "html": "The CrateDB Guide\n\nGuides and tutorials about how to use CrateDB and CrateDB Cloud in practice.\n\nCrateDB is a distributed and scalable SQL database for storing and analyzing massive amounts of data in near real-time, even with complex queries. It is PostgreSQL-compatible, and based on Lucene.\n\nInstallation\n\nInstalling CrateDB\nGetting Started\n\nGetting started with CrateDB\nAdministration\n\nCrateDB Administration\nPerformance Guides\n\nCrateDB Performance Guides\nApplication Domains\n\nLearn how to apply CrateDB’s features to optimally cover use-cases in different application and topic domains.\n\nDocument Store\n\nStoring JSON documents using CrateDB’s `OBJECT` data type\nFull-Text Search\n\nAbout CrateDB’s full-text search capabilities\nIndustrial Data\n\nCrateDB in industrial / IIoT / Industry 4.0 scenarios\nTime Series Data\n\nManaging Time Series Data with CrateDB\nMachine Learning\n\nMachine Learning with CrateDB\nIntegrations\n\nLearn how to use CrateDB with 3rd-party software applications, libraries, and frameworks.\n\nETL\n\nLoad and export data into/from CrateDB\nMetrics\n\nCrateDB with metrics collection agents, brokers, and stores\nData Visualization\n\nData visualization with CrateDB\nBusiness Intelligence\n\nAnalyse information with CrateDB\nMachine Learning\n\nMachine Learning with CrateDB\nSoftware Testing\n\nSoftware testing with CrateDB\nReference Architectures\n\nReference architectures illustrating how CrateDB can be used in a variety of use-cases.\n\nReference Architectures\n\nReference Architectures with CrateDB\n\nTip\n\nPlease also visit the Overview of CrateDB integration tutorials.\n\nSee Also\n\nCrateDB and its documentation are open source projects. Contributions to the pages in this section and subsections are much appreciated. If you can spot a flaw, or would like to contribute additional content, you are most welcome.\n\nYou will find corresponding links within the topmost right navigation element on each page, linking to the relevant page where this project is hosted on GitHub."
  },
  {
    "title": "Search — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/search.html",
    "html": "Search\n\nFrom here you can search these documents. Enter your search words into the box below and click \"search\". Note that the search function will automatically search for all of the words. Pages containing fewer words won't appear in the result list.\n\nSearch"
  },
  {
    "title": "CrateDB Ecosystem Catalog — CrateDB: Clients and Tools",
    "url": "https://cratedb.com/docs/crate/clients-tools/en/latest/index.html#index",
    "html": "CrateDB Ecosystem Catalog\n\nDatabase drivers, libraries, frameworks, and applications for CrateDB.\n\nAbout CrateDB\n\nCrateDB is a distributed and scalable open-source SQL database based on Lucene, with PostgreSQL compatibility. CrateDB clusters store information in the range of billions of records, and terabytes of data, and run analytics in near real time, even with complex queries. CrateDB can be used for enterprise data warehouse workloads, it works across clouds and scales with your data.\n\nConnectivity\n\nThe canonical set of database drivers, client- and developer-applications, and how to configure them to connect to CrateDB.\n\nJust to name a few, the sections below are about the CrateDB Admin UI, the Crash CLI terminal program, connecting with PostgreSQL’s psql client, the DataGrip, and DBeaver IDE applications, the Java/JDBC/Python drivers, the SQLAlchemy and Flink dialects, and more.\n\n IDE\n\nConnect to CrateDB using a database IDE like DataGrip or DBeaver.\n\n CLI\n\nConnect to CrateDB using command-line based terminal programs.\n\n Drivers\n\nList of HTTP and PostgreSQL client drivers, and tutorials.\n\n DataFrame Libraries\n\nConnectivity with DataFrame libraries like pandas and Dask.\n\n ORM Libraries\n\nConnectivity with ORM libraries like SQLAlchemy.\n\nIntegrations\n\nCrateDB integrates with a diverse set of applications and tools concerned with analytics, visualization, and data wrangling, in the areas of data loading and export (ETL), business intelligence (BI), metrics aggregation and monitoring, machine learning, and more.\n\n Overview\n\nLearn how to use CrateDB with popular applications, frameworks, and tools. All on one page.\n\n ETL\n\nETL applications and frameworks for transferring data in and out of CrateDB.\n\n System Metrics\n\nIntegrations and long-term storage for systems monitoring tools like Prometheus and Telegraf.\n\n Data Visualization\n\nVisualize information in your CrateDB cluster.\n\n Business Intelligence\n\nAnalyze information in your CrateDB cluster.\n\n Machine Learning\n\nAdapters and integrations with machine learning frameworks.\n\nNote\n\nContributions to the pages in this section and subsections are much welcome. If you would like to add items about integrations with other tools to this documentation section, please get in touch, or directly edit this page on GitHub. You will find corresponding links within the topmost right navigation element.\n\nSee Also\n\nLooking for the previous content on this page? Visit [Legacy] CrateDB Clients and Tools."
  },
  {
    "title": "Cloud Reference — CrateDB Cloud",
    "url": "https://cratedb.com/docs/cloud/en/latest/reference/index.html",
    "html": "Cloud Reference\n\nCrateDB Cloud is the fully-managed cloud database as a service by CrateDB. With CrateDB Cloud you can deploy, monitor, back up, and scale your clusters in the cloud – without needing to worry about database management.\n\nCrateDB is a distributed, open-source database that combines the performance of NoSQL with the power and simplicity of standard SQL.\n\nNote\n\nThis is an open source documentation project. We host the source code and issue tracker on GitHub."
  },
  {
    "title": "The CrateDB Admin UI — CrateDB: Admin UI",
    "url": "https://cratedb.com/docs/crate/admin-ui/en/latest/index.html#index",
    "html": "latest\nThe CrateDB Admin UI\n\nCrateDB ships with a web administration user interface (or Admin UI).\n\nThe CrateDB Admin UI runs on every CrateDB node. You can use it to inspect and interact with the whole CrateDB cluster in a number of ways.\n\nSee Also\n\nThe CrateDB Admin UI is an open source project and is hosted on GitHub.\n\nTable of contents\n\nConnecting\n\nNavigating\n\nStatus bar\n\nTabs\n\nConnecting\n\nYou can access the Admin UI via HTTP on port 4200:\n\nhttp://HOSTNAME:4200/\n\n\nReplace HOSTNAME with the hostname of the CrateDB node. If CrateDB is running locally, this will be localhost.\n\nNavigate to this URL in a web browser.\n\nTip\n\nIf you access port 4200 via a client library or command-line tool like curl or wget, the request will be handled by the CrateDB Rest API, and the response will be in JSON.\n\nNavigating\n\nThis is what the Admin UI looks like when it first loads:\n\nTake note of the status bar (at the top) and the tabs (down the left side).\n\nStatus bar\n\nAlong the top of the screen, from left to right, the status bar shows:\n\nCluster name\n\nCrateDB version\n\nNumber of nodes in the cluster\n\nHealth checks\n\nData status\n\nGreen – All data is replicated and available\n\nYellow – Some records are unreplicated\n\nRed – Some data is unavailable\n\nCluster status:\n\nGreen – Good configuration\n\nYellow – Some configuration warnings\n\nRed – Some configuration errors\n\nAverage cluster load (for the past 1 minute, 5 minutes, and 15 minutes)\n\nSettings and notifications menu\n\nTabs\n\nOn the left-hand side, from top to bottom, the tabs are:\n\nOverview screen\n\nSQL console\n\nTables browser\n\nViews browser\n\nShards browser\n\nCluster browser\n\nMonitoring overview\n\nPrivileges browser\n\nHelp screen"
  },
  {
    "title": "CrateDB Cloud on Kubernetes — CrateDB Cloud",
    "url": "https://cratedb.com/docs/cloud/en/latest/tutorials/edge/index.html",
    "html": "CrateDB Cloud on Kubernetes\n\nCrateDB Cloud on Kubernetes is the hybrid cloud database solution integrating CrateDB clusters and the CrateDB Cloud software stack with on-premise or customer-controlled cloud infrastructure.\n\nIntroduction\n\nThe CrateDB Cloud on Kubernetes concept is simple. You bring your own Kubernetes infrastructure - whether in a production site, office, laboratory, or local setup, or in your existing managed cloud infrastructure on AWS, Azure, or GCP.\n\nWherever it may be located, we provide the full experience of CrateDB Cloud to that Kubernetes environment. You keep your existing infrastructure setup and you get all the benefits of CrateDB Cloud on top: from quick deployment to seamless scaling and easy cluster management.\n\nDetails\n\nThe process of getting CrateDB Cloud on Kubernetes running is supported by the CrateDB Cloud Console.\n\nEven so, there are some steps involved, and some requirements have to be met in order for it to work. This section of the documentation intends to serve as an end-to-end walkthrough of the corresponding process and prerequisites.\n\n(cloud-on-kubernetes-tutorials)\n\nTutorials\n\nIn these tutorials, we first introduce the signup and configuration process for a local Kubernetes installation. Next, we explain the process end-to-end for using AKS and EKS services. After that, we outline the installation method for some lightweight Kubernetes distributions, like K3s and Microk8s.\n\nThen, we explain how to set up a custom backup location, so you can keep full ownership of your data backups.\n\nWe also introduce a way to monitor your CrateDB Cloud on Kubernetes cluster in the visualization tool Grafana coupled with Loki and Prometheus.\n\n Introduction\n\nFind about about hardware and software prerequisites for running CrateDB Cloud on Kubernetes.\n\n Managed Kubernetes\n\nIn this section, we provide more specific installation instructions for some managed Kubernetes providers.\n\n Self-hosted Kubernetes\n\nIn this section, we outline installation instructions for some third-party supported self-hosted options.\n\n Custom backup location\n\nThis guide introduces a feature of CrateDB Cloud on Kubernetes that lets users specify their backup location.\n\n Monitoring\n\nLearn how to monitor CrateDB Cloud on Kubernetes using Prometheus, Loki, and Grafana."
  },
  {
    "title": "CrateDB Reference — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/",
    "html": "5.6\nCrateDB Reference\n\nCrateDB is a distributed SQL database that makes it simple to store and analyze massive amounts of machine data in real-time.\n\nNote\n\nThis resource assumes you know the basics. If not, check out the Tutorials section for beginner material.\n\nSee Also\n\nCrateDB is an open source project and is hosted on GitHub.\n\nTable of contents\n\nConcepts\nJoins\nClustering\nStorage and consistency\nResiliency\nCLI tools\ncrate\ncrate-node\nConfiguration\nNode-specific settings\nCluster-wide settings\nSession settings\nLogging\nEnvironment variables\nGeneral use\nData definition\nData manipulation\nQuerying\nBuilt-in functions and operators\nUser-defined functions\nBlobs\nOptimistic Concurrency Control\nInformation schema\nAdministration\nSystem information\nRuntime configuration\nUsers and roles management\nPrivileges\nAuthentication\nSecured communications (SSL/TLS)\nOptimization\nJobs management\nJMX monitoring\nSnapshots\nLogical replication\nCloud discovery\nUsage Data Collector\nSQL syntax\nGeneral SQL\nSQL Statements\nClient interfaces\nHTTP endpoint\nPostgreSQL wire protocol\nAppendices\nRelease Notes\nSQL compatibility\nSQL standard compliance\nResiliency Issues\nGlossary"
  },
  {
    "title": "How-To Guides — CrateDB Cloud",
    "url": "https://cratedb.com/docs/cloud/en/latest/howtos/index.html",
    "html": "How-To Guides\n\nThis is a collection of how-to guides for CrateDB Cloud. Learn how to manage your cluster and organizations below. For the purposes of these guides, we assume you interact with CrateDB Cloud by using the CrateDB Cloud Console. Alternatively, you can use the command-line interface CrateDB Cloud CLI, aka. Croud.\n\nNote\n\nNot all operations supported by the CrateDB Cloud Console are also available via Croud. Most importantly, clusters can only be deployed via the CrateDB Cloud Console.\n\nOrganization Management\n Create Organization\n\nLearn how to create new organization with your account.\n\n Add Users to Organization\n\nLearn how to add users to organizations and manage their privileges.\n\nCluster Management\n Reconfigure Cluster\n\nLearn how to scale your cluster up or down, depending on your needs.\n\n Suspend Cluster\n\nLearn how to temporarily suspend your cluster. You are only charged for storage while in suspended state.\n\n Delete Cluster\n\nLearn how to permanently delete your cluster along with your data.\n\n Restoring Backups\n\nLearn how backups are made and how to restore them.\n\n Logical Replication\n\nLogical replication is a mechanism by which data can automatically be copied across multiple clusters.\n\n Private Endpoints\n\nA private endpoint, or private link, is a mechanism that allows a secure, private connection to your cluster.\n\n Clients, Tools, and Integrations\n\nLearn about compatible client applications and tools, and how to configure your favorite client library to connect to a CrateDB cluster.\n\nNote\n\nThis is an open source documentation project. We host the source code and issue tracker on GitHub ."
  },
  {
    "title": "Deploy cluster with Croud — CrateDB Cloud",
    "url": "https://cratedb.com/docs/cloud/en/latest/tutorials/deploy/croud.html",
    "html": "Deploy cluster with Croud\n\nThis tutorial will outline a step-by-step guide to deploying a cluster using the Cloud CLI application from scratch. The walkthrough assumes you have completed the signup process successfully, and that the croud program is installed on your system.\n\n Croud Reference Documentation\n\nVisit full reference documentation to learn more about Croud.\n\nCurrently, it is not possible to create new subscriptions using Croud, so you will need to use the CrateDB Cloud Console UI.\n\nThe payment processing and billing is powered by Stripe. It is also possible to subscribe using Azure and AWS.\n\nWarning\n\nBecause of the current implementation of subscriptions, it is NOT recommended to deploy your first cluster with Croud. You need to have an existing subscription to deploy a cluster using Croud. To create a subscription, use UI.\n\nDeploy a CrateDB cluster using the CrateDB Cloud Web Console.\n\nLog in\n\nWhen working with Croud, the first step is always logging into your account.\n\nCroud, being a CLI application, is operated using commands. Commands should be issued using the following format:\n\nsh$ croud [COMMAND] [OPTIONS]\n\n\nTo log in execute this command:\n\nsh$ croud login\n\n\nThis will open a browser window where you will be prompted for your credentials. You can also log in using azuread, github, or google using the --idp argument. See full Authentication documentation for details.\n\nAfter successfully logging in, you will see this prompt in your browser:\n\nYou have successfully logged into CrateDB Cloud!\nThis window can be closed.\n\nCreate organization\n\nThe first step of deployment is creating an organization which will contain your cluster. If you’ve registered recently, or didn’t delete the organization that was created automatically, you can skip this step:\n\nsh$ croud organizations create --name samplecroudorganization\n\n\nThis will create an organization called “samplecroudorganization”.\n\n(croud-create-project)\n\nCreate project\n\nThe next step is to create a project in your organization. To create a new project execute this command:\n\nsh$ croud projects create --name sampleproject\n\n\nThis will create a new project named sampleproject.\n\n(croud-deploy-cluster)\n\nDeploy cluster\n\nWhen deploying a cluster, these are the required arguments:\n\n--product-name\n\nThe product name to use.\n\n--tier\n\nThe product tier to use.\n\n-p, --project-id\n\nThe project ID to use.\n\n--cluster-name\n\nThe CrateDB cluster name to use.\n\n--version\n\nThe CrateDB version to use.\n\n--username\n\nThe CrateDB username to use.\n\n--password\n\nThe CrateDB password to use.\n\n--subscription-id\n\nThe CrateDB subscription to use.\n\nExample\nsh$ croud clusters deploy /\n  --product-name cr1 /\n  --tier default /\n  --cluster-name my-crate-cluster /\n  --project-id 952cd102-91c1-4837-962a-12ecb71a6ba8 /\n  --version 4.8.1 /\n  --username admin /\n  --password \"as6da9ddasfaad7i902jcv780dmcba\" /\n  --subscription-id 782dfc00-7b25-4f48-8381-b1b096dd1619\n\n\n+--------------------------------------+------------------------+----------+--------------+--------------------------------------+-------------+--------------------------------------------------+\n| id                                   | name                   | numNodes | crateVersion | projectId                            | username    | fqdn                                             |\n|--------------------------------------+------------------------+----------+--------------+--------------------------------------+-------------+--------------------------------------------------|\n| 8d6a7c3c-61d5-11e9-a639-34e12d2331a1 | my-crate-cluster |        1 | 4.8.1        | 952cd102-91c1-4837-962a-12ecb71a6ba8 | admin       | my-crate-cluster.eastus.azure.cratedb.net. |\n+--------------------------------------+------------------------+----------+--------------+--------------------------------------+-------------+--------------------------------------------------+\n==> Info: Cluster creation initiated. It may take a few minutes to complete.\n==> Info: Status: REGISTERED (Your creation request was received and is pending processing.)\n==> Info: Status: IN_PROGRESS (Cluster creation started. Waiting for the node(s) to be created and creating other required resources.)\n==> Success: Operation completed.\n+--------------------------------------+------------------------+----------+--------------+--------------------------------------+-------------+--------------------------------------------------+\n| id                                   | name                   | numNodes | crateVersion | projectId                            | username    | fqdn                                             |\n|--------------------------------------+------------------------+----------+--------------+--------------------------------------+-------------+--------------------------------------------------|\n| 8d6a7c3c-61d5-11e9-a639-34e12d2331a1 | my-crate-cluster |        1 | 4.8.1        | 952cd102-91c1-4837-962a-12ecb71a6ba8 | admin       | my-crate-cluster.eastus.azure.cratedb.net. |\n+--------------------------------------+------------------------+----------+--------------+--------------------------------------+-------------+--------------------------------------------------+\n\n\nNote\n\nParameters tips:\n\nThe minimum length of a password is 24 characters.\n\nTo see all the available products issue: croud products list.\n\nTo find out your project-id issue: croud projects list.\n\nTo find out your subscription-id issue: croud subscriptions list.\n\nNote\n\nComplete documentation on clusters in Croud, including all the deployment arguments, can be found here.\n\nDeploying a cluster with a specific version\n\nYou may want to deploy a cluster with a version other than the latest. You can do this using the --version parameter. Historical Stable and Testing or Nightly builds are all public and available to any user. To use the Nightly/Testing channels, you must use the --channel parameter.\n\nsh$ croud clusters deploy --product-name cr1 /\n  --tier default /\n  --cluster-name my-crate-cluster /\n  --project-id 3ac44505-1d6e-419c-ad23-5d0d572915ba /\n  --version 5.2.0 /\n  --username admin /\n  --password \"as6da9ddasfaad7i902jcv780dmcba\" /\n  --subscription-id 3a35974f-5319-47fb-9a1f-ab85dca75c86 /\n  --channel testing\n\n\nThis command deploys a 5.2.0 version cluster.\n\nAlternatively, you can deploy a cluster with an older version, like this:\n\nsh$ croud clusters deploy /\n  --product-name cr1 /\n  --tier default /\n  --cluster-name my-crate-cluster /\n  --project-id f76d96aa-f1a7-46aa-a89b-8cdd2b3cef15 /\n  --version 4.8.0 /\n  --username admin /\n  --password \"as6da9ddasfaad7i902jcv780dmcba\" /\n  --subscription-id 3a35974f-5319-47fb-9a1f-ab85dca75c86\n\nScale cluster\n\nClusters can be scaled at any time, this allows you to add more nodes or more storage to your cluster.\n\nScaling\n\nRequired arguments:\n\n--cluster-id\n\nThe CrateDB cluster ID to use.\n\n--unit\n\nThe product scale unit to use. This parameter SETS the amount of nodes. I.e.\n\n--unit 0 means 1 node\n\n--unit 1 means 2 nodes\n\n--unit 2 means 3 nodes etc.\n\nThis allows you to scale the performance of your cluster up or down.\n\nExample:\n\nsh$ croud clusters scale \\\n  --project-id 952cd102-91c1-4837-962a-12ecb71a6ba8 \\\n  --cluster-id 8d6a7c3c-61d5-11e9-a639-34e12d2331a1 \\\n  --unit 1\n\n+--------------------------------------+------------------------+----------+\n| id                                   | name                   | numNodes |\n|--------------------------------------+------------------------+----------|\n| 8d6a7c3c-61d5-11e9-a639-34e12d2331a1 | my-crate-cluster |        1 |\n+--------------------------------------+------------------------+----------+\n==> Info: Cluster scaling initiated. It may take a few minutes to complete the changes.\n==> Info: Status: SENT (Your scaling request was sent to the region.)\n==> Info: Status: IN_PROGRESS (Scaling up from 1 to 2 nodes. Waiting for new\nnode(s) to be present.)\n==> Success: Operation completed.\n+--------------------------------------+------------------------+----------+\n| id                                   | name                   | numNodes |\n|--------------------------------------+------------------------+----------|\n| 8d6a7c3c-61d5-11e9-a639-34e12d2331a1 | my-crate-cluster |        2 |\n+--------------------------------------+------------------------+----------+\n\nStorage expansion\n\nRequired arguments:\n\n--cluster-id\n\nThe CrateDB cluster ID to use.\n\n--disk-size-gb\n\nNew size of attached disks (in GiB). This parameter sets the storage to the size specified in parameter. It is not possible to reduce storage.\n\nExample:\n\nsh$ croud clusters expand-storage \\\n  --cluster-id 8d6a7c3c-61d5-11e9-a639-34e12d2331a1 \\\n  --disk-size-gb 512\n+--------------------------------------+------------------------+------------------------------------+\n| id                                   | name                   | hardware_specs                     |\n|--------------------------------------+------------------------+------------------------------------|\n| 8d6a7c3c-61d5-11e9-a639-34e12d2331a1 | my-crate-cluster | Disk size: 256.0 GiB               |\n+--------------------------------------+------------------------+------------------------------------+\n==> Info: Cluster storage expansion initiated. It may take a few minutes to complete the changes.\n==> Info: Status: REGISTERED (Your storage expansion request was received and is pending processing.)\n==> Info: Status: SENT (Your storage expansion request was sent to the region.)\n==> Info: Status: IN_PROGRESS (Suspending cluster and waiting for Persistent Volume Claim(s) to be resized.)\n==> Info: Status: IN_PROGRESS (Starting cluster. Scaling back up to 3 nodes. Waiting for node(s) to be present.)\n==> Success: Operation completed.\n+--------------------------------------+------------------------+------------------------------------+\n| id                                   | name                   | hardware_specs                     |\n|--------------------------------------+------------------------+------------------------------------|\n| 8d6a7c3c-61d5-11e9-a639-34e12d2331a1 | my-crate-cluster | Disk size: 512.0 GiB               |\n+--------------------------------------+------------------------+------------------------------------+\n\n\nWarning\n\nWhen increasing storage size of a cluster, it is temporarily stopped, while the operation finishes.\n\nNote\n\nFor all available arguments for the scaling command, see the cluster scale and cluster storage expansion documentation."
  },
  {
    "title": "Services — CrateDB Cloud",
    "url": "https://cratedb.com/docs/cloud/en/latest/reference/services.html",
    "html": "Services\n\nIn the realm of CrateDB Cloud services, understanding your options is crucial for optimizing both performance and costs. This section of the documentation provides an in-depth look at the various service plans we offer, catering to a wide range of use-cases from small-scale applications to enterprise-level deployments. Our service plans are engineered for scalability, reliability, and performance.\n\nShared\n\nnon-critical workloads\n\nSingle Node\n\nUp to 8 shared vCPUs\n\nUp to 12 GiB RAM\n\nUp to 1 TiB storage\n\nBackups (once per day)\n\nDevelopment support\n\nDedicated\n\nproduction workloads\n\nUp to 9 Nodes\n\nUp to 144 vCPUs\n\nUp to 495 GiB RAM\n\nUp to 72 TiB storage\n\nBackups (once per hour)\n\nBasic Support\n\nAWS / Azure Private Link\n\nUptime SLAs\n\nPremium support available\n\nCustom\n\nlarge production workloads\n\nAny cluster size\n\nCustom compute options\n\nDedicated master nodes\n\nUnlimited Storage\n\nCustom Backups\n\nPremium Support\n\nAWS / Azure Private Link\n\nUptime SLAs\n\n\n\n\nContact us for more information\n\nShared\n\nCrateDB Cloud’s Shared Plan provides an affordable and easy-to-setup option for users who require basic database functionalities. The plan is built around the principle of cost-effectiveness and is particularly well-suited for development, testing, or non-critical production environments.\n\nNode sizes\n\nPlan\n\n\t\n\nSize\n\n\t\n\nvCPUs\n\n\t\n\nRAM\n\n\t\n\nStorage\n\n\n\n\nShared\n\n\t\n\nCRFREE*\n\n\t\n\nup to 2\n\n\t\n\n2 GiB\n\n\t\n\n8 GiB\n\n\n\n\nShared\n\n\t\n\nS2\n\n\t\n\nup to 2\n\n\t\n\n2 GiB\n\n\t\n\n8 GiB to 1TiB\n\n\n\n\nShared\n\n\t\n\nS4\n\n\t\n\nup to 3\n\n\t\n\n4 GiB\n\n\t\n\n8 GiB to 1TiB\n\n\n\n\nShared\n\n\t\n\nS6\n\n\t\n\nup to 4\n\n\t\n\n6 GiB\n\n\t\n\n8 GiB to 1TiB\n\n\n\n\nShared\n\n\t\n\nS12\n\n\t\n\nup to 8\n\n\t\n\n12 GiB\n\n\t\n\n8 GiB to 1TiB\n\nVariable Performance\nSince your cluster will be sharing vCPUs with other clusters, the performance may vary depending on the overall load on the underlying machine. This variability makes it less predictable compared to Dedicated Plans, where your database is running on dedicated resources.\n\nFair-Use Principle\nThe Shared Plan operates on a fair-use principle. All users are expected to utilize the shared resources responsibly so that the system remains equitable and functional for everyone.\n\nNote\n\n*CRFREE: This plan is aimed at new users who want to test and evaluate CrateDB Cloud and is perpetually free to use. Every user can deploy one free tier cluster in their organization without adding a payment method. Free tier clusters will be suspended if they are not used for 7 days, and deleted after 7 more days of inactivity. They cannot be scaled or changed.\n\nDedicated\n\nCrateDB Cloud’s Dedicated Plan is designed to provide robust, scalable, and high-performance database solutions. Unlike the Shared Plan, the Dedicated Plan offers dedicated resources, including dedicated vCPUs, to meet the demands of high-availability and high-throughput environments.\n\nNode sizes\n\nPlan\n\n\t\n\nSize\n\n\t\n\nvCPUs\n\n\t\n\nRAM\n\n\t\n\nStorage\n\n\n\n\nDedicated\n\n\t\n\nCR1\n\n\t\n\n2\n\n\t\n\n7 GiB\n\n\t\n\n32 GiB to 8 TiB\n\n\n\n\nDedicated\n\n\t\n\nCR2\n\n\t\n\n4\n\n\t\n\n14 GiB\n\n\t\n\n32 GiB to 8 TiB\n\n\n\n\nDedicated\n\n\t\n\nCR3\n\n\t\n\n8\n\n\t\n\n28 GiB\n\n\t\n\n32 GiB to 8 TiB\n\n\n\n\nDedicated\n\n\t\n\nCR4\n\n\t\n\n16\n\n\t\n\n55 GiB\n\n\t\n\n32 GiB to 8 TiB\n\nAll Dedicated Plans can be scaled from 1 to 9 nodes. Depeding on the number of nodes, the overall cluster size can be scaled up to the following limits:\n\nCluster sizes\n\nPlan\n\n\t\n\nSize\n\n\t\n\nvCPUs\n\n\t\n\nRAM\n\n\t\n\nStorage\n\n\n\n\nDedicated\n\n\t\n\nCR1\n\n\t\n\nup to 18\n\n\t\n\nup to 63 GiB\n\n\t\n\nup to 72 TiB\n\n\n\n\nDedicated\n\n\t\n\nCR2\n\n\t\n\nup to 36\n\n\t\n\nup to 126 GiB\n\n\t\n\nup to 72 TiB\n\n\n\n\nDedicated\n\n\t\n\nCR3\n\n\t\n\nup to 72\n\n\t\n\nup to 252 GiB\n\n\t\n\nup to 72 TiB\n\n\n\n\nDedicated\n\n\t\n\nCR4\n\n\t\n\nup to 144\n\n\t\n\nup to 495 GiB\n\n\t\n\nup to 72 TiB\n\nRecommended Setup for High Availability\nWhile it’s possible to start with just one node, for applications needing high availability and fault tolerance, we recommend using at least 3 nodes. This ensures that your data is safely replicated and that the cluster can handle node failures gracefully.\n\nCustom\n\nFor organizations with specialized requirements that go beyond the Shared and Dedicated Plans, CrateDB Cloud offers custom solutions tailored to your specific needs. Our sales team and solutions engineers work closely with you to architect and deploy a custom cluster configuration, ensuring optimal performance, scalability, and security for your mission-critical applications.\n\nWhether you have stringent compliance mandates, complex integrations, or unique scalability challenges, our custom solutions provide the flexibility and expertise to meet your business objectives and technical requirements."
  },
  {
    "title": "Tutorials — CrateDB Cloud",
    "url": "https://cratedb.com/docs/cloud/en/latest/tutorials/index.html",
    "html": "Tutorials\n\nWelcome to the tutorials section, the next step in your CrateDB Cloud journey! Here you’ll find comprehensive guides designed to help you make the most of your newly set-up cluster. Whether you are just getting started, or aim to deepen your understanding, these tutorials will walk you through key features and efficient practices to optimize your CrateDB experience.\n\n Time Series\n\nDive into the world of time series data with CrateDB. This tutorial will guide you through the best ways to store, query, and analyze time series data.\n\nIt is perfect for those working with IoT devices, monitoring systems, or any application where time-oriented data is crucial.\n\n Objects\n\nManaging JSON or other structured data? This tutorial on using the object data type in CrateDB is your go-to guide.\n\nLearn how to store nested data efficiently and how to query this data with ease. This tutorial is particularly useful for those dealing with semi-structured data formats like JSON.\n\n Full-Text Search\n\nUnlock the potential of text-based data with CrateDB’s Full-Text Search capabilities.\n\nThis tutorial will teach you how to set up your database for text search, how to create the relevant indices, and how to query your text data efficiently. A must-read for anyone looking to make sense of large volumes of unstructured text data.\n\n Advanced Time Series\n\nThis tutorial demonstrates how to augment time series data with the metadata to enable more comprehensive analysis.\n\nThe techniques and queries allow for unlocking deeper insights and harnessing the full potential of time series data in real-world applications.\n\nFeel free to dive into any of these tutorials to explore the features that are most relevant to your use case. We wish you a happy learning experience."
  },
  {
    "title": "Search — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/search.html",
    "html": "4.8\nSearch\n\nFrom here you can search these documents. Enter your search words into the box below and click \"search\". Note that the search function will automatically search for all of the words. Pages containing fewer words won't appear in the result list.\n\nSearch"
  },
  {
    "title": "Search — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/search.html",
    "html": "3.3\nSearch\n\nFrom here you can search these documents. Enter your search words into the box below and click \"search\". Note that the search function will automatically search for all of the words. Pages containing fewer words won't appear in the result list.\n\nSearch"
  },
  {
    "title": "Quick Start — CrateDB Cloud",
    "url": "https://cratedb.com/docs/cloud/en/latest/tutorials/quick-start.html",
    "html": "Quick Start\n\nThe quickest and easiest way to get started with CrateDB is to create a new cluster in CrateDB Cloud. You can get started by following these simple steps:\n\nCreate User\n\nTo create your user account, you can either set up username and password, or use one of the supported authentication providers:\n\nUsername & Password\n\n\n\nRegister for an account on the CrateDB Cloud sign-in page.\n\nClick on “Username & Password” on the right side.\n\nAt the bottom press “Sign up” and create a new user.\n\nVerify your email address by navigating to the link in the confirmation email.\n\nLog in using the username and password you just set up.\n\nAuthentication Provider\n\nOnce you signed in, you will be redirected to the CrateDB Cloud Console.\n\nNote\n\nIf you sign in through an external authentication provider, a CrateDB Cloud user account will be auto-generated for you.\n\nIf your sign-up was not initiated through an invitation to a pre-existing CrateDB Cloud organization, a default organization named “My Organization” will be created automatically on your behalf. You can rename this organization later via the “Settings”.\n\nDeploy Cluster\n\nAs next step, let’s deploy the first CrateDB cluster. If you are still viewing the “Clusters” page, please follow these steps:\n\nClick on “Deploy cluster” in the middle of the screen.\n\nProvide a cluster name, which will be part of the hostname of your cluster.\n\nSelect one of the available cloud regions.\n\nSelect the “FREE” compute size.\n\nClick the blue “Deploy Cluster” button on the right side.\n\nOnce you deployed the cluster, you will be redirected to the next screen. CrateDB Cloud automatically generates a password for the admin user. You can change the password later if needed.\n\nCopy the provided username and password.\n\nClick “OK” on the bottom right.\n\nYou will be redirected to the cluster overview page. The cluster deployment might take a few minutes. Hold on until the deployment is finished, which will be indicated by a corresponding “Healthy as of a few seconds ago” message on the top left, before continuing with the next step.\n\nNote\n\nThe “FREE” cluster can be started without providing payment details. You can use one free cluster per organization.\n\nThe cluster will be suspended if not used for 7 consecutive days, and will be deleted after an additional 7 days of inactivity.\n\nConnect\n\nEach CrateDB cluster comes with a built-in user interface. You can access it by selecting the blue “Open Admin UI” button. Alternatively, explore the 3rd party tools using the connecting details on the bottom of the page in the “Connecting to your cluster” section.\n\nClick on “Open Admin UI”, and provide authentication credentials.\n\nIn the newly opened page click on “</>” to open the query console.\n\nRun your first query:\n\nSELECT *\nFROM sys.summits \nLIMIT 20;\n\n\nWhile the integrated sys.summits table can be used to run your first queries, you probably want to import your own data or start with one of our sample datasets.\n\nImport Data\n\nTo import data in a CrateDB Cloud cluster, you can make use of the provided import mechanism in the cloud console, which can be found next to the cluster overview page in the “Import” tab.\n\nOwn Data\n\n\n\nClick on “Import” tab on the top menu.\n\nDrag and drop a file on the drop section or click “browse” to locate the file manually.\n\nSelect a file from your local disk.\n\nProvide a valid table name e.g. my_table.\n\nAdjust “Format” and “Compression” if necessary.\n\nClick on the blue “Import” button.\n\nWait until the file is imported successfully\n\nUsing your preferred method (e.g. Admin UI) run the following query to display the first 100 records of your imported data.\n\nSELECT *\nFROM my_table\nLIMIT 100;\n\nSample Dataset"
  },
  {
    "title": "CrateDB Cloud — CrateDB Cloud",
    "url": "https://cratedb.com/docs/cloud/en/latest/#",
    "html": "CrateDB Cloud\n\nCrateDB Cloud is a fully managed, terabyte-scale, and cost-effective analytics database that lets you run analytics over vast amounts of data in near real time, even with complex queries.\n\nIt is an SQL database service for enterprise data warehouse workloads, that works across clouds and scales with your data.\n\nDatabase Features\n\nCrateDB Cloud helps you manage and analyze your data with procedures like machine learning, geospatial analysis, and business intelligence.\n\nCrateDB Cloud’s scalable, distributed analysis engine lets you query terabytes worth of data efficiently.\n\nCrateDB provides a rich data model including container-, geospatial-, and vector-data types, and capabilities for full-text search.\n\nOperational Benefits\n\nCrateDB Cloud is a fully managed enterprise service, allowing you to deploy, monitor, back up, and scale your CrateDB clusters in the cloud without the need to do database management yourself.\n\nWith CrateDB Cloud, there’s no infrastructure to set up or manage, letting you focus on finding meaningful insights using plain SQL, and taking advantage of flexible pricing models across on-demand and flat-rate options.\n\nLearn\n\nUsers around the world rely on CrateDB Cloud clusters to store billions of records and terabytes of data, all accessible without delays. If you want to start using CrateDB Cloud, or make the most of your existing subscription, we are maintaining resources and tutorials to support you correspondingly.\n\n Quick Start\n\nLearn how to sign up and get started with a free cluster.\n\n Tutorials\n\nLearn how to use key features of CrateDB.\n\n Manage\n\nLearn how to manage your cluster.\n\n Web Console\n\nThe web-based management console for all your managed and on-premise clusters.\n\n Croud CLI\n\nA command-line based terminal program to operate your managed clusters.\n\n CrateDB Cloud on Kubernetes\n\nRun your own CrateDB Cloud region using Kubernetes.\n\nDo you want to learn about key database drivers and client applications for CrateDB, such as CrateDB Admin UI, crash, psql, DataGrip, and DBeaver? Discover how to configure these tools and explore CrateDB’s compatibility with analytics, ETL, BI, and monitoring solutions.\n\n Admin UI\n\nEach CrateDB Cloud cluster offers a dedicated Admin UI, which can be used to explore data, schema metadata, and cluster status information.\n\n Clients, Tools, and Integrations\n\nLearn about compatible client applications and tools, and how to configure your favorite client library to connect to a CrateDB cluster.\n\n Import data\n\nLearn how to import data into your CrateDB Cloud clusters.\n\nNote\n\nLike CrateDB itself, this is an open source documentation project. Suggestions for improvements, and source code contributions, are always welcome. "
  },
  {
    "title": "Search — CrateDB Cloud",
    "url": "https://cratedb.com/docs/cloud/en/latest/search.html",
    "html": "Search\n\nFrom here you can search these documents. Enter your search words into the box below and click \"search\". Note that the search function will automatically search for all of the words. Pages containing fewer words won't appear in the result list.\n\nSearch"
  },
  {
    "title": "Search — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/search.html",
    "html": "5.5\nSearch\n\nFrom here you can search these documents. Enter your search words into the box below and click \"search\". Note that the search function will automatically search for all of the words. Pages containing fewer words won't appear in the result list.\n\nSearch"
  },
  {
    "title": "Search — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/search.html",
    "html": "5.6\nSearch\n\nFrom here you can search these documents. Enter your search words into the box below and click \"search\". Note that the search function will automatically search for all of the words. Pages containing fewer words won't appear in the result list.\n\nSearch"
  },
  {
    "title": "Glossary — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/glossary.html",
    "html": "5.6\nGlossary\n\nThis glossary defines key terms used in the CrateDB reference manual.\n\nTable of contents\n\nTerms\n\nB\n\nC\n\nE\n\nF\n\nM\n\nN\n\nO\n\nP\n\nR\n\nS\n\nU\n\nV\n\nTerms\nB\nBinary operator\n\nSee operation.\n\nC\nCLUSTERED BY column\n\nSee routing column.\n\nE\nEvaluation\n\nSee expression.\n\nExpression\n\nAny valid SQL that produces a value (e.g., column references, comparison operators, and functions) through a process known as evaluation.\n\nContrary to a statement.\n\nSee Also\n\nSQL: Value expressions\n\nBuilt-ins: Subquery expressions\n\nData definition: Generation expressions\n\nScalar functions: Conditional functions and expressions\n\nAggregation: Aggregation expressions\n\nF\nFunction\n\nA token (e.g., replace) that takes zero or more arguments (e.g., three strings), performs a specific task, and may return one or more values (e.g., a modified string). Functions that return more than one value are called multi-valued functions.\n\nFunctions may be called in an SQL statement, like so:\n\ncr> SELECT replace('Hello world!', 'world', 'friend') as result;\n+---------------+\n| result        |\n+---------------+\n| Hello friend! |\n+---------------+\nSELECT 1 row in set (... sec)\n\n\nSee Also\n\nScalar functions\n\nAggregate functions\n\nTable functions\n\nWindow functions\n\nUser-defined functions\n\nM\nMetadata gateway\n\nPersists cluster metadata on disk every time the metadata changes. This data is stored persistently across full cluster restarts and recovered after nodes are started again.\n\nSee Also\n\nCluster configuration: Metadata gateway\n\nMulti-valued function\n\nA function that returns two or more values.\n\nSee Also\n\nTable functions\n\nWindow functions\n\nN\nNonscalar\n\nA data type that can have more than one value (e.g., arrays and objects).\n\nContrary to a scalar.\n\nSee Also\n\nGeographic types\n\nContainer types\n\nO\nOperand\n\nSee operator.\n\nOperation\n\nSee operator.\n\nOperator\n\nA reserved keyword (e.g., IN) or sequence of symbols (e.g., >=) that can be used in an SQL statement to manipulate one or more expressions and return a result (e.g., true or false). This process is known as an operation and the expressions can be called operands or arguments.\n\nAn operator that takes one operand is known as a unary operator and an operator that takes two is known as a binary operator.\n\nSee Also\n\nArithmetic operators\n\nComparison operators\n\nArray comparisons\n\nP\nPartition column\n\nA column used to partition a table. Specified by the PARTITIONED BY clause.\n\nAlso known as a PARTITIONED BY column or partitioned column.\n\nA table may be partitioned by one or more columns:\n\nIf a table is partitioned by one column, a new partition is created for every unique value in that partition column\n\nIf a table is partitioned by multiple columns, a new partition is created for every unique combination of row values in those partition columns\n\nSee Also\n\nData definition: Partitioned tables\n\nGenerated columns: Partitioning\n\nCREATE TABLE: PARTITIONED BY clause\n\nALTER TABLE: PARTITION clause\n\nREFRESH: PARTITION clause\n\nOPTIMIZE: PARTITION clause\n\nCOPY TO: PARTITION clause\n\nCOPY FROM: PARTITION clause\n\nCREATE SNAPSHOT: PARTITION clause\n\nRESTORE SNAPSHOT: PARTITION clause\n\nPARTITIONED BY column\n\nSee partition column.\n\nPartitioned column\n\nSee partition column.\n\nR\nRegular expression\n\nAn expression used to search for patterns in a string.\n\nSee Also\n\nWikipedia: Regular expression\n\nData definition: Fulltext analyzers\n\nQuerying: Regular expressions\n\nScalar functions: Regular expressions\n\nTable functions: regexp_matches\n\nRouting column\n\nValues in this column are used to compute a hash which is then used to route the corresponding row to a specific shard.\n\nAlso known as the CLUSTERED BY column.\n\nAll rows that have the same routing column row value are stored in the same shard.\n\nNote\n\nThe routing of rows to a specific shard is not the same as the routing of shards to a specific node (also known as shard allocation).\n\nSee Also\n\nStorage and consistency: Addressing documents\n\nSharding: Routing\n\nCREATE TABLE: CLUSTERED clause\n\nS\nScalar\n\nA data type with a single value (e.g., numbers and strings).\n\nContrary to a nonscalar.\n\nSee Also\n\nPrimitive types\n\nShard allocation\n\nThe process by which CrateDB allocates shards to a specific nodes.\n\nNote\n\nShard allocation is sometimes referred to as shard routing, which is not to be confused with row routing.\n\nSee Also\n\nShard allocation filtering\n\nCluster configuration: Routing allocation\n\nSharding: Number of shards\n\nAltering tables: Changing the number of shards\n\nAltering tables: Reroute shards\n\nShard recovery\n\nThe process by which CrateDB synchronizes a replica shard from a primary shard.\n\nShard recovery can happen during node startup, after node failure, when replicating a primary shard, when moving a shard to another node (i.e., when rebalancing the cluster), or during snapshot restoration.\n\nA shard that is being recovered cannot be queried until the recovery process is complete.\n\nSee Also\n\nCluster settings: Recovery\n\nSystem information: Checked node settings\n\nShard routing\n\nSee shard allocation.\n\nStatement\n\nAny valid SQL that serves as a database instruction (e.g., CREATE TABLE, INSERT, and SELECT) instead of producing a value.\n\nContrary to an expression.\n\nSee Also\n\nData definition\n\nData manipulation\n\nQuerying\n\nSQL Statements\n\nSubquery\n\nA SELECT statement used as a relation in the FROM clause of a parent SELECT statement.\n\nAlso known as a subselect.\n\nSubselect\n\nSee subquery.\n\nU\nUnary operator\n\nSee operation.\n\nUncorrelated subquery\n\nA scalar subquery that does not reference any relations (e.g., tables) in the parent SELECT statement.\n\nSee Also\n\nBuilt-ins: Subquery expressions\n\nV\nValue expression\n\nSee expression."
  },
  {
    "title": "Resiliency Issues — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/resiliency.html",
    "html": "5.6\nResiliency Issues\n\nCrateDB uses Elasticsearch for data distribution and replication. Most of the resiliency issues exist in the Elasticsearch layer and can be tested by Jepsen.\n\nTable of contents\n\nKnown issues\n\nRetry of updates causes double execution\n\nFixed issues\n\nRepeated cluster partitions can cause lost cluster updates\n\nVersion number representing ambiguous row versions\n\nReplicas can fall out of sync when a primary shard fails\n\nLoss of rows due to network partition\n\nDirty reads caused by bad primary handover\n\nChanges are overwritten by old data in danger of lost data\n\nMake table creation resilient to closing and full cluster crashes\n\nUnaware master accepts cluster updates\n\nKnown issues\nRetry of updates causes double execution\nStatus\tWork ongoing (More info)\nSeverity\tModerate\nLikelihood\tVery rare\nCause\tNetwork issues, unresponsive nodes\nWorkloads\tNon-Idempotent writes\n\nScenario\n\nA node with a primary shard receives an update, writes it to disk, but goes offline before having sent a confirmation back to the executing node. When the node comes back online, it receives an update retry and executes the update again.\n\nConsequence\n\nIncorrect data for non-idempotent writes.\n\nFor example:\n\nAn double insert on a table without an explicit primary key would be executed twice and would result in duplicate data.\n\nA double update would incorrectly increment the row version number twice.\n\nFixed issues\nRepeated cluster partitions can cause lost cluster updates\nStatus\tFixed in CrateDB v4.0 (#32006, #32171)\nSeverity\tModerate\nLikelihood\tVery rare\nCause\tNetwork issues, unresponsive nodes\nWorkloads\tAll\n\nScenario\n\nA cluster is partitioned and a new master is elected on the side that has quorum. The cluster is repaired and simultaneously a change is made to the cluster state. The cluster is partitioned again before the new master node has a chance to publish the new cluster state and the partition the master lands on does not have quorum.\n\nConsequence\n\nThe node steps down as master and the uncommunicated state changes are lost.\n\nCluster state is very important and contains information like shard location, schemas, and so on. Lost cluster state updates can cause data loss, reset settings, and problems with table structures.\n\nPartially fixed\n\nThis problem is mostly fixed by #20384 (CrateDB v2.0.x), which uses committed cluster state updates during master election process. This does not fully solve this rare problem but considerably reduces the chance of occurrence. The reason is that if the second partition happens concurrently with a cluster state update and blocks the cluster state commit message from reaching a majority of nodes, it may be that the in flight update is lost. If the now-isolated master can still acknowledge the cluster state update to the client this will result to the loss of an acknowledged change.\n\nVersion number representing ambiguous row versions\nStatus\tFixed in CrateDB v4.0 (#19269, #10708)\nSeverity\tSignificant\nLikelihood\tVery rare\nCause\tNetwork issues, unresponsive nodes\nWorkloads\tVersioned reads with replicated tables while writing.\n\nScenario\n\nA client is writing to a primary shard. The node holding the primary shard is partitioned from the cluster. It usually takes between 30 and 60 seconds (depending on ping configuration) before the master node notices the partition. During this time, the same row is updated on both the primary shard (partitioned) and a replica shard (not partitioned).\n\nConsequence\n\nThere are two different versions of the same row using the same version number. When the primary shard rejoins the cluster and its data is replicated, the update that was made on the replicated shard is lost but the new version number matches the lost update. This will break Optimistic Concurrency Control.\n\nReplicas can fall out of sync when a primary shard fails\nStatus\tFixed in CrateDB v4.0 (#10708)\nSeverity\tModest\nLikelihood\tRare\nCause\tPrimary fails and in-flight writes are only written to a subset of its replicas\nWorkloads\tWrites on replicated table\n\nScenario\n\nWhen a primary shard fails, a replica shard will be promoted to be the primary shard. If there is more than one replica shard, it is possible for the remaining replicas to be out of sync with the new primary shard. This is caused by operations that were in-flight when the primary shard failed and may not have been processed on all replica shards. Currently, the discrepancies are not repaired on primary promotion but instead would be repaired if replica shards are relocated (e.g., from hot to cold nodes); this does mean that the length of time which replicas can be out of sync with the primary shard is unbounded.\n\nConsequence\n\nStale data may be read from replicas.\n\nLoss of rows due to network partition\nStatus\tFixed in Crate v2.0.x (#7572, #14252)\nSeverity\tSignificant\nLikelihood\tVery rare\nCause\tSingle node isolation\nWorkloads\tWrites on replicated table\n\nScenario\n\nA node with a primary shard is partitioned from the cluster. The node continues to accept writes until it notices the network partition. In the meantime, another shard has been elected as the primary. Eventually, the partitioned node rejoins the cluster.\n\nConsequence\n\nData that was written to the original primary shard on the partitioned node is lost as data from the newly elected primary shard replaces it when it rejoins the cluster.\n\nThe risk window depends on your ping configuration. The default configuration of a 30 second ping timeout with three retries corresponds to a 90 second risk window. However, it is very rare for a node to lose connectivity within the cluster but maintain connectivity with clients.\n\nDirty reads caused by bad primary handover\nStatus\tFixed in CrateDB v2.0.x (#15900, #12573)\nSeverity\tModerate\nLikelihood\tRare\nCause\tRace Condition\nWorkloads\tReads\n\nScenario\n\nDuring a primary handover, there is a small risk window when a shard can find out it has been elected as the new primary before the old primary shard notices that it is no longer the primary.\n\nA primary handover can happen in the following scenarios:\n\nA shard is relocated and then elected as the new primary, as two separate but sequential actions. Relocating a shard means creating a new shard and then deleting the old shard.\n\nAn existing replica shard gets promoted to primary because the primary shard was partitioned from the cluster.\n\nConsequence\n\nWrites that occur on the new primary during the risk window will not be replicated to the old shard (which still believes it is the primary) so any subsequent reads on the old shard may return incorrect data.\n\nChanges are overwritten by old data in danger of lost data\nStatus\tFixed in CrateDB v2.0.x (#14671)\nSeverity\tSignificant\nLikelihood\tVery rare\nCause\tNetwork problems\nWorkloads\tWrites\n\nScenario\n\nA node with a primary that contains new data is partitioned from the cluster.\n\nConsequence\n\nCrateDB prefers old data over no data, and so promotes an a shard with stale data as a new primary. The data on the original primary shard is lost. Even if the node with the original primary shard rejoins the cluster, CrateDB has no way of distinguishing correct and incorrect data, so that data replaced with data from the new primary shard.\n\nMake table creation resilient to closing and full cluster crashes\nStatus\tThe issue has been fixed with the following issues. Table recovery: #9126 Reopening tables: #14739 Allocation IDs: #15281\nSeverity\tModest\nLikelihood\tVery Rare\nCause\tEither the cluster fails while recovering a table or the table is closed during shard creation.\nWorkloads\tTable creation\n\nScenario\n\nRecovering a table requires a quorum of shard copies to be available to allocate a primary. This means that a primary cannot be assigned if the cluster dies before enough shards have been allocated. The same happens if a table is closed before enough shard copies were started, making it impossible to reopen the table. Allocation IDs solve this issue by tracking allocated shard copies in the cluster. This makes it possible to safely recover a table in the presence of a single shard copy. Allocation IDs can also distinguish the situation where a table has been created but none of the shards have been started. If such an table was inadvertently closed before at least one shard could be started, a fresh shard will be allocated upon reopening the table.\n\nConsequence\n\nThe primary shard of the table cannot be assigned or a closed table cannot be re-opened.\n\nUnaware master accepts cluster updates\nStatus\tFixed in CrateDB v2.0.x (#13062)\nSeverity\tModerate\nLikelihood\tVery rare\nCause\tNetwork problems\nWorkloads\tDDL statements\n\nScenario\n\nIf a master has lost quorum (i.e. the number of nodes it is in communication with has fallen below the configured minimum) it should step down as master and stop answering requests to perform cluster updates. There is a small risk window between losing quorum and noticing that quorum has been lost, depending on your ping configuration.\n\nConsequence\n\nIf a cluster update request is made to the node between losing quorum and noticing the loss of quorum, that request will be confirmed. However, those updates will be lost because the node will not be able to perform a successful cluster update.\n\nCluster state is very important and contains information like shard location, schemas, and so on. Lost cluster state updates can cause data loss, reset settings, and problems with table structures."
  },
  {
    "title": "SQL standard compliance — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/compliance.html",
    "html": "5.6\nSQL standard compliance\n\nThis page documents the standard SQL (ISO/IEC 9075) features that CrateDB supports, along with implementation notes and any associated caveats.\n\nCaution\n\nThis list is approximate and features that are listed as supported might be nonconforming in their implementation. However, the main reference documentation always contains the most accurate information about the features CrateDB supports and how to use them.\n\nSee Also\n\nSQL compatibility\n\nID\n\n\t\n\nPackage\n\n\t\n\n#\n\n\t\n\nDescription\n\n\t\n\nComments\n\n\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n1\n\n\t\n\nINTEGER and SMALLINT data types\n\n\t\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n2\n\n\t\n\nREAL, DOUBLE PRECISION, and FLOAT data types\n\n\t\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n3\n\n\t\n\nDECIMAL and NUMERIC data types\n\n\t\n\nNot supported in DDL\n\n\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n4\n\n\t\n\nArithmetic operators\n\n\t\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n5\n\n\t\n\nNumeric comparison\n\n\t\n\n\nE011\n\n\t\n\nNumeric data types\n\n\t\n\n6\n\n\t\n\nImplicit casting among the numeric data types\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n1\n\n\t\n\nCHARACTER data type\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n2\n\n\t\n\nCHARACTER VARYING data type\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n3\n\n\t\n\nCharacter literals\n\n\t\n\nOnly simple ‘ quoting\n\n\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n4\n\n\t\n\nCHARACTER_LENGTH function\n\n\t\n\nchar_length only\n\n\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n5\n\n\t\n\nOCTET_LENGTH function\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n6\n\n\t\n\nSUBSTRING function\n\n\t\n\nsubstr scalar\n\n\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n7\n\n\t\n\nCharacter concatenation\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n8\n\n\t\n\nUPPER and LOWER functions\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n9\n\n\t\n\nTRIM function\n\n\t\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n10\n\n\t\n\nImplicit casting among the character string types\n\n\t\n\njust one type\n\n\n\n\nE021\n\n\t\n\nCharacter string types\n\n\t\n\n12\n\n\t\n\nCharacter comparison\n\n\t\n\n\nE031\n\n\t\n\nIdentifiers\n\n\t\t\t\n\n\nE031\n\n\t\n\nIdentifiers\n\n\t\n\n1\n\n\t\n\nDelimited identifiers\n\n\t\n\n\nE031\n\n\t\n\nIdentifiers\n\n\t\n\n2\n\n\t\n\nLower case identifiers\n\n\t\n\n\nE031\n\n\t\n\nIdentifiers\n\n\t\n\n3\n\n\t\n\nTrailing underscore\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\t\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n1\n\n\t\n\nSELECT DISTINCT\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n2\n\n\t\n\nGROUP BY clause\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n4\n\n\t\n\nGROUP BY can contain columns not in <select list>\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n5\n\n\t\n\nSelect list items can be renamed\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n6\n\n\t\n\nHAVING clause\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n7\n\n\t\n\nQualified * in select list\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n8\n\n\t\n\nCorrelation names in the FROM clause\n\n\t\n\n\nE051\n\n\t\n\nBasic query specification\n\n\t\n\n9\n\n\t\n\nRename columns in the FROM clause\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n1\n\n\t\n\nComparison predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n2\n\n\t\n\nBETWEEN predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n3\n\n\t\n\nIN predicate with list of values\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n4\n\n\t\n\nLIKE predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n6\n\n\t\n\nNULL predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n8\n\n\t\n\nEXISTS predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n9\n\n\t\n\nSubqueries in comparison predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n11\n\n\t\n\nSubqueries in IN predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n12\n\n\t\n\nSubqueries in quantified comparison predicate\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n13\n\n\t\n\nCorrelated subqueries\n\n\t\n\n\nE061\n\n\t\n\nBasic predicates and search conditions\n\n\t\n\n14\n\n\t\n\nSearch condition\n\n\t\n\n\nE071\n\n\t\n\nBasic query expressions\n\n\t\n\n1\n\n\t\n\nUNION DISTINCT table operator\n\n\t\n\n\nE071\n\n\t\n\nBasic query expressions\n\n\t\n\n2\n\n\t\n\nUNION ALL table operator\n\n\t\n\n\nE081\n\n\t\n\nBasic Privileges\n\n\t\t\t\n\n\nE081\n\n\t\n\nBasic Privileges\n\n\t\n\n1\n\n\t\n\nSELECT privilege\n\n\t\n\n\nE081\n\n\t\n\nBasic Privileges\n\n\t\n\n2\n\n\t\n\nDELETE privilege\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\t\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n1\n\n\t\n\nAVG\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n2\n\n\t\n\nCOUNT\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n3\n\n\t\n\nMAX\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n4\n\n\t\n\nMIN\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n5\n\n\t\n\nSUM\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n6\n\n\t\n\nALL quantifier\n\n\t\n\n\nE091\n\n\t\n\nSet functions\n\n\t\n\n7\n\n\t\n\nDISTINCT quantifier\n\n\t\n\n\nE101\n\n\t\n\nBasic data manipulation\n\n\t\t\t\n\n\nE101\n\n\t\n\nBasic data manipulation\n\n\t\n\n1\n\n\t\n\nINSERT statement\n\n\t\n\n\nE101\n\n\t\n\nBasic data manipulation\n\n\t\n\n3\n\n\t\n\nSearched UPDATE statement\n\n\t\n\n\nE101\n\n\t\n\nBasic data manipulation\n\n\t\n\n4\n\n\t\n\nSearched DELETE statement\n\n\t\n\n\nE121\n\n\t\n\nBasic cursor support\n\n\t\n\n1\n\n\t\n\nDECLARE CURSOR\n\n\t\n\n\nE121\n\n\t\n\nBasic cursor support\n\n\t\n\n8\n\n\t\n\nCLOSE statement\n\n\t\n\n\nE121\n\n\t\n\nBasic cursor support\n\n\t\n\n10\n\n\t\n\nFETCH statement implicit NEXT\n\n\t\n\n\nE121\n\n\t\n\nBasic cursor support\n\n\t\n\n17\n\n\t\n\nWITH HOLD cursors\n\n\t\n\n\nE131\n\n\t\n\nNull value support (nulls in lieu of values)\n\n\t\t\t\n\n\nE141\n\n\t\n\nBasic integrity constraints\n\n\t\n\n1\n\n\t\n\nNOT NULL constraints\n\n\t\n\n\nE141\n\n\t\n\nBasic integrity constraints\n\n\t\n\n3\n\n\t\n\nPRIMARY KEY constraints\n\n\t\n\n\nE141\n\n\t\n\nBasic integrity constraints\n\n\t\n\n6\n\n\t\n\nCHECK constraints\n\n\t\n\n\nE141\n\n\t\n\nBasic integrity constraints\n\n\t\n\n7\n\n\t\n\nColumn defaults\n\n\t\n\n\nE141\n\n\t\n\nBasic integrity constraints\n\n\t\n\n8\n\n\t\n\nNOT NULL inferred on PRIMARY KEY\n\n\t\n\n\nE151\n\n\t\n\nTransaction support\n\n\t\n\n1\n\n\t\n\nCOMMIT statement\n\n\t\n\n\nE152\n\n\t\n\nBasic SET TRANSACTION statement\n\n\t\t\t\n\n\nE152\n\n\t\n\nBasic SET TRANSACTION statement\n\n\t\n\n1\n\n\t\n\nSET TRANSACTION statement: ISOLATION LEVEL SERIALIZABLE clause\n\n\t\n\nIs ignored\n\n\n\n\nE152\n\n\t\n\nBasic SET TRANSACTION statement\n\n\t\n\n2\n\n\t\n\nSET TRANSACTION statement: READ ONLY and READ WRITE clauses\n\n\t\n\nIs ignored\n\n\n\n\nE161\n\n\t\n\nSQL comments using leading double minus\n\n\t\t\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n1\n\n\t\n\nCOLUMNS view\n\n\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n2\n\n\t\n\nTABLES view\n\n\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n3\n\n\t\n\nVIEWS view\n\n\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n4\n\n\t\n\nTABLE_CONSTRAINTS view\n\n\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n5\n\n\t\n\nREFERENTIAL_CONSTRAINTS view\n\n\t\n\n\nF021\n\n\t\n\nBasic information schema\n\n\t\n\n6\n\n\t\n\nCHECK_CONSTRAINTS view\n\n\t\n\n\nF031\n\n\t\n\nBasic schema manipulation\n\n\t\n\n1\n\n\t\n\nCREATE TABLE statement to create persistent base tables\n\n\t\n\n\nF031\n\n\t\n\nBasic schema manipulation\n\n\t\n\n2\n\n\t\n\nCREATE VIEW statement\n\n\t\n\n\nF031\n\n\t\n\nBasic schema manipulation\n\n\t\n\n3\n\n\t\n\nGRANT statement\n\n\t\n\n\nF031\n\n\t\n\nBasic schema manipulation\n\n\t\n\n4\n\n\t\n\nALTER TABLE statement: ADD COLUMN clause\n\n\t\n\n\nF033\n\n\t\n\nALTER TABLE statement: DROP COLUMN clause\n\n\t\t\t\n\n\nF034\n\n\t\n\nExtended REVOKE statement\n\n\t\t\t\n\n\nF034\n\n\t\n\nExtended REVOKE statement\n\n\t\n\n1\n\n\t\n\nREVOKE statement performed by other than the owner of a schema object\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\t\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n1\n\n\t\n\nInner join (but not necessarily the INNER keyword)\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n2\n\n\t\n\nINNER keyword\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n3\n\n\t\n\nLEFT OUTER JOIN\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n4\n\n\t\n\nRIGHT OUTER JOIN\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n5\n\n\t\n\nOuter joins can be nested\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n7\n\n\t\n\nThe inner table in a left or right outer join can also be used in an inner join\n\n\t\n\n\nF041\n\n\t\n\nBasic joined table\n\n\t\n\n8\n\n\t\n\nAll comparison operators are supported (rather than just =)\n\n\t\n\n\nF051\n\n\t\n\nBasic date and time\n\n\t\n\n1\n\n\t\n\nDATE data type (including support of DATE literal)\n\n\t\n\n\nF051\n\n\t\n\nBasic date and time\n\n\t\n\n3\n\n\t\n\nTIMESTAMP data type (including support of TIMESTAMP literal) with fractional seconds precision of at least 0 and 6\n\n\t\n\n\nF051\n\n\t\n\nBasic date and time\n\n\t\n\n4\n\n\t\n\nComparison predicate on DATE, TIME, and TIMESTAMP data types\n\n\t\n\n\nF051\n\n\t\n\nBasic date and time\n\n\t\n\n5\n\n\t\n\nExplicit CAST between datetime types and character string types\n\n\t\n\n\nF051\n\n\t\n\nBasic date and time\n\n\t\n\n6\n\n\t\n\nCURRENT_DATE\n\n\t\n\n\nF052\n\n\t\n\nIntervals and datetime arithmetic\n\n\t\t\t\n\n\nF111\n\n\t\n\nIsolation levels other than SERIALIZABLE\n\n\t\t\t\n\n\nF111\n\n\t\n\nIsolation levels other than SERIALIZABLE\n\n\t\n\n1\n\n\t\n\nREAD UNCOMMITTED isolation level\n\n\t\n\nIs ignored\n\n\n\n\nF111\n\n\t\n\nIsolation levels other than SERIALIZABLE\n\n\t\n\n2\n\n\t\n\nREAD COMMITTED isolation level\n\n\t\n\nIs ignored\n\n\n\n\nF111\n\n\t\n\nIsolation levels other than SERIALIZABLE\n\n\t\n\n3\n\n\t\n\nREPEATABLE READ isolation level\n\n\t\n\nIs ignored\n\n\n\n\nF131\n\n\t\n\nGrouped operations\n\n\t\n\n1\n\n\t\n\nWHERE, GROUP BY, and HAVING clauses supported in queries with grouped views\n\n\t\n\n\nF131\n\n\t\n\nGrouped operations\n\n\t\n\n3\n\n\t\n\nSet functions supported in queries with grouped views\n\n\t\n\n\nF171\n\n\t\n\nMultiple schemas per user\n\n\t\t\t\n\n\nF201\n\n\t\n\nCAST function\n\n\t\t\t\n\n\nF221\n\n\t\n\nExplicit defaults\n\n\t\t\t\n\n\nF222\n\n\t\n\nINSERT statement: DEFAULT VALUES clause\n\n\t\t\t\n\n\nF261\n\n\t\n\nCASE expression\n\n\t\t\t\n\n\nF261\n\n\t\n\nCASE expression\n\n\t\n\n1\n\n\t\n\nSimple CASE\n\n\t\n\n\nF261\n\n\t\n\nCASE expression\n\n\t\n\n2\n\n\t\n\nSearched CASE\n\n\t\n\n\nF261\n\n\t\n\nCASE expression\n\n\t\n\n3\n\n\t\n\nNULLIF\n\n\t\n\n\nF261\n\n\t\n\nCASE expression\n\n\t\n\n4\n\n\t\n\nCOALESCE\n\n\t\n\n\nF262\n\n\t\n\nExtended CASE expression\n\n\t\t\t\n\n\nF311\n\n\t\n\nSchema definition statement\n\n\t\n\n2\n\n\t\n\nCREATE TABLE for persistent base tables\n\n\t\n\n\nF311\n\n\t\n\nSchema definition statement\n\n\t\n\n3\n\n\t\n\nCREATE VIEW\n\n\t\n\n\nF381\n\n\t\n\nExtended schema manipulation\n\n\t\t\t\n\n\nF381\n\n\t\n\nExtended schema manipulation\n\n\t\n\n1\n\n\t\n\nALTER TABLE statement: ALTER COLUMN clause\n\n\t\n\n\nF381\n\n\t\n\nExtended schema manipulation\n\n\t\n\n2\n\n\t\n\nALTER TABLE statement: ADD CONSTRAINT clause\n\n\t\n\n\nF381\n\n\t\n\nExtended schema manipulation\n\n\t\n\n3\n\n\t\n\nALTER TABLE statement: DROP CONSTRAINT clause\n\n\t\n\n\nF391\n\n\t\n\nLong identifiers\n\n\t\t\t\n\n\nF392\n\n\t\n\nUnicode escapes in identifiers\n\n\t\t\t\n\n\nF401\n\n\t\n\nExtended joined table\n\n\t\n\n2\n\n\t\n\nFULL OUTER JOIN\n\n\t\n\n\nF401\n\n\t\n\nExtended joined table\n\n\t\n\n4\n\n\t\n\nCROSS JOIN\n\n\t\n\n\nF431\n\n\t\n\nRead-only scrollable cursors\n\n\t\t\t\n\n\nF431\n\n\t\n\nRead-only scrollable cursors\n\n\t\n\n1\n\n\t\n\nFETCH with explicit NEXT\n\n\t\n\n\nF431\n\n\t\n\nRead-only scrollable cursors\n\n\t\n\n2\n\n\t\n\nFETCH FIRST\n\n\t\n\n\nF431\n\n\t\n\nRead-only scrollable cursors\n\n\t\n\n3\n\n\t\n\nFETCH LAST\n\n\t\n\n\nF431\n\n\t\n\nRead-only scrollable cursors\n\n\t\n\n4\n\n\t\n\nFETCH PRIOR\n\n\t\n\n\nF431\n\n\t\n\nRead-only scrollable cursors\n\n\t\n\n5\n\n\t\n\nFETCH ABSOLUTE\n\n\t\n\n\nF431\n\n\t\n\nRead-only scrollable cursors\n\n\t\n\n6\n\n\t\n\nFETCH RELATIVE\n\n\t\n\n\nF471\n\n\t\n\nScalar subquery values\n\n\t\t\t\n\n\nF481\n\n\t\n\nExpanded NULL predicate\n\n\t\t\t\n\n\nF501\n\n\t\n\nFeatures and conformance views\n\n\t\n\n1\n\n\t\n\nSQL_FEATURES view\n\n\t\n\n\nF571\n\n\t\n\nTruth value tests\n\n\t\t\t\n\n\nF651\n\n\t\n\nCatalog name qualifiers\n\n\t\t\t\n\n\nF763\n\n\t\n\nCURRENT_SCHEMA\n\n\t\t\t\n\n\nF791\n\n\t\n\nInsensitive cursors\n\n\t\t\t\n\n\nF850\n\n\t\n\nTop-level <order by clause> in <query expression>\n\n\t\t\t\n\n\nF851\n\n\t\n\n<order by clause> in subqueries\n\n\t\t\t\n\n\nF852\n\n\t\n\nTop-level <order by clause> in views\n\n\t\t\t\n\n\nF855\n\n\t\n\nNested <order by clause> in <query expression>\n\n\t\t\t\n\n\nF856\n\n\t\n\nNested <fetch first clause> in <query expression>\n\n\t\t\t\n\n\nF857\n\n\t\n\nTop-level <fetch first clause> in <query expression>\n\n\t\t\t\n\n\nF858\n\n\t\n\n<fetch first clause> in subqueries\n\n\t\t\t\n\n\nF859\n\n\t\n\nTop-level <fetch first clause> in views\n\n\t\t\t\n\n\nF860\n\n\t\n\n<fetch first row count> in <fetch first clause>\n\n\t\t\t\n\n\nS091\n\n\t\n\nBasic array support\n\n\t\t\t\n\nspecial syntax\n\n\n\n\nS091\n\n\t\n\nBasic array support\n\n\t\n\n1\n\n\t\n\nArrays of built-in data types\n\n\t\n\nspecial syntax\n\n\n\n\nS098\n\n\t\n\nARRAY_AGG\n\n\t\t\t\n\n\nT031\n\n\t\n\nBOOLEAN data type\n\n\t\t\t\n\n\nT051\n\n\t\n\nRow types\n\n\t\t\t\n\nLimited to built-in table functions\n\n\n\n\nT054\n\n\t\n\nGREATEST and LEAST\n\n\t\t\t\n\n\nT055\n\n\t\n\nString padding functions\n\n\t\t\t\n\n\nT056\n\n\t\n\nMulti-character trim functions\n\n\t\t\t\n\n\nT071\n\n\t\n\nBIGINT data type\n\n\t\t\t\n\n\nT081\n\n\t\n\nOptional string types maximum length\n\n\t\t\t\n\n\nT121\n\n\t\n\nWITH (excluding RECURSIVE) in query expression\n\n\t\t\t\n\n\nT122\n\n\t\n\nWITH (excluding RECURSIVE) in subquery\n\n\t\t\t\n\n\nT175\n\n\t\n\nGenerated columns\n\n\t\t\t\n\n\nT241\n\n\t\n\nSTART TRANSACTION statement\n\n\t\t\t\n\nIs ignored\n\n\n\n\nT321\n\n\t\n\nBasic SQL-invoked routines\n\n\t\n\n1\n\n\t\n\nUser-defined functions with no overloading\n\n\t\n\n\nT321\n\n\t\n\nBasic SQL-invoked routines\n\n\t\n\n3\n\n\t\n\nFunction invocation\n\n\t\n\n\nT321\n\n\t\n\nBasic SQL-invoked routines\n\n\t\n\n6\n\n\t\n\nROUTINES view\n\n\t\n\n\nT331\n\n\t\n\nBasic roles\n\n\t\t\t\n\n\nT351\n\n\t\n\nBracketed SQL comments (/…/ comments)\n\n\t\t\t\n\n\nT441\n\n\t\n\nABS and MOD functions\n\n\t\t\t\n\n\nT461\n\n\t\n\nSymmetric BETWEEN predicate\n\n\t\t\t\n\n\nT471\n\n\t\n\nResult sets return value\n\n\t\t\t\n\n\nT615\n\n\t\n\nLEAD and LAG functions\n\n\t\t\t\n\n\nT617\n\n\t\n\nFIRST_VALUE and LAST_VALUE function\n\n\t\t\t\n\n\nT618\n\n\t\n\nNTH_VALUE function\n\n\t\t\t\n\n\nT621\n\n\t\n\nEnhanced numeric functions\n\n\t\t\t\n\n\nT626\n\n\t\n\nANY_VALUE aggregation\n\n\t\t\t\n\n\nT631\n\n\t\n\nIN predicate with one list element\n\n\t\t\t\n\n\nT662\n\n\t\n\nUnderscores in numeric literals\n\n\t\t\t"
  },
  {
    "title": "SQL compatibility — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/compatibility.html",
    "html": "5.6\nSQL compatibility\n\nCrateDB provides a standards-based SQL implementation similar to many other SQL databases. In particular, CrateDB aims for compatibility with PostgreSQL. However, CrateDB’s SQL dialect does have some unique characteristics, documented on this page.\n\nSee Also\n\nSQL: Syntax reference\n\nTable of contents\n\nImplementation notes\n\nData types\n\nCreate table\n\nAlter table\n\nSystem information tables\n\nBLOB support\n\nTransactions (BEGIN, START, COMMIT, and ROLLBACK)\n\nUnsupported features and functions\n\nImplementation notes\nData types\n\nCrateDB supports a set of primitive data types. The following table defines how data types of standard SQL map to CrateDB Data types.\n\nStandard SQL\n\n\t\n\nCrateDB\n\n\n\n\ninteger\n\n\t\n\ninteger, int, int4\n\n\n\n\nbit[8]\n\n\t\n\nbyte, char\n\n\n\n\nboolean, bool\n\n\t\n\nboolean\n\n\n\n\nchar [(n)], varchar [(n)]\n\n\t\n\nstring, text, varchar, character varying\n\n\n\n\ntimestamp with time zone\n\n\t\n\ntimestamp with time zone, timestamptz\n\n\n\n\ntimestamp\n\n\t\n\ntimestamp without time zone\n\n\n\n\nsmallint\n\n\t\n\nshort, int2, smallint\n\n\n\n\nbigint\n\n\t\n\nlong, bigint, int8\n\n\n\n\nreal\n\n\t\n\nfloat, real\n\n\n\n\ndouble precision\n\n\t\n\ndouble, double precision\n\nCreate table\n\nCREATE TABLE supports additional storage and table parameters for sharding, replication and routing of data, and does not support inheritance.\n\nAlter table\n\nALTER COLUMN action is not currently supported (see ALTER TABLE).\n\nSystem information tables\n\nThe read-only System information and Information schema tables have a slightly different schema than specified in standard SQL. They provide schema information and can be queried to get real-time statistical data about the cluster, its nodes, and their shards.\n\nBLOB support\n\nStandard SQL defines a binary string type, called BLOB or BINARY LARGE OBJECT. With CrateDB, Binary Data is instead stored in separate BLOB Tables (see Blobs) which can be sharded and replicated.\n\nTransactions (BEGIN, START, COMMIT, and ROLLBACK)\n\nCrateDB is focused on providing analytical capabilities over supporting traditional transactional use cases, and thus it does not provide transaction control. Every statement commits immediately and is replicated within the cluster.\n\nHowever, every row in CrateDB has a version number that is incremented whenever the record is modified. This version number can be used to implement patterns like Optimistic Concurrency Control, which can be used to solve many of the use cases that would otherwise require traditional transactions.\n\nUnsupported features and functions\n\nThese features of standard SQL are not supported:\n\nStored procedures\n\nTriggers\n\nWITH Queries (Common Table Expressions)\n\nSequences\n\nInheritance\n\nConstraints\n\nUnique\n\nForeign key\n\nExclusion constraints\n\nThese functions of standard SQL are either not supported or only partly supported:\n\nAggregate functions\n\nVarious functions available (see Aggregation)\n\nWindow functions\n\nVarious functions available (see Window functions)\n\nENUM support functions\n\nIS DISTINCT FROM\n\nNetwork address functions and operators\n\nMathematical functions\n\nCertain functions supported (see Mathematical functions)\n\nSet returning functions\n\nTrigger functions\n\nXML functions\n\nNote\n\nThe currently supported and unsupported features in CrateDB are exposed in the Information schema table (see sql_features for usage).\n\nCrateDB also supports the PostgreSQL wire protocol.\n\nIf you have use cases for any missing features, functions, or dialect improvements, let us know on GitHub! We are always improving and extending CrateDB and would love to hear your feedback."
  },
  {
    "title": "Search — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/search.html",
    "html": "master\nSearch\n\nFrom here you can search these documents. Enter your search words into the box below and click \"search\". Note that the search function will automatically search for all of the words. Pages containing fewer words won't appear in the result list.\n\nSearch"
  },
  {
    "title": "PostgreSQL wire protocol — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/interfaces/postgres.html",
    "html": "5.6\nPostgreSQL wire protocol\n\nCrateDB supports the PostgreSQL wire protocol v3.\n\nIf a node is started with PostgreSQL wire protocol support enabled it will bind to port 5432 by default. To use a custom port, set the corresponding Ports in the Configuration.\n\nHowever, even though connecting PostgreSQL tools and client libraries is supported, the actual SQL statements have to be supported by CrateDB’s SQL dialect. A notable difference is that CrateDB doesn’t support transactions, which is why clients should generally enable autocommit.\n\nNote\n\nIn order to use setFetchSize in JDBC it is possible to set auto commit to false.\n\nThe client will utilize the fetchSize on SELECT statements and only load up to fetchSize rows into memory.\n\nSee the PostgreSQL JDBC Query docs for more information.\n\nWrite operations will still behave as if auto commit was enabled and commit or rollback calls are ignored.\n\nTable of contents\n\nServer compatibility\n\nStart-up\n\nSSL Support\n\nAuthentication\n\nParameterStatus\n\nDatabase selection\n\nQuery modes\n\nSimple query\n\nExtended query\n\nCopy operations\n\nFunction call\n\nCanceling requests\n\npg_catalog\n\npg_type\n\nOID types\n\nShow transaction isolation\n\nBEGIN, START, and COMMIT statements\n\nClient compatibility\n\nJDBC\n\nLimitations\n\nConnection failover and load balancing\n\nImplementation differences\n\nCopy operations\n\nData types\n\nDates and times\n\nObjects\n\nArrays\n\nDeclaration of arrays\n\nType casts\n\nText search functions and operators\n\nServer compatibility\n\nCrateDB emulates PostgreSQL server version 14.\n\nStart-up\nSSL Support\n\nSSL can be configured using Secured communications (SSL/TLS).\n\nAuthentication\n\nAuthentication methods can be configured using Host-Based Authentication (HBA).\n\nParameterStatus\n\nAfter the authentication succeeded, the server has the possibility to send multiple ParameterStatus messages to the client. These are used to communicate information like server_version (emulates PostgreSQL 9.5) or server_encoding.\n\nCrateDB also sends a message containing the crate_version parameter. This contains the current CrateDB version number.\n\nThis information is useful for clients to detect that they’re connecting to CrateDB instead of a PostgreSQL instance.\n\nDatabase selection\n\nSince CrateDB uses schemas instead of databases, the database parameter sets the default schema name for future queries. If no schema is specified, the schema doc will be used as default. Additionally, the only supported charset is UTF8.\n\nQuery modes\nSimple query\n\nThe PostgreSQL simple query protocol mode is fully implemented.\n\nExtended query\n\nThe PostgreSQL extended query protocol mode is implemented with the following limitations:\n\nThe ParameterDescription message works for the most common use cases except for DDL statements.\n\nTo optimize the execution of bulk operations the execution of statements is delayed until the Sync message is received\n\nCopy operations\n\nCrateDB does not support the COPY sub-protocol, see also Copy operations.\n\nFunction call\n\nThe function call sub-protocol is not supported since it’s a legacy feature.\n\nCanceling requests\n\nPostgreSQL cancelling requests is fully implemented.\n\npg_catalog\n\nFor improved compatibility, the pg_catalog schema is implemented containing following tables:\n\npg_am\n\npg_attrdef\n\npg_attribute\n\npg_class\n\npg_constraint\n\npg_cursors\n\npg_database\n\npg_depend\n\npg_description\n\npg_enum\n\npg_event_trigger\n\npg_index\n\npg_indexes\n\npg_locks\n\npg_namespace\n\npg_proc\n\npg_publication\n\npg_publication_tables\n\npg_range\n\npg_roles\n\npg_settings\n\npg_shdescription\n\npg_stats\n\npg_subscription\n\npg_subscription_rel\n\npg_tables\n\npg_tablespace\n\npg_type\n\npg_views\n\npg_type\n\nSome clients require the pg_catalog.pg_type in order to be able to stream arrays or other non-primitive types.\n\nFor compatibility reasons, there is a trimmed down pg_type table available in CrateDB:\n\ncr> SELECT oid, typname, typarray, typelem, typlen, typtype, typcategory\n... FROM pg_catalog.pg_type\n... ORDER BY oid;\n+------+--------------+----------+---------+--------+---------+-------------+\n|  oid | typname      | typarray | typelem | typlen | typtype | typcategory |\n+------+--------------+----------+---------+--------+---------+-------------+\n|   16 | bool         |     1000 |       0 |      1 | b       | N           |\n|   18 | char         |     1002 |       0 |      1 | b       | S           |\n|   19 | name         |       -1 |       0 |     64 | b       | S           |\n|   20 | int8         |     1016 |       0 |      8 | b       | N           |\n|   21 | int2         |     1005 |       0 |      2 | b       | N           |\n|   23 | int4         |     1007 |       0 |      4 | b       | N           |\n|   24 | regproc      |     1008 |       0 |      4 | b       | N           |\n|   25 | text         |     1009 |       0 |     -1 | b       | S           |\n|   26 | oid          |     1028 |       0 |      4 | b       | N           |\n|   30 | oidvector    |     1013 |      26 |     -1 | b       | A           |\n|  114 | json         |      199 |       0 |     -1 | b       | U           |\n|  199 | _json        |        0 |     114 |     -1 | b       | A           |\n|  600 | point        |     1017 |       0 |     16 | b       | G           |\n|  700 | float4       |     1021 |       0 |      4 | b       | N           |\n|  701 | float8       |     1022 |       0 |      8 | b       | N           |\n|  705 | unknown      |        0 |       0 |     -2 | p       | X           |\n| 1000 | _bool        |        0 |      16 |     -1 | b       | A           |\n| 1002 | _char        |        0 |      18 |     -1 | b       | A           |\n| 1005 | _int2        |        0 |      21 |     -1 | b       | A           |\n| 1007 | _int4        |        0 |      23 |     -1 | b       | A           |\n| 1008 | _regproc     |        0 |      24 |     -1 | b       | A           |\n| 1009 | _text        |        0 |      25 |     -1 | b       | A           |\n| 1014 | _bpchar      |        0 |    1042 |     -1 | b       | A           |\n| 1015 | _varchar     |        0 |    1043 |     -1 | b       | A           |\n| 1016 | _int8        |        0 |      20 |     -1 | b       | A           |\n| 1017 | _point       |        0 |     600 |     -1 | b       | A           |\n| 1021 | _float4      |        0 |     700 |     -1 | b       | A           |\n| 1022 | _float8      |        0 |     701 |     -1 | b       | A           |\n| 1042 | bpchar       |     1014 |       0 |     -1 | b       | S           |\n| 1043 | varchar      |     1015 |       0 |     -1 | b       | S           |\n| 1082 | date         |     1182 |       0 |      8 | b       | D           |\n| 1114 | timestamp    |     1115 |       0 |      8 | b       | D           |\n| 1115 | _timestamp   |        0 |    1114 |     -1 | b       | A           |\n| 1182 | _date        |        0 |    1082 |     -1 | b       | A           |\n| 1184 | timestamptz  |     1185 |       0 |      8 | b       | D           |\n| 1185 | _timestamptz |        0 |    1184 |     -1 | b       | A           |\n| 1186 | interval     |     1187 |       0 |     16 | b       | T           |\n| 1187 | _interval    |        0 |    1186 |     -1 | b       | A           |\n| 1231 | _numeric     |        0 |    1700 |     -1 | b       | A           |\n| 1266 | timetz       |     1270 |       0 |     12 | b       | D           |\n| 1270 | _timetz      |        0 |    1266 |     -1 | b       | A           |\n| 1560 | bit          |     1561 |       0 |     -1 | b       | V           |\n| 1561 | _bit         |        0 |    1560 |     -1 | b       | A           |\n| 1700 | numeric      |     1231 |       0 |     -1 | b       | N           |\n| 2205 | regclass     |     2210 |       0 |      4 | b       | N           |\n| 2210 | _regclass    |        0 |    2205 |     -1 | b       | A           |\n| 2249 | record       |     2287 |       0 |     -1 | p       | P           |\n| 2276 | any          |        0 |       0 |      4 | p       | P           |\n| 2277 | anyarray     |        0 |    2276 |     -1 | p       | P           |\n| 2287 | _record      |        0 |    2249 |     -1 | p       | A           |\n+------+--------------+----------+---------+--------+---------+-------------+\nSELECT 50 rows in set (... sec)\n\n\nNote\n\nThis is just a snapshot of the table.\n\nCheck table information_schema.columns to get information for all supported columns.\n\nOID types\n\nObject Identifiers (OIDs) are used internally by PostgreSQL as primary keys for various system tables.\n\nCrateDB supports the oid type and the following aliases:\n\nName\n\n\t\n\nReference\n\n\t\n\nDescription\n\n\t\n\nExample\n\n\n\n\nregproc\n\n\t\n\npg_proc\n\n\t\n\nA function name\n\n\t\n\nsum\n\n\n\n\nregclass\n\n\t\n\npg_class\n\n\t\n\nA relation name\n\n\t\n\npg_type\n\nCrateDB also supports the oidvector type.\n\nNote\n\nCasting a string or an integer to the regproc type does not result in a function lookup (as it does with PostgreSQL).\n\nInstead:\n\nCasting a string to the regproc type results in an object of the regproc type with a name equal to the string value and an oid equal to an integer hash of the string.\n\nCasting an integer to the regproc type results in an object of the regproc type with a name equal to the string representation of the integer and an oid equal to the integer value.\n\nConsult the CrateDB data types reference for more information about each OID type (including additional type casting behaviour).\n\nShow transaction isolation\n\nFor compatibility with JDBC the SHOW TRANSACTION ISOLATION LEVEL statement is implemented:\n\ncr> show transaction isolation level;\n+-----------------------+\n| transaction_isolation |\n+-----------------------+\n| read uncommitted      |\n+-----------------------+\nSHOW 1 row in set (... sec)\n\nBEGIN, START, and COMMIT statements\n\nFor compatibility with clients that use the PostgresSQL wire protocol (e.g., the Golang lib/pq and pgx drivers), CrateDB will accept the BEGIN, COMMIT, and START TRANSACTION statements. For example:\n\ncr> BEGIN TRANSACTION ISOLATION LEVEL READ UNCOMMITTED,\n...                   READ ONLY,\n...                   NOT DEFERRABLE;\nBEGIN OK, 0 rows affected  (... sec)\n\ncr> COMMIT\nCOMMIT OK, 0 rows affected  (... sec)\n\n\nCrateDB will silently ignore the COMMIT, BEGIN, and START TRANSACTION statements and all respective parameters.\n\nClient compatibility\nJDBC\n\npgjdbc JDBC drivers version 9.4.1209 and above are compatible.\n\nLimitations\n\nReflection methods like conn.getMetaData().getTables(...) won’t work since the required tables are unavailable in CrateDB.\n\nAs a workaround it’s possible to use SHOW TABLES or query the information_schema tables manually using SELECT statements.\n\nOBJECT and GEO_SHAPE columns can be streamed as JSON but require pgjdbc version 9.4.1210 or newer.\n\nMultidimensional arrays will be streamed as JSON encoded string to avoid a protocol limitation where all sub-arrays are required to have the same length.\n\nThe behavior of PreparedStatement.executeBatch in error cases depends on in which stage an error occurs: A BatchUpdateException is thrown if no processing has been done yet, whereas single operations failing after the processing started are indicated by an EXECUTE_FAILED (-3) return value.\n\nTransaction limitations as described above.\n\nHaving escape processing enabled could prevent the usage of Object Literals in case an object key’s starting character clashes with a JDBC escape keyword (see also JDBC escape syntax). Disabling escape processing will remedy this appropriately for pgjdbc version >= 9.4.1212.\n\nConnection failover and load balancing\n\nConnection failover and load balancing is supported as described here: PostgreSQL JDBC connection failover.\n\nNote\n\nIt is not recommended to use the targetServerType parameter since CrateDB has no concept of master-replica nodes.\n\nImplementation differences\n\nThe PostgreSQL Wire Protocol makes it easy to use many PostgreSQL compatible tools and libraries directly with CrateDB. However, many of these tools assume that they are talking to PostgreSQL specifically, and thus rely on SQL extensions and idioms that are unique to PostgreSQL. Because of this, some tools or libraries may not work with other SQL databases such as CrateDB.\n\nCrateDB’s SQL query engine enables real-time search & aggregations for online analytic processing (OLAP) and business intelligence (BI) with the benefit of the ability to scale horizontally. The use-cases of CrateDB are different than those of PostgreSQL, as CrateDB’s specialized storage schema and query execution engine addresses different needs (see Clustering).\n\nThe features listed below cover the main differences in implementation and dialect between CrateDB and PostgreSQL. A detailed comparison between CrateDB’s SQL dialect and standard SQL is outlined in SQL compatibility.\n\nCopy operations\n\nCrateDB does not support the distinct sub-protocol that is used to serve COPY operations and provides another implementation for transferring bulk data using the COPY FROM and COPY TO statements.\n\nData types\nDates and times\n\nAt the moment, CrateDB does not support TIME without a time zone.\n\nAdditionally, CrateDB does not support the INTERVAL input units MILLENNIUM, CENTURY, DECADE, MILLISECOND, or MICROSECOND.\n\nObjects\n\nThe definition of structured values by using JSON types, composite types or HSTORE are not supported. CrateDB alternatively allows the definition of nested documents (of type OBJECT) that store fields containing any CrateDB supported data type, including nested object types.\n\nArrays\nDeclaration of arrays\n\nWhile multidimensional arrays in PostgreSQL must have matching extends for each dimension, CrateDB allows different length nested arrays as this example shows:\n\ncr> select [[1,2,3],[1,2]] from sys.cluster;\n+---------------------+\n| [[1, 2, 3], [1, 2]] |\n+---------------------+\n| [[1, 2, 3], [1, 2]] |\n+---------------------+\nSELECT 1 row in set (... sec)\n\nType casts\n\nCrateDB accepts the Type casting syntax for conversion of one data type to another.\n\nSee Also\n\nPostgreSQL value expressions\n\nCrateDB value expressions\n\nText search functions and operators\n\nThe functions and operators provided by PostgreSQL for full-text search (see PostgreSQL fulltext Search) are not compatible with those provided by CrateDB.\n\nIf you are missing features, functions or dialect improvements and have a great use case for it, let us know on GitHub. We’re always improving and extending CrateDB and we love to hear feedback."
  },
  {
    "title": "Release Notes — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/release-notes/index.html",
    "html": "5.6\nRelease Notes\n\nInformation about individual CrateDB releases, typically including upgrade information and changelog.\n\nNote\n\nThe latest stable, testing, and nightly version is always available from the CrateDB download page.\n\nOlder version of CrateDB are available in the release archives.\n\nVersions\n5.x\n5.6.x\nVersion 5.6.4 - Unreleased\nVersion 5.6.3\nVersion 5.6.2\nVersion 5.6.1\nVersion 5.6.0\n5.5.x\nVersion 5.5.5 - Unreleased\nVersion 5.5.4\nVersion 5.5.3\nVersion 5.5.2\nVersion 5.5.1\nVersion 5.5.0\n5.4.x\nVersion 5.4.8\nVersion 5.4.7\nVersion 5.4.6\nVersion 5.4.5\nVersion 5.4.4\nVersion 5.4.3\nVersion 5.4.2\nVersion 5.4.1\nVersion 5.4.0\n5.3.x\nVersion 5.3.9\nVersion 5.3.8\nVersion 5.3.7\nVersion 5.3.6\nVersion 5.3.5\nVersion 5.3.4\nVersion 5.3.3\nVersion 5.3.2\nVersion 5.3.1\nVersion 5.3.0\n5.2.x\nVersion 5.2.11\nVersion 5.2.10\nVersion 5.2.9\nVersion 5.2.8\nVersion 5.2.7\nVersion 5.2.6\nVersion 5.2.5\nVersion 5.2.4\nVersion 5.2.3\nVersion 5.2.2\nVersion 5.2.1\nVersion 5.2.0\n5.1.x\nVersion 5.1.4\nVersion 5.1.3\nVersion 5.1.2\nVersion 5.1.1\nVersion 5.1.0\n5.0.x\nVersion 5.0.3\nVersion 5.0.2\nVersion 5.0.1\nVersion 5.0.0\n4.x\n4.8.x\nVersion 4.8.4\nVersion 4.8.3\nVersion 4.8.2\nVersion 4.8.1\nVersion 4.8.0\n4.7.x\nVersion 4.7.3\nVersion 4.7.2\nVersion 4.7.1\nVersion 4.7.0\n4.6.x\nVersion 4.6.8\nVersion 4.6.7\nVersion 4.6.6\nVersion 4.6.5\nVersion 4.6.4\nVersion 4.6.3\nVersion 4.6.2\nVersion 4.6.1\nVersion 4.6.0\n4.5.x\nVersion 4.5.5\nVersion 4.5.4\nVersion 4.5.3\nVersion 4.5.2\nVersion 4.5.1\nVersion 4.5.0\n4.4.x\nVersion 4.4.3\nVersion 4.4.2\nVersion 4.4.1\nVersion 4.4.0\n4.3.x\nVersion 4.3.4\nVersion 4.3.3\nVersion 4.3.2\nVersion 4.3.1\nVersion 4.3.0\n4.2.x\nVersion 4.2.7\nVersion 4.2.6\nVersion 4.2.5\nVersion 4.2.4\nVersion 4.2.3\nVersion 4.2.2\nVersion 4.2.1\nVersion 4.2.0\n4.1.x\nVersion 4.1.8\nVersion 4.1.7\nVersion 4.1.6\nVersion 4.1.5\nVersion 4.1.4\nVersion 4.1.3\nVersion 4.1.2\nVersion 4.1.1\nVersion 4.1.0\n4.0.x\nVersion 4.0.12\nVersion 4.0.11\nVersion 4.0.10\nVersion 4.0.9\nVersion 4.0.8\nVersion 4.0.7\nVersion 4.0.6\nVersion 4.0.5\nVersion 4.0.4\nVersion 4.0.3\nVersion 4.0.2\nVersion 4.0.1\nVersion 4.0.0\n3.x\n3.3.x\nVersion 3.3.6\nVersion 3.3.5\nVersion 3.3.4\nVersion 3.3.3\nVersion 3.3.2\nVersion 3.3.1\nVersion 3.3.0\n3.2.x\nVersion 3.2.8\nVersion 3.2.7\nVersion 3.2.6\nVersion 3.2.5\nVersion 3.2.4\nVersion 3.2.3\nVersion 3.2.2\nVersion 3.2.1\nVersion 3.2.0\n3.1.x\nVersion 3.1.6\nVersion 3.1.5\nVersion 3.1.4\nVersion 3.1.3\nVersion 3.1.2\nVersion 3.1.1\nVersion 3.1.0\n3.0.x\nVersion 3.0.7\nVersion 3.0.6\nVersion 3.0.5\nVersion 3.0.4\nVersion 3.0.3\nVersion 3.0.2\nVersion 3.0.1\nVersion 3.0.0\n2.x\n2.3.x\nVersion 2.3.11\nVersion 2.3.10\nVersion 2.3.9\nVersion 2.3.8\nVersion 2.3.7\nVersion 2.3.6\nVersion 2.3.5\nVersion 2.3.4\nVersion 2.3.3\nVersion 2.3.2\nVersion 2.3.1\nVersion 2.3.0\n2.2.x\nVersion 2.2.7\nVersion 2.2.6\nVersion 2.2.5\nVersion 2.2.4\nVersion 2.2.3\nVersion 2.2.2\nVersion 2.2.1\nVersion 2.2.0\n2.1.x\nVersion 2.1.10\nVersion 2.1.9\nVersion 2.1.8\nVersion 2.1.7\nVersion 2.1.6\nVersion 2.1.5\nVersion 2.1.4\nVersion 2.1.3\nVersion 2.1.2\nVersion 2.1.1\nVersion 2.1.0\n2.0.x\nVersion 2.0.7\nVersion 2.0.6\nVersion 2.0.5\nVersion 2.0.4\nVersion 2.0.3\nVersion 2.0.2\nVersion 2.0.1\nVersion 2.0.0\n1.x\n1.2.x\nVersion 1.2.0\n1.1.x\nVersion 1.1.6\nVersion 1.1.5\nVersion 1.1.4\nVersion 1.1.3\nVersion 1.1.2\nVersion 1.1.1\nVersion 1.1.0\n1.0.x\nVersion 1.0.6\nVersion 1.0.5\nVersion 1.0.4\nVersion 1.0.3\nVersion 1.0.2\nVersion 1.0.1\nVersion 1.0.0\nOlder Versions\n\nFor older versions, see the 0.57.8 CHANGES.txt file."
  },
  {
    "title": "SQL Statements — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/statements/index.html",
    "html": "5.6\nSQL Statements\n\nTable of contents\n\nALTER CLUSTER\nALTER PUBLICATION\nALTER TABLE\nALTER ROLE\nALTER USER\nANALYZE\nBEGIN\nCLOSE\nCOMMIT\nCOPY FROM\nCOPY TO\nCREATE ANALYZER\nCREATE BLOB TABLE\nCREATE FUNCTION\nCREATE PUBLICATION\nCREATE REPOSITORY\nCREATE SNAPSHOT\nCREATE SUBSCRIPTION\nCREATE TABLE\nCREATE TABLE AS\nCREATE ROLE\nCREATE USER\nCREATE VIEW\nDEALLOCATE\nDECLARE\nDELETE\nDENY\nDISCARD\nDROP ANALYZER\nDROP FUNCTION\nDROP PUBLICATION\nDROP REPOSITORY\nDROP SNAPSHOT\nDROP SUBSCRIPTION\nDROP TABLE\nDROP ROLE\nDROP USER\nDROP VIEW\nEND\nEXPLAIN\nFETCH\nGRANT\nINSERT\nKILL\nOPTIMIZE\nREFRESH\nRESTORE SNAPSHOT\nREVOKE\nSELECT\nSET and RESET\nSET LICENSE\nSET AND RESET SESSION AUTHORIZATION\nSET TRANSACTION\nSHOW (session settings)\nSHOW COLUMNS\nSHOW CREATE TABLE\nSHOW SCHEMAS\nSHOW TABLES\nSTART TRANSACTION\nUPDATE\nVALUES\nWITH"
  },
  {
    "title": "HTTP endpoint — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/interfaces/http.html",
    "html": "5.6\nHTTP endpoint\n\nCrateDB provides a HTTP Endpoint that can be used to submit SQL queries. The endpoint is accessible under <servername:port>/_sql.\n\nSQL statements are sent to the _sql endpoint in json format, whereby the statement is sent as value associated to the key stmt.\n\nSee Also\n\nData manipulation\n\nA simple SELECT statement can be submitted like this:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' \\\n... -d '{\"stmt\":\"select name, position from locations order by id limit 2\"}'\n{\n  \"cols\": [\n    \"name\",\n    \"position\"\n  ],\n  \"rows\": [\n    [\n      \"North West Ripple\",\n      1\n    ],\n    [\n      \"Outer Eastern Rim\",\n      2\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nNote\n\nWe’re using a simple command line invocation of curl here so you can see how to run this by hand in the terminal. For the rest of the examples in this document, we use here documents (i.e. EOF) for multi line readability.\n\nTable of contents\n\nParameter substitution\n\nDefault schema\n\nColumn types\n\nAvailable data types\n\nBulk operations\n\nError handling\n\nError codes\n\nBulk errors\n\nParameter substitution\n\nIn addition to the stmt key the request body may also contain an args key which can be used for SQL parameter substitution.\n\nThe SQL statement has to be changed to use placeholders where the values should be inserted. Placeholders can either be numbered (in the form of $1, $2, etc.) or unnumbered using a question mark ?.\n\nThe placeholders will then be substituted with values from an array that is expected under the args key:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\":\n...     \"select date,position from locations\n...     where date <= \\$1 and position < \\$2 order by position\",\n...   \"args\": [\"1979-10-12\", 3]\n... }\n... EOF\n{\n  \"cols\": [\n    \"date\",\n    \"position\"\n  ],\n  \"rows\": [\n    [\n      308534400000,\n      1\n    ],\n    [\n      308534400000,\n      2\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nNote\n\nIn this example the placeholders start with an backslash due to shell escaping.\n\nWarning\n\nParameter substitution must not be used within subscript notation.\n\nFor example, column[?] is not allowed.\n\nThe same query using question marks as placeholders looks like this:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\":\n...     \"select date,position from locations\n...     where date <= ? and position < ? order by position\",\n...   \"args\": [\"1979-10-12\", 3]\n... }\n... EOF\n{\n  \"cols\": [\n    \"date\",\n    \"position\"\n  ],\n  \"rows\": [\n    [\n      308534400000,\n      1\n    ],\n    [\n      308534400000,\n      2\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nNote\n\nWith some queries the row count is not ascertainable. In this cases rowcount is -1.\n\nDefault schema\n\nIt is possible to set a default schema while querying the CrateDB cluster via _sql end point. In such case the HTTP request should contain the Default-Schema header with the specified schema name:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' \\\n... -H 'Default-Schema: doc' -d@- <<- EOF\n... {\n...   \"stmt\":\"select name, position from locations order by id limit 2\"\n... }\n... EOF\n{\n  \"cols\": [\n    \"name\",\n    \"position\"\n  ],\n  \"rows\": [\n    [\n      \"North West Ripple\",\n      1\n    ],\n    [\n      \"Outer Eastern Rim\",\n      2\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nIf the schema name is not specified in the header, the default doc schema will be used instead.\n\nColumn types\n\nCrateDB can respond a list col_types with the data type ID of every responded column. This way one can know what exact data type a column is holding.\n\nIn order to get the list of column data types, a types query parameter must be passed to the request:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql?types' -d@- <<- EOF\n... {\n...   \"stmt\":\n...     \"select date, position from locations\n...      where date <= \\$1 and position < \\$2 order by position\",\n...   \"args\": [\"1979-10-12\", 3]\n... }\n... EOF\n{\n  \"cols\": [\n    \"date\",\n    \"position\"\n  ],\n  \"col_types\": [\n    11,\n    9\n  ],\n  \"rows\": [\n    [\n      308534400000,\n      1\n    ],\n    [\n      308534400000,\n      2\n    ]\n  ],\n  \"rowcount\": 2,\n  \"duration\": ...\n}\n\n\nThe Array collection data type is displayed as a list where the first value is the collection type and the second is the inner type. The inner type could also be a collection.\n\nExample of JSON representation of a column list of (String, Integer[]):\n\n\"column_types\": [ 4, [ 100, 9 ] ]\n\nAvailable data types\n\nIDs of all currently available data types:\n\nID\n\n\t\n\nData type\n\n\n\n\n0\n\n\t\n\nNULL\n\n\n\n\n1\n\n\t\n\nNot supported\n\n\n\n\n2\n\n\t\n\nCHAR\n\n\n\n\n3\n\n\t\n\nBOOLEAN\n\n\n\n\n4\n\n\t\n\nTEXT\n\n\n\n\n5\n\n\t\n\nIP\n\n\n\n\n6\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\n7\n\n\t\n\nREAL\n\n\n\n\n8\n\n\t\n\nSMALLINT\n\n\n\n\n9\n\n\t\n\nINTEGER\n\n\n\n\n10\n\n\t\n\nBIGINT\n\n\n\n\n11\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\n12\n\n\t\n\nOBJECT\n\n\n\n\n13\n\n\t\n\nGEO_POINT\n\n\n\n\n14\n\n\t\n\nGEO_SHAPE\n\n\n\n\n15\n\n\t\n\nTIMESTAMP WITHOUT TIME ZONE\n\n\n\n\n16\n\n\t\n\nUnchecked object\n\n\n\n\n17\n\n\t\n\nINTERVAL\n\n\n\n\n19\n\n\t\n\nREGPROC\n\n\n\n\n20\n\n\t\n\nTIME\n\n\n\n\n21\n\n\t\n\nOIDVECTOR\n\n\n\n\n22\n\n\t\n\nNUMERIC\n\n\n\n\n23\n\n\t\n\nREGCLASS\n\n\n\n\n24\n\n\t\n\nDATE\n\n\n\n\n25\n\n\t\n\nBIT\n\n\n\n\n26\n\n\t\n\nJSON\n\n\n\n\n27\n\n\t\n\nCHARACTER\n\n\n\n\n28\n\n\t\n\nFLOAT VECTOR\n\n\n\n\n100\n\n\t\n\nARRAY\n\nBulk operations\n\nThe REST endpoint allows to issue bulk operations which are executed as single calls on the back-end site. It can be compared to prepared statement.\n\nA bulk operation can be expressed simply as an SQL statement.\n\nSupported bulk SQL statements are:\n\nInsert\n\nUpdate\n\nDelete\n\nInstead of the args (Parameter substitution) key, use the key bulk_args. This allows to specify a list of lists, containing all the records which shall be processed. The inner lists need to match the specified columns.\n\nThe bulk response contains a results array, with a row count for each bulk operation. Those results are in the same order as the issued operations of the bulk operation.\n\nThe following example describes how to issue an insert bulk operation and insert three records at once:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\": \"INSERT INTO locations (id, name, kind, description)\n...           VALUES (?, ?, ?, ?)\",\n...   \"bulk_args\": [\n...     [1337, \"Earth\", \"Planet\", \"An awesome place to spend some time on.\"],\n...     [1338, \"Sun\", \"Star\", \"An extraordinarily hot place.\"],\n...     [1339, \"Titan\", \"Moon\", \"Titan, where it rains fossil fuels.\"]\n...   ]\n... }\n... EOF\n{\n  \"cols\": [],\n  \"duration\": ...,\n  \"results\": [\n    {\n      \"rowcount\": 1\n    },\n    {\n      \"rowcount\": 1\n    },\n    {\n      \"rowcount\": 1\n    }\n  ]\n}\n\nError handling\n\nQueries that are invalid or cannot be satisfied will result in an error response. The response will contain an error code, an error message and in some cases additional arguments that are specific to the error code.\n\nClient libraries should use the error code to translate the error into an appropriate exception:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\":\"select name, position from foo.locations\"\n... }\n... EOF\n{\n  \"error\": {\n    \"message\": \"SchemaUnknownException[Schema 'foo' unknown]\",\n    \"code\": 4045\n  }\n}\n\n\nTo get more insight into what exactly went wrong an additional error_trace GET parameter can be specified to return the stack trace:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql?error_trace=true' -d@- <<- EOF\n... {\n...   \"stmt\":\"select name, position from foo.locations\"\n... }\n... EOF\n{\n  \"error\": {\n    \"message\": \"SchemaUnknownException[Schema 'foo' unknown]\",\n    \"code\": 4045\n  },\n  \"error_trace\": \"...\"\n}\n\n\nNote\n\nThis parameter is intended for CrateDB developers or for users requesting support for CrateDB. Client libraries shouldn’t make use of this option and not include the stack trace.\n\nError codes\n\nCode\n\n\t\n\nError\n\n\n\n\n4000\n\n\t\n\nThe statement contains an invalid syntax or unsupported SQL statement\n\n\n\n\n4001\n\n\t\n\nThe statement contains an invalid analyzer definition.\n\n\n\n\n4002\n\n\t\n\nThe name of the relation is invalid.\n\n\n\n\n4003\n\n\t\n\nField type validation failed\n\n\n\n\n4004\n\n\t\n\nPossible feature not supported (yet)\n\n\n\n\n4005\n\n\t\n\nAlter table using a table alias is not supported.\n\n\n\n\n4006\n\n\t\n\nThe used column alias is ambiguous.\n\n\n\n\n4007\n\n\t\n\nThe operation is not supported on this relation, as it is not accessible.\n\n\n\n\n4008\n\n\t\n\nThe name of the column is invalid.\n\n\n\n\n4009\n\n\t\n\nCrateDB License is expired. (Deprecated.)\n\n\n\n\n4010\n\n\t\n\nUser is not authorized to perform the SQL statement.\n\n\n\n\n4011\n\n\t\n\nMissing privilege for user.\n\n\n\n\n4031\n\n\t\n\nOnly read operations are allowed on this node.\n\n\n\n\n4041\n\n\t\n\nUnknown relation.\n\n\n\n\n4042\n\n\t\n\nUnknown analyzer.\n\n\n\n\n4043\n\n\t\n\nUnknown column.\n\n\n\n\n4044\n\n\t\n\nUnknown type.\n\n\n\n\n4045\n\n\t\n\nUnknown schema.\n\n\n\n\n4046\n\n\t\n\nUnknown Partition.\n\n\n\n\n4047\n\n\t\n\nUnknown Repository.\n\n\n\n\n4048\n\n\t\n\nUnknown Snapshot.\n\n\n\n\n4049\n\n\t\n\nUnknown user-defined function.\n\n\n\n\n40410\n\n\t\n\nUnknown user.\n\n\n\n\n4091\n\n\t\n\nA document with the same primary key exists already.\n\n\n\n\n4092\n\n\t\n\nA VersionConflict. Might be thrown if an attempt was made to update the same document concurrently.\n\n\n\n\n4093\n\n\t\n\nA relation with the same name exists already.\n\n\n\n\n4094\n\n\t\n\nThe used table alias contains tables with different schema.\n\n\n\n\n4095\n\n\t\n\nA repository with the same name exists already.\n\n\n\n\n4096\n\n\t\n\nA snapshot with the same name already exists in the repository.\n\n\n\n\n4097\n\n\t\n\nA partition for the same values already exists in this table.\n\n\n\n\n4098\n\n\t\n\nA user-defined function with the same signature already exists.\n\n\n\n\n4099\n\n\t\n\nA user with the same name already exists.\n\n\n\n\n5000\n\n\t\n\nUnhandled server error.\n\n\n\n\n5001\n\n\t\n\nThe execution of one or more tasks failed.\n\n\n\n\n5002\n\n\t\n\nOne or more shards are not available.\n\n\n\n\n5003\n\n\t\n\nThe query failed on one or more shards\n\n\n\n\n5004\n\n\t\n\nCreating a snapshot failed\n\n\n\n\n5030\n\n\t\n\nThe query was killed by a kill statement\n\nBulk errors\n\nIf a bulk operation fails, the resulting row count will be -2 and the resulting object may contain an error_message depending on the resulting error:\n\nsh$ curl -sS -H 'Content-Type: application/json' \\\n... -X POST '127.0.0.1:4200/_sql' -d@- <<- EOF\n... {\n...   \"stmt\": \"INSERT into locations (name, id) values (?,?)\",\n...   \"bulk_args\": [\n...     [\"Mars\", 1341],\n...     [\"Sun\", 1341]\n...   ]\n... }\n... EOF\n{\n  \"cols\": [],\n  \"duration\": ...,\n  \"results\": [\n    {\n      \"rowcount\": 1\n    },\n    {\n      \"rowcount\": -2\n    }\n  ]\n}\n\n\nNote\n\nEvery bulk operation will be executed, independent if one of the operation fails."
  },
  {
    "title": "General SQL — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/general/index.html",
    "html": "5.6\nGeneral SQL\n\nTable of contents\n\nConstraints\nValue expressions\nLexical structure"
  },
  {
    "title": "Usage Data Collector — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/udc.html",
    "html": "5.6\nUsage Data Collector\n\nThe CrateDB Usage Data Collector (UDC) is a sub-system that gathers usage data, reporting it to the UDC server at https://udc.crate.io. It is easy to disable, and does not collect any data that is confidential. For more information about what is being sent, see below.\n\nCrateDB uses this information as a form of automatic, effortless feedback from the community. We want to verify that we are doing the right thing by matching download statistics with usage statistics. After each release, we can see if there is a larger retention span of the server software.\n\nThe data collected is clearly stated here. If any future versions of this system collect additional data, we will clearly announce those changes.\n\nCrateDB is concerned about your privacy. We do not disclose any personally identifiable information.\n\nTable of contents\n\nTechnical information\n\nAdmin UI tracking\n\nConfiguration\n\nHow to disable UDC\n\nBy configuration\n\nBy system property\n\nTechnical information\n\nTo gather good statistics about CrateDB usage, UDC collects this information:\n\nName\n\n\t\n\nDescription\n\n\n\n\nKernel Version\n\n\t\n\nThe build number, and if there are any modifications to the kernel.\n\n\n\n\nCluster Id\n\n\t\n\nA randomized globally unique ID created every time the whole cluster is restarted.\n\n\n\n\nMaster\n\n\t\n\nBoolean whether the current node is master.\n\n\n\n\nPing Count\n\n\t\n\nUDC holds an internal counter per node which is incremented for every ping, and reset on every restart of the a node.\n\n\n\n\nCrateDB Version\n\n\t\n\nThe CrateDB version.\n\n\n\n\nJava Version\n\n\t\n\nThe Java version CrateDB is currently running with.\n\n\n\n\nHardware Address\n\n\t\n\nMAC address to uniquely identify instances behind firewalls.\n\n\n\n\nProcessor count\n\n\t\n\nNumber of available CPUs as reported by Runtime.availableProcessors\n\n\n\n\nEnterprise\n\n\t\n\nIdentifies whether the Enterprise Edition is used. 1\n\nAfter startup, UDC waits for 10 minutes before sending the first ping. It does this for two reasons; first, we don’t want the startup to be slower because of UDC, and secondly, we want to keep pings from automatic tests to a minimum. By default, UDC is sending pings every 24 hours. The ping to the UDC servers is done with a HTTP GET.\n\nAdmin UI tracking\n\nSince Admin UI v0.16.0 we are tracking the user ID along with the cluster ID to know how many active users are currently using CrateDB.\n\nConfiguration\n\nThe Usage Data Collector can be configured by adapting the crate.yml configuration file or adding a system property setting. Refer to Usage data collector to see how these settings can be accessed and how they are configured.\n\nHow to disable UDC\n\nBelow are two ways you can disable UDC. However we hope you support us offering the open source edition, and leave UDC on, so we learn how many people use CrateDB.\n\nBy configuration\n\nJust add following to your crate.yml configuration file:\n\nudc.enabled:  false\n\nBy system property\n\nIf you do not want to make any change to the jars or to the configuration, a system property setting like this will also make sure that UDC is never activated:\n\n-Cudc.enabled=false\n\n1\n\nThe “CrateDB Enterprise Edition” has been dissolved starting with CrateDB 4.5.0, see also Farewell to the CrateDB Enterprise License."
  },
  {
    "title": "Cloud discovery — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/discovery.html",
    "html": "5.6\nCloud discovery\n\nTable of contents\n\nAmazon EC2 discovery\n\nAmazon EC2 discovery\n\nCrateDB has native discovery support when running a cluster with Amazon Web Services (AWS). The discovery mechanism uses the Amazon EC2 API to generate the list of hosts for the unicast host discovery (see Unicast host discovery).\n\nThere is a best practice how to configure and run a CrateDB cluster on Amazon EC2."
  },
  {
    "title": "Snapshots — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/snapshots.html",
    "html": "5.6\nSnapshots\n\nTable of contents\n\nSnapshot\n\nCreating a repository\n\nCreating a snapshot\n\nRestore\n\nRestore data granularity\n\nCleanup\n\nDropping snapshots\n\nDropping repositories\n\nSnapshot\n\nIn CrateDB, backups are called Snapshots. They represent the state of the tables in a CrateDB cluster at the time the Snapshot was created. A Snapshot is always stored in a Repository which has to be created first.\n\nCaution\n\nYou cannot snapshot BLOB tables.\n\nCreating a repository\n\nRepositories are used to store, manage and restore snapshots.\n\nThey are created using the CREATE REPOSITORY statement:\n\ncr> CREATE REPOSITORY where_my_snapshots_go TYPE fs\n... WITH (location='repo_path', compress=true);\nCREATE OK, 1 row affected (... sec)\n\n\nRepositories are uniquely identified by their name. Every repository has a specific type which determines how snapshots are stored.\n\nCrateDB supports different repository types, see Types.\n\nThe creation of a repository configures it inside the CrateDB cluster. In general no data is written, no snapshots inside repositories changed or deleted. This way you can tell the CrateDB cluster about existing repositories which already contain snapshots.\n\nCreating a repository with the same name will result in an error:\n\ncr> CREATE REPOSITORY where_my_snapshots_go TYPE fs\n... WITH (location='another_repo_path', compress=false);\nRepositoryAlreadyExistsException[Repository 'where_my_snapshots_go' already exists]\n\nCreating a snapshot\n\nSnapshots are created inside a repository and can contain any number of tables. The CREATE SNAPSHOT statement is used to create a snapshots:\n\ncr> CREATE SNAPSHOT where_my_snapshots_go.snapshot1 ALL\n... WITH (wait_for_completion=true, ignore_unavailable=true);\nCREATE OK, 1 row affected (... sec)\n\n\nA snapshot is referenced by the name of the repository and the snapshot name, separated by a dot. If ALL is used, all user created tables of the cluster (except blob tables) are stored inside the snapshot.\n\nIt’s possible to only save a specific subset of tables in the snapshot by listing them explicitly:\n\ncr> CREATE SNAPSHOT where_my_snapshots_go.snapshot2 TABLE quotes, doc.locations\n... WITH (wait_for_completion=true);\nCREATE OK, 1 row affected (... sec)\n\n\nEven single partition of Partitioned tables can be selected for backup. This is especially useful if old partitions need to be deleted but it should be possible to restore them if needed:\n\ncr> CREATE SNAPSHOT where_my_snapshots_go.snapshot3 TABLE\n...    locations,\n...    parted_table PARTITION (date='1970-01-01')\n... WITH (wait_for_completion=true);\nCREATE OK, 1 row affected (... sec)\n\n\nSnapshots are incremental. Snapshots of the same cluster created later only store data not already contained in the repository.\n\nAll examples above are used with the argument wait_for_completion set to true. As described in the CREATE REPOSITORY reference documentation, by doing this, the statement will only respond (successfully or not) when the snapshot is fully created. Otherwise the snapshot will be created in the background and the statement will immediately respond as successful. The status of a created snapshot can be retrieved by querying the sys.snapshots system table.\n\nRestore\n\nCaution\n\nIf you are restoring a snapshot into a newer version of CrateDB, be sure to check the Release Notes for upgrade instructions.\n\nCaution\n\nIf you try to restore a table that already exists, CrateDB will return an error. However, if you try to restore metadata or cluster settings that already exist, they will be overwritten.\n\nOnce a snapshot is created, it can be used to restore its tables to the state when the snapshot was created.\n\nTo get basic information about snapshots the sys.snapshots table can be queried:\n\ncr> SELECT repository, name, state, concrete_indices\n... FROM sys.snapshots\n... ORDER BY repository, name;\n+-----------------------+-----------+---------+--------------------...-+\n| repository            | name      | state   | concrete_indices       |\n+-----------------------+-----------+---------+--------------------...-+\n| where_my_snapshots_go | snapshot1 | SUCCESS | [...]                  |\n| where_my_snapshots_go | snapshot2 | SUCCESS | [...]                  |\n| where_my_snapshots_go | snapshot3 | SUCCESS | [...]                  |\n+-----------------------+-----------+---------+--------------------...-+\nSELECT 3 rows in set (... sec)\n\n\nTo restore a table from a snapshot we have to drop it beforehand:\n\ncr> DROP TABLE quotes;\nDROP OK, 1 row affected (... sec)\n\n\nRestoring a snapshot using the RESTORE SNAPSHOT statement.:\n\ncr> RESTORE SNAPSHOT where_my_snapshots_go.snapshot2\n... TABLE quotes\n... WITH (wait_for_completion=true);\nRESTORE OK, 1 row affected (... sec)\n\n\nIn this case only the quotes table from snapshot where_my_snapshots_go.snapshot2 is restored.\n\nIt’s not possible to restore tables that exist in the current cluster:\n\ncr> RESTORE SNAPSHOT where_my_snapshots_go.snapshot2 TABLE quotes;\nRelationAlreadyExists[Relation 'doc.quotes' already exists.]\n\n\nSingle partitions can be either imported into an existing partitioned table the partition belongs to.\n\nTo monitor the progress of RESTORE SNAPSHOT operations please query the sys.snapshot_restore table.\n\ncr> RESTORE SNAPSHOT where_my_snapshots_go.snapshot3 TABLE\n...    parted_table PARTITION (date='1970-01-01')\n... WITH (wait_for_completion=true);\nRESTORE OK, 1 row affected (... sec)\n\n\nOr if no matching partition table exists, it will be implicitly created during restore.\n\nCaution\n\nThis is only possible with CrateDB version 0.55.5 or greater!\n\nSnapshots of single partitions that have been created with earlier versions of CrateDB may be restored, but lead to orphaned partitions!\n\nWhen using CrateDB prior to 0.55.5 you will have to create the table schema first before restoring.\n\ncr> RESTORE SNAPSHOT where_my_snapshots_go.snapshot3 TABLE\n...    parted_table PARTITION (date=0)\n... WITH (wait_for_completion=true);\nRESTORE OK, 1 row affected (... sec)\n\nRestore data granularity\n\nYou are not limited to only being able to restore individual tables (or table partitions). For example:\n\nYou can use ALL instead of listing all tables to restore the whole snapshot, including all metadata and settings.\n\nYou can use TABLES to restore all tables but no metadata or settings. On the other hand, you can use METADATA to restore everything but tables.\n\nYou can use USERMANAGEMENT to restore database users, roles and their privileges.\n\nSee the RESTORE SNAPSHOT documentation for all possible options.\n\nCleanup\nDropping snapshots\n\nDropping a snapshot deletes all files inside the repository that are only referenced by this snapshot. Due to its incremental nature this might be very few files (e.g. for intermediate snapshots). Snapshots are dropped using the DROP SNAPSHOT command:\n\ncr> DROP SNAPSHOT where_my_snapshots_go.snapshot3;\nDROP OK, 1 row affected (... sec)\n\nDropping repositories\n\nIf a repository is not needed anymore, it can be dropped using the DROP REPOSITORY statement:\n\ncr> DROP REPOSITORY \"OldRepository\";\nDROP OK, 1 row affected (... sec)\n\n\nThis statement, like CREATE REPOSITORY, does not manipulate repository contents but only deletes stored configuration for this repository in the cluster state, so it’s not accessible any more."
  },
  {
    "title": "Logical replication — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/logical-replication.html",
    "html": "5.6\nLogical replication\n\nTable of contents\n\nPublication\n\nSubscription\n\nSecurity\n\nMonitoring\n\nLogical replication is a method of data replication across multiple clusters. CrateDB uses a publish and subscribe model where subscribers pull data from the publications of the publisher they subscribed to.\n\nReplicated tables on a subscriber can again be published further to other clusters and thus chaining subscriptions is possible.\n\nNote\n\nA replicated index on a subscriber is read-only.\n\nLogical replication is useful for the following use cases:\n\nConsolidating data from multiple clusters into a single one for aggregated reports.\n\nEnsure high availability if one cluster becomes unavailable.\n\nReplicating between different compatible versions of CrateDB. Replicating tables created on a cluster with higher major/minor version to a cluster with lower major/minor version is not supported.\n\nSee Also\n\nreplication.logical.ops_batch_size replication.logical.reads_poll_duration replication.logical.recovery.chunk_size replication.logical.recovery.max_concurrent_file_chunks\n\nPublication\n\nA publication is the upstream side of logical replication and it’s created on the cluster which acts as a data source.\n\nEach table can be added to multiple publications if needed. Publications can only contain tables. All operation types (INSERT, UPDATE, DELETE and schema changes) are replicated.\n\nEvery publication can have multiple subscribers.\n\nA publication is created using the CREATE PUBLICATION command. The individual tables can be added or removed dynamically using ALTER PUBLICATION. Publications can be removed using the DROP PUBLICATION command.\n\nCaution\n\nThe publishing cluster must have soft_deletes.enabled set to true so that a subscribing cluster can catch up with all changes made during replication pauses caused by network issues or explicitly done by a user.\n\nAlso, soft_deletes.retention_lease.period should be greater than or equal to replication.logical.reads_poll_duration.\n\nSubscription\n\nA subscription is the downstream side of logical replication. A subscription defines the connection to another database and set of publications to which it wants to subscribe. By default, the subscription creation triggers the replication process on the subscriber cluster. The subscriber cluster behaves in the same way as any other CrateDB cluster and can be used as a publisher for other clusters by defining its own publications.\n\nA cluster can have multiple subscriptions. It is also possible for a cluster to have both subscriptions and publications. A cluster cannot subscribe to locally already existing tables, therefore it is not possible to setup a bi-directional replication (both sides subscribing to ALL TABLES leads to a cluster trying to replicate its own tables from another cluster). However, two clusters still can cross-subscribe to each other if one cluster subscribes to locally non-existing tables of another cluster and vice versa.\n\nA subscription is added using the CREATE SUBSCRIPTION command and can be removed using the DROP SUBSCRIPTION command. A subscription starts replicating on its creation and stops on its removal (if no failure happen in-between).\n\nPublished tables must not exist on the subscriber. A cluster cannot subscribe to a table on another cluster if it exists already on its side, therefore it’s not possible to drop and re-create a subscription without starting from scratch i.e removing all replicated tables.\n\nOnly regular tables (including partitions) may be the target of a replication. For example, you can not replicate system tables or views.\n\nThe tables are matched between the publisher and the subscriber using the fully qualified table name. Replication to differently-named tables on the subscriber is not supported.\n\nSecurity\n\nTo create, alter or drop a publication, a user must have the AL privilege on the cluster. Only the owner (the user who created the publication) or a superuser is allowed to ALTER or DROP a publication. To add tables to a publication, the user must have DQL, DML, and DDL privileges on the table. When a user creates a publication that publishes all tables automatically, only those tables where the user has DQL, DML, and DDL privileges will be published. The user a subscriber uses to connect to the publisher must have DQL privileges on the published tables. Tables, included into a publication but not available for a subscriber due to lack of DQL privilege, will not be replicated.\n\nTo create or drop a subscription, a user must have the AL privilege on the cluster. Only the owner (the user who created the subscription) or a superuser is allowed to DROP a subscription.\n\nCaution\n\nA network setup that allows the two clusters to communicate is a pre-requisite for a working publication/subscription setup. See HBA.\n\nMonitoring\n\nAll publications are listed in the pg_publication table. More details for a publication are available in the pg_publication_tables table. It lists the replicated tables for a specific publication.\n\nAll subscriptions are listed in the pg_subscription table. More details for a subscription are available in the pg_subscription_rel table. The table contains detailed information about the replication state per table, including error messages if there was an error."
  },
  {
    "title": "Jobs management — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/jobs-management.html",
    "html": "5.6\nJobs management\n\nEach executed SQL statement results in a corresponding job. Jobs that are currently executing are logged in the system table sys.jobs (see Jobs, operations, and logs).\n\nTo obtain the UUID of a job, stats needs to be enabled (see Collecting stats). Job logging can be disabled by setting the queue size to zero.\n\nKilling an active job forces CrateDB to stop its execution on the cluster immediately. There are two different SQL commands available for killing jobs.\n\nThe KILL ALL statement stops every single job on each node that is running. It returns the total number of contexts of all jobs that have been killed. A job can have contexts on multiple nodes.\n\ncr> kill all;\nKILL OK, ... rows affected (... sec)\n\n\nKILL job_id kills one single job with the specified job_id. Like KILL ALL it returns the total number of contexts of that job killed on all nodes.\n\ncr> kill '175011ce-9bbc-45f2-a86a-5b7f993a93a6';\nKILL OK, ... rows affected (... sec)\n\n\nSee KILL for detailed syntax information on KILL statements."
  },
  {
    "title": "JMX monitoring — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/monitoring.html",
    "html": "5.6\nJMX monitoring\n\nThe JMX monitoring feature exposes query metrics via the JMX API.\n\nTable of contents\n\nSetup\n\nEnable collecting stats\n\nEnable the JMX API\n\nUsing Docker\n\nJMX Beans\n\nQueryStats MBean\n\nNodeStatus MBean\n\nNodeInfo MXBean\n\nConnections MBean\n\nThreadPools MXBean\n\nCircuitBreakers MXBean\n\nExposing JMX via HTTP\n\nSetup\nEnable collecting stats\n\nBy default, Collecting stats is enabled. You can disable collecting stats via the CrateDB configuration file or by running this statement:\n\ncr> SET GLOBAL \"stats.enabled\" = FALSE;\n\nEnable the JMX API\n\nTo monitor CrateDB using the JMX API, you must set the following system properties before you start CrateDB:\n\ncom.sun.management.jmxremote\ncom.sun.management.jmxremote.port=<JMX_PORT>\ncom.sun.management.jmxremote.ssl=false\ncom.sun.management.jmxremote.authenticate=false\n\n\nHere, <JMX_PORT> sets the port number of your JMX server. JMX SSL and authentication are currently not supported.\n\nMore information about the JMX monitoring properties can be found in the JMX documentation.\n\nYou can set the Java system properties with the -D option:\n\nsh$ ./bin/crate -Dcom.sun.management.jmxremote \\\n...             -Dcom.sun.management.jmxremote.port=7979 \\\n...             -Dcom.sun.management.jmxremote.ssl=false \\\n...             -Dcom.sun.management.jmxremote.authenticate=false\n\n\nHowever, the recommended way to set system properties is via the CRATE_JAVA_OPTS environment variable, like so:\n\nsh$ export CRATE_JAVA_OPTS=\"$CRATE_JAVA_OPTS \\\n      -Dcom.sun.management.jmxremote \\\n      -Dcom.sun.management.jmxremote.port=7979 \\\n      -Dcom.sun.management.jmxremote.ssl=false \\\n      -Dcom.sun.management.jmxremote.authenticate=false\"\nsh$ ./bin/crate\n\n\nIf you’re using the CrateDB Debian or RPM packages, you can set this environment variable via the /etc/default/crate configuration file.\n\nUsing Docker\n\nTo enable JMX monitoring when running CrateDB in a Docker container you have to set the following additional Java system properties:\n\n-Djava.rmi.server.hostname=<RMI_HOSTNAME>\n-Dcom.sun.management.jmxremote.rmi.port=<RMI_PORT>\n\n\nHere, <RMI_HOSTNAME> is the IP address or hostname of the Docker host and <RMI_PORT> is the statically assigned port of the RMI server. For convenience, <RMI_PORT> can be set to the same port the JMX server listens on.\n\nThe <RMI_HOSTNAME> and <RMI_PORT> can be used by JMX clients (e.g. JConsole or VisualVM) to connect to the JMX server.\n\nHere’s an example Docker command:\n\nsh> docker run -d --env CRATE_HEAP_SIZE=1g -e CRATE_JAVA_OPTS=\"\\\n      -Dcom.sun.management.jmxremote\n      -Dcom.sun.management.jmxremote.port=7979 \\\n      -Dcom.sun.management.jmxremote.ssl=false \\\n      -Dcom.sun.management.jmxremote.authenticate=false \\\n      -Dcom.sun.management.jmxremote.rmi.port=7979 \\\n      -Djava.rmi.server.hostname=<RMI_HOSTNAME>\" \\\n      -p 7979:7979 crate -Cnetwork.host=_site_\n\n\nHere, again, <RMI_HOSTNAME> is the IP address or hostname of the Docker host.\n\nJMX Beans\nQueryStats MBean\n\nThe QueryStats MBean exposes the sum of durations, in milliseconds, total and failed count of all statements executed since the node was started, grouped by type, for SELECT, UPDATE, DELETE, INSERT, MANAGEMENT, DDL, COPY and UNDEFINED queries.\n\nMetrics can be accessed using the JMX MBean object name io.crate.monitoring:type=QueryStats and the following attributes:\n\nStatements total count since the node was started:\n\nSelectQueryTotalCount\n\nInsertQueryTotalCount\n\nUpdateQueryTotalCount\n\nDeleteQueryTotalCount\n\nManagementQueryTotalCount\n\nDDLQueryTotalCount\n\nCopyQueryTotalCount\n\nUndefinedQueryTotalCount\n\nStatements failed count since the node was started:\n\nSelectQueryFailedCount\n\nInsertQueryFailedCount\n\nUpdateQueryFailedCount\n\nDeleteQueryFailedCount\n\nManagementQueryFailedCount\n\nDDLQueryFailedCount\n\nCopyQueryFailedCount\n\nUndefinedQueryFailedCount\n\nThe sum of the durations, in milliseconds, since the node was started, of all statement executions grouped by type:\n\nSelectQuerySumOfDurations\n\nInsertQuerySumOfDurations\n\nUpdateQuerySumOfDurations\n\nDeleteQuerySumOfDurations\n\nManagementQuerySumOfDurations\n\nDDLQuerySumOfDurations\n\nCopyQuerySumOfDurations\n\nUndefinedQuerySumOfDurations\n\nNodeStatus MBean\n\nThe NodeStatus JMX MBean exposes the status of the current node as boolean values.\n\nNodeStatus can be accessed using the JMX MBean object name io.crate.monitoring:type=NodeStatus and the following attributes:\n\nReady\n\nDefines if the node is able to process SQL statements.\n\nNodeInfo MXBean\n\nThe NodeInfo JMX MXBean exposes information about the current node.\n\nNodeInfo can be accessed using the JMX MXBean object name io.crate.monitoring:type=NodeInfo and the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nNodeId\n\n\t\n\nProvides the unique identifier of the node in the cluster.\n\n\n\n\nNodeName\n\n\t\n\nProvides the human friendly name of the node.\n\n\n\n\nClusterStateVersion\n\n\t\n\nProvides the version of the current applied cluster state.\n\n\n\n\nShardStats\n\n\t\n\nStatistics about the number of shards located on the node.\n\n\n\n\nShardInfo\n\n\t\n\nDetailed information about the shards located on the node.\n\nShardStats returns a CompositeData object containing statistics about the number of shards located on the node with the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nTotal\n\n\t\n\nThe number of shards located on the node.\n\n\n\n\nPrimaries\n\n\t\n\nThe number of primary shards located on the node.\n\n\n\n\nReplicas\n\n\t\n\nThe number of replica shards located on the node.\n\n\n\n\nUnassigned\n\n\t\n\nThe number of unassigned shards in the cluster. If the node is the elected master node in the cluster, this will show the total number of unassigned shards in the cluster, otherwise 0.\n\nShardInfo returns an Array of CompositeData objects containing detailed information about the shards located on the node with the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nId\n\n\t\n\nThe shard id. This shard id is managed by the system, ranging from 0 up to the number of configured shards of the table.\n\n\n\n\nTable\n\n\t\n\nThe name of the table this shard belongs to.\n\n\n\n\nPartitionIdent\n\n\t\n\nThe partition ident of a partitioned table. Empty for non-partitioned tables.\n\n\n\n\nRoutingState\n\n\t\n\nThe current state of the shard in the routing table. Possible states are:\n\nUNASSIGNED\n\nINITIALIZING\n\nSTARTED\n\nRELOCATING\n\n\n\n\nState\n\n\t\n\nThe current state of the shard. Possible states are:\n\nCREATED\n\nRECOVERING\n\nPOST_RECOVERY\n\nSTARTED\n\nRELOCATED\n\nCLOSED\n\nINITIALIZING\n\nUNASSIGNED\n\n\n\n\nSize\n\n\t\n\nThe estimated cumulated size in bytes of all files of this shard.\n\nConnections MBean\n\nThe Connections MBean exposes information about any open connections to a CrateDB node.\n\nIt can be accessed using the io.crate.monitoring:type=Connections object name and has the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nHttpOpen\n\n\t\n\nThe number of currently established connections via HTTP\n\n\n\n\nHttpTotal\n\n\t\n\nThe number of total connections established via HTTP over the life time of a node\n\n\n\n\nPsqlOpen\n\n\t\n\nThe number of currently established connections via the PostgreSQL protocol\n\n\n\n\nPsqlTotal\n\n\t\n\nThe number of total connections established via the PostgreSQL protocol over the life time of a node\n\n\n\n\nTransportOpen\n\n\t\n\nThe number of currently established connections via the transport protocol\n\nThreadPools MXBean\n\nThe ThreadPools MXBean exposes statistical information about the used thread pools of a CrateDB node.\n\nIt can be accessed using the io.crate.monitoring:type=ThreadPools object name and has following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nGeneric\n\n\t\n\nThread pool statistics of the generic thread pool.\n\n\n\n\nSearch\n\n\t\n\nThread pool statistics of the search thread pool used by read statements on user generated tables.\n\n\n\n\nWrite\n\n\t\n\nThread pool statistics of the write thread pool used for writing and deleting data.\n\n\n\n\nManagement\n\n\t\n\nThread pool statistics of the management thread pool used by management tasks like stats collecting, repository information, shard allocations, etc.\n\n\n\n\nFlush\n\n\t\n\nThread pool statistics of the flush thread pool used for fsyncing to disk and merging segments in the storage engine.\n\n\n\n\nRefresh\n\n\t\n\nThread pool statistics of the refresh thread pool used for automatic and on-demand refreshing of tables\n\n\n\n\nSnapshot\n\n\t\n\nThread pool statistics of the snapshot thread pool used for creating and restoring snapshots.\n\n\n\n\nForceMerge\n\n\t\n\nThread pool statistics of the force_merge thread pool used when running an optimize statement.\n\n\n\n\nListener\n\n\t\n\nThread pool statistics of the listener thread pool used on client nodes for asynchronous result listeners.\n\n\n\n\nGet\n\n\t\n\nThread pool statistics of the get thread pool used when querying sys.nodes or sys.shards.\n\n\n\n\nFetchShardStarted\n\n\t\n\nThread pool statistics of the fetch_shard_started thread pool used on shard allocation .\n\n\n\n\nFetchShardStore\n\n\t\n\nThread pool statistics of the fetch_shard_store used on shard replication.\n\nEach of them returns a CompositeData object containing detailed statistics of each thread pool with the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\npoolSize\n\n\t\n\nThe current number of threads in the pool.\n\n\n\n\nlargestPoolSize\n\n\t\n\nThe largest number of threads that have ever simultaneously been in the pool.\n\n\n\n\nqueueSize\n\n\t\n\nThe current number of tasks in the queue.\n\n\n\n\nactive\n\n\t\n\nThe approximate number of threads that are actively executing tasks.\n\n\n\n\ncompleted\n\n\t\n\nThe approximate total number of tasks that have completed execution.\n\n\n\n\nrejected\n\n\t\n\nThe number of rejected executions.\n\nCircuitBreakers MXBean\n\nThe CircuitBreaker MXBean exposes statistical information about all available circuit breakers of a CrateDB node.\n\nIt can be accessed using the io.crate.monitoring:type=CircuitBreakers object name and has following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nParent\n\n\t\n\nStatistics of the parent circuit breaker containing summarized counters across all circuit breakers.\n\n\n\n\nQuery\n\n\t\n\nStatistics of the query circuit breaker used to account memory usage of SQL execution including intermediate states e.g. on aggregation and resulting rows.\n\n\n\n\nJobsLog\n\n\t\n\nStatistics of the jobs_log circuit breaker used to account memory usage of the sys.jobs_log table.\n\n\n\n\nOperationsLog\n\n\t\n\nStatistics of the operations_log circuit breaker used to account memory usage of the sys.operations_log table.\n\n\n\n\nFieldData\n\n\t\n\nStatistics of the field_data circuit breaker used for estimating the amount of memory a field will require to be loaded into memory.\n\n\n\n\nInFlightRequests\n\n\t\n\nStatistics of the in_flight_requests circuit breaker used to account memory usage of all incoming requests on transport or HTTP level.\n\n\n\n\nRequest\n\n\t\n\nStatistics of the request circuit breaker used to account memory usage of per-request data structure.\n\nEach of them returns a CompositeData object containing detailed statistics of each circuit breaker with the following attributes:\n\nName\n\n\t\n\nDescription\n\n\n\n\nname\n\n\t\n\nThe circuit breaker name this statistic belongs to.\n\n\n\n\nused\n\n\t\n\nThe currently accounted used memory estimations.\n\n\n\n\nlimit\n\n\t\n\nThe configured limit when to trip.\n\n\n\n\ntrippedCount\n\n\t\n\nThe total number of occurred trips.\n\nExposing JMX via HTTP\n\nThe JMX metrics and a readiness endpoint can be exposed via HTTP (e.g. to be used by Prometheus) by using the Crate JMX HTTP Exporter Java agent. See the README in the Crate JMX HTTP Exporter repository for more information."
  },
  {
    "title": "Secured communications (SSL/TLS) — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/ssl.html",
    "html": "5.6\nSecured communications (SSL/TLS)\n\nYou can encrypt the internal communication between CrateDB nodes and the external communication with HTTP and PostgreSQL clients. When you configure encryption, CrateDB secures connections using Transport Layer Security (TLS).\n\nYou can enable SSL on a per-protocol basis:\n\nIf you enable SSL for HTTP, all connections will require HTTPS.\n\nBy default, if you enable SSL for the PostgreSQL wire protocol, clients can negotiate on a per-connection basis whether to use SSL. However, you can enforce SSL via Host-Based Authentication.\n\nIf you enable SSL for the CrateDB transport protocol (used for intra-node communication), nodes only accept SSL connections (ssl.transport.mode set to on).\n\nTip\n\nYou can use on SSL mode to configure a multi-zone cluster to ensure encryption for nodes communicating between zones. Please note, that SSL has to be on in all nodes as communication is point-2-point, and intra-zone communication will also be encrypted.\n\nTable of contents\n\nSSL/TLS configuration\n\nConfiguring the Keystore\n\nConfiguring a separate Truststore\n\nConnecting to a CrateDB node using HTTPS\n\nConnect to a CrateDB node using the Admin UI\n\nConnect to a CrateDB node using Crash\n\nConnect to a CrateDB node using REST\n\nConnecting to a CrateDB node using the PostgreSQL wire protocol with SSL/TLS\n\nConnect to a CrateDB node using JDBC\n\nConnect to a CrateDB node using psql\n\nSetting up a Keystore/Truststore with a certificate chain\n\nGenerate Keystore with a private key\n\nGenerate a certificate signing request\n\nOptional: Use a self-signed certificate to act as a Certificate Authority (CA)\n\nGenerate a self-signed certificate\n\nGenerate a signed cert\n\nImport the CA certificate into the Keystore\n\nImport CA into Truststore\n\nImport the signed certificate\n\nConfiguring CrateDB\n\nSSL/TLS configuration\n\nTo enable SSL a keystore and a few configuration changes are necessary. These changes need to be made in the crate.yml file on each node that should have secure communications enabled.\n\nSkip to Generate Keystore with a private key for a step-by-step instruction on how to create a keystore.\n\nOnce the keystore (and optional truststore) is created, continue with the following steps:\n\nSet ssl.psql.enabled or ssl.http.enabled to true.\n\nSet ssl.transport.mode to on.\n\nConfiguring the Keystore\n\n(Optional) Configuring a separate Truststore\n\nNote\n\nCrateDB monitors SSL files such as keystore and truststore that are configured as values of the node settings. If any of these files are updated CrateDB dynamically reloads them. The polling frequency of the files is set via the ssl.resource_poll_interval setting.\n\nConfiguring the Keystore\n\nSSL/TLS needs a keystore. The keystore holds the node certificate(s) which should be signed by a certificate authority (CA). A third-party CA or your organization’s existing CA can be used.\n\nWhen a client connects to a node using SSL/TLS, the client receives the certificate provided by the node and will determine if the node’s certificate is valid, trusted, and matches the hostname or IP address it is trying to connect to.\n\nCaution\n\nTechnically, it’s possible to disable CA checks for certificates on the client. It is strongly recommended however to use certificates signed by an official CA or by a private CA (company PKI) that is also known to the client. This will help to ensure that establishing trust is as painless as possible.\n\nSee Generate Keystore with a private key for information about how to create a keystore.\n\nOnce the keystore is prepared, define the absolute file path to the keystore .jks file on the node using ssl.keystore_filepath setting.\n\nNote\n\nMake sure that the keystore file has the right permissions and is accessible by the system user crate.\n\nAlso, define the password needed to decrypt the keystore by using the ssl.keystore_password setting.\n\nUse ssl.keystore_key_password setting to define the key password used when creating the Keystore.\n\nFor a full list of the settings needed to configure SSL/TLS, refer to SSL configuration reference.\n\nConfiguring a separate Truststore\n\nTrusted CA certificates can be stored in a node’s keystore or a separate truststore can be used to store them.\n\nIf you want to use a separate truststore, create a node truststore and import the CA certificate(s) you want to trust. Once the truststore is prepared, define the absolute file path of the truststore .jks file on the node using the ssl.truststore_filepath setting.\n\nNote\n\nMake sure that the truststore file has the right permissions and is accessible by the system user crate.\n\nAlso define the password needed to decrypt the keystore by using the ssl.truststore_password setting.\n\nFor a full list of the settings needed to configure SSL/TLS, refer to SSL configuration reference.\n\nConnecting to a CrateDB node using HTTPS\nConnect to a CrateDB node using the Admin UI\n\nCrate’s HTTP endpoint remains unchanged. When you have turned on secure communication, it will use HTTPS instead of plain HTTP. Simply point your browser to the same URL you used before but changing the protocol to HTTPS:\n\nFor example, https://localhost:4200 becomes https://localhost:4200. If you have not configured the CrateDB node’s keystore with a signed certificate from a Certificate Authority (CA), then you will get something like the following: NET::ERR_CERT_AUTHORITY_INVALID. You either need to get your certificate signed from one of the CAs included in your browser or import your owned certificates into the browser. A third option is storing an exception for the CrateDB node certification in your browser after verifying that this is indeed a certificate you trust.\n\nConnect to a CrateDB node using Crash\n\nYou can connect to a CrateDB node using a secure communication:\n\ncrash --hosts https://localhost:4200\n\n\nTo validate the provided certificates, please see the options --verify-ssl and --key-file.\n\nConnect to a CrateDB node using REST\n\nIssue your REST requests to the node using the https:// protocol. You may have to configure your client to validate the received certificate accordingly.\n\nConnecting to a CrateDB node using the PostgreSQL wire protocol with SSL/TLS\nConnect to a CrateDB node using JDBC\n\nJDBC needs to validate the CrateDB node’s identity by checking that the node certificate is signed by a trusted authority. If the certificate is signed by a certificate authority (CA) that is known to the Java runtime, there is nothing further to do (as Java comes with copies of the most common CA’s certificates).\n\nIf you have a certificate that is signed by a CA not known to the Java runtime, you need to configure a truststore which contains the node’s certificate and provide the path to the truststore file along with the password when starting your Java application:\n\njava -Djavax.net.ssl.trustStore=mystore -Djavax.net.ssl.trustStorePassword=mypassword com.mycompany.MyApp\n\n\nIn case you face any issues extra debugging information is available by adding -Djavax.net.debug=ssl to your command line.\n\nLast but not least, the connection parameter ssl=true must be added to the connection URL so that the JDBC driver will try and establish an SSL connection.\n\nFor further information, visit JDBC SSL documentation.\n\nConnect to a CrateDB node using psql\n\nBy default, psql attempts to use SSL if available on the node. For further information including the different SSL modes please visit the PSQL documentation.\n\nSetting up a Keystore/Truststore with a certificate chain\n\nIn case you need to setup a Keystore or a Truststore, here are the commands to get you started. All the commands use a validity of 36500 days (about 100 years). You might want to use less.\n\nGenerate Keystore with a private key\n\nThe first step is to create a Keystore with a private key using the RSA algorithm. The “first and last name” is the common name (CN) which should overlap with the URL the service it is used with.\n\nCommand:\n\nkeytool -keystore keystore -genkey -keyalg RSA -alias server -validity 36500\n\n\nOutput:\n\nEnter keystore password:\nRe-enter new password:\nWhat is your first and last name?\n  [Unknown]:  ssl.crate.io\nWhat is the name of your organizational unit?\n  [Unknown]:  Cryptography Department\nWhat is the name of your organization?\n  [Unknown]:  Crate.io GmbH\nWhat is the name of your City or Locality?\n  [Unknown]:  Berlin\nWhat is the name of your State or Province?\n  [Unknown]:  Berlin\nWhat is the two-letter country code for this unit?\n  [Unknown]:  DE\nIs CN=ssl.crate.io, OU=Cryptography Department, O=Crate.io GmbH, L=Berlin, ST=Berlin, C=DE correct?\n  [no]:  yes\n\nEnter key password for <server>\n    (RETURN if same as keystore password):\nRe-enter new password:\n\nGenerate a certificate signing request\n\nTo establish trust for this key, we need to sign it. This is done by generating a certificate signing request.\n\nIf you have access to a certificate authority (CA), you can skip the next steps and get the signed certificate from the CA using the signing request which we will generate with the command below. If you don’t have access to a CA, then follow the optional steps after this step to establish your own CA.\n\nCommand:\n\nkeytool -keystore keystore -certreq -alias server -keyalg RSA -file server.csr\n\n\nOutput:\n\nEnter keystore password:\nEnter key password for <server>\n\nOptional: Use a self-signed certificate to act as a Certificate Authority (CA)\n\nNote\n\nOnly follow these optional steps if you want to create your own Certificate Authority (CA). Otherwise, please request a signed certificate from one of the CAs bundled with Java.\n\nGenerate a self-signed certificate\n\nIf you don’t get your certificate signed from one of the official CAs, you might want to create your own CA with a self-signed certificate. The common name (CN) should overlap with the CN of the server key generated in the first step. For example, ssl.crate.io overlaps with *.crate.io.\n\nNote\n\nIn this step by step guide it is shown how to create a server certificate. If you want to create a client certificate the steps are almost the same with the exception of providing a common name that is equivalent to the crate username as described in client certificate authentication method.\n\nCommand:\n\nopenssl req -x509 -sha256 -nodes -days 36500 -newkey rsa:2048 \\\n    -keyout rootCA.key -out rootCA.crt\n\n\nOutput:\n\nGenerating a 2048 bit RSA private key\n.......................................................................+++\n.............................................................+++\nwriting new private key to 'rootCA.key'\n-----\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n-----\nCountry Name (2 letter code) [AU]:AT\nState or Province Name (full name) [Some-State]:Vorarlberg\nLocality Name (eg, city) []:Dornbirn\nOrganization Name (eg, company) [Internet Widgits Pty Ltd]:Crate.io\nOrganizational Unit Name (eg, section) []:Cryptography Department\nCommon Name (e.g. server FQDN or YOUR name) []:*.crate.io\nEmail Address []:info@crate.io\n\nGenerate a signed cert\n\nIn order that the server can prove itself to have a valid and trusted domain it is required that the server certificate contains subjectAltName.\n\nCreate a file called ssl.ext with the following content. In section [alt_names] list valid domain names of the server:\n\nauthorityKeyIdentifier=keyid,issuer\nbasicConstraints=CA:FALSE\nkeyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment\nsubjectAltName = @alt_names\n\n[alt_names]\nDNS.1 = www.example.com\n\n\nNow you can generate a signed cert from our certificate signing request.\n\nCommand:\n\nopenssl x509 -req -in server.csr -CA rootCA.crt -CAkey rootCA.key \\\n    -CAcreateserial -out server.crt -sha256 -days 36500 -extfile ssl.ext\n\n\nOutput:\n\nSignature ok\nsubject=/C=DE/ST=Berlin/L=Berlin/O=Crate.io GmbH/OU=Cryptography Department/CN=ssl.crate.io\nGetting CA Private Key\n\nImport the CA certificate into the Keystore\n\nThe CA needs to be imported to the Keystore for the certificate chain to be available when we import our signed certificate.\n\nCommand:\n\nkeytool -import -keystore keystore -file rootCA.crt -alias theCARoot\n\n\nOutput:\n\nEnter keystore password:\nOwner: EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT\nIssuer: EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT\nSerial number: f13562ec6184401e\nValid from: Mon Jun 12 13:09:17 CEST 2017 until: Wed May 19 13:09:17 CEST 2117\nCertificate fingerprints:\n     MD5:  BB:A1:79:53:FE:71:EC:61:2A:19:81:E8:0E:E8:C9:81\n     SHA1: 96:66:C1:01:49:17:D1:19:FB:DB:83:86:50:3D:3D:AD:DA:F7:C6:A9\n     SHA256: 69:82:C5:24:9A:A1:AE:DF:80:29:7A:26:92:C1:A5:9F:AF:7D:03:56:CC:C3:E9:73:3B:FD:85:66:35:D6:8A:9B\n     Signature algorithm name: SHA256withRSA\n     Version: 3\n\nExtensions:\n\n#1: ObjectId: 2.5.29.35 Criticality=false\nAuthorityKeyIdentifier [\nKeyIdentifier [\n0000: CD 29 4E 07 3D C3 7C D0   16 45 FB 0A CE 8D B4 98  .)N.=....E......\n0010: B7 A8 4C 79                                        ..Ly\n]\n[EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT]\nSerialNumber: [    f13562ec 6184401e]\n]\n\n#2: ObjectId: 2.5.29.19 Criticality=false\nBasicConstraints:[\n  CA:true\n  PathLen:2147483647\n]\n\n#3: ObjectId: 2.5.29.14 Criticality=false\nSubjectKeyIdentifier [\nKeyIdentifier [\n0000: CD 29 4E 07 3D C3 7C D0   16 45 FB 0A CE 8D B4 98  .)N.=....E......\n0010: B7 A8 4C 79                                        ..Ly\n]\n]\n\nTrust this certificate? [no]:  yes\nCertificate was added to keystore\n\nImport CA into Truststore\n\nIf we are using our own CA, we should also import the certificate to the Truststore, such that it is available for clients which want to verify signatures.\n\nCommand:\n\nkeytool -import -keystore truststore -file rootCA.crt -alias theCARoot\n\n\nOutput:\n\nEnter keystore password:\nRe-enter new password:\nOwner: EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT\nIssuer: EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT\nSerial number: f13562ec6184401e\nValid from: Mon Jun 12 13:09:17 CEST 2017 until: Wed May 19 13:09:17 CEST 2117\nCertificate fingerprints:\n     MD5:  BB:A1:79:53:FE:71:EC:61:2A:19:81:E8:0E:E8:C9:81\n     SHA1: 96:66:C1:01:49:17:D1:19:FB:DB:83:86:50:3D:3D:AD:DA:F7:C6:A9\n     SHA256: 69:82:C5:24:9A:A1:AE:DF:80:29:7A:26:92:C1:A5:9F:AF:7D:03:56:CC:C3:E9:73:3B:FD:85:66:35:D6:8A:9B\n     Signature algorithm name: SHA256withRSA\n     Version: 3\n\nExtensions:\n\n#1: ObjectId: 2.5.29.35 Criticality=false\nAuthorityKeyIdentifier [\nKeyIdentifier [\n0000: CD 29 4E 07 3D C3 7C D0   16 45 FB 0A CE 8D B4 98  .)N.=....E......\n0010: B7 A8 4C 79                                        ..Ly\n]\n[EMAILADDRESS=info@crate.io, CN=*.crate.io, OU=Cryptography Department, O=Crate.io, L=Dornbirn, ST=Vorarlberg, C=AT]\nSerialNumber: [    f13562ec 6184401e]\n]\n\n#2: ObjectId: 2.5.29.19 Criticality=false\nBasicConstraints:[\n  CA:true\n  PathLen:2147483647\n]\n\n#3: ObjectId: 2.5.29.14 Criticality=false\nSubjectKeyIdentifier [\nKeyIdentifier [\n0000: CD 29 4E 07 3D C3 7C D0   16 45 FB 0A CE 8D B4 98  .)N.=....E......\n0010: B7 A8 4C 79                                        ..Ly\n]\n]\n\nTrust this certificate? [no]:  yes\nCertificate was added to keystore\n\nImport the signed certificate\n\nNow we have a signed certificate, signed by either from a official CA or from our own CA. Let’s import it to the Keystore.\n\nCommand:\n\nkeytool -import -keystore keystore -file server.crt -alias server\n\n\nOutput:\n\nEnter keystore password:\nEnter key password for <server>\nCertificate reply was installed in keystore\n\nConfiguring CrateDB\n\nFinally, you want to supply the Keystore/Truststore in the CrateDB configuration, see Secured communications (SSL/TLS)."
  },
  {
    "title": "Optimization — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/optimization.html",
    "html": "5.6\nOptimization\n\nTable of contents\n\nIntroduction\n\nMultiple table optimization\n\nPartition optimization\n\nIntroduction\n\nIn CrateDB every table (or if partitioned every partition) consists of segments. When inserting/deleting/updating data new segments are created following as an append-only strategy, which gives the advantage of fast writes but on the other hand can result into a big number of segments. As the number of segments increases the read operations become slower since more segments need to be visited. Moreover each segment consumes file handles, memory and CPU. CrateDB solves this problem by merging segments automatically in the background. Small segments are merged into bigger segments, which, in turn, are merged into even bigger segments. Furthermore any deleted rows and documents are not copied to the new bigger segment during this process.\n\nIf required one or more tables or table partitions can be optimized explicitly in order to improve performance. A few parameters can also be configured for the optimization process, like the max number of segments you wish to have when optimization is completed, or if you only wish to merge segments with deleted data, etc. See OPTIMIZE for detailed description of parameters.\n\ncr> OPTIMIZE table locations;\nOPTIMIZE OK, 1 row affected (... sec)\n\n\nNote\n\nSystem tables cannot be optimized.\n\nMultiple table optimization\n\nIf needed, multiple tables can be defined comma-separated in a single SQL request. The result message is printed if the request on every given table is completed.\n\ncr> OPTIMIZE TABLE locations, parted_table;\nOPTIMIZE OK, 2 rows affected (... sec)\n\n\nNote\n\nIf one or more tables or partitions do not exist, none of the given tables/partitions are optimized and an error is returned. The error returns only the first non-existent table/partition.\n\nPartition optimization\n\nAdditionally it is possible to define a specific PARTITION of a partitioned table which should be optimized (see Partitioned tables).\n\nBy using the PARTITION clause in the optimize statement a separate request for a given partition can be performed. That means that only specific partitions of a partitioned table are optimized. For further details on how to create an optimize request on partitioned tables see the SQL syntax and its synopsis (see OPTIMIZE).\n\ncr> OPTIMIZE TABLE parted_table PARTITION (day='2014-04-08');\nOPTIMIZE OK, 1 row affected (... sec)\n\n\nIn case the PARTITION clause is omitted all partitions will be optimized. If a table has many partitions this should be avoided due to performance reasons."
  },
  {
    "title": "Authentication — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/auth/index.html",
    "html": "5.6\nAuthentication\n\nTable of contents\n\nAuthentication Methods\nTrust method\nPassword authentication method\nClient certificate authentication method\nHost-Based Authentication (HBA)\nAuthentication against CrateDB\nAuthenticating as a superuser\nAuthenticating to Admin UI\nNode-to-node communication"
  },
  {
    "title": "Privileges — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/privileges.html",
    "html": "5.6\nPrivileges\n\nTo execute statements, a user needs to have the required privileges.\n\nTable of contents\n\nIntroduction\n\nPrivilege Classes\n\nPrivilege types\n\nDQL\n\nDML\n\nDDL\n\nAL\n\nHierarchical inheritance of privileges\n\nBehavior of GRANT, DENY and REVOKE\n\nGRANT\n\nDENY\n\nREVOKE\n\nList privileges\n\nRoles inheritance\n\nIntroduction\n\nInheritance\n\nPrivileges resolution\n\nGRANT\n\nREVOKE\n\nIntroduction\n\nCrateDB has a superuser (crate) which has the privilege to do anything. The privileges of other users and roles have to be managed using the GRANT, DENY or REVOKE statements.\n\nThe privileges that can be granted, denied or revoked are:\n\nDQL\n\nDML\n\nDDL\n\nAL\n\nSkip to Privilege types for details.\n\nPrivilege Classes\n\nThe privileges can be granted on different classes:\n\nCLUSTER\n\nSCHEMA\n\nTABLE and VIEW\n\nSkip to Hierarchical inheritance of privileges for details.\n\nA user with AL on level CLUSTER can grant privileges they have themselves to other users or roles as well.\n\nPrivilege types\nDQL\n\nGranting Data Query Language (DQL) privilege to a user or role, indicates that this user/role is allowed to execute SELECT, SHOW, REFRESH and COPY TO statements, as well as using the available user-defined functions, on the object for which the privilege applies.\n\nDML\n\nGranting Data Manipulation Language (DML) privilege to a user or role, indicates that this user/role is allowed to execute INSERT, COPY FROM, UPDATE and DELETE statements, on the object for which the privilege applies.\n\nDDL\n\nGranting Data Definition Language (DDL) privilege to a user or role, indicates that this user/role is allowed to execute the following statements on objects for which the privilege applies:\n\nCREATE TABLE\n\nDROP TABLE\n\nCREATE VIEW\n\nDROP VIEW\n\nCREATE FUNCTION\n\nDROP FUNCTION\n\nCREATE REPOSITORY\n\nDROP REPOSITORY\n\nCREATE SNAPSHOT\n\nDROP SNAPSHOT\n\nRESTORE SNAPSHOT\n\nALTER TABLE\n\nAL\n\nGranting Administration Language (AL) privilege to a user or role, enables the user/role to execute the following statements:\n\nCREATE USER/ROLE\n\nDROP USER/ROLE\n\nSET GLOBAL\n\nAll statements enabled via the AL privilege operate on a cluster level. So granting this on a schema or table level will have no effect.\n\nHierarchical inheritance of privileges\n\nPrivileges can be managed on three different levels, namely: CLUSTER, SCHEMA, and TABLE/VIEW.\n\nWhen a privilege is assigned on a certain level, the privilege will propagate down the hierarchy. Privileges defined on a lower level will always override those from a higher level:\n\n  cluster\n    ||\n  schema\n   /  \\\ntable view\n\n\nThis statement will grant DQL privilege to user riley on all the tables and functions of the doc schema:\n\ncr> GRANT DQL ON SCHEMA doc TO riley;\nGRANT OK, 1 row affected (... sec)\n\n\nThis statement will deny DQL privilege to user riley on the doc schema table doc.accounting. However, riley will still have DQL privilege on all the other tables of the doc schema:\n\ncr> DENY DQL ON TABLE doc.accounting TO riley;\nDENY OK, 1 row affected (... sec)\n\n\nNote\n\nIn CrateDB, schemas are just namespaces that are created and dropped implicitly. Therefore, when GRANT, DENY or REVOKE are invoked on a schema level, CrateDB takes the schema name provided without further validation.\n\nPrivileges can be managed on all schemas and tables of the cluster, except the information_schema.\n\nViews are on the same hierarchy with tables, i.e. a privilege on a view is gained through a GRANT on either the view itself, the schema the view belongs to, or a cluster-wide privilege. Privileges on relations which are referenced in the view do not grant any privileges on the view itself. On the contrary, even if the user/role does not have any privileges on a view’s referenced relations but on the view itself, the user/role can still access the relations through the view. For example:\n\ncr> CREATE VIEW first_customer as SELECT * from doc.accounting ORDER BY id LIMIT 1\nCREATE OK, 1 row affected (... sec)\n\n\nPreviously we had issued a DENY for user riley on doc.accounting but we can still access it through the view because we have access to it through the doc schema:\n\ncr> SELECT id from first_customer;\n+----+\n| id |\n+----+\n|  1 |\n+----+\nSELECT 1 row in set (... sec)\n\n\nSee Also\n\nViews: Privileges\n\nBehavior of GRANT, DENY and REVOKE\n\nNote\n\nYou can only grant, deny, or revoke privileges for an existing user or role. You must first create a user/role and then configure privileges.\n\nGRANT\n\nTo grant a privilege to an existing user or role on the whole cluster, we use the GRANT SQL statement, for example:\n\ncr> GRANT DML TO wolfgang;\nGRANT OK, 1 row affected (... sec)\n\n\nDQL privilege can be granted on the sys schema to user wolfgang, like this:\n\ncr> GRANT DQL ON SCHEMA sys TO wolfgang;\nGRANT OK, 1 row affected (... sec)\n\n\nThe following statement will grant all privileges on table doc.books to user wolfgang:\n\ncr> GRANT ALL PRIVILEGES ON TABLE doc.books TO wolfgang;\nGRANT OK, 4 rows affected (... sec)\n\n\nUsing “ALL PRIVILEGES” is a shortcut to grant all the currently grantable privileges to a user or role.\n\nNote\n\nIf no schema is specified in the table ident, the table will be looked up in the current schema.\n\nIf a user/role with the name specified in the SQL statement does not exist the statement returns an error:\n\ncr> GRANT DQL TO layla;\nRoleUnknownException[Role 'layla' does not exist]\n\n\nTo grant ALL PRIVILEGES to user will on the cluster, we can use the following syntax:\n\ncr> GRANT ALL PRIVILEGES TO will;\nGRANT OK, 4 rows affected (... sec)\n\n\nUsing ALL PRIVILEGES is a shortcut to grant all the currently grantable privileges to a user or role, namely DQL, DML and DDL.\n\nPrivileges can be granted to multiple users/roles in the same statement, like so:\n\ncr> GRANT DDL ON TABLE doc.books TO wolfgang, will;\nGRANT OK, 1 row affected (... sec)\n\nDENY\n\nTo deny a privilege to an existing user or role on the whole cluster, use the DENY SQL statement, for example:\n\ncr> DENY DDL TO will;\nDENY OK, 1 row affected (... sec)\n\n\nDQL privilege can be denied on the sys schema to user wolfgang like this:\n\ncr> DENY DQL ON SCHEMA sys TO wolfgang;\nDENY OK, 1 row affected (... sec)\n\n\nThe following statement will deny DQL privilege on table doc.books to user wolfgang:\n\ncr> DENY DQL ON TABLE doc.books TO wolfgang;\nDENY OK, 1 row affected (... sec)\n\n\nDENY ALL or DENY ALL PRIVILEGES will deny all privileges to a user or role, on the cluster it can be used like this:\n\ncr> DENY ALL TO will;\nDENY OK, 3 rows affected (... sec)\n\nREVOKE\n\nTo revoke a privilege that was previously granted or denied to a user or role use the REVOKE SQL statement, for example the DQL privilege that was previously denied to user wolfgang on the sys schema, can be revoked like this:\n\ncr> REVOKE DQL ON SCHEMA sys FROM wolfgang;\nREVOKE OK, 1 row affected (... sec)\n\n\nThe privileges that were granted and denied to user wolfgang on doc.books can be revoked like this:\n\ncr> REVOKE ALL ON TABLE doc.books FROM wolfgang;\nREVOKE OK, 4 rows affected (... sec)\n\n\nThe privileges that were granted to user will on the cluster can be revoked like this:\n\ncr> REVOKE ALL FROM will;\nREVOKE OK, 4 rows affected (... sec)\n\n\nNote\n\nThe REVOKE statement can remove only privileges that have been granted or denied through the GRANT or DENY statements. If the privilege on a specific object was not explicitly granted, the REVOKE statement has no effect. The effect of the REVOKE statement will be reflected in the row count.\n\nNote\n\nWhen a privilege is revoked from a user or role, it can still be active for that user/role, if the user/role inherits it, from another role.\n\nList privileges\n\nCrateDB exposes the privileges of users and roles of the database through the sys.privileges system table.\n\nBy querying the sys.privileges table you can get all information regarding the existing privileges. E.g.:\n\ncr> SELECT * FROM sys.privileges order by grantee, class, ident;\n+---------+----------+---------+----------------+-------+------+\n| class   | grantee  | grantor | ident          | state | type |\n+---------+----------+---------+----------------+-------+------+\n| SCHEMA  | riley    | crate   | doc            | GRANT | DQL  |\n| TABLE   | riley    | crate   | doc.accounting | DENY  | DQL  |\n| TABLE   | will     | crate   | doc.books      | GRANT | DDL  |\n| CLUSTER | wolfgang | crate   | NULL           | GRANT | DML  |\n+---------+----------+---------+----------------+-------+------+\nSELECT 4 rows in set (... sec)\n\nRoles inheritance\nIntroduction\n\nYou can grant, or revoke roles for an existing user or role. This allows to group granted or denied privileges and inherit them to other users or roles.\n\nYou must first create usesr and roles and then grant roles to other roles or users. You can configure the privileges of each role before or after granting roles to other roles or users.\n\nNote\n\nRoles can be granted to other roles or users, but users (roles which can also login to the database) cannot be granted to other roles or users.\n\nNote\n\nSuperuser crate cannot be granted to other users or roles, and roles cannot be granted to it.\n\nInheritance\n\nThe inheritance can span multiple levels, so you can have role_a which is granted to role_b, which in turn is granted to role_c, and so on. Each role can be granted to multiple other roles and each role or user can be granted multiple other roles. Cycles cannot be created, for example:\n\ncr> GRANT role_a TO role_b;\nGRANT OK, 1 row affected (... sec)\n\ncr> GRANT role_b TO role_c;\nGRANT OK, 1 row affected (... sec)\n\ncr> GRANT role_c TO role_a;\nSQLParseException[Cannot grant role role_c to role_a, role_a is a parent role of role_c and a cycle will be created]\n\nPrivileges resolution\n\nWhen a user executes a statement, the privileges mechanism will check first if the user has been granted the required privileges, if not, it will check if the roles which this user has been granted have those privileges and if not, it will continue checking the roles granted to those parent roles of the user and so on. For example:\n\ncr> GRANT role_a TO role_b;\nGRANT OK, 1 row affected (... sec)\n\ncr> GRANT role_b TO role_c;\nGRANT OK, 1 row affected (... sec)\n\ncr> GRANT DQL ON TABLE sys.users TO role_a;\nGRANT OK, 1 row affected (... sec)\n\ncr> GRANT role_c TO john;\nGRANT OK, 1 row affected (... sec)\n\n\nUser john is able to query sys.users, as even though he lacks DQL privilege on the table, he is granted role_c which in turn is granted role_b which is granted role_a, and role has the DQL privilege on sys.users.\n\nKeep in mind that DENY has precedence over GRANT. If a role has been both granted and denied a privilege (directly or through role inheritance), then DENY will take effect. For example, GRANT is inherited from a role and DENY directly set on the user:\n\ncr> GRANT DQL ON TABLE sys.users TO role_a;\nGRANT OK, 1 row affected (... sec)\n\ncr> GRANT role_a TO john\nGRANT OK, 1 row affected (... sec)\n\ncr> DENY DQL ON TABLE sys.users TO john\nDENY OK, 1 row affected (... sec)\n\n\nUser john cannot query sys.users.\n\nAnother example with DENY in effect, inherited from a role:\n\ncr> GRANT DQL ON TABLE sys.users TO role_a;\nGRANT OK, 1 row affected (... sec)\n\ncr> DENY DQL ON TABLE sys.users TO role_b;\nDENY OK, 1 row affected (... sec)\n\ncr> GRANT role_a, role_b TO john;\nGRANT OK, 2 rows affected (... sec)\n\n\nUser john cannot query sys.users.\n\nGRANT\n\nTo grant an existing role to an existing user or role on the whole cluster, we use the GRANT SQL statement, for example:\n\ncr> GRANT role_dql TO wolfgang;\nGRANT OK, 1 row affected (... sec)\n\n\nDML privilege can be granted on the sys schema to role role_dml, so, by inheritance, to user wolfgang as well, like this:\n\ncr> GRANT DQL ON SCHEMA sys TO role_dql;\nGRANT OK, 1 row affected (... sec)\n\n\nThe following statements will grant all privileges on table doc.books to role role_all_on_books, and by inheritance to user wolfgang as well:\n\ncr> GRANT role_all_on_books TO wolfgang;\nGRANT OK, 1 row affected (... sec)\n\ncr> GRANT ALL PRIVILEGES ON TABLE doc.books TO role_all_on_books;\nGRANT OK, 4 rows affected (... sec)\n\n\nIf a role with the name specified in the SQL statement does not exist the statement returns an error:\n\ncr> GRANT DDL TO role_ddl;\nRoleUnknownException[Role 'role_ddl' does not exist]\n\n\nMultiple roles can be granted to multiple users/roles in the same statement, like so:\n\ncr> GRANT role_dql, role_all_on_books TO layla, will;\nGRANT OK, 4 rows affected (... sec)\n\n\nNotice that 4 rows affected is returned, as in total there are 2 users, will and layla and each of them is granted two roles: role_dql and role_all_on_books.\n\nREVOKE\n\nTo revoke a role that was previously granted to a user or role use the REVOKE SQL statement. For example role role_dql which was previously granted to users wolfgang,``layla`` and will, can be revoked like this:\n\ncr> REVOKE role_dql FROM wolfgang, layla, will;\nREVOKE OK, 3 rows affected (... sec)\n\n\nIf a privilege is revoked from a role which is granted to other roles or users, the privilege is automatically revoked also for those roles and users, for example if we revoke privileges on table doc.books from role_all_on_books:\n\ncr> REVOKE ALL PRIVILEGES ON TABLE doc.books FROM role_all_on_books;\nREVOKE OK, 4 rows affected (... sec)\n\n\nuser wolfgang, who is granted the role role_all_on_books, also looses those privileges.\n\nIf a user is granted the same privilege by inheriting two different roles, when revoking one of the roles, the user still keeps the privilege. For example if user john gets granted `role_dql and role_dml:\n\ncr> GRANT DQL TO role_dql;\nGRANT OK, 1 row affected (... sec)\n\ncr> GRANT DQL, DML TO role_dml;\nGRANT OK, 2 rows affected (... sec)\n\ncr> GRANT role_dql, role_dml TO john;\nGRANT OK, 2 rows affected (... sec)\n\n\nand then we revoke role_dql from john:\n\ncr> REVOKE role_dql FROM john;\nREVOKE OK, 1 row affected (... sec)\n\n\njohn still has DQL privilege since it inherits it from role_dml which is still granted to him."
  },
  {
    "title": "Users and roles management — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/user-management.html",
    "html": "5.6\nUsers and roles management\n\nUsers and roles account information is stored in the cluster metadata of CrateDB and supports the following statements to create, alter and drop users and roles:\n\nCREATE USER\n\nCREATE ROLE\n\nALTER USER or ALTER ROLE\n\nDROP USER or DROP ROLE\n\nThese statements are database management statements that can be invoked by superusers that already exist in the CrateDB cluster. The CREATE USER, CREATE ROLE, DROP USER and DROP ROLE statements can also be invoked by users with the AL privilege. ALTER USER or ALTER ROLE can be invoked by users to change their own password, without requiring any privilege.\n\nWhen CrateDB is started, the cluster contains one predefined superuser. This user is called crate. It is not possible to create any other superusers.\n\nThe definition of all users and roles, including hashes of their passwords, together with their privileges is backed up together with the cluster’s metadata when a snapshot is created, and it is restored when using the ALL, METADATA, or USERMANAGEMENT keywords with the:ref:sql-restore-snapshot command.\n\nTable of contents\n\nROLES\n\nCREATE ROLE\n\nALTER ROLE\n\nDROP ROLE\n\nList roles\n\nUSERS\n\nCREATE USER\n\nALTER USER\n\nDROP USER\n\nList users\n\nROLES\n\nRoles are entities that are not allowed to login, but can be assigned privileges and they can be granted to other roles, thus creating a role hierarchy, or directly to users. For example, a role myschema_dql_role can be granted with DQL privileges on schema myschema and afterwards the role can be granted to a user, which will automatically inherit those privileges from the myschema_dql_role. A role myschema_dml_role can be granted with DML privileges on schema myschema and can also be granted the role myschema_dql_role, thus gaining also DQL privileges. When myschema_dml_role is granted to a user, this user will automatically have both DQL and DML privileges on myschema.\n\nCREATE ROLE\n\nTo create a new role for the CrateDB database cluster use the CREATE ROLE SQL statement:\n\ncr> CREATE ROLE role_a;\nCREATE OK, 1 row affected (... sec)\n\n\nTip\n\nNewly created roles do not have any privileges. After creating a role, you should configure user privileges.\n\nFor example, to grant all privileges to the role_a user, run:\n\ncr> GRANT ALL PRIVILEGES TO role_a;\nGRANT OK, 4 rows affected (... sec)\n\n\nThe name parameter of the statement follows the principles of an identifier which means that it must be double-quoted if it contains special characters (e.g. whitespace) or if the case needs to be maintained:\n\ncr> CREATE ROLE \"Custom Role\";\nCREATE OK, 1 row affected (... sec)\n\n\nIf a role or user with the name specified in the SQL statement already exists the statement returns an error:\n\ncr> CREATE ROLE \"Custom Role\";\nRoleAlreadyExistsException[Role 'Custom Role' already exists]\n\nALTER ROLE\n\nALTER ROLE and ALTER USER SQL statements are not supported for roles, only for users.\n\nDROP ROLE\n\nTo remove an existing role from the CrateDB database cluster use the DROP ROLE or DROP USER SQL statement:\n\ncr> DROP ROLE role_c;\nDROP OK, 1 row affected (... sec)\n\ncr> DROP USER role_d;\nDROP OK, 1 row affected (... sec)\n\n\nIf a role with the name specified in the SQL statement does not exist, the statement returns an error:\n\ncr> DROP ROLE role_d;\nRoleUnknownException[Role 'role_d' does not exist]\n\nList roles\n\nCrateDB exposes database roles via the read-only Roles system table. The sys.roles table shows all roles in the cluster which can be used to group privileges.\n\nTo list all existing roles query the table:\n\ncr> SELECT name, granted_roles FROM sys.roles order by name;\n+--------+------------------------------------------+\n| name   | granted_roles                            |\n+--------+------------------------------------------+\n| role_a | []                                       |\n| role_b | [{\"grantor\": \"crate\", \"role\": \"role_c\"}] |\n| role_c | []                                       |\n+--------+------------------------------------------+\nSELECT 3 rows in set (... sec)\n\nUSERS\nCREATE USER\n\nTo create a new user for the CrateDB database cluster use the CREATE USER SQL statement:\n\ncr> CREATE USER user_a;\nCREATE OK, 1 row affected (... sec)\n\n\nTip\n\nNewly created users do not have any privileges. After creating a user, you should configure user privileges.\n\nFor example, to grant all privileges to the user_a user, run:\n\ncr> GRANT ALL PRIVILEGES TO user_a;\nGRANT OK, 4 rows affected (... sec)\n\n\nIt can be used to connect to the database cluster using available authentication methods. You can specify the user’s password in the WITH clause of the CREATE statement. This is required if you want to use the Password authentication method:\n\ncr> CREATE USER user_b WITH (password = 'a_secret_password');\nCREATE OK, 1 row affected (... sec)\n\n\nThe username parameter of the statement follows the principles of an identifier which means that it must be double-quoted if it contains special characters (e.g. whitespace) or if the case needs to be maintained:\n\ncr> CREATE USER \"Custom User\";\nCREATE OK, 1 row affected (... sec)\n\n\nIf a user with the username specified in the SQL statement already exists the statement returns an error:\n\ncr> CREATE USER \"Custom User\";\nRoleAlreadyExistsException[Role 'Custom User' already exists]\n\nALTER USER\n\nTo alter the password for an existing user from the CrateDB database cluster use the ALTER ROLE or ALTER USER SQL statements:\n\ncr> ALTER USER user_a SET (password = 'pass');\nALTER OK, 1 row affected (... sec)\n\n\nThe password can be reset (cleared) if specified as NULL:\n\ncr> ALTER USER user_a SET (password = NULL);\nALTER OK, 1 row affected (... sec)\n\n\nNote\n\nThe built-in superuser crate has no password and it is not possible to set a new password for this user.\n\nDROP USER\n\nTo remove an existing user from the CrateDB database cluster use the DROP ROLE or DROP USER SQL statements:\n\ncr> DROP USER user_c;\nDROP OK, 1 row affected (... sec)\n\ncr> DROP ROLE user_d;\nDROP OK, 1 row affected (... sec)\n\n\nIf a user with the username specified in the SQL statement does not exist the statement returns an error:\n\ncr> DROP USER user_d;\nRoleUnknownException[Role 'user_d' does not exist]\n\n\nNote\n\nIt is not possible to drop the built-in superuser crate.\n\nList users\n\nCrateDB exposes database users via the read-only Users system table. The sys.users table shows all users in the cluster which can be used for authentication. The initial superuser crate which is available for all CrateDB clusters is also part of that list.\n\nTo list all existing users query the table:\n\ncr> SELECT name, granted_roles, password, superuser FROM sys.users order by name;\n+--------+----------------------------------------------------------------------------------+----------+-----------+\n| name   | granted_roles                                                                    | password | superuser |\n+--------+----------------------------------------------------------------------------------+----------+-----------+\n| crate  | []                                                                               | NULL     | TRUE      |\n| user_a | [{\"grantor\": \"crate\", \"role\": \"role_a\"}, {\"grantor\": \"crate\", \"role\": \"role_b\"}] | NULL     | FALSE     |\n| user_b | []                                                                               | ******** | FALSE     |\n+--------+----------------------------------------------------------------------------------+----------+-----------+\nSELECT 3 rows in set (... sec)\n\n\nNote\n\nCrateDB also supports retrieving the current connected user using the system information functions: CURRENT_USER, USER and SESSION_USER."
  },
  {
    "title": "Runtime configuration — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/runtime-config.html",
    "html": "5.6\nRuntime configuration\n\nThe CrateDB cluster can be configured at runtime using the SET and RESET statement. See the Cluster Settings configuration section for details about the supported settings.\n\nIf SET is used with PERSISTENT the change will survive a cluster restart, if used with TRANSIENT the value will reset to the default value or to the value in the configuration file on a restart.\n\ncr> SET GLOBAL PERSISTENT stats.enabled = false;\nSET OK, 1 row affected (... sec)\n\ncr> select sys.cluster.settings['stats']['enabled'] from sys.cluster;\n+------------------------------+\n| settings['stats']['enabled'] |\n+------------------------------+\n| FALSE                        |\n+------------------------------+\nSELECT 1 row in set (... sec)\n\n\nYou can change multiple values at once:\n\ncr> SET GLOBAL TRANSIENT stats.enabled = true,\n... stats.jobs_log_size = 1024, stats.operations_log_size = 4096;\nSET OK, 1 row affected (... sec)\n\ncr> select settings['stats']['enabled'],\n...   settings['stats']['jobs_log_size'],\n...   settings['stats']['operations_log_size']\n... from sys.cluster;\n+-...------------+-...------------------+-...------------------------+\n| ...['enabled'] | ...['jobs_log_size'] | ...['operations_log_size'] |\n+-...------------+-...------------------+-...------------------------+\n| TRUE           |                 1024 |                       4096 |\n+-...------------+-...------------------+-...------------------------+\nSELECT 1 row in set (... sec)\n\n\nIts also possible to save a complete nested object of settings:\n\ncr> SET GLOBAL TRANSIENT stats = {\n...   jobs_log_size = 2048,\n...   operations_log_size = 8192\n... };\nSET OK, 1 row affected (... sec)\n\ncr> SELECT\n...   settings['stats']['jobs_log_size'] AS jobs_size,\n...   settings['stats']['operations_log_size'] AS op_size\n... FROM sys.cluster;\n+-----------+---------+\n| jobs_size | op_size |\n+-----------+---------+\n|      2048 |    8192 |\n+-----------+---------+\nSELECT 1 row in set (... sec)\n\n\nUsing the RESET statement, a setting will be reset to either on node startup defined configuration file value or to its default value:\n\ncr> RESET GLOBAL stats.enabled, stats.operations_log_size;\nRESET OK, 1 row affected (... sec)\n\ncr> SELECT\n...   settings['stats']['jobs_log_size'] AS jobs_size,\n...   settings['stats']['operations_log_size'] AS op_size\n... FROM sys.cluster;\n+-----------+---------+\n| jobs_size | op_size |\n+-----------+---------+\n|      2048 |   10000 |\n+-----------+---------+\nSELECT 1 row in set (... sec)\n\n\nRESET can also be done on objects:\n\ncr> RESET GLOBAL stats;\nRESET OK, 1 row affected (... sec)\n\ncr> SELECT\n...   settings['stats']['jobs_log_size'] AS jobs_size,\n...   settings['stats']['operations_log_size'] AS op_size\n... FROM sys.cluster;\n+-----------+---------+\n| jobs_size | op_size |\n+-----------+---------+\n|     10000 |   10000 |\n+-----------+---------+\nSELECT 1 row in set (... sec)\n"
  },
  {
    "title": "System information — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/system-information.html",
    "html": "5.6\nSystem information\n\nCrateDB provides the sys schema which contains virtual tables. These tables are read-only and can be queried to get statistical real-time information about the cluster, its nodes and their shards:\n\nTable of contents\n\nCluster\n\nCluster license\n\nlicense\n\nCluster settings\n\nNodes\n\nid\n\nname\n\nhostname\n\nrest_url\n\nattributes\n\nport\n\nload\n\nmem\n\nheap\n\nversion\n\ncluster_state_version\n\nfs\n\nthread_pools\n\nos\n\ncgroup limitations\n\nUptime limitations\n\nos_info\n\nnetwork\n\nconnections\n\nprocess\n\nNode checks\n\nAcknowledge failed checks\n\nDescription of checked node settings\n\nRecovery expected data nodes\n\nRecovery after data nodes\n\nRecovery after time\n\nRouting allocation disk watermark high\n\nRouting allocation disk watermark low\n\nMaximum shards per node\n\nShards\n\nTable schema\n\nExample\n\nSegments\n\nJobs, operations, and logs\n\nJobs\n\nTable schema\n\nJobs metrics\n\nsys.jobs_metrics Table schema\n\nClassification\n\nOperations\n\nTable schema\n\nLogs\n\nsys.jobs_log Table schema\n\nsys.operations_log Table schema\n\nCluster checks\n\nCurrent Checks\n\nNumber of partitions\n\nTables need to be recreated\n\nCrateDB table version compatibility scheme\n\nAvoiding reindex using partitioned tables\n\nHow to reindex\n\nLicense check\n\nHealth\n\nHealth definition\n\nRepositories\n\nSnapshots\n\nSnapshot Restore\n\nSummits\n\nUsers\n\nRoles\n\nPrivileges\n\nAllocations\n\nShard table permissions\n\nsys jobs tables permissions\n\npg_stats\n\npg_publication\n\npg_publication_tables\n\npg_subscription\n\npg_subscription_rel\n\nCluster\n\nBasic information about the CrateDB cluster can be retrieved from the sys.cluster table:\n\nName\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nA unique ID generated by the system.\n\n\t\n\nTEXT\n\n\n\n\nlicense\n\n\t\n\nThe current CrateDB license information. Always NULL. This exists for backward compatibility\n\n\t\n\nOBJECT\n\n\n\n\nname\n\n\t\n\nThe cluster name.\n\n\t\n\nTEXT\n\n\n\n\nmaster_node\n\n\t\n\nNode ID of the node which currently operates as master\n\n\t\n\nTEXT\n\n\n\n\nsettings\n\n\t\n\nThe cluster settings.\n\n\t\n\nOBJECT\n\nThe result has at most 1 row:\n\ncr> select name from sys.cluster;\n+-----------------+\n| name            |\n+-----------------+\n| Testing-CrateDB |\n+-----------------+\nSELECT 1 row in set (... sec)\n\nCluster license\n\nThe sys.cluster.license expression returns information about the currently registered license.\n\nNote\n\nLicenses were removed in CrateDB 4.5. Accordingly, these values are deprecated and return NULL in CrateDB 4.5 and higher.\n\nlicense\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nlicense\n\n\t\nThe current CrateDB license information\n\nor NULL on CrateDB CE.\n\n\t\n\nOBJECT\n\n\n\n\nlicense['expiry_date']\n\n\t\n\nThe Dates and times on which the license expires.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nlicense['issued_to']\n\n\t\n\nThe organisation for which the license is issued.\n\n\t\n\nTEXT\n\n\n\n\nlicense['max_nodes']\n\n\t\n\nThe maximum number of nodes the license is valid for.\n\n\t\n\nINTEGER\n\nCluster settings\n\nThe sys.cluster.settings expression returns information about the currently applied cluster settings.\n\ncr> select settings from sys.cluster;\n+-----------------------------------------------------...-+\n| settings                                                |\n+-----------------------------------------------------...-+\n| {\"bulk\": {...}, \"cluster\": {...}, \"gateway\": {...}, ... |\n+-----------------------------------------------------...-+\nSELECT 1 row in set (... sec)\n\ncr> select column_name, data_type from information_schema.columns\n... where column_name like 'settings%'\n... and table_name = 'cluster';\n+-----------------------------------------------------------------------------------+--------------+\n| column_name                                                                       | data_type    |\n+-----------------------------------------------------------------------------------+--------------+\n| settings                                                                          | object       |\n| settings['bulk']                                                                  | object       |\n| settings['bulk']['request_timeout']                                               | text         |\n| settings['cluster']                                                               | object       |\n| settings['cluster']['graceful_stop']                                              | object       |\n| settings['cluster']['graceful_stop']['force']                                     | boolean      |\n| settings['cluster']['graceful_stop']['min_availability']                          | text         |\n| settings['cluster']['graceful_stop']['timeout']                                   | text         |\n| settings['cluster']['info']                                                       | object       |\n| settings['cluster']['info']['update']                                             | object       |\n| settings['cluster']['info']['update']['interval']                                 | text         |\n| settings['cluster']['max_shards_per_node']                                        | integer      |\n| settings['cluster']['routing']                                                    | object       |\n| settings['cluster']['routing']['allocation']                                      | object       |\n| settings['cluster']['routing']['allocation']['allow_rebalance']                   | text         |\n| settings['cluster']['routing']['allocation']['balance']                           | object       |\n| settings['cluster']['routing']['allocation']['balance']['index']                  | real         |\n| settings['cluster']['routing']['allocation']['balance']['shard']                  | real         |\n| settings['cluster']['routing']['allocation']['balance']['threshold']              | real         |\n| settings['cluster']['routing']['allocation']['cluster_concurrent_rebalance']      | integer      |\n| settings['cluster']['routing']['allocation']['disk']                              | object       |\n| settings['cluster']['routing']['allocation']['disk']['threshold_enabled']         | boolean      |\n| settings['cluster']['routing']['allocation']['disk']['watermark']                 | object       |\n| settings['cluster']['routing']['allocation']['disk']['watermark']['flood_stage']  | text         |\n| settings['cluster']['routing']['allocation']['disk']['watermark']['high']         | text         |\n| settings['cluster']['routing']['allocation']['disk']['watermark']['low']          | text         |\n| settings['cluster']['routing']['allocation']['enable']                            | text         |\n| settings['cluster']['routing']['allocation']['exclude']                           | object       |\n| settings['cluster']['routing']['allocation']['exclude']['_host']                  | text         |\n| settings['cluster']['routing']['allocation']['exclude']['_id']                    | text         |\n| settings['cluster']['routing']['allocation']['exclude']['_ip']                    | text         |\n| settings['cluster']['routing']['allocation']['exclude']['_name']                  | text         |\n| settings['cluster']['routing']['allocation']['include']                           | object       |\n| settings['cluster']['routing']['allocation']['include']['_host']                  | text         |\n| settings['cluster']['routing']['allocation']['include']['_id']                    | text         |\n| settings['cluster']['routing']['allocation']['include']['_ip']                    | text         |\n| settings['cluster']['routing']['allocation']['include']['_name']                  | text         |\n| settings['cluster']['routing']['allocation']['node_concurrent_recoveries']        | integer      |\n| settings['cluster']['routing']['allocation']['node_initial_primaries_recoveries'] | integer      |\n| settings['cluster']['routing']['allocation']['require']                           | object       |\n| settings['cluster']['routing']['allocation']['require']['_host']                  | text         |\n| settings['cluster']['routing']['allocation']['require']['_id']                    | text         |\n| settings['cluster']['routing']['allocation']['require']['_ip']                    | text         |\n| settings['cluster']['routing']['allocation']['require']['_name']                  | text         |\n| settings['cluster']['routing']['allocation']['total_shards_per_node']             | integer      |\n| settings['cluster']['routing']['rebalance']                                       | object       |\n| settings['cluster']['routing']['rebalance']['enable']                             | text         |\n| settings['gateway']                                                               | object       |\n| settings['gateway']['expected_data_nodes']                                        | integer      |\n| settings['gateway']['expected_nodes']                                             | integer      |\n| settings['gateway']['recover_after_data_nodes']                                   | integer      |\n| settings['gateway']['recover_after_nodes']                                        | integer      |\n| settings['gateway']['recover_after_time']                                         | text         |\n| settings['indices']                                                               | object       |\n| settings['indices']['breaker']                                                    | object       |\n| settings['indices']['breaker']['query']                                           | object       |\n| settings['indices']['breaker']['query']['limit']                                  | text         |\n| settings['indices']['breaker']['request']                                         | object       |\n| settings['indices']['breaker']['request']['limit']                                | text         |\n| settings['indices']['breaker']['total']                                           | object       |\n| settings['indices']['breaker']['total']['limit']                                  | text         |\n| settings['indices']['recovery']                                                   | object       |\n| settings['indices']['recovery']['internal_action_long_timeout']                   | text         |\n| settings['indices']['recovery']['internal_action_timeout']                        | text         |\n| settings['indices']['recovery']['max_bytes_per_sec']                              | text         |\n| settings['indices']['recovery']['recovery_activity_timeout']                      | text         |\n| settings['indices']['recovery']['retry_delay_network']                            | text         |\n| settings['indices']['recovery']['retry_delay_state_sync']                         | text         |\n| settings['indices']['replication']                                                | object       |\n| settings['indices']['replication']['retry_timeout']                               | text         |\n| settings['logger']                                                                | object_array |\n| settings['logger']['level']                                                       | text_array   |\n| settings['logger']['name']                                                        | text_array   |\n| settings['memory']                                                                | object       |\n| settings['memory']['allocation']                                                  | object       |\n| settings['memory']['allocation']['type']                                          | text         |\n| settings['memory']['operation_limit']                                             | integer      |\n| settings['overload_protection']                                                   | object       |\n| settings['overload_protection']['dml']                                            | object       |\n| settings['overload_protection']['dml']['initial_concurrency']                     | integer      |\n| settings['overload_protection']['dml']['max_concurrency']                         | integer      |\n| settings['overload_protection']['dml']['min_concurrency']                         | integer      |\n| settings['overload_protection']['dml']['queue_size']                              | integer      |\n| settings['replication']                                                           | object       |\n| settings['replication']['logical']                                                | object       |\n| settings['replication']['logical']['ops_batch_size']                              | integer      |\n| settings['replication']['logical']['reads_poll_duration']                         | text         |\n| settings['replication']['logical']['recovery']                                    | object       |\n| settings['replication']['logical']['recovery']['chunk_size']                      | text         |\n| settings['replication']['logical']['recovery']['max_concurrent_file_chunks']      | integer      |\n| settings['statement_timeout']                                                     | text         |\n| settings['stats']                                                                 | object       |\n| settings['stats']['breaker']                                                      | object       |\n| settings['stats']['breaker']['log']                                               | object       |\n| settings['stats']['breaker']['log']['jobs']                                       | object       |\n| settings['stats']['breaker']['log']['jobs']['limit']                              | text         |\n| settings['stats']['breaker']['log']['operations']                                 | object       |\n| settings['stats']['breaker']['log']['operations']['limit']                        | text         |\n| settings['stats']['enabled']                                                      | boolean      |\n| settings['stats']['jobs_log_expiration']                                          | text         |\n| settings['stats']['jobs_log_filter']                                              | text         |\n| settings['stats']['jobs_log_persistent_filter']                                   | text         |\n| settings['stats']['jobs_log_size']                                                | integer      |\n| settings['stats']['operations_log_expiration']                                    | text         |\n| settings['stats']['operations_log_size']                                          | integer      |\n| settings['stats']['service']                                                      | object       |\n| settings['stats']['service']['interval']                                          | text         |\n| settings['stats']['service']['max_bytes_per_sec']                                 | text         |\n| settings['udc']                                                                   | object       |\n| settings['udc']['enabled']                                                        | boolean      |\n| settings['udc']['initial_delay']                                                  | text         |\n| settings['udc']['interval']                                                       | text         |\n| settings['udc']['url']                                                            | text         |\n+-----------------------------------------------------------------------------------+--------------+\nSELECT ... rows in set (... sec)\n\n\nFor further details, see the Cluster Settings configuration section.\n\nNodes\n\nTo get information about the nodes query for sys.nodes.\n\nThis table can be queried for one, multiple or all nodes within a cluster.\n\nThe table schema is as follows:\n\nid\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nA unique ID within the cluster generated by the system.\n\n\t\n\nTEXT\n\nname\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nname\n\n\t\n\nThe node name within a cluster. The system will choose a random name. You can also customize the node name, see Node-specific settings.\n\n\t\n\nTEXT\n\nhostname\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nhostname\n\n\t\n\nThe specified host name of the machine the node is running on.\n\n\t\n\nTEXT\n\nrest_url\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nrest_url\n\n\t\n\nFull HTTP(s) address where the REST API of the node is exposed, including schema, hostname (or IP) and port.\n\n\t\n\nTEXT\n\nattributes\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nattributes\n\n\t\n\nThe custom attributes set for the node, e.g. if node.attr.color is blue, and node.attr.location is east`, the value of this column would be: ``{color=blue, location=east}\n\n\t\n\nOBJECT\n\nport\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nport\n\n\t\n\nThe specified ports for both HTTP and binary transport interfaces. You can also customize the ports setting, see Ports.\n\n\t\n\nOBJECT\n\n\n\n\nport['http']\n\n\t\n\nCrateDB’s HTTP port.\n\n\t\n\nINTEGER\n\n\n\n\nport['transport']\n\n\t\n\nCrateDB’s binary transport port.\n\n\t\n\nINTEGER\n\n\n\n\nport['psql']\n\n\t\n\nThe PostgreSQL wire protocol port.\n\n\t\n\nINTEGER\n\nload\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nload\n\n\t\n\nSystem load statistics\n\n\t\n\nOBJECT\n\n\n\n\nload['1']\n\n\t\n\nAverage load over the last 1 minute.\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\nload['5']\n\n\t\n\nAverage load over the last 5 minutes.\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\nload['15']\n\n\t\n\nAverage load over the last 15 minutes.\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\nload['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the load probe.\n\n\t\n\nBIGINT\n\nmem\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nmem\n\n\t\n\nMemory utilization statistics of the host.\n\n\t\n\nOBJECT\n\n\n\n\nmem['used']\n\n\t\n\nCurrently used memory in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nmem['used_percent']\n\n\t\n\nCurrently used memory in percent of total.\n\n\t\n\nSMALLINT\n\n\n\n\nmem['free']\n\n\t\n\nCurrently available memory in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nmem['free_percent']\n\n\t\n\nCurrently available memory in percent of total.\n\n\t\n\nSMALLINT\n\n\n\n\nmem['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the memory probe.\n\n\t\n\nBIGINT\n\nheap\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nheap\n\n\t\n\nHeap memory utilization statistics.\n\n\t\n\nOBJECT\n\n\n\n\nheap['used']\n\n\t\n\nCurrently used heap memory in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nheap['max']\n\n\t\n\nMaximum available heap memory. You can specify the max heap memory CrateDB should use in the Configuration.\n\n\t\n\nBIGINT\n\n\n\n\nheap['free']\n\n\t\n\nCurrently available heap memory in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nheap['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the heap probe.\n\n\t\n\nBIGINT\n\nversion\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nversion\n\n\t\n\nCrateDB version information.\n\n\t\n\nOBJECT\n\n\n\n\nversion['number']\n\n\t\n\nVersion string in format \"major.minor.hotfix\"\n\n\t\n\nTEXT\n\n\n\n\nversion['build_hash']\n\n\t\n\nSHA hash of the GitHub commit which this build was built from.\n\n\t\n\nTEXT\n\n\n\n\nversion['build_snapshot']\n\n\t\n\nIndicates whether this build is a snapshot build.\n\n\t\n\nBOOLEAN\n\n\n\n\nversion['minimum_index_compatibility_version']\n\n\t\n\nIndicates the minimum compatible index version which is supported.\n\n\t\n\nTEXT\n\n\n\n\nversion['minimum_wire_compatibility_version']\n\n\t\n\nIndicates the minimum compatible wire protocol version which is supported.\n\n\t\n\nTEXT\n\ncluster_state_version\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\ncluster_state_version\n\n\t\n\nThe current version of the cluster state. The cluster state is an immutable structure and that is recreated when a change is published.\n\n\t\n\nBIGINT\n\nfs\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nfs\n\n\t\n\nUtilization statistics about the file system.\n\n\t\n\nOBJECT\n\n\n\n\nfs['total']\n\n\t\n\nAggregated usage statistic of all disks on the host.\n\n\t\n\nOBJECT\n\n\n\n\nfs['total']['size']\n\n\t\n\nTotal size of all disks in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['used']\n\n\t\n\nTotal used space of all disks in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['available']\n\n\t\n\nTotal available space of all disks in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['reads']\n\n\t\n\nTotal number of reads on all disks.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['bytes_read']\n\n\t\n\nTotal size of reads on all disks in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['writes']\n\n\t\n\nTotal number of writes on all disks.\n\n\t\n\nBIGINT\n\n\n\n\nfs['total']['bytes_written']\n\n\t\n\nTotal size of writes on all disks in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['disks']\n\n\t\n\nUsage statistics of individual disks on the host.\n\n\t\n\nARRAY\n\n\n\n\nfs['disks']['dev']\n\n\t\n\nDevice name\n\n\t\n\nTEXT\n\n\n\n\nfs['disks']['size']\n\n\t\n\nTotal size of the disk in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['disks']['used']\n\n\t\n\nUsed space of the disk in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['disks']['available']\n\n\t\n\nAvailable space of the disk in bytes.\n\n\t\n\nBIGINT\n\n\n\n\nfs['data']\n\n\t\n\nInformation about data paths used by the node.\n\n\t\n\nARRAY\n\n\n\n\nfs['data']['dev']\n\n\t\n\nDevice name\n\n\t\n\nTEXT\n\n\n\n\nfs['data']['path']\n\n\t\n\nFile path where the data of the node resides.\n\n\t\n\nTEXT\n\nthread_pools\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nthread_pools\n\n\t\n\nUsage statistics of Java thread pools.\n\n\t\n\nARRAY\n\n\n\n\nthread_pools['name']\n\n\t\n\nName of the pool.\n\n\t\n\nTEXT\n\n\n\n\nthread_pools['active']\n\n\t\n\nNumber of currently running thread in the thread pool.\n\n\t\n\nINTEGER\n\n\n\n\nthread_pools['rejected']\n\n\t\n\nTotal number of rejected threads in the thread pool.\n\n\t\n\nBIGINT\n\n\n\n\nthread_pools['largest']\n\n\t\n\nLargest number of threads that have ever simultaneously been in the pool.\n\n\t\n\nINTEGER\n\n\n\n\nthread_pools['completed']\n\n\t\n\nTotal number of completed thread in the thread pool.\n\n\t\n\nBIGINT\n\n\n\n\nthread_pools['threads']\n\n\t\n\nSize of the thread pool.\n\n\t\n\nINTEGER\n\n\n\n\nthread_pools['queue']\n\n\t\n\nNumber of thread currently in the queue.\n\n\t\n\nINTEGER\n\nos\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nos\n\n\t\n\nOperating system stats\n\n\t\n\nOBJECT\n\n\n\n\nos['uptime']\n\n\t\n\nSystem uptime in milliseconds\n\nRequires allowing system calls on Windows and macOS. See notes in Uptime limitations.\n\n\t\n\nBIGINT\n\n\n\n\nos['timestamp']\n\n\t\n\nUNIX timestamp in millisecond resolution\n\n\t\n\nBIGINT\n\n\n\n\nos['cpu']\n\n\t\n\nInformation about CPU utilization\n\n\t\n\nOBJECT\n\n\n\n\nos['cpu']['used']\n\n\t\n\nSystem CPU usage as percentage\n\n\t\n\nSMALLINT\n\n\n\n\nos['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the OS probe.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']\n\n\t\n\nInformation about cgroups (Linux only)\n\n\t\n\nOBJECT\n\n\n\n\nos['cgroup']['cpuacct']\n\n\t\n\nInformation about CPU accounting\n\n\t\n\nOBJECT\n\n\n\n\nos['cgroup']['cpuacct']['control_group']\n\n\t\n\nThe path to the CPU accounting cgroup\n\n\t\n\nTEXT\n\n\n\n\nos['cgroup']['cpuacct']['usage_nanos']\n\n\t\n\nThe total CPU time (in nanoseconds) consumed by all tasks in this cgroup.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['cpu']\n\n\t\n\nInformation about the CPU subsystem\n\n\t\n\nOBJECT\n\n\n\n\nos['cgroup']['cpu']['control_group']\n\n\t\n\nThe path to the CPU cgroup\n\n\t\n\nTEXT\n\n\n\n\nos['cgroup']['cpu']['cfs_period_micros']\n\n\t\n\nThe period of time (in microseconds) the cgroup access to the CPU gets reallocated.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['cpu']['cfs_quota_micros']\n\n\t\n\nThe total amount of time (in microseconds) for which all tasks in the cgroup can run during one period (cfs_period_micros).\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['cpu']['num_elapsed_periods']\n\n\t\n\nThe nr. of period intervals (cfs_period_micros) that have elapsed.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['cpu']['num_times_throttled']\n\n\t\n\nThe nr. of times tasks in the cgroup have been throttled.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['cpu']['time_throttled_nanos']\n\n\t\n\nThe total time (in nanoseconds) for which tasks in the cgroup have been throttled.\n\n\t\n\nBIGINT\n\n\n\n\nos['cgroup']['mem']\n\n\t\n\nInformation about memory resources used by tasks in a cgroup.\n\n\t\n\nOBJECT\n\n\n\n\nos['cgroup']['mem']['control_group']\n\n\t\n\nThe path to the memory cgroup\n\n\t\n\nTEXT\n\n\n\n\nos['cgroup']['mem']['usage_bytes']\n\n\t\n\nThe total current memory usage by processes in the cgroup.\n\n\t\n\nTEXT\n\n\n\n\nos['cgroup']['mem']['limit_bytes']\n\n\t\n\nThe max. amount of user memory in the cgroup.\n\n\t\n\nTEXT\n\nThe CPU information values are cached for 1s. They might differ from the actual values at query time. Use the probe timestamp to get the time of collection. When analyzing the CPU usage over time, always use os['probe_timestamp'] to calculate the time difference between 2 probes.\n\ncgroup limitations\n\nNote\n\ncgroup metrics only work if the stats are available from /sys/fs/cgroup/cpu and /sys/fs/cgroup/cpuacct.\n\nUptime limitations\n\nNote\n\nos[‘uptime’] required a system call when running CrateDB on Windows or macOS, however, system calls are not permitted by default. If you require this metric you need to allow system calls by setting bootstrap.seccomp to false. This setting must be set in the crate.yml or via command line argument and cannot be changed at runtime.\n\nos_info\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nos_info\n\n\t\n\nOperating system information\n\n\t\n\nOBJECT\n\n\n\n\nos_info['available_processors']\n\n\t\n\nNumber of processors that are available in the JVM. This is usually equal to the number of cores of the CPU.\n\n\t\n\nINTEGER\n\n\n\n\nos_info['name']\n\n\t\n\nName of the operating system (ex: Linux, Windows, macOS)\n\n\t\n\nTEXT\n\n\n\n\nos_info['arch']\n\n\t\n\nName of the JVM architecture (ex: amd64, x86)\n\n\t\n\nTEXT\n\n\n\n\nos_info['version']\n\n\t\n\nVersion of the operating system\n\n\t\n\nTEXT\n\n\n\n\nos_info['jvm']\n\n\t\n\nInformation about the JVM (Java Virtual Machine)\n\n\t\n\nOBJECT\n\n\n\n\nos_info['jvm']['version']\n\n\t\n\nThe JVM version\n\n\t\n\nTEXT\n\n\n\n\nos_info['jvm']['vm_name']\n\n\t\n\nThe name of the JVM (e.g. OpenJDK, Java HotSpot(TM) )\n\n\t\n\nTEXT\n\n\n\n\nos_info['jvm']['vm_vendor']\n\n\t\n\nThe vendor name of the JVM\n\n\t\n\nTEXT\n\n\n\n\nos_info['jvm']['vm_version']\n\n\t\n\nThe version of the JVM\n\n\t\n\nTEXT\n\nnetwork\n\nNetwork statistics are deprecated in CrateDB 2.3 and may completely be removed in subsequent versions. All BIGINT columns always return 0.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nnetwork\n\n\t\n\nStatistics about network activity on the host.\n\n\t\n\nOBJECT\n\n\n\n\nnetwork['probe_timestamp']\n\n\t\n\nUnix timestamp at the time of collection of the network probe.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']\n\n\t\n\nTCP network activity on the host.\n\n\t\n\nOBJECT\n\n\n\n\nnetwork['tcp']['connections']\n\n\t\n\nInformation about TCP network connections.\n\n\t\n\nOBJECT\n\n\n\n\nnetwork['tpc']['connections']['initiated']\n\n\t\n\nTotal number of initiated TCP connections.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tpc']['connections']['accepted']\n\n\t\n\nTotal number of accepted TCP connections.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tpc']['connections']['curr_established']\n\n\t\n\nTotal number of currently established TCP connections.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']['connections']['dropped']\n\n\t\n\nTotal number of dropped TCP connections.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']['connections']['embryonic_dropped']\n\n\t\n\nTotal number of TCP connections that have been dropped before they were accepted.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']['packets']\n\n\t\n\nInformation about TCP packets.\n\n\t\n\nOBJECT\n\n\n\n\nnetwork['tpc']['packets']['sent']\n\n\t\n\nTotal number of TCP packets sent.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']['packets']['received']\n\n\t\n\nTotal number of TCP packets received.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tpc']['packets']['retransmitted']\n\n\t\n\nTotal number of TCP packets retransmitted due to an error.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']['packets']['errors_received']\n\n\t\n\nTotal number of TCP packets that contained checksum errors, had a bad offset, were dropped because of a lack of memory or were too short.\n\n\t\n\nBIGINT\n\n\n\n\nnetwork['tcp']]['packets']['rst_sent']\n\n\t\n\nTotal number of RST packets sent due to left unread data in queue when socket is closed. See tools.ietf.org.\n\n\t\n\nBIGINT\n\nconnections\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nhttp\n\n\t\n\nNumber of connections established via HTTP\n\n\t\n\nOBJECT\n\n\n\n\nhttp['open']\n\n\t\n\nThe currently open connections established via HTTP\n\n\t\n\nBIGINT\n\n\n\n\nhttp['total']\n\n\t\n\nThe total number of connections that have been established via HTTP over the life time of a CrateDB node\n\n\t\n\nBIGINT\n\n\n\n\npsql\n\n\t\n\nNumber of connections established via PostgreSQL protocol\n\n\t\n\nOBJECT\n\n\n\n\npsql['open']\n\n\t\n\nThe currently open connections established via PostgreSQL protocol\n\n\t\n\nBIGINT\n\n\n\n\npsql['total']\n\n\t\n\nThe total number of connections that have been established via PostgreSQL protocol over the life time of a CrateDB node\n\n\t\n\nBIGINT\n\n\n\n\ntransport\n\n\t\n\nNumber of connections established via Transport protocol\n\n\t\n\nOBJECT\n\n\n\n\ntransport['open']\n\n\t\n\nThe currently open connections established via Transport protocol\n\n\t\n\nBIGINT\n\nprocess\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nprocess\n\n\t\n\nStatistics about the CrateDB process.\n\n\t\n\nOBJECT\n\n\n\n\nprocess['open_file_descriptors']\n\n\t\n\nNumber of currently open file descriptors used by the CrateDB process.\n\n\t\n\nBIGINT\n\n\n\n\nprocess['max_open_file_descriptors']\n\n\t\n\nThe maximum number of open file descriptors CrateDB can use.\n\n\t\n\nBIGINT\n\n\n\n\nprocess['probe_timestamp']\n\n\t\n\nThe system UNIX timestamp at the moment of the probe collection.\n\n\t\n\nBIGINT\n\n\n\n\nprocess['cpu']\n\n\t\n\nInformation about the CPU usage of the CrateDB process.\n\n\t\n\nOBJECT\n\n\n\n\nprocess['cpu']['percent']\n\n\t\n\nThe CPU usage of the CrateDB JVM process given in percent.\n\n\t\n\nSMALLINT\n\nThe CPU information values are cached for 1s. They might differ from the actual values at query time. Use the probe timestamp to get the time of the collect. When analyzing the CPU usage over time, always use process['probe_timestamp'] to calculate the time difference between 2 probes.\n\nNote\n\nIf one of the queried nodes is not responding within three seconds it returns null every column except id and name. This behaviour could be used to detect hanging nodes.\n\nNode checks\n\nThe table sys.node_checks exposes a list of internal node checks and results of their validation.\n\nThe table schema is the following:\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe unique check ID.\n\n\t\n\nINTEGER\n\n\n\n\nnode_id\n\n\t\n\nThe unique node ID.\n\n\t\n\nTEXT\n\n\n\n\nseverity\n\n\t\n\nThe level of severity. The higher the value of the field the higher severity.\n\n\t\n\nINTEGER\n\n\n\n\ndescription\n\n\t\n\nThe description message for the setting check.\n\n\t\n\nTEXT\n\n\n\n\npassed\n\n\t\n\nThe flag determines whether the check for the setting has passed.\n\n\t\n\nBOOLEAN\n\n\n\n\nacknowledged\n\n\t\n\nThe flag determines whether the check for this setting has been acknowledged by the user in order to ignored the value of passed column. This column can be updated.\n\n\t\n\nBOOLEAN\n\nExample query:\n\ncr> select id, node_id, description from sys.node_checks order by id, node_id;\n+----+---------...-+--------------------------------------------------------------...-+\n| id | node_id     | description                                                      |\n+----+---------...-+--------------------------------------------------------------...-+\n|  1 | ...         | It has been detected that the 'gateway.expected_data_nodes' s... |\n|  2 | ...         | The cluster setting 'gateway.recover_after_data_nodes' (or th... |\n|  3 | ...         | If any of the \"expected data nodes\" recovery settings are set... |\n|  5 | ...         | The high disk watermark is exceeded on the node. The cluster ... |\n|  6 | ...         | The low disk watermark is exceeded on the node. The cluster w... |\n|  7 | ...         | The flood stage disk watermark is exceeded on the node. Table... |\n|  8 | ...         | The amount of shards on the node reached 90 % of the limit of... |\n+----+---------...-+--------------------------------------------------------------...-+\nSELECT 7 rows in set (... sec)\n\nAcknowledge failed checks\n\nIt is possible to acknowledge every check by updating the acknowledged column. By doing this, specially CrateDB’s built-in Admin UI won’t complain anymore about failing checks.\n\nImagine we’ve added a new node to our cluster, but as the gateway.expected_data_nodes column can only be set via config-file or command-line argument, the check for this setting will not pass on the already running nodes until the config-file or command-line argument on these nodes is updated and the nodes are restarted (which is not what we want on a healthy well running cluster).\n\nIn order to make the Admin UI accept a failing check (so the checks label goes green again), we must acknowledge this check by updating it’s acknowledged flag:\n\ncr> update sys.node_checks set acknowledged = true where id = 1;\nUPDATE OK, 1 row affected (... sec)\n\n\nCaution\n\nUpdates on this column are transient, so changed values are lost after the affected node is restarted.\n\nDescription of checked node settings\nRecovery expected data nodes\n\nThis check looks at the gateway.expected_data_nodes setting and checks if its value matches the actual number of data nodes present in the cluster. If the actual number of nodes is below the expected number, the warning is raised to indicate some nodes are down. If the actual number is greater, this is flagged to indicate the setting should be updated.\n\nNote\n\nFor backward compatibility, setting the deprecated gateway.expected_nodes instead is still supported. It counts all nodes, not only data-carrying nodes.\n\nRecovery after data nodes\n\nThis check looks at the gateway.recover_after_data_nodes setting and checks if its value is greater than half the configured expected number, but not greater than the configured expected number.\n\nNote\n\nFor backward compatibility, setting the deprecated gateway.recover_after_nodes instead is still supported.\n\n(E / 2) < R <= E\n\n\nHere, R is the number of recovery nodes and E is the number of expected (data) nodes.\n\nIf recovery is started when some nodes are down, CrateDB proceeds on the basis the nodes that are down may not be coming back, and it will create new replicas and rebalance shards as necessary. This is throttled, and it can be controlled with routing allocation settings, but depending on the context, you may prefer to delay recovery if the nodes are only down for a short period of time, so it is advisable to review the documentation around the settings involved and configure them carefully.\n\nRecovery after time\n\nIf gateway.recover_after_data_nodes is set, then gateway.recover_after_time must not be set to 0s, otherwise the gateway.recover_after_data_nodes setting wouldn’t have any effect.\n\nNote\n\nFor backward compatibility, setting the deprecated gateway.recover_after_nodes instead is still supported.\n\nRouting allocation disk watermark high\n\nThe check for the cluster.routing.allocation.disk.watermark.high setting verifies that the high watermark is not exceeded on the current node. The usage of each disk for configured CrateDB data paths is verified against the threshold setting. If one or more verification fails the check is marked as not passed.\n\nRouting allocation disk watermark low\n\nThe check for the cluster.routing.allocation.disk.watermark.low which controls the low watermark for the node disk usage. The check verifies that the low watermark is not exceeded on the current node. The verification is done against each disk for configured CrateDB data paths. The check is not passed if the verification for one or more disk fails.\n\nMaximum shards per node\n\nThe check verifies that the amount of shards on the current node is less than 90 percent of cluster.max_shards_per_node. Creating new tables or partitions which would push the number of shards beyond 100 % of the limit will be rejected.\n\nShards\n\nThe table sys.shards contains real-time statistics for all shards of all (non-system) tables.\n\nTable schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nnode\n\n\t\n\nInformation about the node the shard is located at.\n\n\t\n\nOBJECT\n\n\n\n\nnode['name']\n\n\t\n\nThe name of the node the shard is located at.\n\n\t\n\nTEXT\n\n\n\n\nnode['id']\n\n\t\n\nThe id of the node the shard is located at.\n\n\t\n\nTEXT\n\n\n\n\nblob_path\n\n\t\n\nPath to the directory which contains the blob files of the shard, or null if the shard is not a blob shard.\n\n\t\n\nTEXT\n\n\n\n\nid\n\n\t\n\nThe shard id. This shard id is managed by the system, ranging from 0 up to the number of configured shards of the table.\n\n\t\n\nINTEGER\n\n\n\n\nmin_lucene_version\n\n\t\n\nShows the oldest Lucene segment version used in this shard.\n\n\t\n\nTEXT\n\n\n\n\nnum_docs\n\n\t\n\nThe total amount of documents within a shard.\n\n\t\n\nBIGINT\n\n\n\n\noprhan_partition\n\n\t\n\nTrue if this shard belongs to an orphaned partition which doesn’t belong to any table anymore.\n\n\t\n\nBOOLEAN\n\n\n\n\npartition_ident\n\n\t\n\nThe partition ident of a partitioned table. Empty for non-partitioned tables.\n\n\t\n\nTEXT\n\n\n\n\npath\n\n\t\n\nPath to the shard directory on the filesystem. This directory contains state and index files.\n\n\t\n\nTEXT\n\n\n\n\nprimary\n\n\t\n\nIndicates if this shard is the primary shard.\n\n\t\n\nBOOLEAN\n\n\n\n\nrecovery\n\n\t\n\nRecovery statistics for a shard.\n\n\t\n\nOBJECT\n\n\n\n\nrecovery['files']\n\n\t\n\nFile recovery statistics\n\n\t\n\nOBJECT\n\n\n\n\nrecovery['files']['percent']\n\n\t\n\nPercentage of files already recovered.\n\n\t\n\nREAL\n\n\n\n\nrecovery['files']['recovered']\n\n\t\n\nNumber of files recovered in the shard. Includes both existing and reused files.\n\n\t\n\nINTEGER\n\n\n\n\nrecovery['files']['reused']\n\n\t\n\nTotal number of files reused from a local copy while recovering the shard.\n\n\t\n\nINTEGER\n\n\n\n\nrecovery['files']['used']\n\n\t\n\nTotal number of files in the shard.\n\n\t\n\nINTEGER\n\n\n\n\nrecovery['size']\n\n\t\n\nRecovery statistics for the shard in bytes\n\n\t\n\nOBJECT\n\n\n\n\nrecovery['size']['percent']\n\n\t\n\nPercentage of bytes already recovered\n\n\t\n\nREAL\n\n\n\n\nrecovery['size']['recovered']\n\n\t\n\nNumber of bytes recovered. Includes both existing and re-used bytes.\n\n\t\n\nBIGINT\n\n\n\n\nrecovery['size']['reused']\n\n\t\n\nNumber of bytes re-used from a local copy while recovering the shard.\n\n\t\n\nBIGINT\n\n\n\n\nrecovery['size']['used']\n\n\t\n\nTotal number of bytes in the shard.\n\n\t\n\nBIGINT\n\n\n\n\nrecovery['stage']\n\n\t\n\nRecovery stage:\n\ninit: Recovery has not started\n\nindex: Reading the Lucene index meta-data and copying bytes from source to destination\n\nstart: Starting the engine, opening the index for use\n\ntranslog: Replaying transaction log\n\nfinalize: Cleanup\n\ndone: Complete\n\n\t\n\nTEXT\n\n\n\n\nrecovery['total_time']\n\n\t\n\nReturns elapsed time from the start of the shard recovery.\n\n\t\n\nBIGINT\n\n\n\n\nrecovery['type']\n\n\t\n\nRecovery type:\n\ngateway\n\nsnapshot\n\nreplica\n\nrelocating\n\n\t\n\nTEXT\n\n\n\n\nrelocating_node\n\n\t\n\nThe id of the node to which the shard is getting relocated to.\n\n\t\n\nTEXT\n\n\n\n\nrouting_state\n\n\t\n\nThe current state of the shard in the routing table. Possible states are:\n\nUNASSIGNED\n\nINITIALIZING\n\nSTARTED\n\nRELOCATING\n\n\t\n\nTEXT\n\n\n\n\nschema_name\n\n\t\n\nThe schema name of the table the shard belongs to\n\n\t\n\nTEXT\n\n\n\n\nsize\n\n\t\n\nThe current size in bytes. This value is cached for a short period and may return slightly outdated values.\n\n\t\n\nBIGINT\n\n\n\n\nstate\n\n\t\n\nThe current state of the shard. Possible states are:\n\nCREATED\n\nRECOVERING\n\nPOST_RECOVERY\n\nSTARTED\n\nRELOCATED\n\nCLOSED\n\nINITIALIZING\n\nUNASSIGNED\n\n\t\n\nTEXT\n\n\n\n\nclosed\n\n\t\n\nThe state of the table associated with the shard.\n\n\t\n\nBOOLEAN\n\n\n\n\ntable_name\n\n\t\n\nThe name of the table this shard belongs to\n\n\t\n\nTEXT\n\n\n\n\nseq_no_stats\n\n\t\n\nContains information about internal sequence numbering and checkpoints for these sequence numbers.\n\n\t\n\nOBJECT\n\n\n\n\nseq_no_stats['max_seq_no']\n\n\t\n\nThe highest sequence number that has been issued so far on the shard.\n\n\t\n\nBIGINT\n\n\n\n\nseq_no_stats['local_checkpoint']\n\n\t\n\nThe highest sequence number for which all lower sequence number of been processed on this shard. Due to concurrent indexing this can be lower than max_seq_no.\n\n\t\n\nBIGINT\n\n\n\n\nseq_no_stats['global_checkpoint']\n\n\t\n\nThe highest sequence number for which the local shard can guarantee that all lower sequence numbers have been processed on all active shard copies.\n\n\t\n\nBIGINT\n\n\n\n\ntranslog_stats\n\n\t\n\nContains information for the translog of the shard.\n\n\t\n\nOBJECT\n\n\n\n\ntranslog_stats['size']\n\n\t\n\nThe current size of the translog file in bytes.\n\n\t\n\nBIGINT\n\n\n\n\ntranslog_stats['uncommitted_size']\n\n\t\n\nThe size in bytes of the translog that has not been committed to Lucene yet.\n\n\t\n\nBIGINT\n\n\n\n\ntranslog_stats['number_of_operations']\n\n\t\n\nThe number of operations recorded in the translog.\n\n\t\n\nINTEGER\n\n\n\n\ntranslog_stats['uncommitted_operations']\n\n\t\n\nThe number of operations in the translog which have not been committed to Lucene yet.\n\n\t\n\nINTEGER\n\n\n\n\nretention_leases\n\n\t\n\nVersioned collection of retention leases.\n\n\t\n\nOBJECT\n\n\n\n\nflush_stats\n\n\t\n\nFlush information. Shard relocation resets this information.\n\n\t\n\nOBJECT\n\n\n\n\nflush_stats['count']\n\n\t\n\nThe total amount of flush operations that happened on the shard.\n\n\t\n\nBIGINT\n\n\n\n\nflush_stats['periodic_count']\n\n\t\n\nThe number of periodic flushes. Each periodic flush also counts as a regular flush. A periodic flush can happen after writes depending on settings like the translog flush threshold.\n\n\t\n\nBIGINT\n\n\n\n\nflush_stats['total_time_ns']\n\n\t\n\nThe total time spent on flush operations on the shard.\n\n\t\n\nBIGINT\n\nNote\n\nThe sys.shards table is subject to Shard table permissions.\n\nExample\n\nFor example, you can query shards like this:\n\ncr> select schema_name as schema,\n...   table_name as t,\n...   id,\n...   partition_ident as p_i,\n...   num_docs as docs,\n...   primary,\n...   relocating_node as r_n,\n...   routing_state as r_state,\n...   state,\n...   orphan_partition as o_p\n... from sys.shards where table_name = 'locations' and id = 1;\n+--------+-----------+----+-----+------+---------+------+---------+---------+-------+\n| schema | t         | id | p_i | docs | primary | r_n  | r_state |  state  | o_p   |\n+--------+-----------+----+-----+------+---------+------+---------+---------+-------+\n| doc    | locations |  1 |     |    8 | TRUE    | NULL | STARTED | STARTED | FALSE |\n+--------+-----------+----+-----+------+---------+------+---------+---------+-------+\nSELECT 1 row in set (... sec)\n\nSegments\n\nThe sys.segments table contains information about the Lucene segments of the shards.\n\nThe segment information is useful to understand the behaviour of the underlying Lucene file structures for troubleshooting and performance optimization of shards.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nsegment_name\n\n\t\n\nName of the segment, derived from the segment generation and used internally to create file names in the directory of the shard.\n\n\t\n\nTEXT\n\n\n\n\nshard_id\n\n\t\n\nID of the effected shard.\n\n\t\n\nINTEGER\n\n\n\n\ntable_schema\n\n\t\n\nSchema name of the table of the shard.\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nTable name of the shard.\n\n\t\n\nTEXT\n\n\n\n\npartition_ident\n\n\t\n\nThe partition ident of a partitioned table. Empty for non-partitioned tables.\n\n\t\n\nTEXT\n\n\n\n\nnode\n\n\t\n\nInformation about the node the shard is located at.\n\n\t\n\nOBJECT\n\n\n\n\nnode['name']\n\n\t\n\nThe name of the node the shard is located at.\n\n\t\n\nTEXT\n\n\n\n\nnode['id']\n\n\t\n\nThe id of the node the shard is located at.\n\n\t\n\nTEXT\n\n\n\n\ngeneration\n\n\t\n\nGeneration number of the segment, increments for each segment written.\n\n\t\n\nLONG\n\n\n\n\nnum_docs\n\n\t\n\nNumber of non-deleted Lucene documents in this segment.\n\n\t\n\nINTEGER\n\n\n\n\ndeleted_docs\n\n\t\n\nNumber of deleted Lucene documents in this segment.\n\n\t\n\nINTEGER\n\n\n\n\nsize\n\n\t\n\nDisk space used by the segment in bytes.\n\n\t\n\nLONG\n\n\n\n\nmemory\n\n\t\n\nUnavailable starting from CrateDB 5.0. Always returns -1.\n\n\t\n\nLONG\n\n\n\n\ncommitted\n\n\t\n\nIndicates if the segments are synced to disk. Segments that are synced can survive a hard reboot.\n\n\t\n\nBOOLEAN\n\n\n\n\nprimary\n\n\t\n\nDescribes if this segment is part of a primary shard.\n\n\t\n\nBOOLEAN\n\n\n\n\nsearch\n\n\t\n\nIndicates if the segment is searchable. If false, the segment has most likely been written to disk but needs a refresh to be searchable.\n\n\t\n\nBOOLEAN\n\n\n\n\nversion\n\n\t\n\nVersion of Lucene used to write the segment.\n\n\t\n\nTEXT\n\n\n\n\ncompound\n\n\t\n\nIf true, Lucene merges all files from the segment into a single file to save file descriptors.\n\n\t\n\nBOOLEAN\n\n\n\n\nattributes\n\n\t\n\nContains information about whether high compression was enabled.\n\n\t\n\nOBJECT\n\nNote\n\nThe information in the sys.segments table is expensive to calculate and therefore this information should be retrieved with awareness that it can have performance implications on the cluster.\n\nNote\n\nThe sys.shards table is subject to Shard table permissions.\n\nJobs, operations, and logs\n\nTo let you inspect the activities currently taking place in a cluster, CrateDB provides system tables that let you track current cluster jobs and operations. See Jobs Table and Operations Table.\n\nJobs and operations that finished executing are additionally recorded in memory. There are two retention policies available to control how many records should be kept.\n\nOne option is to configure the maximum number of records which should be kept. Once the configured table size is reached, the older log records are deleted as newer records are added. This is configurable using stats.jobs_log_size and stats.operations_log_size.\n\nAnother option is to configure an expiration time for the records. In this case, the records in the logs tables are periodically cleared if they are older than the expiry time. This behaviour is configurable using stats.jobs_log_expiration and stats.operations_log_expiration.\n\nIn addition to these retention policies, there is a memory limit in place preventing these tables from taking up too much memory. The amount of memory that can be used to store the jobs can be configured using stats.breaker.log.jobs.limit and stats.breaker.log.operations.limit. If the memory limit is reached, an error message will be logged and the log table will be cleared completely.\n\nIt is also possible to define a filter which must match for jobs to be recorded after they finished executing. This can be useful to only record slow queries or queries that failed due to an error. This filter can be configured using the stats.jobs_log_filer setting.\n\nFurthermore, there is a second filter setting which also results in a log entry in the regular CrateDB log file for all finished jobs that match this filter. This can be configured using stats.jobs_log_persistent_filter. This could be used to create a persistent slow query log.\n\nJobs\n\nThe sys.jobs table is a constantly updated view of all jobs that are currently being executed in the cluster.\n\nTable schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe job UUID.\n\nThis job ID is generated by the system.\n\n\t\n\nTEXT\n\n\n\n\nnode\n\n\t\n\nInformation about the node that created the job.\n\n\t\n\nOBJECT\n\n\n\n\nnode['id']\n\n\t\n\nThe id of the node.\n\n\t\n\nTEXT\n\n\n\n\nnode['name']\n\n\t\n\nThe name of the node.\n\n\t\n\nTEXT\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the job started.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nstmt\n\n\t\n\nShows the data query or manipulation statement represented by this job.\n\n\t\n\nTEXT\n\n\n\n\nusername\n\n\t\n\nThe user who is executing the statement.\n\n\t\n\nTEXT\n\nThe field username corresponds to the SESSION_USER that is performing the query:\n\ncr> select stmt, username, started from sys.jobs where stmt like 'sel% from %jobs%';\n+---------------------------------------------------------------------------------+----------+-...-----+\n| stmt                                                                            | username | started |\n+---------------------------------------------------------------------------------+----------+-...-----+\n| select stmt, username, started from sys.jobs where stmt like 'sel% from %jobs%' | crate    | ...     |\n+---------------------------------------------------------------------------------+----------+-...-----+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nIf the user management module is not available, the username is given as crate.\n\nEvery request that queries data or manipulates data is considered a “job” if it is a valid query. Requests that are not valid queries (for example, a request that tries to query a non-existent table) will not show up as jobs.\n\nNote\n\nThe sys.jobs table is subject to sys jobs tables permissions.\n\nJobs metrics\n\nThe sys.jobs_metrics table provides an overview of the query latency in the cluster. Jobs metrics are not persisted across node restarts.\n\nThe metrics are aggregated for each node and each unique classification of the statements.\n\nNote\n\nIn order to reduce the memory requirements for these metrics, the times are statistically sampled and therefore may have slight inaccuracies. In addition, durations are only tracked up to 10 minutes. Statements taking longer than that are capped to 10 minutes.\n\nsys.jobs_metrics Table schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nnode\n\n\t\n\nAn object containing the id and name of the node on which the metrics have been sampled.\n\n\t\n\nOBJECT\n\n\n\n\nclassification\n\n\t\n\nAn object containing the statement classification.\n\n\t\n\nOBJECT\n\n\n\n\nclassification['type']\n\n\t\n\nThe general type of the statement. Types are: INSERT, SELECT, UPDATE, DELETE, COPY, DDL, and MANAGEMENT.\n\n\t\n\nTEXT\n\n\n\n\nclassification['labels']\n\n\t\n\nLabels are only available for certain statement types that can be classified more accurately than just by their type.\n\n\t\n\nTEXT_ARRAY\n\n\n\n\ntotal_count\n\n\t\n\nTotal number of queries executed\n\n\t\n\nBIGINT\n\n\n\n\nfailed_count\n\n\t\n\nTotal number of queries that failed to complete successfully.\n\n\t\n\nBIGINT\n\n\n\n\nsum_of_durations\n\n\t\n\nSum of durations in ms of all executed queries per statement type.\n\n\t\n\nBIGINT\n\n\n\n\nstdev\n\n\t\n\nThe standard deviation of the query latencies\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\nmean\n\n\t\n\nThe mean query latency in ms\n\n\t\n\nDOUBLE PRECISION\n\n\n\n\nmax\n\n\t\n\nThe maximum query latency in ms\n\n\t\n\nBIGINT\n\n\n\n\nmin\n\n\t\n\nThe minimum query latency in ms\n\n\t\n\nBIGINT\n\n\n\n\npercentiles\n\n\t\n\nAn object containing different percentiles\n\n\t\n\nOBJECT\n\nClassification\n\nCertain statement types (such as SELECT statements) have additional labels in their classification. These labels are the names of the logical plan operators that are involved in the query.\n\nFor example, the following UNION statement:\n\nSELECT name FROM t1 where id = 1\nUNION ALL\nSELECT name FROM t2 where id < 2\n\n\nwould result in the following labels:\n\nUnion` for the UNION ALL\n\nGet for the left SELECT\n\nCollect for the right SELECT\n\nNote\n\nLabels may be subject to change as they only represent internal properties of the statement!\n\nOperations\n\nThe sys.operations table is a constantly updated view of all operations that are currently being executed in the cluster:\n\ncr> select node['name'], job_id, name, used_bytes from sys.operations\n... order by name limit 1;\n+--------------+--------...-+-----...-+------------+\n| node['name'] | job_id     | name    | used_bytes |\n+--------------+--------...-+-----...-+------------+\n| crate        | ...        | ...     | ...        |\n+--------------+--------...-+-----...-+------------+\nSELECT 1 row in set (... sec)\n\n\nAn operation is a node-specific sub-component of a job (for when a job involves multi-node processing). Jobs that do not require multi-node processing will not produce any operations.\n\nTable schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe operation UUID.\n\nThis operation ID is generated by the system.\n\n\t\n\nTEXT\n\n\n\n\njob_id\n\n\t\n\nThe job id this operation belongs to.\n\n\t\n\nTEXT\n\n\n\n\nname\n\n\t\n\nThe name of the operation.\n\n\t\n\nTEXT\n\n\n\n\nnode\n\n\t\n\nInformation about the node that created the operation.\n\n\t\n\nOBJECT\n\n\n\n\nnode['id']\n\n\t\n\nThe id of the node.\n\n\t\n\nTEXT\n\n\n\n\nnode['name']\n\n\t\n\nThe name of the node.\n\n\t\n\nTEXT\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the operation started.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nused_bytes\n\n\t\n\nCurrently loaded amount of data by the operation.\n\n\t\n\nBIGINT\n\nNote\n\nIn some cases, operations are generated for internal CrateDB work that does not directly correspond to a user request. These entries do not have corresponding entries in sys.jobs.\n\nLogs\n\nThe sys.jobs and sys.operations tables have corresponding log tables: sys.jobs_log and sys.operations_log.\n\nsys.jobs_log Table schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe job ID.\n\n\t\n\nTEXT\n\n\n\n\nended\n\n\t\n\nThe point in time when the job finished.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nerror\n\n\t\n\nIf the job encountered an error, this will hold the error message.\n\n\t\n\nTEXT\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the job started.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nstmt\n\n\t\n\nShows the data query or manipulation statement executed by the job.\n\n\t\n\nTEXT\n\n\n\n\nusername\n\n\t\n\nThe user who executed the statement.\n\n\t\n\nTEXT\n\n\n\n\nclassification\n\n\t\n\nAn object containing the statement classification.\n\n\t\n\nOBJECT\n\n\n\n\nclassification['type']\n\n\t\n\nThe general type of the statement. Types are: INSERT, SELECT, UPDATE, DELETE,``COPY``, DDL, and MANAGEMENT.\n\n\t\n\nTEXT\n\n\n\n\nclassification['labels']\n\n\t\n\nLabels are only available for certain statement types that can be classified more accurately than just by their type.\n\n\t\n\nTEXT_ARRAY\n\n\n\n\nnode\n\n\t\n\nInformation about the node that created the job.\n\n\t\n\nOBJECT\n\n\n\n\nnode['id']\n\n\t\n\nThe id of the node.\n\n\t\n\nTEXT\n\n\n\n\nnode['name']\n\n\t\n\nThe name of the node.\n\n\t\n\nTEXT\n\nNote\n\nYou can control which jobs are recorded using the stats.jobs_log_filter\n\nNote\n\nThe sys.jobs_log table is subject to sys jobs tables permissions.\n\nsys.operations_log Table schema\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe operation ID.\n\n\t\n\nTEXT\n\n\n\n\njob_id\n\n\t\n\nThe job id.\n\n\t\n\nTEXT\n\n\n\n\nended\n\n\t\n\nThe point in time when the operation finished.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nerror\n\n\t\n\nIf the operation encountered an error, this will hold the error message.\n\n\t\n\nTEXT\n\n\n\n\nname\n\n\t\n\nThe name of the operation.\n\n\t\n\nTEXT\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the operation started.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nused_bytes\n\n\t\n\nThe amount of data loaded by the operation.\n\n\t\n\nBIGINT\n\nAfter a job or operation finishes, the corresponding entry will be moved into the corresponding log table:\n\ncr> select id, stmt, username, started, ended, error\n... from sys.jobs_log order by ended desc limit 2;\n+-...+----------------------------------------------...-+----------+-...-----+-...---+-------+\n| id | stmt                                             | username | started | ended | error |\n+-...+----------------------------------------------...-+----------+-...-----+-...---+-------+\n| ...| select node['name'], ...                         | crate    | ...     | ...   |  NULL |\n| ...| select stmt, username, started from sys.jobs ... | crate    | ...     | ...   |  NULL |\n+-...+----------------------------------------------...-+----------+-...-----+-...---+-------+\nSELECT 2 rows in set (... sec)\n\n\nInvalid queries are also logged in the sys.jobs_log table, i.e. queries that never make it to the sys.jobs table because they could not be executed.\n\nThe log tables are bound by a fixed size (stats.jobs_log_size) or by an expiration time (stats.jobs_log_expiration)\n\nSee Collecting stats for information on how to configure logs.\n\nCaution\n\nIf you deactivate statistics tracking, the logs tables will be truncated.\n\nCluster checks\n\nThe table sys.checks exposes a list of internal cluster checks and results of their validation.\n\nThe sys.checks table looks like this:\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nid\n\n\t\n\nThe unique check id.\n\n\t\n\nINTEGER\n\n\n\n\nseverity\n\n\t\n\nThe level of severity. The higher the value of the field the higher severity.\n\n\t\n\nINTEGER\n\n\n\n\ndescription\n\n\t\n\nThe description message for the setting check.\n\n\t\n\nTEXT\n\n\n\n\npassed\n\n\t\n\nThe flag determines whether the check for the setting has passed.\n\n\t\n\nBOOLEAN\n\nHere’s an example query:\n\ncr> select id, description from sys.checks order by id;\n+----+--------------------------------------------------------------...-+\n| id | description                                                      |\n+----+--------------------------------------------------------------...-+\n|  2 | The total number of partitions of one or more partitioned tab... |\n|  3 | The following tables need to be recreated for compatibility w... |\n+----+--------------------------------------------------------------...-+\nSELECT 2 rows in set (... sec)\n\n\nCluster checks are also indicated in the CrateDB admin console. When all cluster checks (and all Node checks) pass, the Checks icon will be green. Here’s what it looks like when some checks are failing at the CRITICAL severity level:\n\nCurrent Checks\nNumber of partitions\n\nThis check warns if any partitioned table has more than 1000 partitions to detect the usage of a high cardinality field for partitioning.\n\nTables need to be recreated\n\nWarning\n\nDo not attempt to upgrade your cluster to a newer major version if this cluster check is failing. Follow the instructions below to get this cluster check passing.\n\nThis check warns you if your cluster contains tables that you need to reindex before you can upgrade to a future major version of CrateDB.\n\nIf you try to upgrade to a later major CrateDB version without reindexing the tables, CrateDB will refuse to start.\n\nCrateDB table version compatibility scheme\n\nCrateDB maintains backward compatibility for tables created in majorVersion - 1:\n\nTable Origin\n\n\t\n\nCurrent Version\n\n\t\n\nCurrent Version\n\n\t\n\nCurrent Version\n\n\n\t\n\n3.x\n\n\t\n\n4.x\n\n\t\n\n5.x\n\n\n\n\n3.x\n\n\t\n\n✔️\n\n\t\n\n✔️\n\n\t\n\n❌\n\n\n\n\n4.x\n\n\t\n\n❌\n\n\t\n\n✔️\n\n\t\n\n✔️\n\n\n\n\n5.x\n\n\t\n\n❌\n\n\t\n\n❌\n\n\t\n\n✔️\n\nAvoiding reindex using partitioned tables\n\nReindexing tables is an expensive operation which can take a long time. If you are storing time series data for a certain retention period and intend to delete old data, it is possible to use the partitioned tables to avoid reindex operations.\n\nYou will have to use a partition column that denotes time. For example, if you have a retention period of nine months, you could partition a table by a month column. Then, every month, the system will create a new partition. This new partition is created using the active CrateDB version and is compatible with the next major CrateDB version. Now to achieve your goal of avoiding a reindex, you must manually delete any partition older than nine months. If you do that, then after nine months you rolled through all partitions and the remaining nine are compatible with the next major CrateDB version.\n\nHow to reindex\n\nUse SHOW CREATE TABLE to get the schema required to create an empty copy of the table to recreate:\n\ncr> SHOW CREATE TABLE rx.metrics;\n+-----------------------------------------------------+\n| SHOW CREATE TABLE rx.metrics                        |\n+-----------------------------------------------------+\n| CREATE TABLE IF NOT EXISTS \"rx\".\"metrics\" (         |\n|    \"id\" TEXT NOT NULL,                                       |\n|    \"temperature\" REAL,                              |\n|    PRIMARY KEY (\"id\")                               |\n| )                                                   |\n| CLUSTERED BY (\"id\") INTO 4 SHARDS                   |\n| WITH (                                              |\n|    \"allocation.max_retries\" = 5,                    |\n|    \"blocks.metadata\" = false,                       |\n|    \"blocks.read\" = false,                           |\n|    \"blocks.read_only\" = false,                      |\n|    \"blocks.read_only_allow_delete\" = false,         |\n|    \"blocks.write\" = false,                          |\n|    codec = 'default',                               |\n|    column_policy = 'strict',                        |\n|    \"mapping.total_fields.limit\" = 1000,             |\n|    max_ngram_diff = 1,                              |\n|    max_shingle_diff = 3,                            |\n|    number_of_replicas = '0-1',                      |\n|    \"routing.allocation.enable\" = 'all',             |\n|    \"routing.allocation.total_shards_per_node\" = -1, |\n|    \"store.type\" = 'fs',                             |\n|    \"translog.durability\" = 'REQUEST',               |\n|    \"translog.flush_threshold_size\" = 536870912,     |\n|    \"translog.sync_interval\" = 5000,                 |\n|    \"unassigned.node_left.delayed_timeout\" = 60000,  |\n|    \"write.wait_for_active_shards\" = '1'             |\n| )                                                   |\n+-----------------------------------------------------+\nSHOW 1 row in set (... sec)\n\n\nCreate a new temporary table, using the schema returned from SHOW CREATE TABLE:\n\ncr> CREATE TABLE rx.tmp_metrics (id TEXT PRIMARY KEY, temperature REAL);\nCREATE OK, 1 row affected (... sec)\n\n\nCopy the data:\n\ncr> INSERT INTO rx.tmp_metrics (id, temperature) (SELECT id, temperature FROM rx.metrics);\nINSERT OK, 2 rows affected (... sec)\n\n\nSwap the tables:\n\ncr> ALTER CLUSTER SWAP TABLE rx.tmp_metrics TO rx.metrics;\nALTER OK, 1 row affected  (... sec)\n\n\nConfirm the new your_table contains all data and has the new version:\n\ncr> SELECT count(*) FROM rx.metrics;\n+----------+\n| count(*) |\n+----------+\n|        2 |\n+----------+\nSELECT 1 row in set (... sec)\n\ncr> SELECT version['created'] FROM information_schema.tables\n... WHERE table_schema = 'rx' AND table_name = 'metrics';\n+--------------------+\n| version['created'] |\n+--------------------+\n| 5.6.4              |\n+--------------------+\nSELECT 1 row in set (... sec)\n\n\nDrop the old table, as it is now obsolete:\n\ncr> DROP TABLE rx.tmp_metrics;\nDROP OK, 1 row affected  (... sec)\n\n\nAfter you reindexed all tables, this cluster check will pass.\n\nNote\n\nSnapshots of your tables created prior to them being upgraded will not work with future versions of CrateDB. For this reason, you should create a new snapshot for each of your tables. (See Snapshots.)\n\nLicense check\n\nNote\n\nThis check was removed in version 4.5 because CrateDB no longer requires an enterprise license, see also Farewell to the CrateDB Enterprise License.\n\nThis check warns you when your license is close to expiration, is already expired, or if the cluster contains more nodes than allowed by your license. It will yield a MEDIUM alert when your license is valid for less than 15 days and a HIGH alert when your license is valid for less than a day. All other cases, like already expired or max-nodes-violation, it will result in a HIGH alert. We recommend that you request a new license when this check triggers, in order to avoid the situation where operations are rejected due to an invalid license.\n\nHealth\n\nThe sys.health table lists the health of each table and table partition. The health is computed by checking the states of the shard of each table/partition.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\ntable_name\n\n\t\n\nThe table name.\n\n\t\n\nTEXT\n\n\n\n\ntable_schema\n\n\t\n\nThe schema of the table.\n\n\t\n\nTEXT\n\n\n\n\npartition_ident\n\n\t\n\nThe ident of the partition. NULL for non-partitioned tables.\n\n\t\n\nTEXT\n\n\n\n\nhealth\n\n\t\n\nThe health label. Can be RED, YELLOW or GREEN.\n\n\t\n\nTEXT\n\n\n\n\nseverity\n\n\t\n\nThe health as a smallint value. Useful when ordering on health.\n\n\t\n\nSMALLINT\n\n\n\n\nmissing_shards\n\n\t\n\nThe number of not assigned or started shards.\n\n\t\n\nINTEGER\n\n\n\n\nunderreplicated_shards\n\n\t\n\nThe number of shards which are not fully replicated.\n\n\t\n\nINTEGER\n\nBoth missing_shards and underreplicated_shards might return -1 if the cluster is in an unhealthy state that prevents the exact number from being calculated. This could be the case when the cluster can’t elect a master, because there are not enough eligible nodes available.\n\ncr> select * from sys.health order by severity desc, table_name;\n+--------+----------------+-----------------+----------+------------+--------------+------------------------+\n| health | missing_shards | partition_ident | severity | table_name | table_schema | underreplicated_shards |\n+--------+----------------+-----------------+----------+------------+--------------+------------------------+\n| GREEN  |              0 |            NULL |        1 | locations  | doc          |                      0 |\n| GREEN  |              0 |            NULL |        1 | quotes     | doc          |                      0 |\n+--------+----------------+-----------------+----------+------------+--------------+------------------------+\nSELECT 2 rows in set (... sec)\n\n\nThe health with the highest severity will always define the health of the query scope.\n\nExample of getting a cluster health (health of all tables):\n\ncr> select health from sys.health order by severity desc limit 1;\n+--------+\n| health |\n+--------+\n| GREEN  |\n+--------+\nSELECT 1 row in set (... sec)\n\nHealth definition\n\nHealth\n\n\t\n\nDescription\n\n\n\n\nRED\n\n\t\n\nAt least one primary shard is missing (primary shard not started or unassigned).\n\n\n\n\nYELLOW\n\n\t\n\nAt least one shard is underreplicated (replica shard not started or unassigned).\n\n\n\n\nGREEN\n\n\t\n\nAll primary and replica shards have been started.\n\nNote\n\nThe sys.health table is subject to Shard table permissions as it will expose a summary of table shard states.\n\nRepositories\n\nThe table sys.repositories lists all configured repositories that can be used to create, manage and restore snapshots (see Snapshots).\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nname\n\n\t\n\nThe repository name\n\n\t\n\nTEXT\n\n\n\n\ntype\n\n\t\n\nThe type of the repository determining how and where the repository stores its snapshots.\n\n\t\n\nTEXT\n\n\n\n\nsettings\n\n\t\n\nThe configuration settings the repository has been created with. The specific settings depend on the repository type, see CREATE REPOSITORY.\n\n\t\n\nOBJECT\n\ncr> SELECT name, type, settings FROM sys.repositories\n... ORDER BY name;\n+---------+------+---------------------------------------------------...--+\n| name    | type | settings                                               |\n+---------+------+---------------------------------------------------...--+\n| my_repo | fs   | {\"compress\": \"true\", \"location\": \"repo_location\", ...} |\n+---------+------+---------------------------------------------------...--+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nSensitive user account information will be masked and thus not visible to the user.\n\nSnapshots\n\nThe table sys.snapshots lists all existing snapshots in all configured repositories (see Snapshots).\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nname\n\n\t\n\nThe name of the snapshot\n\n\t\n\nTEXT\n\n\n\n\nrepository\n\n\t\n\nThe name of the repository that contains this snapshot.\n\n\t\n\nTEXT\n\n\n\n\nconcrete_indices\n\n\t\n\nContains the names of all tables and partitions that are contained in this snapshot how they are represented as ES index names.\n\n\t\n\nARRAY(TEXT)\n\n\n\n\ntables\n\n\t\n\nContains the fully qualified names of all tables within the snapshot.\n\n\t\n\nARRAY(TEXT)\n\n\n\n\ntable_partitions\n\n\t\n\nContains the table schema, table name and partition values of partitioned tables within the snapshot.\n\n\t\n\nARRAY(OBJECT)\n\n\n\n\nstarted\n\n\t\n\nThe point in time when the creation of the snapshot started. Changes made after that are not stored in this snapshot.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nfinished\n\n\t\n\nThe point in time when the snapshot creation finished.\n\n\t\n\nTIMESTAMP WITH TIME ZONE\n\n\n\n\nstate\n\n\t\n\nThe current state of the snapshot. One of: IN_PROGRESS, SUCCESS, PARTIAL, or FAILED.\n\n\t\n\nTEXT\n\n\n\n\nversion\n\n\t\n\nAn internal version this snapshot was created with.\n\n\t\n\nTEXT\n\n\n\n\nfailures\n\n\t\n\nA list of failures that occurred while taking the snapshot. If taking the snapshot was successful this is empty.\n\n\t\n\nARRAY(TEXT)\n\nSnapshot/Restore operates on a per-shard basis. Hence, the state column indicates whether all (SUCCESS), some (PARTIAL), or no shards(FAILED) have been backed up. PARTIAL snapshots are the result of some primaries becoming unavailable while taking the snapshot when there are no replicas at hand (cluster state is RED). If there are replicas of the (now unavailable) primaries (cluster state is YELLOW) the snapshot succeeds and all shards are included (state SUCCESS). Building on a PARTIAL snapshot will include all primaries again.\n\nWarning\n\nIn case of a PARTIAL state another snapshot should be created in order to guarantee a full backup! Only SUCCESS includes all shards.\n\nThe concrete_indices column contains the names of all Elasticsearch indices that were stored in the snapshot. A normal CrateDB table maps to one Elasticsearch index, a partitioned table maps to one Elasticsearch index per partition. The mapping follows the following pattern:\n\nCrateDB table / partition name\n\n\t\n\nconcrete_indices entry\n\n\n\n\ndoc.my_table\n\n\t\n\nmy_table\n\n\n\n\nmy_schema.my_table\n\n\t\n\nmy_schema.my_table\n\n\n\n\ndoc.parted_table (value=null)\n\n\t\n\n.partitioned.my_table.0400\n\n\n\n\nmy_schema.parted_table (value=null)\n\n\t\n\nmy_schema..partitioned.my_table.0400\n\ncr> SELECT \"repository\", name, state, concrete_indices\n... FROM sys.snapshots order by \"repository\", name;\n+------------+-------------+---------+-----------------...-+\n| repository | name        | state   | concrete_indices    |\n+------------+-------------+---------+-----------------...-+\n| my_repo    | my_snapshot | SUCCESS | [...]               |\n+------------+-------------+---------+-----------------...-+\nSELECT 1 row in set (... sec)\n\nSnapshot Restore\n\nThe sys.snapshot_restore table contains information about the current state of snapshot restore operations.\n\npg_stats schema\n\nName\n\n\t\n\nDescription\n\n\t\n\nType\n\n\n\n\nid\n\n\t\n\nThe UUID of the restore snapshot operation.\n\n\t\n\nTEXT\n\n\n\n\nrepository\n\n\t\n\nThe name of the repository that contains the snapshot.\n\n\t\n\nTEXT\n\n\n\n\nsnapshot\n\n\t\n\nThe name of the snapshot.\n\n\t\n\nTEXT\n\n\n\n\nstate\n\n\t\n\nThe current state of the snapshot restore operations. Possible states are: INIT, STARTED, SUCCESS, and FAILURE.\n\n\t\n\nTEXT\n\n\n\n\nshards['table_schema']\n\n\t\n\nThe schema name of the table of the shard.\n\n\t\n\nTEXT\n\n\n\n\nshards['table_name']\n\n\t\n\nThe table name of the shard.\n\n\t\n\nTEXT\n\n\n\n\nshards['partition_ident']\n\n\t\n\nThe identifier of the partition of the shard. NULL if the is not partitioned.\n\n\t\n\nTEXT\n\n\n\n\nshards['shard_id']\n\n\t\n\nThe ID of the shard.\n\n\t\n\nINTEGER\n\n\n\n\nshards['state']\n\n\t\n\nThe restore state of the shard. Possible states are: INIT, STARTED, SUCCESS, and FAILURE.\n\n\t\n\nTEXT\n\nTo get more information about the restoring snapshots and shards one can join the sys.snapshot_restore with sys.shards or sys.snapshots table.\n\nSummits\n\nThe sys.summits table contains the information about the mountains in the Alps higher than 2000m. The mountain names from the table are also used to generate random nodes names.\n\nUsers\n\nThe sys.users table contains all existing database users in the cluster.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nname\n\n\t\n\nThe name of the database user.\n\n\t\n\nTEXT\n\n\n\n\nsuperuser\n\n\t\n\nFlag to indicate whether the user is a superuser.\n\n\t\n\nBOOLEAN\n\n\n\n\npassword\n\n\t\n\n******** if there is a password set or NULL if there is not.\n\n\t\n\nTEXT\n\n\n\n\ngranted_roles\n\n\t\n\nA list of parent roles granted to the user\n\n\t\n\nARRAY\n\n\n\n\ngranted_roles[role]\n\n\t\n\nThe name of the role granted to the user\n\n\t\n\nTEXT\n\n\n\n\ngranted_roles[grantor]\n\n\t\n\nThe name of user who granted the role to the user\n\n\t\n\nTEXT\n\nRoles\n\nThe sys.roles table contains all existing database roles in the cluster.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nname\n\n\t\n\nThe name of the database user.\n\n\t\n\nTEXT\n\n\n\n\ngranted_roles\n\n\t\n\nA list of parent roles granted to the user\n\n\t\n\nARRAY\n\n\n\n\ngranted_roles[role]\n\n\t\n\nThe name of the role granted to the user\n\n\t\n\nTEXT\n\n\n\n\ngranted_roles[grantor]\n\n\t\n\nThe name of user who granted the role to the user\n\n\t\n\nTEXT\n\nPrivileges\n\nThe sys.privileges table contains all privileges for each user and role of the database.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nclass\n\n\t\n\nThe class on which the privilege applies\n\n\t\n\nTEXT\n\n\n\n\ngrantee\n\n\t\n\nThe name of the database user or role for which the privilege is granted or denied\n\n\t\n\nTEXT\n\n\n\n\ngrantor\n\n\t\n\nThe name of the database user who granted or denied the privilege\n\n\t\n\nTEXT\n\n\n\n\nident\n\n\t\n\nThe name of the database object on which the privilege applies\n\n\t\n\nTEXT\n\n\n\n\nstate\n\n\t\n\nEither GRANT or DENY, which indicates if the user or role has been granted or denied access to the specific database object\n\n\t\n\nARRAY\n\n\n\n\ntype\n\n\t\n\nThe type of access for the specific database object\n\n\t\n\nTEXT\n\nAllocations\n\nThe sys.allocations table contains information about shards and their allocation state. The table contains:\n\nshards that are unassigned and why they are unassigned\n\nshards that are assigned but cannot be moved or rebalanced and why they remain on their current node\n\nIt can help to identify problems if shard allocations behave different than expected, e.g. when a shard stays unassigned or a shard does not move off a node.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\ntable_schema\n\n\t\n\nSchema name of the table of the shard.\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nTable name of the shard.\n\n\t\n\nTEXT\n\n\n\n\npartition_ident\n\n\t\n\nIdentifier of the partition of the shard. NULL if the table is not partitioned.\n\n\t\n\nTEXT\n\n\n\n\nshard_id\n\n\t\n\nID of the effected shard.\n\n\t\n\nINTEGER\n\n\n\n\nnode_id\n\n\t\n\nID of the node on which the shard resides. NULL if the shard is unassigned.\n\n\t\n\nTEXT\n\n\n\n\nprimary\n\n\t\n\nWhether the shard is a primary shard.\n\n\t\n\nBOOLEAN\n\n\n\n\ncurrent_state\n\n\t\n\nCurrent state of the shard. Possible states are: UNASSIGNED, INITIALIZING, STARTED, RELOCATING\n\n\t\n\nTEXT\n\n\n\n\nexplanation\n\n\t\n\nExplanation why the shard cannot be allocated, moved or rebalanced.\n\n\t\n\nTEXT\n\n\n\n\ndecisions\n\n\t\n\nA list of decisions that describe in detail why the shard in the current state.\n\n\t\n\nARRAY\n\n\n\n\ndecisions['node_id']\n\n\t\n\nID of the node of the decision.\n\n\t\n\nTEXT\n\n\n\n\ndecisions['node_name']\n\n\t\n\nName of the node of the decision.\n\n\t\n\nTEXT\n\n\n\n\ndecisions['explanations']\n\n\t\n\nDetailed list of human readable explanations why the node decided whether to allocate or rebalance the shard. Returns NULL if there is no need to rebalance the shard.\n\n\t\n\nARRAY\n\nNote\n\nThe sys.allocations table is subject to Shard table permissions.\n\nShard table permissions\n\nAccessing tables that return shards (sys.shards, sys.allocations) is subjected to the same privileges constraints as the other tables. Namely, in order to query them, the connected user needs to have the DQL privilege on that particular table, either directly or inherited from the SCHEMA or CLUSTER (for more information on privileges inheritance see Hierarchical Inheritance of Privileges).\n\nHowever, being able to query shard returning system tables will not allow the user to retrieve all the rows in the table, as they may contain information related to tables, which the connected user does not have any privileges for. The only rows that will be returned will be the ones the user is allowed to access.\n\nFor example, if the user john has any privilege on the doc.books table but no privilege at all on doc.locations, when john issues a SELECT * FROM sys.shards statement, the shards information related to the doc.locations table will not be returned.\n\nsys jobs tables permissions\n\nAccessing sys.jobs and sys.jobs_log tables is subjected to the same privileges constraints as other tables. To query them, the current user needs to have the DQL privilege on that particular table, either directly or inherited from the SCHEMA or CLUSTER.\n\nA user that doesn’t have superuser privileges is allowed to retrieve only their own job logs entries, while a user with superuser privileges has access to all.\n\npg_stats\n\nThe pg_stats table in the pg_catalog system schema contains statistical data about the contents of the CrateDB cluster.\n\nEntries are periodically created or updated in the interval configured with the stats.service.interval setting.\n\nAlternatively the statistics can also be updated using the ANALYZE command.\n\nThe table contains 1 entry per column for each table in the cluster which has been analyzed.\n\npg_stats schema\n\nName\n\n\t\n\nType\n\n\t\n\nDescription\n\n\n\n\nschemaname\n\n\t\n\ntext\n\n\t\n\nName of the schema containing the table.\n\n\n\n\ntablename\n\n\t\n\ntext\n\n\t\n\nName of the table.\n\n\n\n\nattname\n\n\t\n\ntext\n\n\t\n\nName of the column.\n\n\n\n\ninherited\n\n\t\n\nbool\n\n\t\n\nAlways false in CrateDB; For compatibility with PostgreSQL.\n\n\n\n\nnull_frac\n\n\t\n\nreal\n\n\t\n\nFraction of column entries that are null.\n\n\n\n\navg_width\n\n\t\n\ninteger\n\n\t\n\nAverage size in bytes of column’s entries.\n\n\n\n\nn_distinct\n\n\t\n\nreal\n\n\t\n\nAn approximation of the number of distinct values in a column.\n\n\n\n\nmost_common_vals\n\n\t\n\nstring[]\n\n\t\n\nA list of the most common values in the column. null if no values seem. more common than others.\n\n\n\n\nmost_common_freqs\n\n\t\n\nreal[]\n\n\t\n\nA list of the frequencies of the most common values. The size of the array always matches most_common_vals. If most_common_vals is null this is null as well.\n\n\n\n\nhistogram_bounds\n\n\t\n\nstring[]\n\n\t\n\nA list of values that divide the column’s values into groups of approximately equal population. The values in most_common_vals, if present, are omitted from this histogram calculation.\n\n\n\n\ncorrelation\n\n\t\n\nreal\n\n\t\n\nAlways 0.0. This column exists for PostgreSQL compatibility.\n\n\n\n\nmost_common_elems\n\n\t\n\nstring[]\n\n\t\n\nAlways null. Exists for PostgreSQL compatibility.\n\n\n\n\nmost_common_elem_freqs\n\n\t\n\nreal[]\n\n\t\n\nAlways null. Exists for PostgreSQL compatibility.\n\n\n\n\nelem_count_histogram\n\n\t\n\nreal[]\n\n\t\n\nAlways null. Exists for PostgreSQL compatibility.\n\nNote\n\nNot all data types support creating statistics. So some columns may not show up in the table.\n\npg_publication\n\nThe pg_publication table in the pg_catalog system schema contains all publications created in the cluster.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\noid\n\n\t\n\nRow identifier.\n\n\t\n\nINTEGER\n\n\n\n\npubname\n\n\t\n\nName of the publication.\n\n\t\n\nTEXT\n\n\n\n\npubowner\n\n\t\n\noid of the owner of the publication.\n\n\t\n\nINTEGER\n\n\n\n\npuballtables\n\n\t\n\nWhether this publication includes all tables in the cluster, including tables created in the future.\n\n\t\n\nBOOLEAN\n\n\n\n\npubinsert\n\n\t\n\nWhether INSERT operations are replicated for tables in the publication. Always true.\n\n\t\n\nBOOLEAN\n\n\n\n\npubupdate\n\n\t\n\nWhether UPDATE operations are replicated for tables in the publication. Always true.\n\n\t\n\nBOOLEAN\n\n\n\n\npubdelete\n\n\t\n\nWhether DELETE operations are replicated for tables in the publication. Always true.\n\n\t\n\nBOOLEAN\n\npg_publication_tables\n\nThe pg_publication_tables table in the pg_catalog system schema contains tables replicated by a publication.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\npubname\n\n\t\n\nName of the publication.\n\n\t\n\nTEXT\n\n\n\n\nschemaname\n\n\t\n\nName of the schema containing table.\n\n\t\n\nTEXT\n\n\n\n\ntablename\n\n\t\n\nName of the table.\n\n\t\n\nTEXT\n\npg_subscription\n\nThe pg_subscription table in the pg_catalog system schema contains all subscriptions created in the cluster.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\noid\n\n\t\n\nRow identifier.\n\n\t\n\nINTEGER\n\n\n\n\nsubdbid\n\n\t\n\nnoop value, always 0.\n\n\t\n\nINTEGER\n\n\n\n\nsubname\n\n\t\n\nName of the subscription.\n\n\t\n\nTEXT\n\n\n\n\nsubowner\n\n\t\n\noid of the owner of the subscription.\n\n\t\n\nINTEGER\n\n\n\n\nsubenabled\n\n\t\n\nWhether the subscription is enabled, always true.\n\n\t\n\nBOOLEAN\n\n\n\n\nsubbinary\n\n\t\n\nNoop value, always true.\n\n\t\n\nBOOLEAN\n\n\n\n\nsubstream\n\n\t\n\nNoop value, always true.\n\n\t\n\nBOOLEAN\n\n\n\n\nsubconninfo\n\n\t\n\nConnection string to the publishing cluster.\n\n\t\n\nTEXT\n\n\n\n\nsubslotname\n\n\t\n\nNoop value, always NULL.\n\n\t\n\nTEXT\n\n\n\n\nsubsynccommit\n\n\t\n\nNoop value, always NULL.\n\n\t\n\nTEXT\n\n\n\n\nsubpublications\n\n\t\n\nArray of subscribed publication names. These publications are defined in the publishing cluster.\n\n\t\n\nARRAY\n\npg_subscription_rel\n\nThe pg_subscription_rel table in the pg_catalog system schema contains the state for each replicated relation in each subscription.\n\nColumn Name\n\n\t\n\nDescription\n\n\t\n\nReturn Type\n\n\n\n\nsrsubid\n\n\t\n\nReference to subscription.\n\n\t\n\nINTEGER\n\n\n\n\nsrrelid\n\n\t\n\nReference to relation.\n\n\t\n\nREGCLASS\n\n\n\n\nsrsubstate\n\n\t\n\nReplication state of the relation. State code: i - initializing; d - restoring; r - monitoring, i.e. waiting for new changes; e - error.\n\n\t\n\nTEXT\n\n\n\n\nsrsubstate_reason\n\n\t\n\nError message if there was a replication error for the relation or NULL.\n\n\t\n\nTEXT\n\n\n\n\nsrsublsn\n\n\t\n\nNoop value, always NULL.\n\n\t\n\nLONG"
  },
  {
    "title": "Information schema — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/information-schema.html",
    "html": "5.6\nInformation schema\n\ninformation_schema is a special schema that contains virtual tables which are read-only and can be queried to get information about the state of the cluster.\n\nTable of contents\n\nAccess\n\nVirtual tables\n\ntables\n\nsettings\n\nviews\n\ncolumns\n\ntable_constraints\n\nkey_column_usage\n\ntable_partitions\n\nroutines\n\nschemata\n\nsql_features\n\ncharacter_sets\n\nAccess\n\nWhen the user management is enabled, accessing the information_schema is open to all users and it does not require any privileges.\n\nHowever, being able to query information_schema tables will not allow the user to retrieve all the rows in the table, as it can contain information related to tables over which the connected user does not have any privileges. The only rows that will be returned will be the ones the user is allowed to access.\n\nFor example, if the user john has any privilege on the doc.books table but no privilege at all on doc.locations, when john issues a SELECT * FROM information_schema.tables statement, the tables information related to the doc.locations table will not be returned.\n\nVirtual tables\ntables\n\nThe information_schema.tables virtual table can be queried to get a list of all available tables and views and their settings, such as number of shards or number of replicas.\n\ncr> SELECT table_schema, table_name, table_type, number_of_shards, number_of_replicas\n... FROM information_schema.tables\n... ORDER BY table_schema ASC, table_name ASC;\n+--------------------+-------------------------+------------+------------------+--------------------+\n| table_schema       | table_name              | table_type | number_of_shards | number_of_replicas |\n+--------------------+-------------------------+------------+------------------+--------------------+\n| doc                | galaxies                | VIEW       |             NULL | NULL               |\n| doc                | locations               | BASE TABLE |                2 | 0                  |\n| doc                | partitioned_table       | BASE TABLE |                4 | 0-1                |\n| doc                | quotes                  | BASE TABLE |                2 | 0                  |\n| information_schema | character_sets          | BASE TABLE |             NULL | NULL               |\n| information_schema | columns                 | BASE TABLE |             NULL | NULL               |\n| information_schema | key_column_usage        | BASE TABLE |             NULL | NULL               |\n| information_schema | referential_constraints | BASE TABLE |             NULL | NULL               |\n| information_schema | routines                | BASE TABLE |             NULL | NULL               |\n| information_schema | schemata                | BASE TABLE |             NULL | NULL               |\n| information_schema | sql_features            | BASE TABLE |             NULL | NULL               |\n| information_schema | table_constraints       | BASE TABLE |             NULL | NULL               |\n| information_schema | table_partitions        | BASE TABLE |             NULL | NULL               |\n| information_schema | tables                  | BASE TABLE |             NULL | NULL               |\n| information_schema | views                   | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_am                   | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_attrdef              | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_attribute            | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_class                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_constraint           | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_cursors              | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_database             | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_depend               | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_description          | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_enum                 | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_event_trigger        | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_index                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_indexes              | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_locks                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_namespace            | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_proc                 | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_publication          | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_publication_tables   | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_range                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_roles                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_settings             | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_shdescription        | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_stats                | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_subscription         | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_subscription_rel     | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_tables               | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_tablespace           | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_type                 | BASE TABLE |             NULL | NULL               |\n| pg_catalog         | pg_views                | BASE TABLE |             NULL | NULL               |\n| sys                | allocations             | BASE TABLE |             NULL | NULL               |\n| sys                | checks                  | BASE TABLE |             NULL | NULL               |\n| sys                | cluster                 | BASE TABLE |             NULL | NULL               |\n| sys                | health                  | BASE TABLE |             NULL | NULL               |\n| sys                | jobs                    | BASE TABLE |             NULL | NULL               |\n| sys                | jobs_log                | BASE TABLE |             NULL | NULL               |\n| sys                | jobs_metrics            | BASE TABLE |             NULL | NULL               |\n| sys                | node_checks             | BASE TABLE |             NULL | NULL               |\n| sys                | nodes                   | BASE TABLE |             NULL | NULL               |\n| sys                | operations              | BASE TABLE |             NULL | NULL               |\n| sys                | operations_log          | BASE TABLE |             NULL | NULL               |\n| sys                | privileges              | BASE TABLE |             NULL | NULL               |\n| sys                | repositories            | BASE TABLE |             NULL | NULL               |\n| sys                | roles                   | BASE TABLE |             NULL | NULL               |\n| sys                | segments                | BASE TABLE |             NULL | NULL               |\n| sys                | shards                  | BASE TABLE |             NULL | NULL               |\n| sys                | snapshot_restore        | BASE TABLE |             NULL | NULL               |\n| sys                | snapshots               | BASE TABLE |             NULL | NULL               |\n| sys                | summits                 | BASE TABLE |             NULL | NULL               |\n| sys                | users                   | BASE TABLE |             NULL | NULL               |\n+--------------------+-------------------------+------------+------------------+--------------------+\nSELECT 64 rows in set (... sec)\n\n\nThe table also contains additional information such as the specified routing column and partition columns:\n\ncr> SELECT table_name, clustered_by, partitioned_by\n... FROM information_schema.tables\n... WHERE table_schema = 'doc'\n... ORDER BY table_schema ASC, table_name ASC;\n+-------------------+--------------+----------------+\n| table_name        | clustered_by | partitioned_by |\n+-------------------+--------------+----------------+\n| galaxies          | NULL         | NULL           |\n| locations         | id           | NULL           |\n| partitioned_table | _id          | [\"date\"]       |\n| quotes            | id           | NULL           |\n+-------------------+--------------+----------------+\nSELECT 4 rows in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nDescription\n\n\t\n\nData Type\n\n\n\n\nblobs_path\n\n\t\n\nThe data path of the blob table\n\n\t\n\nTEXT\n\n\n\n\nclosed\n\n\t\n\nThe state of the table\n\n\t\n\nBOOLEAN\n\n\n\n\nclustered_by\n\n\t\n\nThe routing column used to cluster the table\n\n\t\n\nTEXT\n\n\n\n\ncolumn_policy\n\n\t\n\nDefines whether the table uses a STRICT or a DYNAMIC Column policy\n\n\t\n\nTEXT\n\n\n\n\nnumber_of_replicas\n\n\t\n\nThe number of replicas the table currently has\n\n\t\n\nINTEGER\n\n\n\n\nnumber_of_shards\n\n\t\n\nThe number of shards the table is currently distributed across\n\n\t\n\nINTEGER\n\n\n\n\npartitioned_by\n\n\t\n\nThe partition columns (used to partition the table)\n\n\t\n\nTEXT\n\n\n\n\nreference_generation\n\n\t\n\nSpecifies how values in the self-referencing column are generated\n\n\t\n\nTEXT\n\n\n\n\nrouting_hash_function\n\n\t\n\nThe name of the hash function used for internal routing\n\n\t\n\nTEXT\n\n\n\n\nself_referencing_column_name\n\n\t\n\nThe name of the column that uniquely identifies each row (always _id)\n\n\t\n\nTEXT\n\n\n\n\nsettings\n\n\t\n\nWITH\n\n\t\n\nOBJECT\n\n\n\n\ntable_catalog\n\n\t\n\nRefers to the table_schema\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nThe name of the table\n\n\t\n\nTEXT\n\n\n\n\ntable_schema\n\n\t\n\nThe name of the schema the table belongs to\n\n\t\n\nTEXT\n\n\n\n\ntable_type\n\n\t\n\nThe type of the table (BASE TABLE for tables, VIEW for views)\n\n\t\n\nTEXT\n\n\n\n\nversion\n\n\t\n\nA collection of version numbers relevant to the table\n\n\t\n\nOBJECT\n\nsettings\n\nTable settings specify configuration parameters for tables. Some settings can be set during Cluster runtime and others are only applied on cluster restart.\n\nThis list of table settings in WITH shows detailed information of each parameter.\n\nTable parameters can be applied with CREATE TABLE on creation of a table. With ALTER TABLE they can be set on already existing tables.\n\nThe following statement creates a new table and sets the refresh interval of shards to 500 ms and sets the shard allocation for primary shards only:\n\ncr> create table parameterized_table (id integer, content text)\n... with (\"refresh_interval\"=500, \"routing.allocation.enable\"='primaries');\nCREATE OK, 1 row affected (... sec)\n\n\nThe settings can be verified by querying information_schema.tables:\n\ncr> select settings['routing']['allocation']['enable'] as alloc_enable,\n...   settings['refresh_interval'] as refresh_interval\n... from information_schema.tables\n... where table_name='parameterized_table';\n+--------------+------------------+\n| alloc_enable | refresh_interval |\n+--------------+------------------+\n| primaries    |              500 |\n+--------------+------------------+\nSELECT 1 row in set (... sec)\n\n\nOn existing tables this needs to be done with ALTER TABLE statement:\n\ncr> alter table parameterized_table\n... set (\"routing.allocation.enable\"='none');\nALTER OK, -1 rows affected (... sec)\n\nviews\n\nThe table information_schema.views contains the name, definition and options of all available views.\n\ncr> SELECT table_schema, table_name, view_definition\n... FROM information_schema.views\n... ORDER BY table_schema ASC, table_name ASC;\n+--------------+------------+-------------------------+\n| table_schema | table_name | view_definition         |\n+--------------+------------+-------------------------+\n| doc          | galaxies   | SELECT                  |\n|              |            |   \"id\"                  |\n|              |            | , \"name\"                |\n|              |            | , \"description\"         |\n|              |            | FROM \"locations\"        |\n|              |            | WHERE \"kind\" = 'Galaxy' |\n+--------------+------------+-------------------------+\nSELECT 1 row in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nDescription\n\n\t\n\nData Type\n\n\n\n\ntable_catalog\n\n\t\n\nThe catalog of the table of the view (refers to table_schema)\n\n\t\n\nTEXT\n\n\n\n\ntable_schema\n\n\t\n\nThe schema of the table of the view\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nThe name of the table of the view\n\n\t\n\nTEXT\n\n\n\n\nview_definition\n\n\t\n\nThe SELECT statement that defines the view\n\n\t\n\nTEXT\n\n\n\n\ncheck_option\n\n\t\n\nNot applicable for CrateDB, always return NONE\n\n\t\n\nTEXT\n\n\n\n\nis_updatable\n\n\t\n\nWhether the view is updatable. Not applicable for CrateDB, always returns FALSE\n\n\t\n\nBOOLEAN\n\n\n\n\nowner\n\n\t\n\nThe user that created the view\n\n\t\n\nTEXT\n\nNote\n\nIf you drop the table of a view, the view will still exist and show up in the information_schema.tables and information_schema.views tables.\n\ncolumns\n\nThis table can be queried to get a list of all available columns of all tables and views and their definition like data type and ordinal position inside the table:\n\ncr> select table_name, column_name, ordinal_position as pos, data_type\n... from information_schema.columns\n... where table_schema = 'doc' and table_name not like 'my_table%'\n... order by table_name asc, column_name asc;\n+-------------------+--------------------------------+-----+--------------------------+\n| table_name        | column_name                    | pos | data_type                |\n+-------------------+--------------------------------+-----+--------------------------+\n| locations         | date                           |   3 | timestamp with time zone |\n| locations         | description                    |   6 | text                     |\n| locations         | id                             |   1 | integer                  |\n| locations         | information                    |  11 | object_array             |\n| locations         | information['evolution_level'] |  13 | smallint                 |\n| locations         | information['population']      |  12 | bigint                   |\n| locations         | inhabitants                    |   7 | object                   |\n| locations         | inhabitants['description']     |   9 | text                     |\n| locations         | inhabitants['interests']       |   8 | text_array               |\n| locations         | inhabitants['name']            |  10 | text                     |\n| locations         | kind                           |   4 | text                     |\n| locations         | landmarks                      |  14 | text_array               |\n| locations         | name                           |   2 | text                     |\n| locations         | position                       |   5 | integer                  |\n| partitioned_table | date                           |   3 | timestamp with time zone |\n| partitioned_table | id                             |   1 | bigint                   |\n| partitioned_table | title                          |   2 | text                     |\n| quotes            | id                             |   1 | integer                  |\n| quotes            | quote                          |   2 | text                     |\n+-------------------+--------------------------------+-----+--------------------------+\nSELECT 19 rows in set (... sec)\n\n\nYou can even query this table’s own columns (attention: this might lead to infinite recursion of your mind, beware!):\n\ncr> select column_name, data_type, ordinal_position\n... from information_schema.columns\n... where table_schema = 'information_schema'\n... and table_name = 'columns' order by column_name asc;\n+--------------------------+------------+------------------+\n| column_name              | data_type  | ordinal_position |\n+--------------------------+------------+------------------+\n| character_maximum_length | integer    |                1 |\n| character_octet_length   | integer    |                2 |\n| character_set_catalog    | text       |                3 |\n| character_set_name       | text       |                4 |\n| character_set_schema     | text       |                5 |\n| check_action             | integer    |                6 |\n| check_references         | text       |                7 |\n| collation_catalog        | text       |                8 |\n| collation_name           | text       |                9 |\n| collation_schema         | text       |               10 |\n| column_default           | text       |               11 |\n| column_details           | object     |               12 |\n| column_details['name']   | text       |               13 |\n| column_details['path']   | text_array |               14 |\n| column_name              | text       |               15 |\n| data_type                | text       |               16 |\n| datetime_precision       | integer    |               17 |\n| domain_catalog           | text       |               18 |\n| domain_name              | text       |               19 |\n| domain_schema            | text       |               20 |\n| generation_expression    | text       |               21 |\n| identity_cycle           | boolean    |               22 |\n| identity_generation      | text       |               23 |\n| identity_increment       | text       |               24 |\n| identity_maximum         | text       |               25 |\n| identity_minimum         | text       |               26 |\n| identity_start           | text       |               27 |\n| interval_precision       | integer    |               28 |\n| interval_type            | text       |               29 |\n| is_generated             | text       |               30 |\n| is_identity              | boolean    |               31 |\n| is_nullable              | boolean    |               32 |\n| numeric_precision        | integer    |               33 |\n| numeric_precision_radix  | integer    |               34 |\n| numeric_scale            | integer    |               35 |\n| ordinal_position         | integer    |               36 |\n| table_catalog            | text       |               37 |\n| table_name               | text       |               38 |\n| table_schema             | text       |               39 |\n| udt_catalog              | text       |               40 |\n| udt_name                 | text       |               41 |\n| udt_schema               | text       |               42 |\n+--------------------------+------------+------------------+\nSELECT 42 rows in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nDescription\n\n\t\n\nData Type\n\n\n\n\ntable_catalog\n\n\t\n\nRefers to the table_schema\n\n\t\n\nTEXT\n\n\n\n\ntable_schema\n\n\t\n\nSchema name containing the table\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nTable Name\n\n\t\n\nTEXT\n\n\n\n\ncolumn_name\n\n\t\n\nColumn Name For fields in object columns this is not an identifier but a path and therefore must not be double quoted when programmatically obtained.\n\n\t\n\nTEXT\n\n\n\n\nordinal_position\n\n\t\n\nThe position of the column within the table\n\n\t\n\nINTEGER\n\n\n\n\nis_nullable\n\n\t\n\nWhether the column is nullable\n\n\t\n\nBOOLEAN\n\n\n\n\ndata_type\n\n\t\n\nThe data type of the column\n\nFor further information see Data types\n\n\t\n\nTEXT\n\n\n\n\ncolumn_default\n\n\t\n\nThe default expression of the column\n\n\t\n\nTEXT\n\n\n\n\ncharacter_maximum_length\n\n\t\n\nIf the data type is a character type then return the declared length limit; otherwise NULL.\n\n\t\n\nINTEGER\n\n\n\n\ncharacter_octet_length\n\n\t\n\nNot implemented (always returns NULL)\n\nPlease refer to TEXT type\n\n\t\n\nINTEGER\n\n\n\n\nnumeric_precision\n\n\t\n\nIndicates the number of significant digits for a numeric data_type. For all other data types this column is NULL.\n\n\t\n\nINTEGER\n\n\n\n\nnumeric_precision_radix\n\n\t\n\nIndicates in which base the value in the column numeric_precision for a numeric data_type is exposed. This can either be 2 (binary) or 10 (decimal). For all other data types this column is NULL.\n\n\t\n\nINTEGER\n\n\n\n\nnumeric_scale\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nINTEGER\n\n\n\n\ndatetime_precision\n\n\t\n\nContains the fractional seconds precision for a timestamp data_type. For all other data types this column is null.\n\n\t\n\nINTEGER\n\n\n\n\ninterval_type\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ninterval_precision\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nINTEGER\n\n\n\n\ncharacter_set_catalog\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncharacter_set_schema\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncharacter_set_name\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncollation_catalog\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncollation_schema\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncollation_name\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ndomain_catalog\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ndomain_schema\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ndomain_name\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nudt_catalog\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nudt_schema\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nudt_name\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncheck_references\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\ncheck_action\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nINTEGER\n\n\n\n\ngeneration_expression\n\n\t\n\nThe expression used to generate ad column. If the column is not generated NULL is returned.\n\n\t\n\nTEXT\n\n\n\n\nis_generated\n\n\t\n\nReturns ALWAYS or NEVER wether the column is generated or not.\n\n\t\n\nTEXT\n\n\n\n\nis_identity\n\n\t\n\nNot implemented (always returns false)\n\n\t\n\nBOOLEAN\n\n\n\n\nidentity_cycle\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nBOOLEAN\n\n\n\n\nidentity_generation\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nidentity_increment\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nidentity_maximum\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nidentity_minimum\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\n\n\n\nidentity_start\n\n\t\n\nNot implemented (always returns NULL)\n\n\t\n\nTEXT\n\ntable_constraints\n\nThis table can be queried to get a list of all defined table constraints, their type, name and which table they are defined in.\n\nNote\n\nCurrently only PRIMARY_KEY constraints are supported.\n\ncr> select table_schema, table_name, constraint_name, constraint_type as type\n... from information_schema.table_constraints\n... where table_name = 'tables'\n...   or table_name = 'quotes'\n...   or table_name = 'documents'\n...   or table_name = 'tbl'\n... order by table_schema desc, table_name asc limit 10;\n+--------------------+------------+------------------------+-------------+\n| table_schema       | table_name | constraint_name        | type        |\n+--------------------+------------+------------------------+-------------+\n| information_schema | tables     | tables_pk              | PRIMARY KEY |\n| doc                | quotes     | quotes_pk              | PRIMARY KEY |\n| doc                | quotes     | doc_quotes_id_not_null | CHECK       |\n| doc                | tbl        | doc_tbl_col_not_null   | CHECK       |\n+--------------------+------------+------------------------+-------------+\nSELECT 4 rows in set (... sec)\n\nkey_column_usage\n\nThis table may be queried to retrieve primary key information from all user tables:\n\ncr> select constraint_name, table_name, column_name, ordinal_position\n... from information_schema.key_column_usage\n... where table_name = 'students'\n+-----------------+------------+-------------+------------------+\n| constraint_name | table_name | column_name | ordinal_position |\n+-----------------+------------+-------------+------------------+\n| students_pk     | students   | id          |                1 |\n| students_pk     | students   | department  |                2 |\n+-----------------+------------+-------------+------------------+\nSELECT 2 rows in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nDescription\n\n\t\n\nData Type\n\n\n\n\nconstraint_catalog\n\n\t\n\nRefers to table_catalog\n\n\t\n\nTEXT\n\n\n\n\nconstraint_schema\n\n\t\n\nRefers to table_schema\n\n\t\n\nTEXT\n\n\n\n\nconstraint_name\n\n\t\n\nName of the constraint\n\n\t\n\nTEXT\n\n\n\n\ntable_catalog\n\n\t\n\nRefers to table_schema\n\n\t\n\nTEXT\n\n\n\n\ntable_schema\n\n\t\n\nName of the schema that contains the table that contains the constraint\n\n\t\n\nTEXT\n\n\n\n\ntable_name\n\n\t\n\nName of the table that contains the constraint\n\n\t\n\nTEXT\n\n\n\n\ncolumn_name\n\n\t\n\nName of the column that contains the constraint\n\n\t\n\nTEXT\n\n\n\n\nordinal_position\n\n\t\n\nPosition of the column within the constraint (starts with 1)\n\n\t\n\nINTEGER\n\ntable_partitions\n\nThis table can be queried to get information about all partitioned tables, Each partition of a table is represented as one row. The row contains the information table name, schema name, partition ident, and the values of the partition. values is a key-value object with the partition column (or columns) as key(s) and the corresponding value as value(s).\n\ncr> insert into a_partitioned_table (id, content) values (1, 'content_a');\nINSERT OK, 1 row affected (... sec)\n\ncr> alter table a_partitioned_table set (number_of_shards=5);\nALTER OK, -1 rows affected (... sec)\n\ncr> insert into a_partitioned_table (id, content) values (2, 'content_b');\nINSERT OK, 1 row affected (... sec)\n\n\nThe following example shows a table where the column content of table a_partitioned_table has been used to partition the table. The table has two partitions. The partitions are introduced when data is inserted where content is content_a, and content_b.:\n\ncr> select table_name, table_schema as schema, partition_ident, \"values\"\n... from information_schema.table_partitions\n... order by table_name, partition_ident;\n+---------------------+--------+--------------------+--------------------------+\n| table_name          | schema | partition_ident    | values                   |\n+---------------------+--------+--------------------+--------------------------+\n| a_partitioned_table | doc    | 04566rreehimst2vc4 | {\"content\": \"content_a\"} |\n| a_partitioned_table | doc    | 04566rreehimst2vc8 | {\"content\": \"content_b\"} |\n+---------------------+--------+--------------------+--------------------------+\nSELECT 2 rows in set (... sec)\n\n\nThe second partition has been created after the number of shards for future partitions have been changed on the partitioned table, so they show 5 instead of 4:\n\ncr> select table_name, partition_ident,\n... number_of_shards, number_of_replicas\n... from information_schema.table_partitions\n... order by table_name, partition_ident;\n+---------------------+--------------------+------------------+--------------------+\n| table_name          | partition_ident    | number_of_shards | number_of_replicas |\n+---------------------+--------------------+------------------+--------------------+\n| a_partitioned_table | 04566rreehimst2vc4 |                4 | 0-1                |\n| a_partitioned_table | 04566rreehimst2vc8 |                5 | 0-1                |\n+---------------------+--------------------+------------------+--------------------+\nSELECT 2 rows in set (... sec)\n\nroutines\n\nThe routines table contains tokenizers, token-filters, char-filters, custom analyzers created by CREATE ANALYZER statements (see Creating a custom analyzer), and functions created by CREATE FUNCTION statements:\n\ncr> select routine_name, routine_type\n... from information_schema.routines\n... group by routine_name, routine_type\n... order by routine_name asc limit 5;\n+----------------------+--------------+\n| routine_name         | routine_type |\n+----------------------+--------------+\n| PathHierarchy        | TOKENIZER    |\n| apostrophe           | TOKEN_FILTER |\n| arabic               | ANALYZER     |\n| arabic_normalization | TOKEN_FILTER |\n| arabic_stem          | TOKEN_FILTER |\n+----------------------+--------------+\nSELECT 5 rows in set (... sec)\n\n\nFor example you can use this table to list existing tokenizers like this:\n\ncr> select routine_name\n... from information_schema.routines\n... where routine_type='TOKENIZER'\n... order by routine_name asc limit 10;\n+----------------+\n| routine_name   |\n+----------------+\n| PathHierarchy  |\n| char_group     |\n| classic        |\n| edge_ngram     |\n| keyword        |\n| letter         |\n| lowercase      |\n| ngram          |\n| path_hierarchy |\n| pattern        |\n+----------------+\nSELECT 10 rows in set (... sec)\n\n\nOr get an overview of how many routines and routine types are available:\n\ncr> select count(*), routine_type\n... from information_schema.routines\n... group by routine_type\n... order by routine_type;\n+----------+--------------+\n| count(*) | routine_type |\n+----------+--------------+\n|       45 | ANALYZER     |\n|        3 | CHAR_FILTER  |\n|       16 | TOKENIZER    |\n|       61 | TOKEN_FILTER |\n+----------+--------------+\nSELECT 4 rows in set (... sec)\n\n\nSchema\n\nName\n\n\t\n\nData Type\n\n\n\n\nroutine_name\n\n\t\n\nTEXT\n\n\n\n\nroutine_type\n\n\t\n\nTEXT\n\n\n\n\nroutine_body\n\n\t\n\nTEXT\n\n\n\n\nroutine_schema\n\n\t\n\nTEXT\n\n\n\n\ndata_type\n\n\t\n\nTEXT\n\n\n\n\nis_deterministic\n\n\t\n\nBOOLEAN\n\n\n\n\nroutine_definition\n\n\t\n\nTEXT\n\n\n\n\nspecific_name\n\n\t\n\nTEXT\n\nroutine_name\n\nName of the routine (might be duplicated in case of overloading)\n\nroutine_type\n\nType of the routine. Can be FUNCTION, ANALYZER, CHAR_FILTER, TOKEN_FILTER or TOKEN_FILTER.\n\nroutine_schema\n\nThe schema where the routine was defined. If it doesn’t apply, then NULL.\n\nroutine_body\n\nThe language used for the routine implementation. If it doesn’t apply, then NULL.\n\ndata_type\n\nThe return type of the function. If it doesn’t apply, then NULL.\n\nis_deterministic\n\nIf the routine is deterministic then True, else False (NULL if it doesn’t apply).\n\nroutine_definition\n\nThe function definition (NULL if it doesn’t apply).\n\nspecific_name\n\nUsed to uniquely identify the function in a schema, even if the function is overloaded. Currently the specific name contains the types of the function arguments. As the format might change in the future, it should be only used to compare it to other instances of specific_name.\n\nschemata\n\nThe schemata table lists all existing schemas. The blob, information_schema, and sys schemas are always available. The doc schema is available after the first user table is created.\n\ncr> select schema_name from information_schema.schemata order by schema_name;\n+--------------------+\n| schema_name        |\n+--------------------+\n| blob               |\n| doc                |\n| information_schema |\n| pg_catalog         |\n| sys                |\n+--------------------+\nSELECT 5 rows in set (... sec)\n\nsql_features\n\nThe sql_features table outlines supported and unsupported SQL features of CrateDB based to the current SQL standard (see SQL standard compliance):\n\ncr> select feature_name, is_supported, sub_feature_id, sub_feature_name\n... from information_schema.sql_features\n... where feature_id='F501';\n+--------------------------------+--------------+----------------+--------------------+\n| feature_name                   | is_supported | sub_feature_id | sub_feature_name   |\n+--------------------------------+--------------+----------------+--------------------+\n| Features and conformance views | FALSE        |                |                    |\n| Features and conformance views | TRUE         | 1              | SQL_FEATURES view  |\n| Features and conformance views | FALSE        | 2              | SQL_SIZING view    |\n| Features and conformance views | FALSE        | 3              | SQL_LANGUAGES view |\n+--------------------------------+--------------+----------------+--------------------+\nSELECT 4 rows in set (... sec)\n\n\nName\n\n\t\n\nData Type\n\n\t\n\nNullable\n\n\n\n\nfeature_id\n\n\t\n\nTEXT\n\n\t\n\nNO\n\n\n\n\nfeature_name\n\n\t\n\nTEXT\n\n\t\n\nNO\n\n\n\n\nsub_feature_id\n\n\t\n\nTEXT\n\n\t\n\nNO\n\n\n\n\nsub_feature_name\n\n\t\n\nTEXT\n\n\t\n\nNO\n\n\n\n\nis_supported\n\n\t\n\nTEXT\n\n\t\n\nNO\n\n\n\n\nis_verified_by\n\n\t\n\nTEXT\n\n\t\n\nYES\n\n\n\n\ncomments\n\n\t\n\nTEXT\n\n\t\n\nYES\n\nfeature_id\n\nIdentifier of the feature\n\nfeature_name\n\nDescriptive name of the feature by the Standard\n\nsub_feature_id\n\nIdentifier of the sub feature; If it has zero-length, this is a feature\n\nsub_feature_name\n\nDescriptive name of the sub feature by the Standard; If it has zero-length, this is a feature\n\nis_supported\n\nYES if the feature is fully supported by the current version of CrateDB, NO if not\n\nis_verified_by\n\nIdentifies the conformance test used to verify the claim;\n\nAlways NULL since the CrateDB development group does not perform formal testing of feature conformance\n\ncomments\n\nEither NULL or shows a comment about the supported status of the feature\n\ncharacter_sets\n\nThe character_sets table identifies the character sets available in the current database.\n\nIn CrateDB there is always a single entry listing UTF8:\n\ncr> SELECT character_set_name, character_repertoire FROM information_schema.character_sets;\n+--------------------+----------------------+\n| character_set_name | character_repertoire |\n+--------------------+----------------------+\n| UTF8               | UCS                  |\n+--------------------+----------------------+\nSELECT 1 row in set (... sec)\n\n\nColumn Name\n\n\t\n\nReturn Type\n\n\t\n\nDescription\n\n\n\n\ncharacter_set_catalog\n\n\t\n\nTEXT\n\n\t\n\nNot implemented, this column is always null.\n\n\n\n\ncharacter_set_schema\n\n\t\n\nTEXT\n\n\t\n\nNot implemented, this column is always null.\n\n\n\n\ncharacter_set_name\n\n\t\n\nTEXT\n\n\t\n\nName of the character set\n\n\n\n\ncharacter_repertoire\n\n\t\n\nTEXT\n\n\t\n\nCharacter repertoire\n\n\n\n\nform_of_use\n\n\t\n\nTEXT\n\n\t\n\nCharacter encoding form, same as character_set_name\n\n\n\n\ndefault_collate_catalog\n\n\t\n\nTEXT\n\n\t\n\nName of the database containing the default collation (Always crate)\n\n\n\n\ndefault_collate_schema\n\n\t\n\nTEXT\n\n\t\n\nName of the schema containing the default collation (Always NULL)\n\n\n\n\ndefault_collate_name\n\n\t\n\nTEXT\n\n\t\n\nName of the default collation (Always NULL)"
  },
  {
    "title": "Optimistic Concurrency Control — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/occ.html",
    "html": "5.6\nOptimistic Concurrency Control\n\nTable of contents\n\nIntroduction\n\nOptimistic update\n\nOptimistic delete\n\nKnown limitations\n\nIntroduction\n\nEven though CrateDB does not support transactions, Optimistic Concurrency Control can be achieved by using the internal system columns _seq_no and _primary_term.\n\nEvery new primary shard row has an initial sequence number of 0. This value is increased by 1 on every insert, delete or update operation the primary shard executes. The primary term will be incremented when a shard is promoted to primary so the user can know if they are executing an update against the most up to date cluster configuration.\n\nIt’s possible to fetch the _seq_no and _primary_term by selecting them:\n\ncr> SELECT id, type, _seq_no, _primary_term FROM sensors ORDER BY 1;\n+-----+-------+---------+---------------+\n| id  | type  | _seq_no | _primary_term |\n+-----+-------+---------+---------------+\n| ID1 | DHT11 |       0 |             1 |\n| ID2 | DHT21 |       0 |             1 |\n+-----+-------+---------+---------------+\nSELECT 2 rows in set (... sec)\n\n\nThese _seq_no and _primary_term values can now be used on updates and deletes.\n\nNote\n\nOptimistic concurrency control only works using the = operator, checking for the exact _seq_no and _primary_term your update or delete is based on.\n\nOptimistic update\n\nQuerying for the correct _seq_no and _primary_term ensures that no concurrent update and cluster configuration change has taken place:\n\ncr> UPDATE sensors SET last_verification = '2020-01-10 09:40'\n... WHERE\n...   id = 'ID1'\n...   AND \"_seq_no\" = 0\n...   AND \"_primary_term\" = 1;\nUPDATE OK, 1 row affected (... sec)\n\n\nUpdating a row with a wrong or outdated sequence number or primary term will not execute the update and results in 0 affected rows:\n\ncr> UPDATE sensors SET last_verification = '2020-01-10 09:40'\n... WHERE\n...   id = 'ID1'\n...   AND \"_seq_no\" = 42\n...   AND \"_primary_term\" = 5;\nUPDATE OK, 0 rows affected (... sec)\n\nOptimistic delete\n\nThe same can be done when deleting a row:\n\ncr> DELETE FROM sensors WHERE id = 'ID2'\n...   AND \"_seq_no\" = 0\n...   AND \"_primary_term\" = 1;\nDELETE OK, 1 row affected (... sec)\n\nKnown limitations\n\nThe _seq_no and _primary_term columns can only be used when specifying the whole primary key in a query. For example, the query below is not possible with the database schema used for testing, because type is not declared as a primary key:\n\ncr> DELETE FROM sensors WHERE type = 'DHT11'\n...   AND \"_seq_no\" = 3\n...   AND \"_primary_term\" = 1;\nUnsupportedFeatureException[\"_seq_no\" and \"_primary_term\" columns can only be used\ntogether in the WHERE clause with equals comparisons and if there are also equals\ncomparisons on primary key columns]\n\n\nIn order to use the optimistic concurrency control mechanism, both the _seq_no and _primary_term columns need to be specified. It is not possible to only specify one of them. For example, the query below will result in an error:\n\ncr> DELETE FROM sensors WHERE id = 'ID1' AND \"_seq_no\" = 3;\nVersioningValidationException[\"_seq_no\" and \"_primary_term\" columns can only be used\ntogether in the WHERE clause with equals comparisons and if there are also equals\ncomparisons on primary key columns]\n\n\nNote\n\nBoth DELETE and UPDATE commands will return a row count of 0, if the given required version does not match the actual version of the relevant row."
  },
  {
    "title": "Blobs — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/blobs.html",
    "html": "5.6\nBlobs\n\nCrateDB includes support to store binary large objects. By utilizing CrateDB’s cluster features the files can be replicated and sharded just like regular data.\n\nTable of contents\n\nCreating a table for blobs\n\nCustom location for storing blob data\n\nGlobal by configuration\n\nPer blob table setting\n\nList\n\nAltering a blob table\n\nDeleting a blob table\n\nUsing blob tables\n\nUploading\n\nDownloading\n\nDeleting\n\nCreating a table for blobs\n\nBefore adding blobs a blob table must be created. Blob tables can be sharded. This makes it possible to distribute binaries over multiple nodes. Lets use the CrateDB shell crash to issue the SQL statement:\n\nsh$ crash -c \"create blob table myblobs clustered into 3 shards with (number_of_replicas=0)\"\nCREATE OK, 1 row affected (... sec)\n\n\nNow CrateDB is configured to allow blobs to be management under the /_blobs/myblobs endpoint.\n\nCustom location for storing blob data\n\nIt is possible to define a custom directory path for storing blob data which can be completely different than the normal data path. Best use case for this is storing normal data on a fast SSD and blob data on a large cheap spinning disk.\n\nThe custom blob data path can be set either globally by configuration or while creating a blob table. The path can be either absolute or relative and must be creatable/writable by the user CrateDB is running as. A relative path value is relative to CRATE_HOME.\n\nBlob data will be stored under this path with the following layout:\n\n/<blobs.path>/nodes/<NODE_NO>/indices/<INDEX_UUID>/<SHARD_ID>/blobs\n\nGlobal by configuration\n\nJust uncomment or add following entry at the CrateDB configuration in order to define a custom path globally for all blob tables:\n\nblobs.path: /path/to/blob/data\n\n\nAlso see Configuration.\n\nPer blob table setting\n\nIt is also possible to define a custom blob data path per table instead of global by configuration. Also per table setting take precedence over the configuration setting.\n\nSee CREATE BLOB TABLE for details.\n\nCreating a blob table with a custom blob data path:\n\nsh$ crash -c \"create blob table myblobs clustered into 3 shards with (blobs_path='/tmp/crate_blob_data')\" # doctest: +SKIP\nCREATE OK, 1 row affected (... sec)\n\nList\n\nTo list all blobs inside a blob table a SELECT statement can be used:\n\nsh$ crash -c \"select digest, last_modified from blob.myblobs\"\n+------------------------------------------+---------------+\n| digest                                   | last_modified |\n+------------------------------------------+---------------+\n| 4a756ca07e9487f482465a99e8286abc86ba4dc7 | ...           |\n+------------------------------------------+---------------+\nSELECT 1 row in set (... sec)\n\n\nNote\n\nTo query blob tables it is necessary to always specify the schema name blob.\n\nAltering a blob table\n\nThe number of replicas a blob table has can be changed using the ALTER BLOB TABLE clause:\n\nsh$ crash -c \"alter blob table myblobs set (number_of_replicas=0)\"\nALTER OK, -1 rows affected (... sec)\n\nDeleting a blob table\n\nBlob tables can be deleted similar to normal tables:\n\nsh$ crash -c \"drop blob table myblobs\"\nDROP OK, 1 row affected (... sec)\n\nUsing blob tables\n\nThe usage of Blob Tables is only supported using the HTTP/HTTPS protocol. This section describes how binaries can be stored, fetched and deleted.\n\nNote\n\nFor the reason of internal optimization any successful request could lead to a 307 Temporary Redirect response.\n\nUploading\n\nTo upload a blob the SHA1 hash of the blob has to be known upfront since this has to be used as the ID of the new blob. For this example we use a fancy Python one-liner to compute the SHA hash:\n\nsh$ python3 -c 'import hashlib;print(hashlib.sha1(\"contents\".encode(\"utf-8\")).hexdigest())'\n4a756ca07e9487f482465a99e8286abc86ba4dc7\n\n\nThe blob can now be uploaded by issuing a PUT request:\n\nsh$ curl -isSX PUT '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7' -d 'contents'\nHTTP/1.1 201 Created\ncontent-length: 0\n\n\nIf a blob already exists with the given hash a 409 Conflict is returned:\n\nsh$ curl -isSX PUT '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7' -d 'contents'\nHTTP/1.1 409 Conflict\ncontent-length: 0\n\nDownloading\n\nTo download a blob simply use a GET request:\n\nsh$ curl -sS '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7'\ncontents\n\n\nIf the blob doesn’t exist a 404 Not Found error is returned:\n\nsh$ curl -isS '127.0.0.1:4200/_blobs/myblobs/e5fa44f2b31c1fb553b6021e7360d07d5d91ff5e'\nHTTP/1.1 404 Not Found\ncontent-length: 0\n\n\nTo determine if a blob exists without downloading it, a HEAD request can be used:\n\nsh$ curl -sS -I '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7'\nHTTP/1.1 200 OK\ncontent-length: 8\naccept-ranges: bytes\nexpires: Thu, 31 Dec 2037 23:59:59 GMT\ncache-control: max-age=315360000\n\n\nNote\n\nThe cache headers for blobs are static and basically allows clients to cache the response forever since the blob is immutable.\n\nDeleting\n\nTo delete a blob simply use a DELETE request:\n\nsh$ curl -isS -XDELETE '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7'\nHTTP/1.1 204 No Content\n\n\nIf the blob doesn’t exist a 404 Not Found error is returned:\n\nsh$ curl -isS -XDELETE '127.0.0.1:4200/_blobs/myblobs/4a756ca07e9487f482465a99e8286abc86ba4dc7'\nHTTP/1.1 404 Not Found\ncontent-length: 0\n"
  },
  {
    "title": "User-defined functions — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/user-defined-functions.html",
    "html": "5.6\nUser-defined functions\n\nTable of contents\n\nCREATE OR REPLACE\n\nSupported types\n\nOverloading\n\nDeterminism\n\nDROP FUNCTION\n\nSupported languages\n\nJavaScript\n\nJavaScript supported types\n\nWorking with NUMBERS\n\nCREATE OR REPLACE\n\nCrateDB supports user-defined functions. See CREATE FUNCTION for a full syntax description.\n\nCREATE FUNCTION defines a new function:\n\ncr> CREATE FUNCTION my_subtract_function(integer, integer)\n... RETURNS integer\n... LANGUAGE JAVASCRIPT\n... AS 'function my_subtract_function(a, b) { return a - b; }';\nCREATE OK, 1 row affected  (... sec)\n\ncr> SELECT doc.my_subtract_function(3, 1) AS col;\n+-----+\n| col |\n+-----+\n|   2 |\n+-----+\nSELECT 1 row in set (... sec)\n\n\nCREATE OR REPLACE FUNCTION will either create a new function or replace an existing function definition:\n\ncr> CREATE OR REPLACE FUNCTION log10(bigint)\n... RETURNS double precision\n... LANGUAGE JAVASCRIPT\n... AS 'function log10(a) {return Math.log(a)/Math.log(10); }';\nCREATE OK, 1 row affected  (... sec)\n\ncr> SELECT doc.log10(10) AS col;\n+-----+\n| col |\n+-----+\n| 1.0 |\n+-----+\nSELECT 1 row in set (... sec)\n\n\nIt is possible to use named function arguments in the function signature. For example, the calculate_distance function signature has two geo_point arguments named start and end:\n\ncr> CREATE OR REPLACE FUNCTION calculate_distance(\"start\" geo_point, \"end\" geo_point)\n... RETURNS real\n... LANGUAGE JAVASCRIPT\n... AS 'function calculate_distance(start, end) {\n...       return Math.sqrt(\n...            Math.pow(end[0] - start[0], 2),\n...            Math.pow(end[1] - start[1], 2));\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\n\nNote\n\nArgument names are used for query documentation purposes only. You cannot reference arguments by name in the function body.\n\nOptionally, a schema-qualified function name can be defined. If you omit the schema, the current session schema is used:\n\ncr> CREATE OR REPLACE FUNCTION my_schema.log10(bigint)\n... RETURNS double precision\n... LANGUAGE JAVASCRIPT\n... AS 'function log10(a) { return Math.log(a)/Math.log(10); }';\nCREATE OK, 1 row affected  (... sec)\n\n\nNote\n\nIn order to improve the PostgreSQL server compatibility CrateDB allows the creation of user defined functions against the pg_catalog schema. However, the creation of user defined functions against the read-only System information and Information schema schemas is prohibited.\n\nSupported types\n\nFunction arguments and return values can be any of the supported data types. The values passed into a function must strictly correspond to the specified argument data types.\n\nNote\n\nThe value returned by the function will be casted to the return type provided in the definition if required. An exception will be thrown if the cast is not successful.\n\nOverloading\n\nWithin a specific schema, you can overload functions by defining functions with the same name but a different set of arguments:\n\ncr> CREATE FUNCTION my_schema.my_multiply(integer, integer)\n... RETURNS integer\n... LANGUAGE JAVASCRIPT\n... AS 'function my_multiply(a, b) { return a * b; }';\nCREATE OK, 1 row affected  (... sec)\n\n\nThis would overload the my_multiply function with different argument types:\n\ncr> CREATE FUNCTION my_schema.my_multiply(bigint, bigint)\n... RETURNS bigint\n... LANGUAGE JAVASCRIPT\n... AS 'function my_multiply(a, b) { return a * b; }';\nCREATE OK, 1 row affected  (... sec)\n\n\nThis would overload the my_multiply function with more arguments:\n\ncr> CREATE FUNCTION my_schema.my_multiply(bigint, bigint, bigint)\n... RETURNS bigint\n... LANGUAGE JAVASCRIPT\n... AS 'function my_multiply(a, b, c) { return a * b * c; }';\nCREATE OK, 1 row affected  (... sec)\n\n\nCaution\n\nIt is considered bad practice to create functions that have the same name as the CrateDB built-in functions.\n\nNote\n\nIf you call a function without a schema name, CrateDB will look it up in the built-in functions first and only then in the user-defined functions available in the search_path.\n\nTherefore a built-in function with the same name as a user-defined function will hide the latter, even if it contains a different set of arguments. However, such functions can still be called if the schema name is explicitly provided.\n\nDeterminism\n\nCaution\n\nUser-defined functions need to be deterministic, meaning that they must always return the same result value when called with the same argument values, because CrateDB might cache the returned values and reuse the value if the function is called multiple times with the same arguments.\n\nDROP FUNCTION\n\nFunctions can be dropped like this:\n\ncr> DROP FUNCTION doc.log10(bigint);\nDROP OK, 1 row affected  (... sec)\n\n\nAdding IF EXISTS prevents from raising an error if the function doesn’t exist:\n\ncr> DROP FUNCTION IF EXISTS doc.log10(integer);\nDROP OK, 1 row affected  (... sec)\n\n\nOptionally, argument names can be specified within the drop statement:\n\ncr> DROP FUNCTION IF EXISTS doc.calculate_distance(start_point geo_point, end_point geo_point);\nDROP OK, 1 row affected  (... sec)\n\n\nOptionally, you can provide a schema:\n\ncr> DROP FUNCTION my_schema.log10(bigint);\nDROP OK, 1 row affected  (... sec)\n\nSupported languages\n\nCurrently, CrateDB only supports JavaScript for user-defined functions.\n\nJavaScript\n\nThe user defined function JavaScript is compatible with the ECMAScript 2019 specification.\n\nCrateDB uses the GraalVM JavaScript engine as a JavaScript (ECMAScript) language execution runtime. The GraalVM JavaScript engine is a Java application that works on the stock Java Virtual Machines (VMs). The interoperability between Java code (host language) and JavaScript user-defined functions (guest language) is guaranteed by the GraalVM Polyglot API.\n\nPlease note: CrateDB does not use the GraalVM JIT compiler as optimizing compiler. However, the stock host Java VM JIT compilers can JIT-compile, optimize, and execute the GraalVM JavaScript codebase to a certain extent.\n\nThe execution context for guest JavaScript is created with restricted privileges to allow for the safe execution of less trusted guest language code. The guest language application context for each user-defined function is created with default access modifiers, so any access to managed resources is denied. The only exception is the host language interoperability configuration which explicitly allows access to Java lists and arrays. Please refer to GraalVM Security Guide for more detailed information.\n\nAlso, even though user-defined functions implemented with ECMA-compliant JavaScript, objects that are normally accessible with a web browser (e.g. window, console, and so on) are not available.\n\nNote\n\nGraalVM treats objects provided to JavaScript user-defined functions as close as possible to their respective counterparts and therefore by default only a subset of prototype functions are available in user-defined functions. For CrateDB 4.6 and earlier the object prototype was disabled.\n\nPlease refer to the GraalVM JavaScript Compatibility FAQ to learn more about the compatibility.\n\nJavaScript supported types\n\nJavaScript functions can handle all CrateDB data types. However, for some return types the function output must correspond to the certain format.\n\nIf a function requires geo_point as a return type, then the JavaScript function must return a double precision array of size 2, WKT string or GeoJson object.\n\nHere is an example of a JavaScript function returning a double array:\n\ncr> CREATE FUNCTION rotate_point(point geo_point, angle real)\n... RETURNS geo_point\n... LANGUAGE JAVASCRIPT\n... AS 'function rotate_point(point, angle) {\n...       var cos = Math.cos(angle);\n...       var sin = Math.sin(angle);\n...       var x = cos * point[0] - sin * point[1];\n...       var y = sin * point[0] + cos * point[1];\n...       return [x, y];\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\n\nBelow is an example of a JavaScript function returning a WKT string, which will be cast to geo_point:\n\ncr> CREATE FUNCTION symmetric_point(point geo_point)\n... RETURNS geo_point\n... LANGUAGE JAVASCRIPT\n... AS 'function symmetric_point (point, angle) {\n...       var x = - point[0],\n...           y = - point[1];\n...       return \"POINT (\\\" + x + \\\", \\\" + y +\\\")\";\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\n\nSimilarly, if the function specifies the geo_shape return data type, then the JavaScript function should return a GeoJson object or WKT string:\n\ncr> CREATE FUNCTION line(\"start\" array(double precision), \"end\" array(double precision))\n... RETURNS object\n... LANGUAGE JAVASCRIPT\n... AS 'function line(start, end) {\n...        return { \"type\": \"LineString\", \"coordinates\" : [start_point, end_point] };\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\n\nNote\n\nIf the return value of the JavaScript function is undefined, it is converted to NULL.\n\nWorking with NUMBERS\n\nThe JavaScript engine interprets numbers as java.lang.Double, java.lang.Long, or java.lang.Integer, depending on the computation performed. In most cases, this is not an issue, since the return type of the JavaScript function will be cast to the return type specified in the CREATE FUNCTION statement, although cast might result in a loss of precision.\n\nHowever, when you try to cast DOUBLE PRECISION to TIMESTAMP WITH TIME ZONE, it will be interpreted as UTC seconds and will result in a wrong value:\n\ncr> CREATE FUNCTION utc(bigint, bigint, bigint)\n... RETURNS TIMESTAMP WITH TIME ZONE\n... LANGUAGE JAVASCRIPT\n... AS 'function utc(year, month, day) {\n...       return Date.UTC(year, month, day, 0, 0, 0);\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\ncr> SELECT date_format(utc(2016,04,6)) as epoque;\n+------------------------------+\n| epoque                       |\n+------------------------------+\n| 48314-07-22T00:00:00.000000Z |\n+------------------------------+\nSELECT 1 row in set (... sec)\n\n\nTo avoid this behavior, the numeric value should be divided by 1000 before it is returned:\n\ncr> CREATE FUNCTION utc(bigint, bigint, bigint)\n... RETURNS TIMESTAMP WITH TIME ZONE\n... LANGUAGE JAVASCRIPT\n... AS 'function utc(year, month, day) {\n...       return Date.UTC(year, month, day, 0, 0, 0)/1000;\n...    }';\nCREATE OK, 1 row affected  (... sec)\n\ncr> SELECT date_format(utc(2016,04,6)) as epoque;\n+-----------------------------+\n| epoque                      |\n+-----------------------------+\n| 2016-05-06T00:00:00.000000Z |\n+-----------------------------+\nSELECT 1 row in set (... sec)\n"
  },
  {
    "title": "Querying — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/dql/index.html",
    "html": "5.6\nQuerying\n\nThis section provides an overview of how to query CrateDB.\n\nSee Also\n\nGeneral use: Data definition\n\nGeneral use: Data manipulation\n\nTable of contents\n\nSelecting data\nIntroduction\nFROM clause\nJoins\nDISTINCT clause\nWHERE clause\nComparison operators\nArray comparisons\nEXISTS\nContainer data types\nAggregation\nWindow functions\nGROUP BY\nWITH Queries (Common Table Expressions)\nJoins\nCross joins\nInner joins\nOuter joins\nJoin conditions\nAvailable join algorithms\nLimitations\nUnion\nUnion All\nUnion Distinct\nUnion of object types\nUnion of different types\nRefresh\nIntroduction\nMultiple Table Refresh\nPartition Refresh\nFulltext search\nMATCH Predicate\nUsage\nSearching On Multiple Columns\nNegative Search\nFilter By _score\nGeo search\nIntroduction\nMATCH predicate\nExact queries"
  },
  {
    "title": "Built-in functions and operators — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/builtins/index.html",
    "html": "5.6\nBuilt-in functions and operators\n\nThis chapter provides an overview of built-in functions and operators.\n\nTable of contents\n\nScalar functions\nString functions\nDate and time functions\nGeo functions\nMathematical functions\nRegular expression functions\nArray functions\nObject functions\nConditional functions and expressions\nSystem information functions\nSpecial functions\nAggregation\nAggregate expressions\nAggregate functions\nLimitations\nArithmetic operators\nBit operators\nTable functions\nScalar functions\nempty_row( )\nunnest( array [ array , ] )\npg_catalog.generate_series(start, stop, [step])\npg_catalog.generate_subscripts(array, dim, [reverse])\nregexp_matches(source, pattern [, flags])\npg_catalog.pg_get_keywords()\ninformation_schema._pg_expandarray(array)\nComparison operators\nBasic operators\nWHERE clause operators\nArray comparisons\nIN (value [, ...])\nANY/SOME (array expression)\nALL (array_expression)\nSubquery expressions\nIN (subquery)\nANY/SOME (subquery)\nALL (subquery)\nWindow functions\nWindow function call\nWindow definition\nGeneral-purpose window functions\nAggregate window functions"
  },
  {
    "title": "Data manipulation — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/dml.html",
    "html": "5.6\nData manipulation\n\nThis section provides an overview of how to manipulate data (e.g., inserting rows) with CrateDB.\n\nSee Also\n\nGeneral use: Data definition\n\nGeneral use: Querying\n\nTable of contents\n\nInserting data\n\nInserting data by query\n\nUpserts (ON CONFLICT DO UPDATE SET)\n\nUpdating data\n\nDeleting data\n\nImport and export\n\nImporting data\n\nExample\n\nDetailed error reporting\n\nExporting data\n\nInserting data\n\nInserting data to CrateDB is done by using the SQL INSERT statement.\n\nNote\n\nThe column list is always ordered based on the column position in the CREATE TABLE statement of the table. If the insert columns are omitted, the values in the VALUES clauses must correspond to the table columns in that order.\n\nInserting a row:\n\ncr> insert into locations (id, date, description, kind, name, position)\n... values (\n...   '14',\n...   '2013-09-12T21:43:59.000Z',\n...   'Blagulon Kappa is the planet to which the police are native.',\n...   'Planet',\n...   'Blagulon Kappa',\n...   7\n... );\nINSERT OK, 1 row affected (... sec)\n\n\nWhen inserting rows with the VALUES clause all data is validated in terms of data types compatibility and compliance with defined constraints, and if there are any issues an error message is returned and no rows are inserted.\n\nInserting multiple rows at once (aka. bulk insert) can be done by defining multiple values for the INSERT statement:\n\ncr> insert into locations (id, date, description, kind, name, position) values\n... (\n...   '16',\n...   '2013-09-14T21:43:59.000Z',\n...   'Blagulon Kappa II is the planet to which the police are native.',\n...   'Planet',\n...   'Blagulon Kappa II',\n...   19\n... ),\n... (\n...   '17',\n...   '2013-09-13T16:43:59.000Z',\n...   'Brontitall is a planet with a warm, rich atmosphere and no mountains.',\n...   'Planet',\n...   'Brontitall',\n...   10\n... );\nINSERT OK, 2 rows affected (... sec)\n\n\nWhen inserting into tables containing Generated columns or Base Columns having the Default clause specified, their values can be safely omitted. They are generated upon insert:\n\ncr> CREATE TABLE debit_card (\n...   owner text,\n...   num_part1 integer,\n...   num_part2 integer,\n...   check_sum integer GENERATED ALWAYS AS ((num_part1 + num_part2) * 42),\n...   \"user\" text DEFAULT 'crate'\n... );\nCREATE OK, 1 row affected (... sec)\n\ncr> insert into debit_card (owner, num_part1, num_part2) values\n... ('Zaphod Beeblebrox', 1234, 5678);\nINSERT OK, 1 row affected (... sec)\n\ncr> select * from debit_card;\n+-------------------+-----------+-----------+-----------+-------+\n| owner             | num_part1 | num_part2 | check_sum | user  |\n+-------------------+-----------+-----------+-----------+-------+\n| Zaphod Beeblebrox |      1234 |      5678 |    290304 | crate |\n+-------------------+-----------+-----------+-----------+-------+\nSELECT 1 row in set (... sec)\n\n\nFor Generated columns, if the value is given, it is validated against the generation clause of the column and the currently inserted row:\n\ncr> insert into debit_card (owner, num_part1, num_part2, check_sum) values\n... ('Arthur Dent', 9876, 5432, 642935);\nSQLParseException[Given value 642935 for generated column check_sum does not match calculation ((num_part1 + num_part2) * 42) = 642936]\n\nInserting data by query\n\nIt is possible to insert data using a query instead of values. Column data types of source and target table can differ as long as the values are castable. This gives the opportunity to restructure the tables data, renaming a field, changing a field’s data type or convert a normal table into a partitioned one.\n\nCaution\n\nWhen inserting data from a query, there is no error message returned when rows fail to be inserted, they are instead skipped, and the number of rows affected is decreased to reflect the actual number of rows for which the operation succeeded.\n\nExample of changing a field’s data type, in this case, changing the position data type from integer to smallint:\n\ncr> create table locations2 (\n...     id text primary key,\n...     name text,\n...     date timestamp with time zone,\n...     kind text,\n...     position smallint,\n...     description text\n... ) clustered by (id) into 2 shards with (number_of_replicas = 0);\nCREATE OK, 1 row affected (... sec)\n\ncr> insert into locations2 (id, name, date, kind, position, description)\n... (\n...     select id, name, date, kind, position, description\n...     from locations\n...     where position < 10\n... );\nINSERT OK, 14 rows affected (... sec)\n\n\nExample of creating a new partitioned table out of the locations table with data partitioned by year:\n\ncr> create table locations_parted (\n...     id text primary key,\n...     name text,\n...     year text primary key,\n...     date timestamp with time zone,\n...     kind text,\n...     position integer\n... ) clustered by (id) into 2 shards\n... partitioned by (year) with (number_of_replicas = 0);\nCREATE OK, 1 row affected (... sec)\n\ncr> insert into locations_parted (id, name, year, date, kind, position)\n... (\n...     select\n...         id,\n...         name,\n...         date_format('%Y', date),\n...         date,\n...         kind,\n...         position\n...     from locations\n... );\nINSERT OK, 16 rows affected (... sec)\n\n\nResulting partitions of the last insert by query:\n\ncr> select table_name, partition_ident, values, number_of_shards, number_of_replicas\n... from information_schema.table_partitions\n... where table_name = 'locations_parted'\n... order by partition_ident;\n+------------------+-----------------+------------------+------------------+--------------------+\n| table_name       | partition_ident | values           | number_of_shards | number_of_replicas |\n+------------------+-----------------+------------------+------------------+--------------------+\n| locations_parted | 042j2e9n74      | {\"year\": \"1979\"} |                2 |                  0 |\n| locations_parted | 042j4c1h6c      | {\"year\": \"2013\"} |                2 |                  0 |\n+------------------+-----------------+------------------+------------------+--------------------+\nSELECT 2 rows in set (... sec)\n\n\nNote\n\nlimit, offset and order by are not supported inside the query statement.\n\nUpserts (ON CONFLICT DO UPDATE SET)\n\nThe ON CONFLICT DO UPDATE SET clause is used to update the existing row if inserting is not possible because of a duplicate-key conflict if a document with the same PRIMARY KEY already exists. This is type of operation is commonly referred to as an upsert, being a combination of “update” and “insert”.\n\ncr> SELECT\n...     name,\n...     visits,\n...     extract(year from last_visit) AS last_visit\n... FROM uservisits ORDER BY NAME;\n+----------+--------+------------+\n| name     | visits | last_visit |\n+----------+--------+------------+\n| Ford     |      1 | 2013       |\n| Trillian |      3 | 2013       |\n+----------+--------+------------+\nSELECT 2 rows in set (... sec)\n\ncr> INSERT INTO uservisits (id, name, visits, last_visit) VALUES\n... (\n...     0,\n...     'Ford',\n...     1,\n...     '2015-01-12'\n... ) ON CONFLICT (id) DO UPDATE SET\n...     visits = visits + 1;\nINSERT OK, 1 row affected (... sec)\n\ncr> SELECT\n...     name,\n...     visits,\n...     extract(year from last_visit) AS last_visit\n... FROM uservisits WHERE id = 0;\n+------+--------+------------+\n| name | visits | last_visit |\n+------+--------+------------+\n| Ford |      2 | 2013       |\n+------+--------+------------+\nSELECT 1 row in set (... sec)\n\n\nIt’s possible to refer to values which would be inserted if no duplicate-key conflict occurred, by using the special excluded table. This table is especially useful in multiple-row inserts, to refer to the current rows values:\n\ncr> INSERT INTO uservisits (id, name, visits, last_visit) VALUES\n... (\n...     0,\n...     'Ford',\n...     2,\n...     '2016-01-13'\n... ),\n... (\n...     1,\n...     'Trillian',\n...     5,\n...     '2016-01-15'\n... ) ON CONFLICT (id) DO UPDATE SET\n...     visits = visits + excluded.visits,\n...     last_visit = excluded.last_visit;\nINSERT OK, 2 rows affected (... sec)\n\ncr> SELECT\n...     name,\n...     visits,\n...     extract(year from last_visit) AS last_visit\n... FROM uservisits ORDER BY name;\n+----------+--------+------------+\n| name     | visits | last_visit |\n+----------+--------+------------+\n| Ford     |      4 | 2016       |\n| Trillian |      8 | 2016       |\n+----------+--------+------------+\nSELECT 2 rows in set (... sec)\n\n\nThis can also be done when using a query instead of values:\n\ncr> CREATE TABLE uservisits2 (\n...   id integer primary key,\n...   name text,\n...   visits integer,\n...   last_visit timestamp with time zone\n... ) CLUSTERED BY (id) INTO 2 SHARDS WITH (number_of_replicas = 0);\nCREATE OK, 1 row affected (... sec)\n\ncr> INSERT INTO uservisits2 (id, name, visits, last_visit)\n... (\n...     SELECT id, name, visits, last_visit\n...     FROM uservisits\n... );\nINSERT OK, 2 rows affected (... sec)\n\ncr> INSERT INTO uservisits2 (id, name, visits, last_visit)\n... (\n...     SELECT id, name, visits, last_visit\n...     FROM uservisits\n... ) ON CONFLICT (id) DO UPDATE SET\n...     visits = visits + excluded.visits,\n...     last_visit = excluded.last_visit;\nINSERT OK, 2 rows affected (... sec)\n\ncr> SELECT\n...     name,\n...     visits,\n...     extract(year from last_visit) AS last_visit\n... FROM uservisits ORDER BY name;\n+----------+--------+------------+\n| name     | visits | last_visit |\n+----------+--------+------------+\n| Ford     |      4 | 2016       |\n| Trillian |      8 | 2016       |\n+----------+--------+------------+\nSELECT 2 rows in set (... sec)\n\n\nSee Also\n\nSQL syntax: ON CONFLICT DO UPDATE SET\n\nUpdating data\n\nIn order to update documents in CrateDB the SQL UPDATE statement can be used:\n\ncr> update locations set description = 'Updated description'\n... where name = 'Bartledan';\nUPDATE OK, 1 row affected (... sec)\n\n\nUpdating nested objects is also supported:\n\ncr> update locations set inhabitants['name'] = 'Human' where name = 'Bartledan';\nUPDATE OK, 1 row affected (... sec)\n\n\nIt’s also possible to reference a column within the expression, for example to increment a number like this:\n\ncr> update locations set position = position + 1 where position < 3;\nUPDATE OK, 6 rows affected (... sec)\n\n\nNote\n\nIf the same documents are updated concurrently an VersionConflictException might occur. CrateDB contains a retry logic that tries to resolve the conflict automatically.\n\nDeleting data\n\nDeleting rows in CrateDB is done using the SQL DELETE statement:\n\ncr> delete from locations where position > 3;\nDELETE OK, ... rows affected (... sec)\n\nImport and export\nImporting data\n\nUsing the COPY FROM statement, CrateDB nodes can import data from local files or files that are available over the network.\n\nThe supported data formats are JSON and CSV. The format is inferred from the file extension, if possible. Alternatively the format can also be provided as an option (see WITH). If the format is not provided and cannot be inferred from the file extension, it will be processed as JSON.\n\nJSON files must contain a single JSON object per line.\n\nExample JSON data:\n\n{\"id\": 1, \"quote\": \"Don't panic\"}\n{\"id\": 2, \"quote\": \"Ford, you're turning into a penguin. Stop it.\"}\n\n\nCSV files must contain a header with comma-separated values, which will be added as columns.\n\nExample CSV data:\n\nid,quote\n1,\"Don't panic\"\n2,\"Ford, you're turning into a penguin. Stop it.\"\n\n\nNote\n\nThe COPY FROM statement will convert and validate your data.\n\nValues for generated columns will be computed if the data does not contain them, otherwise they will be imported and validated\n\nFurthermore, column names in your data are considered case sensitive (as if they were quoted in a SQL statement).\n\nFor further information, including how to import data to Partitioned tables, take a look at the COPY FROM reference.\n\nExample\n\nHere’s an example statement:\n\ncr> COPY quotes FROM 'file:///tmp/import_data/quotes.json';\nCOPY OK, 3 rows affected (... sec)\n\n\nThis statement imports data from the /tmp/import_data/quotes.json file into a table named quotes.\n\nNote\n\nThe file you specify must be available on one of the CrateDB nodes. This statement will not work with files that are local to your client.\n\nFor the above statement, every node in the cluster will attempt to import data from a file located at /tmp/import_data/quotes.json relative to the crate process (i.e., if you are running CrateDB inside a container, the file must also be inside the container).\n\nIf you want to import data from a file that on your local computer using COPY FROM, you must first transfer the file to one of the CrateDB nodes.\n\nConsult the COPY FROM reference for additional information.\n\nIf you want to import all files inside the /tmp/import_data directory on every CrateDB node, you can use a wildcard, like so:\n\ncr> COPY quotes FROM '/tmp/import_data/*' WITH (bulk_size = 4);\nCOPY OK, 3 rows affected (... sec)\n\n\nThis wildcard can also be used to only match certain files in a directory:\n\ncr> COPY quotes FROM '/tmp/import_data/qu*.json';\nCOPY OK, 3 rows affected (... sec)\n\nDetailed error reporting\n\nIf the RETURN_SUMMARY clause is specified, a result set containing information about failures and successfully imported records is returned.\n\ncr> COPY locations FROM '/tmp/import_data/locations_with_failure/locations*.json' RETURN SUMMARY;\n +--...--+----------...--------+---------------+-------------+--------------------...-------------------------------------+\n | node  | uri                 | success_count | error_count | errors                                                     |\n +--...--+----------...--------+---------------+-------------+--------------------...-------------------------------------+\n | {...} | .../locations1.json |             6 |           0 | {}                                                         |\n | {...} | .../locations2.json |             5 |           2 | {\"Cannot cast value...{\"count\": ..., \"line_numbers\": ...}} |\n +--...--+----------...--------+---------------+-------------+--------------------...-------------------------------------+\n COPY 2 rows in set (... sec)\n\n\nIf an error happens while processing the URI in general, the error_count and success_count columns will contains NULL values to indicate that no records were processed.\n\ncr> COPY locations FROM '/tmp/import_data/not-existing.json' RETURN SUMMARY;\n +--...--+-----------...---------+---------------+-------------+------------------------...------------------------+\n | node  | uri                   | success_count | error_count | errors                                            |\n +--...--+-----------...---------+---------------+-------------+------------------------...------------------------+\n | {...} | .../not-existing.json |          NULL |        NULL | {\"...not-existing.json (...)\": {\"count\": 1, ...}} |\n +--...--+-----------...---------+---------------+-------------+------------------------...------------------------+\nCOPY 1 row in set (... sec)\n\n\nSee COPY FROM for more information.\n\nExporting data\n\nData can be exported using the COPY TO statement. Data is exported in a distributed way, meaning each node will export its own data.\n\nReplicated data is not exported. So every row of an exported table is stored only once.\n\nThis example shows how to export a given table into files named after the table and shard ID with gzip compression:\n\ncr> REFRESH TABLE quotes;\nREFRESH OK...\n\ncr> COPY quotes TO DIRECTORY '/tmp/' with (compression='gzip');\nCOPY OK, 3 rows affected ...\n\n\nInstead of exporting a whole table, rows can be filtered by an optional WHERE clause condition. This is useful if only a subset of the data needs to be exported:\n\ncr> COPY quotes WHERE match(quote_ft, 'time') TO DIRECTORY '/tmp/' WITH (compression='gzip');\nCOPY OK, 2 rows affected ...\n\n\nFor further details see COPY TO."
  },
  {
    "title": "Environment variables — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/config/environment.html",
    "html": "5.6\nEnvironment variables\n\nCrateDB can be configured with some environment variables.\n\nThere are many different ways to set environment variables, depending on how CrateDB is being deployed.\n\nHere is a trivial example:\n\nsh$ export CRATE_HOME=/tmp/crate\nsh$ ./bin/crate\n\n\nHere, we set CRATE_HOME to /tmp/crate, export it so that sub-processes of the shell have access, and then start CrateDB.\n\nCrateDB supports two kinds of environment variables:\n\nApplication variables\n\nJava Virtual Machine (JVM) variables\n\nWe use “application” here to distinguish between CrateDB running as a Java application and the JVM itself, which runs CrateDB.\n\nTable of contents\n\nApplication variables\n\nJVM variables\n\nGeneral\n\nApplication variables\nCRATE_HOME: directory path\n\nThe home directory of the CrateDB installation.\n\nThis directory is used as the root for the configuration directory, data directory, log directory, and so on.\n\nIf you have installed CrateDB from a package, this variable should be set for you.\n\nIf you are installing manually, in most cases, this should be set to the directory from which you would normally execute bin/crate, i.e. the root directory of the basic installation.\n\nJVM variables\nGeneral\nCRATE_JAVA_OPTS: Java options\n\nThe Java options to use when running CrateDB.\n\nFor example, you could change the stack size like this:\n\nCRATE_JAVA_OPTS=-Xss500k\n\n\nSee Also\n\nFor more information about Java options, consult the documentation for Microsoft Windows or Unix-like operating systems.\n\nCRATE_HEAP_SIZE: size\n\nThe Java heap size, i.e. the amount of memory that can be used.\n\nYou can set the heap size to four gigabytes like this:\n\nCRATE_HEAP_SIZE=4g\n\n\nUse g for gigabytes or m for megabytes.\n\nSee Also\n\nAppropriate memory configuration is important for optimal performance.\n\nCRATE_HEAP_DUMP_PATH: file or directory path (default: varies)\n\nThe directory to be used for heap dumps in the case of a crash.\n\nIf a directory path is configured, new heap dumps will be written to that directory every time CrateDB crashes.\n\nIf a file path is configured (i.e. the last node of the path is non-existent or exists and is a file) CrateDB will overwrite that file with a heap dump every time it crashes.\n\nDefault values are as follows:\n\nFor a basic installation, the process working directory\n\nIf you have installed a CrateDB Linux package, /var/lib/crate\n\nWhen running CrateDB on Docker, /data/data\n\nWarning\n\nMake sure there is enough disk space available for heap dumps."
  },
  {
    "title": "Data definition — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/ddl/index.html",
    "html": "5.6\nData definition\n\nThis section provides an overview of how to create tables and perform other data-definition related operations with CrateDB.\n\nSee Also\n\nGeneral use: Data manipulation\n\nGeneral use: Querying\n\nTable of contents\n\nCreating tables\nTable definition\nTable configuration\nData types\nOverview\nPrimitive types\nContainer types\nFLOAT_VECTOR\nGeographic types\nType casting\nPostgreSQL compatibility\nSystem columns\nGenerated columns\nGeneration expressions\nLast modified dates\nPartitioning\nConstraints\nPRIMARY KEY\nNULL\nNOT NULL\nCHECK\nStorage\nColumn store\nPartitioned tables\nIntroduction\nCreation\nInformation schema\nInsert\nUpdate\nDelete\nQuerying\nAlter\nLimitations\nConsistency notes related to concurrent DML statement\nSharding\nIntroduction\nNumber of shards\nRouting\nReplication\nTable configuration\nShard recovery\nUnderreplication\nShard allocation filtering\nSettings\nSpecial attributes\nColumn policy\nstrict\ndynamic\nFulltext indices\nIndex definition\nDisable indexing\nPlain index (default)\nCreating a custom analyzer\nExtending a built-in analyzer\nFulltext analyzers\nOverview\nBuilt-in analyzers\nBuilt-in tokenizers\nBuilt-in token filters\nBuilt-in char filter\nShow Create Table\nViews\nCreating views\nQuerying views\nDropping views\nAltering tables\nUpdating parameters\nAdding columns\nRenaming columns\nClosing and opening tables\nRenaming tables\nReroute shards"
  },
  {
    "title": "Logging — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/config/logging.html",
    "html": "5.6\nLogging\n\nCrateDB supports two kinds of logging:\n\nApplication logging with Log4j\n\nJava Virtual Machine (JVM) garbage collection logging\n\nWe use “application” here to distinguish between CrateDB running as a Java application and the JVM itself, which runs CrateDB.\n\nBecause garbage collection logging is a native feature of the JVM it behaves differently and is configured differently.\n\nTable of contents\n\nApplication logging\n\nLog4j\n\nConfiguration file\n\nLog levels\n\nRun-time configuration\n\nJVM logging\n\nGarbage collection\n\nEnvironment variables\n\nApplication logging\nLog4j\n\nCrateDB uses Log4j.\n\nConfiguration file\n\nYou can configure Log4j with the log4j2.properties file in the CrateDB configuration directory.\n\nThe log4j2.properties file is formatted using YAML and simplifies Log4j configuration by allowing you to use the PropertyConfigurator but without having to tediously repeat the log4j prefix.\n\nHere’s one example:\n\nrootLogger.level = info\nrootLogger.appenderRef.console.ref = console\n\n# log query execution errors for easier debugging\nlogger.action.name = org.crate.action.sql\nlogger.action.level = debug\n\nappender.console.type = Console\nappender.console.name = console\nappender.console.layout.type = PatternLayout\nappender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c{1.}] %marker%m%n\n\n\nAnd here is a snippet of the generated properties ready for use with log4j. You get the point.\n\nSee Also\n\nConsult the PropertyConfigurator documentation or the configuration section of the Log4j documentation for more information.\n\nLog levels\n\nPossible log levels are the same as for Log4j, in order of increasing importance:\n\nTRACE\n\nDEBUG\n\nINFO\n\nWARN\n\nERROR\n\nLog levels must be provided as string literals in the SET statement.\n\nNote\n\nBe careful using the TRACE log level because it’s extremely verbose, can obscure other important log messages and even fill up entire data disks in some cases.\n\nRun-time configuration\n\nIt’s possible to set the log level of loggers at runtime using SET, like so:\n\nSET GLOBAL TRANSIENT \"logger.action\" = 'INFO';\n\n\nIn this example, the log level INFO is applied to the action logger.\n\nIn addition to being able to configure any of the standard loggers, you can configure the root (i.e. default) logger using logger._root.\n\nAs with any setting, you can inspect the current configuration by querying the sys.cluster table.\n\nTip\n\nRun-time logging configuration is particularly useful if you are debugging a problem and you want to increase the log level without restarting nodes.\n\nRun-time logging configuration is applied across the whole cluster, and overrides the start-up configuration defined in each respective log4j2.properties file.\n\nCaution\n\nThe RESET statement is supported but logging configuration is only reset when the whole cluster is restarted.\n\nJVM logging\n\nCrateDB exposes some native JVM logging functionality.\n\nGarbage collection\n\nCrateDB logs JVM garbage collection times using the built-in garbage collection logging of the JVM.\n\nEnvironment variables\n\nThe following environment variables can be used to configure garbage collection logging.\n\nCRATE_DISABLE_GC_LOGGING: boolean integer (default: 0)\n\nWhether to disable garbage collection logging.\n\nSet to 1 to disable.\n\nNote\n\nSince CrateDB 3.0, Garbage collection logging is enabled by default.\n\nCRATE_GC_LOG_DIR: path to logs directory (default: varies)\n\nThe log file directory.\n\nFor a basic installation, the logs directory in the CRATE_HOME directory is the default.\n\nIf you have installed a CrateDB Linux package, the default directory is /var/log/crate instead.\n\nCRATE_GC_LOG_SIZE: file size (default: 64m)\n\nMaximum file size of log files before they are rotated.\n\nCRATE_GC_LOG_FILES: number (default: 16)\n\nThe amount of files kept in rotation.\n\nCaution\n\nWith the default configuration of 16 rotated 64 megabyte log files, garbage collection logs will grow to occupy one gigabyte on disk.\n\nMake sure you have enough available disk space for configuration."
  },
  {
    "title": "Session settings — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/config/session.html",
    "html": "5.6\nSession settings\n\nTable of contents\n\nUsage\n\nSupported session settings\n\nSession settings only apply to the currently connected client session.\n\nUsage\n\nTo configure a modifiable session setting, use SET, for example:\n\nSET search_path TO myschema, doc;\n\n\nTo retrieve the current value of a session setting, use SHOW e.g:\n\nSHOW search_path;\n\n\nBesides using SHOW, it is also possible to use the current_setting scalar function.\n\nSupported session settings\nsearch_path\nDefault: pg_catalog, doc\nModifiable: yes\n\nThe list of schemas to be searched when a relation is referenced without a schema.\n\nCrateDB will try to resolve an unqualified relation name against the configured search_path by iterating over the configured schemas in the order they were declared. The first matching relation in the search_path is used. CrateDB will report an error if there is no match.\n\nNote\n\nThis setting mirrors the PostgreSQL search_path setting.\n\nSome PostgreSQL clients require access to various tables in the pg_catalog schema. Usually, this is to extract information about built-in data types or functions.\n\nCrateDB implements the system pg_catalog schema and it automatically includes it in the search_path before the configured schemas, unless it is already explicitly in the schema configuration.\n\napplication_name\nDefault: null\nModifiable: yes\n\nAn arbitrary application name that can be set to identify an application that connects to a CrateDB node.\n\nSome clients set this implicitly to their client name.\n\nstatement_timeout\nDefault: '0'\nModifiable: yes\n\nThe maximum duration of any statement before it gets cancelled. If 0 (the default), queries are allowed to run infinitely and don’t get cancelled automatically.\n\nThe value is an INTERVAL with a maximum of 2147483647 milliseconds. That’s roughly 24 days.\n\nmemory.operation_limit\nDefault: 0\nModifiable: yes\n\nThis is an experimental expert setting defining the maximal amount of memory in bytes that an individual operation can consume before triggering an error.\n\n0 means unlimited. In that case only the global circuit breaker limits apply.\n\nThere is no 1:1 mapping from SQL statement to operation. Some SQL statements have no corresponding operation. Other SQL statements can have more than one operation. You can use the sys.operations view to get some insights, but keep in mind that both, operations which are used to execute a query, and their name could change with any release, including hotfix releases.\n\nenable_hashjoin\nDefault: true\nModifiable: yes\n\nAn experimental setting which enables CrateDB to consider whether a JOIN operation should be evaluated using the HashJoin implementation instead of the Nested-Loops implementation.\n\nNote\n\nIt is not always possible or efficient to use the HashJoin implementation. Having this setting enabled, will only add the option of considering it, it will not guarantee it. See also the available join algorithms for more insights on this topic.\n\nerror_on_unknown_object_key\nDefault: true\nModifiable: yes\n\nThis setting controls the behaviour of querying unknown object keys to dynamic objects. CrateDB will throw an error by default if any of the queried object keys are unknown or will return a null if the setting is set to false.\n\ndatestyle\nDefault: ISO\nModifiable: yes\n\nShows the display format for date and time values. Only the ISO style is supported. Optionally provided pattern conventions for the order of date parts (Day, Month, Year) are ignored.\n\nNote\n\nThe session setting currently has no effect in CrateDB and exists for compatibility with PostgreSQL. Trying to set this to a date format style other than ISO will raise an exception.\n\nmax_index_keys\nDefault: 32\nModifiable: no\n\nShows the maximum number of index keys.\n\nNote\n\nThe session setting has no effect in CrateDB and exists for compatibility with PostgreSQL.\n\nmax_identifier_length\nDefault: 255\nModifiable: no\n\nShows the maximum length of identifiers in bytes.\n\nserver_version_num\nDefault: 100500\nModifiable: no\n\nShows the emulated PostgreSQL server version.\n\nserver_version\nDefault: 10.5\nModifiable: no\n\nShows the emulated PostgreSQL server version.\n\nstandard_conforming_strings\nDefault: on\nModifiable: no\n\nCauses '...' strings to treat backslashes literally.\n\noptimizer\nDefault: true\nModifiable: yes\n\nThis setting indicates whether a query optimizer rule is activated. The name of the query optimizer rule has to be provided as a suffix as part of the setting e.g. SET optimizer_rewrite_collect_to_get = false.\n\nNote\n\nThe optimizer setting is for advanced use only and can significantly impact the performance behavior of the queries.\n\noptimizer_eliminate_cross_join\nDefault: true\nModifiable: yes\n\nThis setting indicates if the cross join elimination rule of the optimizer rule is activated.\n\nWarning\n\nExperimental session settings might be removed in the future even in minor feature releases."
  },
  {
    "title": "Cluster-wide settings — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/config/cluster.html",
    "html": "5.6\nCluster-wide settings\n\nAll current applied cluster settings can be read by querying the sys.cluster.settings column. Most cluster settings can be changed at runtime. This is documented at each setting.\n\nTable of contents\n\nNon-runtime cluster-wide settings\n\nCollecting stats\n\nShard limits\n\nUsage data collector\n\nGraceful stop\n\nBulk operations\n\nDiscovery\n\nUnicast host discovery\n\nDiscovery via DNS\n\nDiscovery on Amazon EC2\n\nRouting allocation\n\nShard balancing\n\nAttribute-based shard allocation\n\nCluster-wide attribute awareness\n\nCluster-wide attribute filtering\n\nDisk-based shard allocation\n\nRecovery\n\nMemory management\n\nQuery circuit breaker\n\nRequest circuit breaker\n\nAccounting circuit breaker\n\nStats circuit breakers\n\nTotal circuit breaker\n\nThread pools\n\nSettings for fixed thread pools\n\nOverload Protection\n\nMetadata\n\nMetadata gateway\n\nLogical Replication\n\nNon-runtime cluster-wide settings\n\nCluster wide settings which cannot be changed at runtime need to be specified in the configuration of each node in the cluster.\n\nCaution\n\nCluster settings specified via node configurations are required to be exactly the same on every node in the cluster for proper operation of the cluster.\n\nCollecting stats\nstats.enabled\nDefault: true\nRuntime: yes\n\nA boolean indicating whether or not to collect statistical information about the cluster.\n\nCaution\n\nThe collection of statistical information incurs a slight performance penalty, as details about every job and operation across the cluster will cause data to be inserted into the corresponding system tables.\n\nstats.jobs_log_size\nDefault: 10000\nRuntime: yes\n\nThe maximum number of job records kept to be kept in the sys.jobs_log table on each node.\n\nA job record corresponds to a single SQL statement to be executed on the cluster. These records are used for performance analytics. A larger job log produces more comprehensive stats, but uses more RAM.\n\nOlder job records are deleted as newer records are added, once the limit is reached.\n\nSetting this value to 0 disables collecting job information.\n\nstats.jobs_log_expiration\nDefault: 0s (disabled)\nRuntime: yes\n\nThe job record expiry time in seconds.\n\nJob records in the sys.jobs_log table are periodically cleared if they are older than the expiry time. This setting overrides stats.jobs_log_size.\n\nIf the value is set to 0, time based log entry eviction is disabled.\n\nNote\n\nIf both the stats.operations_log_size and stats.operations_log_expiration settings are disabled, jobs will not be recorded.\n\nstats.jobs_log_filter\nDefault: true (Include everything)\nRuntime: yes\n\nAn expression to determine if a job should be recorded into sys.jobs_log. The expression must evaluate to a boolean. If it evaluates to true the statement will show up in sys.jobs_log until it’s evicted due to one of the other rules. (expiration or size limit reached).\n\nThe expression may reference all columns contained in sys.jobs_log. A common use case is to include only jobs that took a certain amount of time to execute:\n\ncr> SET GLOBAL \"stats.jobs_log_filter\" = $$ended - started > '5 minutes'::interval$$;\n\nstats.jobs_log_persistent_filter\nDefault: false (Include nothing)\nRuntime: yes\n\nAn expression to determine if a job should also be recorded to the regular CrateDB log. Entries that match this filter will be logged under the StatementLog logger with the INFO level.\n\nThis is similar to stats.jobs_log_filter except that these entries are persisted to the log file. This should be used with caution and shouldn’t be set to an expression that matches many queries as the logging operation will block on IO and can therefore affect performance.\n\nA common use case is to use this for slow query logging.\n\nstats.operations_log_size\nDefault: 10000\nRuntime: yes\n\nThe maximum number of operations records to be kept in the sys.operations_log table on each node.\n\nA job consists of one or more individual operations. Operations records are used for performance analytics. A larger operations log produces more comprehensive stats, but uses more RAM.\n\nOlder operations records are deleted as newer records are added, once the limit is reached.\n\nSetting this value to 0 disables collecting operations information.\n\nstats.operations_log_expiration\nDefault: 0s (disabled)\nRuntime: yes\n\nEntries of sys.operations_log are cleared by a periodically job when they are older than the specified expire time. This setting overrides stats.operations_log_size. If the value is set to 0 the time based log entry eviction is disabled.\n\nNote\n\nIf both settings stats.operations_log_size and stats.operations_log_expiration are disabled, no job information will be collected.\n\nstats.service.interval\nDefault: 24h\nRuntime: yes\n\nDefines the refresh interval to refresh tables statistics used to produce optimal query execution plans.\n\nThis field expects a time value either as a bigint or double precision or alternatively as a string literal with a time suffix (ms, s, m, h, d, w).\n\nIf the value provided is 0 then the refresh is disabled.\n\nCaution\n\nUsing a very small value can cause a high load on the cluster.\n\nstats.service.max_bytes_per_sec\nDefault: 40mb\nRuntime: yes\n\nSpecifies the maximum number of bytes per second that can be read on data nodes to collect statistics. If this is set to a positive number, the underlying I/O operations of the ANALYZE statement are throttled.\n\nIf the value provided is 0 then the throttling is disabled.\n\nShard limits\ncluster.max_shards_per_node\nDefault: 1000\nRuntime: yes\n\nThe maximum amount of shards per node.\n\nAny operations that would result in the creation of additional shard copies that would exceed this limit are rejected.\n\nFor example. If you have 999 shards in the current cluster and you try to create a new table, the create table operation will fail.\n\nSimilarly, if a write operation would lead to the creation of a new partition, the statement will fail.\n\nEach shard on a node requires some memory and increases the size of the cluster state. Having too many shards per node will impact the clusters stability and it is therefore discouraged to raise the limit above 1000.\n\nNote\n\nThe maximum amount of shards per node setting is also used for the Maximum shards per node check.\n\nUsage data collector\n\nThe settings of the Usage Data Collector are read-only and cannot be set during runtime. Please refer to Usage Data Collector to get further information about its usage.\n\nudc.enabled\nDefault: true\nRuntime: no\n\ntrue: Enables the Usage Data Collector.\n\nfalse: Disables the Usage Data Collector.\n\nudc.initial_delay\nDefault: 10m\nRuntime: no\n\nThe delay for first ping after start-up.\n\nThis field expects a time value either as a bigint or double precision or alternatively as a string literal with a time suffix (ms, s, m, h, d, w).\n\nudc.interval\nDefault: 24h\nRuntime: no\n\nThe interval a UDC ping is sent.\n\nThis field expects a time value either as a bigint or double precision or alternatively as a string literal with a time suffix (ms, s, m, h, d, w).\n\nudc.url\nDefault: https://udc.crate.io\nRuntime: no\n\nThe URL the ping is sent to.\n\nGraceful stop\n\nBy default, when the CrateDB process stops it simply shuts down, possibly making some shards unavailable which leads to a red cluster state and lets some queries fail that required the now unavailable shards. In order to safely shutdown a CrateDB node, the graceful stop procedure can be used.\n\nThe following cluster settings can be used to change the shutdown behaviour of nodes of the cluster:\n\ncluster.graceful_stop.min_availability\nDefault: primaries\nRuntime: yes\nAllowed values: none | primaries | full\n\nnone: No minimum data availability is required. The node may shut down even if records are missing after shutdown.\n\nprimaries: At least all primary shards need to be available after the node has shut down. Replicas may be missing.\n\nfull: All records and all replicas need to be available after the node has shut down. Data availability is full.\n\nNote\n\nThis option is ignored if there is only 1 node in a cluster!\n\ncluster.graceful_stop.timeout\nDefault: 2h\nRuntime: yes\n\nDefines the maximum waiting time in milliseconds for the reallocation process to finish. The force setting will define the behaviour when the shutdown process runs into this timeout.\n\nThe timeout expects a time value either as a bigint or double precision or alternatively as a string literal with a time suffix (ms, s, m, h, d, w).\n\ncluster.graceful_stop.force\nDefault: false\nRuntime: yes\n\nDefines whether graceful stop should force stopping of the node if it runs into the timeout which is specified with the cluster.graceful_stop.timeout setting.\n\nBulk operations\n\nSQL DML Statements involving a huge amount of rows like COPY FROM, INSERT or UPDATE can take an enormous amount of time and resources. The following settings change the behaviour of those queries.\n\nbulk.request_timeout\nDefault: 1m\nRuntime: yes\n\nDefines the timeout of internal shard-based requests involved in the execution of SQL DML Statements over a huge amount of rows.\n\nDiscovery\n\nData sharding and work splitting are at the core of CrateDB. This is how we manage to execute very fast queries over incredibly large datasets. In order for multiple CrateDB nodes to work together a cluster needs to be formed. The process of finding other nodes with which to form a cluster is called discovery. Discovery runs when a CrateDB node starts and when a node is not able to reach the master node and continues until a master node is found or a new master node is elected.\n\ndiscovery.seed_hosts\nDefault: 127.0.0.1\nRuntime: no\n\nIn order to form a cluster with CrateDB instances running on other nodes a list of seed master-eligible nodes needs to be provided. This setting should normally contain the addresses of all the master-eligible nodes in the cluster. In order to seed the discovery process the nodes listed here must be live and contactable. This setting contains either an array of hosts or a comma-delimited string. By default a node will bind to the available loopback and scan for local ports between 4300 and 4400 to try to connect to other nodes running on the same server. This default behaviour provides local auto clustering without any configuration. Each value should be in the form of host:port or host (where port defaults to the setting transport.tcp.port).\n\nNote\n\nIPv6 hosts must be bracketed.\n\ncluster.initial_master_nodes\nDefault: not set\nRuntime: no\n\nContains a list of node names, full-qualified hostnames or IP addresses of the master-eligible nodes which will vote in the very first election of a cluster that’s bootstrapping for the first time. By default this is not set, meaning it expects this node to join an already formed cluster. In development mode, with no discovery settings configured, this step is performed by the nodes themselves, but this auto-bootstrapping is designed to aim development and is not safe for production. In production you must explicitly list the names or IP addresses of the master-eligible nodes whose votes should be counted in the very first election.\n\ndiscovery.type\nDefault: zen\nRuntime: no\nAllowed values: zen | single-node\n\nSpecifies whether CrateDB should form a multiple-node cluster. By default, CrateDB discovers other nodes when forming a cluster and allows other nodes to join the cluster later. If discovery.type is set to single-node, CrateDB forms a single-node cluster and the node won’t join any other clusters. This can be useful for testing. It is not recommend to use this for production setups. The single-node mode also skips bootstrap checks.\n\nCaution\n\nIf a node is started without any initial_master_nodes or a discovery_type set to single-node (e.g., the default configuration), it will never join a cluster even if the configuration is subsequently changed.\n\nIt is possible to force the node to forget its current cluster state by using the crate-node CLI tool. However, be aware that this may result in data loss.\n\nUnicast host discovery\n\nAs described above, CrateDB has built-in support for statically specifying a list of addresses that will act as the seed nodes in the discovery process using the discovery.seed_hosts setting.\n\nCrateDB also has support for several different mechanisms of seed nodes discovery. Currently there are two other discovery types: via DNS and via EC2 API.\n\nWhen a node starts up with one of these discovery types enabled, it performs a lookup using the settings for the specified mechanism listed below. The hosts and ports retrieved from the mechanism will be used to generate a list of unicast hosts for node discovery.\n\nThe same lookup is also performed by all nodes in a cluster whenever the master is re-elected (see Cluster Meta Data).\n\ndiscovery.seed_providers\nDefault: not set\nRuntime: no\nAllowed values: srv, ec2\n\nSee also: Discovery.\n\nDiscovery via DNS\n\nCrate has built-in support for discovery via DNS. To enable DNS discovery the discovery.seed_providers setting needs to be set to srv.\n\nThe order of the unicast hosts is defined by the priority, weight and name of each host defined in the SRV record. For example:\n\n_crate._srv.example.com. 3600 IN SRV 2 20 4300 crate1.example.com.\n_crate._srv.example.com. 3600 IN SRV 1 10 4300 crate2.example.com.\n_crate._srv.example.com. 3600 IN SRV 2 10 4300 crate3.example.com.\n\n\nwould result in a list of discovery nodes ordered like:\n\ncrate2.example.com:4300, crate3.example.com:4300, crate1.example.com:4300\n\ndiscovery.srv.query\nRuntime: no\n\nThe DNS query that is used to look up SRV records, usually in the format _service._protocol.fqdn If not set, the service discovery will not be able to look up any SRV records.\n\ndiscovery.srv.resolver\nRuntime: no\n\nThe hostname or IP of the DNS server used to resolve DNS records. If this is not set, or the specified hostname/IP is not resolvable, the default (system) resolver is used.\n\nOptionally a custom port can be specified using the format hostname:port.\n\nDiscovery on Amazon EC2\n\nCrateDB has built-in support for discovery via the EC2 API. To enable EC2 discovery the discovery.seed_providers settings needs to be set to ec2.\n\ndiscovery.ec2.access_key\nRuntime: no\n\nThe access key ID to identify the API calls.\n\ndiscovery.ec2.secret_key\nRuntime: no\n\nThe secret key to identify the API calls.\n\nFollowing settings control the discovery:\n\ndiscovery.ec2.groups\nRuntime: no\n\nA list of security groups; either by ID or name. Only instances with the given group will be used for unicast host discovery.\n\ndiscovery.ec2.any_group\nDefault: true\nRuntime: no\n\nDefines whether all (false) or just any (true) security group must be present for the instance to be used for discovery.\n\ndiscovery.ec2.host_type\nDefault: private_ip\nRuntime: no\nAllowed values: private_ip, public_ip, private_dns, public_dns\n\nDefines via which host type to communicate with other instances.\n\ndiscovery.ec2.availability_zones\nRuntime: no\n\nA list of availability zones. Only instances within the given availability zone will be used for unicast host discovery.\n\ndiscovery.ec2.tag.<name>\nRuntime: no\n\nEC2 instances for discovery can also be filtered by tags using the discovery.ec2.tag. prefix plus the tag name.\n\nE.g. to filter instances that have the environment tags with the value dev your setting will look like: discovery.ec2.tag.environment: dev.\n\ndiscovery.ec2.endpoint\nRuntime: no\n\nIf you have your own compatible implementation of the EC2 API service you can set the endpoint that should be used.\n\nRouting allocation\ncluster.routing.allocation.enable\nDefault: all\nRuntime: yes\nAllowed values: all | none | primaries | new_primaries\n\nall allows all shard allocations, the cluster can allocate all kinds of shards.\n\nnone allows no shard allocations at all. No shard will be moved or created.\n\nprimaries only primaries can be moved or created. This includes existing primary shards.\n\nnew_primaries allows allocations for new primary shards only. This means that for example a newly added node will not allocate any replicas. However it is still possible to allocate new primary shards for new indices. Whenever you want to perform a zero downtime upgrade of your cluster you need to set this value before gracefully stopping the first node and reset it to all after starting the last updated node.\n\nNote\n\nThis allocation setting has no effect on the recovery of primary shards! Even when cluster.routing.allocation.enable is set to none, nodes will recover their unassigned local primary shards immediately after restart.\n\ncluster.routing.rebalance.enable\nDefault: all\nRuntime: yes\nAllowed values: all | none | primaries | replicas\n\nEnables or disables rebalancing for different types of shards:\n\nall allows shard rebalancing for all types of shards.\n\nnone disables shard rebalancing for any types.\n\nprimaries allows shard rebalancing only for primary shards.\n\nreplicas allows shard rebalancing only for replica shards.\n\ncluster.routing.allocation.allow_rebalance\nDefault: indices_all_active\nRuntime: yes\nAllowed values: always | indices_primary_active | indices_all_active\n\nDefines when rebalancing will happen based on the total state of all the indices shards in the cluster.\n\nDefaults to indices_all_active to reduce chatter during initial recovery.\n\ncluster.routing.allocation.cluster_concurrent_rebalance\nDefault: 2\nRuntime: yes\n\nDefines how many concurrent rebalancing tasks are allowed across all nodes.\n\ncluster.routing.allocation.node_initial_primaries_recoveries\nDefault: 4\nRuntime: yes\n\nDefines how many concurrent primary shard recoveries are allowed on a node.\n\nSince primary recoveries use data that is already on disk (as opposed to inter-node recoveries), recovery should be fast and so this setting can be higher than node_concurrent_recoveries.\n\ncluster.routing.allocation.node_concurrent_recoveries\nDefault: 2\nRuntime: yes\n\nDefines how many concurrent recoveries are allowed on a node.\n\nShard balancing\n\nYou can configure how CrateDB attempts to balance shards across a cluster by specifying one or more property weights. CrateDB will consider a cluster to be balanced when no further allowed action can bring the weighted properties of each node closer together.\n\nNote\n\nBalancing may be restricted by other settings (e.g., attribute-based and disk-based shard allocation).\n\ncluster.routing.allocation.balance.shard\nDefault: 0.45f\nRuntime: yes\n\nDefines the weight factor for shards allocated on a node (float). Raising this raises the tendency to equalize the number of shards across all nodes in the cluster.\n\ncluster.routing.allocation.balance.index\nDefault: 0.55f\nRuntime: yes\n\nDefines a factor to the number of shards per index allocated on a specific node (float). Increasing this value raises the tendency to equalize the number of shards per index across all nodes in the cluster.\n\ncluster.routing.allocation.balance.threshold\nDefault: 1.0f\nRuntime: yes\n\nMinimal optimization value of operations that should be performed (non negative float). Increasing this value will cause the cluster to be less aggressive about optimising the shard balance.\n\nAttribute-based shard allocation\n\nYou can control how shards are allocated to specific nodes by setting custom attributes on each node (e.g., server rack ID or node availability zone). After doing this, you can define cluster-wide attribute awareness and then configure cluster-wide attribute filtering.\n\nSee Also\n\nFor an in-depth example of using custom node attributes, check out the multi-zone setup how-to guide.\n\nCluster-wide attribute awareness\n\nTo make use of custom attributes for attribute-based shard allocation, you must configure cluster-wide attribute awareness.\n\ncluster.routing.allocation.awareness.attributes\nRuntime: no\n\nYou may define custom node attributes which can then be used to do awareness based on the allocation of a shard and its replicas.\n\nFor example, let’s say we want to use an attribute named rack_id. We start two nodes with node.attr.rack_id set to rack_one. Then we create a single table with five shards and one replica. The table will be fully deployed on the current nodes (five shards and one replica each, making a total of 10 shards).\n\nNow, if we start two more nodes with node.attr.rack_id set to rack_two, CrateDB will relocate shards to even out the number of shards across the nodes. However, a shard and its replica will not be allocated to nodes sharing the same rack_id value.\n\nThe awareness.attributes setting supports using several values.\n\ncluster.routing.allocation.awareness.force.*.values\nRuntime: no\n\nAttributes on which shard allocation will be forced. Here, * is a placeholder for the awareness attribute, which can be configured using the cluster.routing.allocation.awareness.attributes setting.\n\nFor example, let’s say we configured forced shard allocation for an awareness attribute named zone with values set to zone1, zone2. Start two nodes with node.attr.zone set to zone1. Then, create a table with five shards and one replica. The table will be created, but only five shards will be allocated (with no replicas). The replicas will only be allocated when we start one or more nodes with node.attr.zone set to zone2.\n\nCluster-wide attribute filtering\n\nTo control how CrateDB uses custom attributes for attribute-based shard allocation, you must configure cluster-wide attribute filtering.\n\nNote\n\nCrateDB will retroactively enforce filter definitions. If a new filter would prevent newly created matching shards from being allocated to a node, CrateDB would also move any existing matching shards away from that node.\n\ncluster.routing.allocation.include.*\nRuntime: yes\n\nOnly allocate shards on nodes where at least one of the specified values matches the attribute.\n\nFor example:\n\ncluster.routing.allocation.include.zone: \"zone1,zone2\"`\n\ncluster.routing.allocation.exclude.*\nRuntime: yes\n\nOnly allocate shards on nodes where none of the specified values matches the attribute.\n\nFor example:\n\ncluster.routing.allocation.exclude.zone: \"zone1\"\n\ncluster.routing.allocation.require.*\nRuntime: yes\n\nUsed to specify a number of rules, which all of them must match for a node in order to allocate a shard on it.\n\nDisk-based shard allocation\ncluster.routing.allocation.disk.threshold_enabled\nDefault: true\nRuntime: yes\n\nPrevent shard allocation on nodes depending of the disk usage.\n\ncluster.routing.allocation.disk.watermark.low\nDefault: 85%\nRuntime: yes\n\nDefines the lower disk threshold limit for shard allocations. New shards will not be allocated on nodes with disk usage greater than this value. It can also be set to an absolute bytes value (like e.g. 500mb) to prevent the cluster from allocating new shards on node with less free disk space than this value.\n\ncluster.routing.allocation.disk.watermark.high\nDefault: 90%\nRuntime: yes\n\nDefines the higher disk threshold limit for shard allocations. The cluster will attempt to relocate existing shards to another node if the disk usage on a node rises above this value. It can also be set to an absolute bytes value (like e.g. 500mb) to relocate shards from nodes with less free disk space than this value.\n\ncluster.routing.allocation.disk.watermark.flood_stage\nDefault: 95%\nRuntime: yes\n\nDefines the threshold on which CrateDB enforces a read-only block on every index that has at least one shard allocated on a node with at least one disk exceeding the flood stage.\n\nNote\n\nblocks.read_only_allow_delete setting is automatically reset to FALSE for the tables if the disk space is freed and the threshold is undershot.\n\ncluster.routing.allocation.disk.watermark settings may be defined as percentages or bytes values. However, it is not possible to mix the value types.\n\nBy default, the cluster will retrieve information about the disk usage of the nodes every 30 seconds. This can also be changed by setting the cluster.info.update.interval setting.\n\nNote\n\nThe watermark settings are also used for the Routing allocation disk watermark low and Routing allocation disk watermark high node check.\n\nSetting cluster.routing.allocation.disk.threshold_enabled to false will disable the allocation decider, but the node checks will still be active and warn users about running low on disk space.\n\ncluster.routing.allocation.total_shards_per_node\nDefault: -1\nRuntime: yes\n\nLimits the number of shards that can be allocated per node. A value of -1 means unlimited.\n\nSetting this to 1000, for example, will prevent CrateDB from assigning more than 1000 shards per node. A node with 1000 shards would be excluded from allocation decisions and CrateDB would attempt to allocate shards to other nodes, or leave shards unassigned if no suitable node can be found.\n\nRecovery\nindices.recovery.max_bytes_per_sec\nDefault: 40mb\nRuntime: yes\n\nSpecifies the maximum number of bytes that can be transferred during shard recovery per seconds. Limiting can be disabled by setting it to 0. This setting allows to control the network usage of the recovery process. Higher values may result in higher network utilization, but also faster recovery process.\n\nindices.recovery.retry_delay_state_sync\nDefault: 500ms\nRuntime: yes\n\nDefines the time to wait after an issue caused by cluster state syncing before retrying to recover.\n\nindices.recovery.retry_delay_network\nDefault: 5s\nRuntime: yes\n\nDefines the time to wait after an issue caused by the network before retrying to recover.\n\nindices.recovery.internal_action_timeout\nDefault: 15m\nRuntime: yes\n\nDefines the timeout for internal requests made as part of the recovery.\n\nindices.recovery.internal_action_long_timeout\nDefault: 30m\nRuntime: yes\n\nDefines the timeout for internal requests made as part of the recovery that are expected to take a long time. Defaults to twice internal_action_timeout.\n\nindices.recovery.recovery_activity_timeout\nDefault: 30m\nRuntime: yes\n\nRecoveries that don’t show any activity for more then this interval will fail. Defaults to internal_action_long_timeout.\n\nindices.recovery.max_concurrent_file_chunks\nDefault: 2\nRuntime: yes\n\nControls the number of file chunk requests that can be sent in parallel per recovery. As multiple recoveries are already running in parallel, controlled by cluster.routing.allocation.node_concurrent_recoveries, increasing this expert-level setting might only help in situations where peer recovery of a single shard is not reaching the total inbound and outbound peer recovery traffic as configured by indices.recovery.max_bytes_per_sec, but is CPU-bound instead, typically when using transport-level security or compression.\n\nMemory management\nmemory.allocation.type\nDefault: on-heap\nRuntime: yes\n\nSupported values are on-heap and off-heap. This influences if memory is preferably allocated in the heap space or in the off-heap/direct memory region.\n\nSetting this to off-heap doesn’t imply that the heap won’t be used anymore. Most allocations will still happen in the heap space but some operations will be allowed to utilize off heap buffers.\n\nWarning\n\nUsing off-heap is considered experimental.\n\nmemory.operation_limit\nDefault: 0\nRuntime: yes\n\nDefault value for the memory.operation_limit session setting. Changing the cluster setting will only affect new sessions, not existing sessions.\n\nQuery circuit breaker\n\nThe Query circuit breaker will keep track of the used memory during the execution of a query. If a query consumes too much memory or if the cluster is already near its memory limit it will terminate the query to ensure the cluster keeps working.\n\nindices.breaker.query.limit\nDefault: 60%\nRuntime: yes\n\nSpecifies the limit for the query breaker. Provided values can either be absolute values (interpreted as a number of bytes), byte sizes (like 1mb) or percentage of the heap size (like 12%). A value of -1 disables breaking the circuit while still accounting memory usage.\n\nRequest circuit breaker\n\nThe request circuit breaker allows an estimation of required heap memory per request. If a single request exceeds the specified amount of memory, an exception is raised.\n\nindices.breaker.request.limit\nDefault: 60%\nRuntime: yes\n\nSpecifies the JVM heap limit for the request circuit breaker.\n\nAccounting circuit breaker\n\nTracks things that are held in memory independent of queries. For example the memory used by Lucene for segments.\n\nindices.breaker.accounting.limit\nDefault: 100%\nRuntime: yes\n\nSpecifies the JVM heap limit for the accounting circuit breaker\n\nCaution\n\nThis setting is deprecated and will be removed in a future release.\n\nStats circuit breakers\n\nSettings that control the behaviour of the stats circuit breaker. There are two breakers in place, one for the jobs log and one for the operations log. For each of them, the breaker limit can be set.\n\nstats.breaker.log.jobs.limit\nDefault: 5%\nRuntime: yes\n\nThe maximum memory that can be used from CRATE_HEAP_SIZE for the sys.jobs_log table on each node.\n\nWhen this memory limit is reached, the job log circuit breaker logs an error message and clears the sys.jobs_log table completely.\n\nstats.breaker.log.operations.limit\nDefault: 5%\nRuntime: yes\n\nThe maximum memory that can be used from CRATE_HEAP_SIZE for the sys.operations_log table on each node.\n\nWhen this memory limit is reached, the operations log circuit breaker logs an error message and clears the sys.operations_log table completely.\n\nTotal circuit breaker\nindices.breaker.total.limit\nDefault: 95%\nRuntime: yes\n\nThe maximum memory that can be used by all aforementioned circuit breakers together.\n\nEven if an individual circuit breaker doesn’t hit its individual limit, queries might still get aborted if several circuit breakers together would hit the memory limit configured in indices.breaker.total.limit.\n\nThread pools\n\nEvery node uses a number of thread pools to schedule operations, each pool is dedicated to specific operations. The most important pools are:\n\nwrite: Used for write operations like index, update or delete. The type defaults to fixed.\n\nsearch: Used for read operations like SELECT statements. The type defaults to fixed.\n\nget: Used for some specific read operations. For example on tables like sys.shards or sys.nodes. The type defaults to fixed.\n\nrefresh: Used for refresh operations. The type defaults to scaling.\n\ngeneric: For internal tasks like cluster state management. The type defaults to scaling.\n\nlogical_replication: For logical replication operations. The type defaults to fixed.\n\nIn addition to those pools, there are also netty worker threads which are used to process network requests and many CPU bound actions like query analysis and optimization.\n\nThe thread pool settings are expert settings which you generally shouldn’t need to touch. They are dynamically sized depending on the number of available CPU cores. If you’re running multiple services on the same machine you instead should change the processors setting.\n\nIncreasing the number of threads for a pool can result in degraded performance due to increased context switching and higher memory footprint.\n\nIf you observe idle CPU cores increasing the thread pool size is rarely the right course of action, instead it can be a sign that:\n\nOperations are blocked on disk IO. Increasing the thread pool size could result in more operations getting queued and blocked on disk IO without increasing throughput but decreasing it due to more memory pressure and additional garbage collection activity.\n\nIndividual operations running single threaded. Not all tasks required to process a SQL statement can be further subdivided and processed in parallel, but many operations default to use one thread per shard. Because of this, you can consider increasing the number of shards of a table to increase the parallelism of a single individual statement and increase CPU core utilization. As an alternative you can try increasing the concurrency on the client side, to have CrateDB process more SQL statements in parallel.\n\nthread_pool.<name>.type\nRuntime: no\nAllowed values: fixed | scaling\n\nfixed holds a fixed size of threads to handle the requests. It also has a queue for pending requests if no threads are available.\n\nscaling ensures that a thread pool holds a dynamic number of threads that are proportional to the workload.\n\nSettings for fixed thread pools\n\nIf the type of a thread pool is set to fixed there are a few optional settings.\n\nthread_pool.<name>.size\nRuntime: no\n\nNumber of threads. The default size of the different thread pools depend on the number of available CPU cores.\n\nthread_pool.<name>.queue_size\nDefault write: 200\nDefault search: 1000\nDefault get: 100\nRuntime: no\n\nSize of the queue for pending requests. A value of -1 sets it to unbounded. If you have burst workloads followed by periods of inactivity it can make sense to increase the queue_size to allow a node to buffer more queries before rejecting new operations. But be aware, increasing the queue size if you have sustained workloads will only increase the system’s memory consumption and likely degrade performance.\n\nOverload Protection\n\nOverload protection settings control how many resources operations like INSERT INTO FROM QUERY or COPY can use.\n\nThe values here serve as a starting point for an algorithm that dynamically adapts the effective concurrency limit based on the round-trip time of requests. Whenever one of these settings is updated, the previously calculated effective concurrency is reset.\n\nChanging settings will only effect new operations, already running operations will continue with the previous settings.\n\noverload_protection.dml.initial_concurrency\nDefault: 5\nRuntime: yes\n\nThe initial number of concurrent operations allowed per target node.\n\noverload_protection.dml.min_concurrency\nDefault: 1\nRuntime: yes\n\nThe minimum number of concurrent operations allowed per target node.\n\noverload_protection.dml.max_concurrency\nDefault: 2000\nRuntime: yes\n\nThe maximum number of concurrent operations allowed per target node.\n\noverload_protection.dml.queue_size\nDefault: 200\nRuntime: yes\n\nHow many operations are allowed to queue up.\n\nMetadata\ncluster.info.update.interval\nDefault: 30s\nRuntime: yes\n\nDefines how often the cluster collect metadata information (e.g. disk usages etc.) if no concrete event is triggered.\n\nMetadata gateway\n\nThe following settings can be used to configure the behavior of the metadata gateway.\n\ngateway.expected_nodes\nDefault: -1\nRuntime: no\n\nThe setting gateway.expected_nodes defines the total number of nodes expected in the cluster. It is evaluated together with gateway.recover_after_nodes to decide if the cluster can start with recovery.\n\nCaution\n\nThis setting is deprecated and will be removed in a future version. Use gateway.expected_data_nodes instead.\n\ngateway.expected_data_nodes\nDefault: -1\nRuntime: no\n\nThe setting gateway.expected_data_nodes defines the total number of data nodes expected in the cluster. It is evaluated together with gateway.recover_after_data_nodes to decide if the cluster can start with recovery.\n\ngateway.recover_after_time\nDefault: 5m\nRuntime: no\n\nThe gateway.recover_after_time setting defines the time to wait for the number of nodes set in gateway.expected_data_nodes (or gateway.expected_nodes) to become available, before starting the recovery, once the number of nodes defined in gateway.recover_after_data_nodes (or gateway.recover_after_nodes) has already been reached. This setting is ignored if gateway.expected_data_nodes or gateway.expected_nodes are set to 0 or 1. It also has no effect if gateway.recover_after_data_nodes is set equal to gateway.expected_data_nodes (or gateway.recover_after_nodes is set equal to gateway.expected_nodes). The cluster also proceeds to immediate recovery, and the default 5 minutes waiting time does not apply, if neither this setting nor expected_nodes and expected_data_nodes are explicitly set.\n\ngateway.recover_after_nodes\nDefault: -1\nRuntime: no\n\nThe gateway.recover_after_nodes setting defines the number of nodes that need to join the cluster before the cluster state recovery can start. If this setting is -1 and gateway.expected_nodes is set, all nodes will need to be started before the cluster state recovery can start. Please note that proceeding with recovery when not all nodes are available could trigger the promotion of shards and the creation of new replicas, generating disk and network load, which may be unnecessary. You can use a combination of this setting with gateway.recovery_after_time to mitigate this risk.\n\nCaution\n\nThis setting is deprecated and will be removed in CrateDB 5.0. Use gateway.recover_after_data_nodes instead.\n\ngateway.recover_after_data_nodes\nDefault: -1\nRuntime: no\n\nThe gateway.recover_after_data_nodes setting defines the number of data nodes that need to be started before the cluster state recovery can start. If this setting is -1 and gateway.expected_data_nodes is set, all data nodes will need to be started before the cluster state recovery can start. Please note that proceeding with recovery when not all data nodes are available could trigger the promotion of shards and the creation of new replicas, generating disk and network load, which may be unnecessary. You can use a combination of this setting with gateway.recovery_after_time to mitigate this risk.\n\nLogical Replication\n\nReplication process can be configured by the following settings. Settings are dynamic and can be changed in runtime.\n\nreplication.logical.ops_batch_size\nDefault: 50000\nMin value: 16\nRuntime: yes\n\nMaximum number of operations to replicate from the publisher cluster per poll. Represents a number to advance a sequence.\n\nreplication.logical.reads_poll_duration\nDefault: 50\nRuntime: yes\n\nThe maximum time (in milliseconds) to wait for changes per poll operation. When a subscriber makes another one request to a publisher, it has reads_poll_duration milliseconds to harvest changes from the publisher.\n\nreplication.logical.recovery.chunk_size\nDefault: 1MB\nMin value: 1KB\nMax value: 1GB\nRuntime: yes\n\nChunk size to transfer files during the initial recovery of a replicating table.\n\nreplication.logical.recovery.max_concurrent_file_chunks\nDefault: 2\nMin value: 1\nMax value: 5\nRuntime: yes\n\nControls the number of file chunk requests that can be sent in parallel between clusters during the recovery."
  },
  {
    "title": "Resiliency — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/concepts/resiliency.html",
    "html": "5.6\nResiliency\n\nDistributed systems are tricky. All sorts of things can go wrong that are beyond your control. The network can go away, disks can fail, hosts can be terminated unexpectedly. CrateDB tries very hard to cope with these sorts of issues while maintaining availability, consistency, and durability.\n\nHowever, as with any distributed system, sometimes, rarely, things can go wrong.\n\nThankfully, for most use-cases, if you follow best practices, you are extremely unlikely to experience resiliency issues with CrateDB.\n\nSee Also\n\nAppendix: Resiliency Issues\n\nTable of contents\n\nMonitoring cluster status\n\nStorage and consistency\n\nDeployment strategies\n\nMonitoring cluster status\n\nThe Admin UI in CrateDB has a status indicator which can be used to determine the stability and health of a cluster.\n\nA green status indicates that all shards have been replicated, are available, and are not being relocated. This is the lowest risk status for a cluster. The status will turn yellow when there is an elevated risk of encountering issues, due to a network failure or the failure of a node in the cluster.\n\nThe status is updated every few seconds (variable on your cluster ping configuration).\n\nStorage and consistency\n\nCode that expects the behavior of an ACID compliant database like MySQL may not always work as expected with CrateDB.\n\nCrateDB does not support ACID transactions, but instead has atomic operations and eventual consistency at the row level. See also Clustering.\n\nEventual consistency is the trade-off that CrateDB makes in exchange for high-availability that can tolerate most hardware and network failures. So you may observe data from different cluster nodes temporarily falling very briefly out-of-sync with each other, although over time they will become consistent.\n\nFor example, you know a row has been written as soon as you get the INSERT OK message. But that row might not be read back by a subsequent SELECT on a different node until after a table refresh (which typically occurs within one second).\n\nYour applications should be designed to work this storage and consistency model.\n\nDeployment strategies\n\nWhen deploying CrateDB you should carefully weigh your need for high-availability and disaster recovery against operational complexity and expense.\n\nWhich strategy you pick is going to depend on the specifics of your situation.\n\nHere are some considerations:\n\nCrateDB is designed to scale horizontally. Make sure that your machines are fit for purpose, i.e. use SSDs, increase RAM up to 64 GB, and use multiple CPU cores when you can. But if you want to dynamically increase (or decrease) the capacity of your cluster, add (or remove) nodes.\n\nIf availability is a concern, you can add nodes across multiple zones (e.g. different data centers or geographical regions). The more available your CrateDB cluster is, the more likely it is to withstand external failures like a zone going down.\n\nIf data durability or read performance is a concern, you can increase the number of table replicas. More table replicas means a smaller chance of permanent data loss due to hardware failures, in exchange for the use of more disk space and more intra-cluster network traffic.\n\nIf disaster recovery is important, you can take regular snapshots and store those snapshots in cold storage. This safeguards data that has already been successfully written and replicated across the cluster.\n\nCrateDB works well as part of a data pipeline, especially if you’re working with high-volume data. If you have a message queue in front of CrateDB, you can configure it with backups and replay the data flow for a specific timeframe. This can be used to recover from issues that affect your data before it has been successfully written and replicated across the cluster.\n\nIndeed, this is the generally recommended way to recover from any of the rare consistency or data-loss issues you might encounter when CrateDB experiences network or hardware failures (see next section)."
  },
  {
    "title": "Node-specific settings — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/config/node.html",
    "html": "5.6\nNode-specific settings\n\nTable of contents\n\nBasics\n\nNode types\n\nGeneral\n\nNetworking\n\nHosts\n\nPorts\n\nAdvanced TCP settings\n\nTransport settings\n\nPaths\n\nPlug-ins\n\nCPU\n\nMemory\n\nGarbage collection\n\nAuthentication\n\nTrust authentication\n\nHost-based authentication\n\nHBA entries\n\nSecured communications (SSL/TLS)\n\nCross-origin resource sharing (CORS)\n\nBlobs\n\nRepositories\n\nQueries\n\nLegacy\n\nJavaScript language\n\nCustom attributes\n\nBasics\ncluster.name\nDefault: crate\nRuntime: no\n\nThe name of the CrateDB cluster the node should join to.\n\nnode.name\nRuntime: no\n\nThe name of the node. If no name is configured a random one will be generated.\n\nNote\n\nNode names must be unique in a CrateDB cluster.\n\nnode.store.allow_mmap\nDefault: true\nRuntime: no\n\nThe setting indicates whether or not memory-mapping is allowed.\n\nNode types\n\nCrateDB supports different types of nodes.\n\nThe following settings can be used to differentiate nodes upon startup:\n\nnode.master\nDefault: true\nRuntime: no\n\nWhether or not this node is able to get elected as master node in the cluster.\n\nnode.data\nDefault: true\nRuntime: no\n\nWhether or not this node will store data.\n\nUsing different combinations of these two settings, you can create four different types of node. Each type of node is differentiated by what types of load it will handle.\n\nTabulating the truth values for node.master and node.data produces a truth table outlining the four different types of node:\n\n\t\n\nMaster\n\n\t\n\nNo master\n\n\n\n\nData\n\n\t\n\nHandle all loads.\n\n\t\n\nHandles client requests and query execution.\n\n\n\n\nNo data\n\n\t\n\nHandles cluster management.\n\n\t\n\nHandles client requests.\n\nNodes marked as node.master will only handle cluster management if they are elected as the cluster master. All other loads are shared equally.\n\nGeneral\nnode.sql.read_only\nDefault: false\nRuntime: no\n\nIf set to true, the node will only allow SQL statements which are resulting in read operations.\n\nstatement_timeout\nDefault: 0\nRuntime: yes\n\nThe maximum duration of any statement before it gets cancelled.\n\nThis value is used as default value for the statement_timeout session setting\n\nIf 0 queries are allowed to run infinitely and don’t get cancelled automatically.\n\nNote\n\nUpdating this setting won’t affect existing sessions, it will only take effect for new sessions.\n\nNetworking\nHosts\nnetwork.host\nDefault: _local_\nRuntime: no\n\nThe IP address CrateDB will bind itself to. This setting sets both the network.bind_host and network.publish_host values.\n\nnetwork.bind_host\nDefault: _local_\nRuntime: no\n\nThis setting determines to which address CrateDB should bind itself to.\n\nnetwork.publish_host\nDefault: _local_\nRuntime: no\n\nThis setting is used by a CrateDB node to publish its own address to the rest of the cluster.\n\nTip\n\nApart from IPv4 and IPv6 addresses there are some special values that can be used for all above settings:\n\n_local_\n\n\t\n\nAny loopback addresses on the system, for example 127.0.0.1.\n\n\n\n\n_site_\n\n\t\n\nAny site-local addresses on the system, for example 192.168.0.1.\n\n\n\n\n_global_\n\n\t\n\nAny globally-scoped addresses on the system, for example 8.8.8.8.\n\n\n\n\n_[INTERFACE]_\n\n\t\n\nAddresses of a network interface, for example _en0_.\n\nPorts\nhttp.port\nRuntime: no\n\nThis defines the TCP port range to which the CrateDB HTTP service will be bound to. It defaults to 4200-4300. Always the first free port in this range is used. If this is set to an integer value it is considered as an explicit single port.\n\nThe HTTP protocol is used for the REST endpoint which is used by all clients except the Java client.\n\nhttp.publish_port\nRuntime: no\n\nThe port HTTP clients should use to communicate with the node. It is necessary to define this setting if the bound HTTP port (http.port) of the node is not directly reachable from outside, e.g. running it behind a firewall or inside a Docker container.\n\ntransport.tcp.port\nRuntime: no\n\nThis defines the TCP port range to which the CrateDB transport service will be bound to. It defaults to 4300-4400. Always the first free port in this range is used. If this is set to an integer value it is considered as an explicit single port.\n\nThe transport protocol is used for internal node-to-node communication.\n\ntransport.publish_port\nRuntime: no\n\nThe port that the node publishes to the cluster for its own discovery. It is necessary to define this setting when the bound tranport port (transport.tcp.port) of the node is not directly reachable from outside, e.g. running it behind a firewall or inside a Docker container.\n\npsql.port\nRuntime: no\n\nThis defines the TCP port range to which the CrateDB Postgres service will be bound to. It defaults to 5432-5532. Always the first free port in this range is used. If this is set to an integer value it is considered as an explicit single port.\n\nAdvanced TCP settings\n\nAny interface that uses TCP (Postgres wire, HTTP & Transport protocols) shares the following settings:\n\nnetwork.tcp.no_delay\nDefault: true\nRuntime: no\n\nEnable or disable the Nagle’s algorithm for buffering TCP packets. Buffering is disabled by default.\n\nnetwork.tcp.keep_alive\nDefault: true\nRuntime: no\n\nConfigures the SO_KEEPALIVE option for sockets, which determines whether they send TCP keepalive probes.\n\nnetwork.tcp.reuse_address\nDefault: true on non-windows machines and false otherwise\nRuntime: no\n\nConfigures the SO_REUSEADDRS option for sockets, which determines whether they should reuse the address.\n\nnetwork.tcp.send_buffer_size\nDefault: -1\nRuntime: no\n\nThe size of the TCP send buffer (SO_SNDBUF socket option). By default not explicitly set.\n\nnetwork.tcp.receive_buffer_size\nDefault: -1\nRuntime: no\n\nThe size of the TCP receive buffer (SO_RCVBUF socket option). By default not explicitly set.\n\nNote\n\nEach setting in this section has its counterpart for HTTP and transport. To provide a protocol specific setting, remove network prefix and use either http or transport instead. For example, no_delay can be configured as http.tcp.no_delay and transport.tcp.no_delay. Please note, that PG interface takes its settings from transport.\n\nTransport settings\ntransport.connect_timeout\nDefault: 30s\nRuntime: no\n\nThe connect timeout for initiating a new connection.\n\ntransport.compress\nDefault: false\nRuntime: no\n\nSet to true to enable compression (DEFLATE) between all nodes.\n\ntransport.ping_schedule\nDefault: -1\nRuntime: no\n\nSchedule a regular application-level ping message to ensure that transport connections between nodes are kept alive. Defaults to -1 (disabled). It is preferable to correctly configure TCP keep-alives instead of using this feature, because TCP keep-alives apply to all kinds of long-lived connections and not just to transport connections.\n\nPaths\n\nNote\n\nRelative paths are relative to CRATE_HOME. Absolute paths override this behavior.\n\npath.conf\nDefault: config\nRuntime: no\n\nFilesystem path to the directory containing the configuration files crate.yml and log4j2.properties.\n\npath.data\nDefault: data\nRuntime: no\n\nFilesystem path to the directory where this CrateDB node stores its data (table data and cluster metadata).\n\nMultiple paths can be set by using a comma separated list and each of these paths will hold full shards (instead of striping data across them). For example:\n\npath.data: /path/to/data1,/path/to/data2\n\n\nWhen CrateDB finds striped shards at the provided locations (from CrateDB <0.55.0), these shards will be migrated automatically on startup.\n\npath.logs\nDefault: logs\nRuntime: no\n\nFilesystem path to a directory where log files should be stored.\n\nCan be used as a variable inside log4j2.properties.\n\nFor example:\n\nappender:\n  file:\n    file: ${path.logs}/${cluster.name}.log\n\npath.repo\nRuntime: no\n\nA list of filesystem or UNC paths where repositories of type fs may be stored.\n\nWithout this setting a CrateDB user could write snapshot files to any directory that is writable by the CrateDB process. To safeguard against this security issue, the possible paths have to be whitelisted here.\n\nSee also location setting of repository type fs.\n\nSee Also\n\nblobs.path\n\nPlug-ins\nplugin.mandatory\nRuntime: no\n\nA list of plug-ins that are required for a node to startup.\n\nIf any plug-in listed here is missing, the CrateDB node will fail to start.\n\nCPU\nprocessors\nRuntime: no\n\nThe number of processors is used to set the size of the thread pools CrateDB is using appropriately. If not set explicitly, CrateDB will infer the number from the available processors on the system.\n\nIn environments where the CPU amount can be restricted (like Docker) or when multiple CrateDB instances are running on the same hardware, the inferred number might be too high. In such a case, it is recommended to set the value explicitly.\n\nMemory\nbootstrap.memory_lock\nDefault: false\nRuntime: no\n\nCrateDB performs poorly when the JVM starts swapping: you should ensure that it never swaps. If set to true, CrateDB will use the mlockall system call on startup to ensure that the memory pages of the CrateDB process are locked into RAM.\n\nGarbage collection\n\nCrateDB logs if JVM garbage collection on different memory pools takes too long. The following settings can be used to adjust these timeouts:\n\nmonitor.jvm.gc.collector.young.warn\nDefault: 1000ms\nRuntime: no\n\nCrateDB will log a warning message if it takes more than the configured timespan to collect the Eden Space (heap).\n\nmonitor.jvm.gc.collector.young.info\nDefault: 700ms\nRuntime: no\n\nCrateDB will log an info message if it takes more than the configured timespan to collect the Eden Space (heap).\n\nmonitor.jvm.gc.collector.young.debug\nDefault: 400ms\nRuntime: no\n\nCrateDB will log a debug message if it takes more than the configured timespan to collect the Eden Space (heap).\n\nmonitor.jvm.gc.collector.old.warn\nDefault: 10000ms\nRuntime: no\n\nCrateDB will log a warning message if it takes more than the configured timespan to collect the Old Gen / Tenured Gen (heap).\n\nmonitor.jvm.gc.collector.old.info\nDefault: 5000ms\nRuntime: no\n\nCrateDB will log an info message if it takes more than the configured timespan to collect the Old Gen / Tenured Gen (heap).\n\nmonitor.jvm.gc.collector.old.debug\nDefault: 2000ms\nRuntime: no\n\nCrateDB will log a debug message if it takes more than the configured timespan to collect the Old Gen / Tenured Gen (heap).\n\nAuthentication\nTrust authentication\nauth.trust.http_default_user\nDefault: crate\nRuntime: no\n\nThe default user that should be used for authentication when clients connect to CrateDB via HTTP protocol and they do not specify a user via the Authorization request header.\n\nauth.trust.http_support_x_real_ip\nDefault: false\nRuntime: no\n\nIf enabled, the HTTP transport will trust the X-Real-IP header sent by the client to determine the client’s IP address. This is useful when CrateDB is running behind a reverse proxy or load-balancer. For improved security, any _local_ IP address (127.0.0.1 and ::1) defined in this header will be ignored.\n\nWarning\n\nEnabling this setting can be a security risk, as it allows clients to impersonate other clients by sending a fake X-Real-IP header.\n\nHost-based authentication\n\nAuthentication settings (auth.host_based.*) are node settings, which means that their values apply only to the node where they are applied and different nodes may have different authentication settings.\n\nauth.host_based.enabled\nDefault: false\nRuntime: no\n\nSetting to enable or disable Host Based Authentication (HBA). It is disabled by default.\n\nHBA entries\n\nThe auth.host_based.config. setting is a group setting that can have zero, one or multiple groups that are defined by their group key (${order}) and their fields (user, address, method, protocol, ssl).\n\n${order}:\nAn identifier that is used as a natural order key when looking up the host\nbased configuration entries. For example, an order key of a will be\nlooked up before an order key of b. This key guarantees that the entry\nlookup order will remain independent from the insertion order of the\nentries.\n\nThe Host-Based Authentication (HBA) setting is a list of predicates that users can specify to restrict or allow access to CrateDB.\n\nThe meaning of the fields of the are as follows:\n\nauth.host_based.config.${order}.user\nRuntime: no\nSpecifies an existing CrateDB username, only crate user (superuser) is\navailable. If no user is specified in the entry, then all existing users\ncan have access.\nauth.host_based.config.${order}.address\nRuntime: no\nThe client machine addresses that the client matches, and which are allowed\nto authenticate. This field may contain an IPv4 address, an IPv6 address or\nan IPv4 CIDR mask. For example: 127.0.0.1 or 127.0.0.1/32. It also\nmay contain a hostname or the special _local_ notation which will match\nboth IPv4 and IPv6 connections from localhost. A hostname specification\nthat starts with a dot (.) matches a suffix of the actual hostname.\nSo .crate.io would match foo.crate.io but not just crate.io. If no address\nis specified in the entry, then access to CrateDB is open for all hosts.\nauth.host_based.config.${order}.method\nRuntime: no\nThe authentication method to use when a connection matches this entry.\nValid values are trust, cert, and password. If no method is\nspecified, the trust method is used by default.\nSee Trust method, Client certificate authentication method and Password authentication method for more\ninformation about these methods.\nauth.host_based.config.${order}.protocol\nRuntime: no\nSpecifies the protocol for which the authentication entry should be used.\nIf no protocol is specified, then this entry will be valid for all\nprotocols that rely on host based authentication see Trust method).\nauth.host_based.config.${order}.ssl\nDefault: optional\nRuntime: no\nSpecifies whether the client must use SSL/TLS to connect to the cluster.\nIf set to on then the client must be connected through SSL/TLS\notherwise is not authenticated. If set to off then the client must\nnot be connected via SSL/TLS otherwise is not authenticated. Finally\noptional, which is the value when the option is completely skipped,\nmeans that the client can be authenticated regardless of SSL/TLS is used\nor not.\n\nExample of config groups:\n\nauth.host_based.config:\n  entry_a:\n    user: crate\n    address: 127.16.0.0/16\n  entry_b:\n    method: trust\n  entry_3:\n    user: crate\n    address: 172.16.0.0/16\n    method: trust\n    protocol: pg\n    ssl: on\n\nSecured communications (SSL/TLS)\n\nSecured communications via SSL allows you to encrypt traffic between CrateDB nodes and clients connecting to them. Connections are secured using Transport Layer Security (TLS).\n\nssl.http.enabled\nDefault: false\nRuntime: no\n\nSet this to true to enable secure communication between the CrateDB node and the client through SSL via the HTTPS protocol.\n\nssl.psql.enabled\nDefault: false\nRuntime: no\n\nSet this to true to enable secure communication between the CrateDB node and the client through SSL via the PostgreSQL wire protocol.\n\nssl.transport.mode\nDefault: legacy\nRuntime: no\n\nFor communication between nodes, choose:\n\noff\n\nSSL cannot be used\n\nlegacy\n\nSSL is not used. If HBA is enabled, transport connections won’t be verified Any reachable host can establish a connection.\n\non\n\nSSL must be used\n\nssl.keystore_filepath\nRuntime: no\n\nThe full path to the node keystore file.\n\nssl.keystore_password\nRuntime: no\n\nThe password used to decrypt the keystore file defined with ssl.keystore_filepath.\n\nssl.keystore_key_password\nRuntime: no\n\nThe password entered at the end of the keytool -genkey command.\n\nNote\n\nOptionally trusted CA certificates can be stored separately from the node’s keystore into a truststore for CA certificates.\n\nssl.truststore_filepath\nRuntime: no\n\nThe full path to the node truststore file. If not defined, then only a keystore will be used.\n\nssl.truststore_password\nRuntime: no\n\nThe password used to decrypt the truststore file defined with ssl.truststore_filepath.\n\nssl.resource_poll_interval\nDefault: 5m\nRuntime: no\n\nThe frequency at which SSL files such as keystore and truststore are polled for changes.\n\nCross-origin resource sharing (CORS)\n\nMany browsers support the same-origin policy which requires web applications to explicitly allow requests across origins. The cross-origin resource sharing settings in CrateDB allow for configuring these.\n\nhttp.cors.enabled\nDefault: false\nRuntime: no\n\nEnable or disable cross-origin resource sharing.\n\nhttp.cors.allow-origin\nDefault: <empty>\nRuntime: no\n\nDefine allowed origins of a request. * allows any origin (which can be a substantial security risk) and by prepending a / the string will be treated as a regular expression. For example /https?:\\/\\/crate.io/ will allow requests from https://crate.io and https://crate.io. This setting disallows any origin by default.\n\nhttp.cors.max-age\nDefault: 1728000 (20 days)\nRuntime: no\n\nMax cache age of a preflight request in seconds.\n\nhttp.cors.allow-methods\nDefault: OPTIONS, HEAD, GET, POST, PUT, DELETE\nRuntime: no\n\nAllowed HTTP methods.\n\nhttp.cors.allow-headers\nDefault: X-Requested-With, Content-Type, Content-Length\nRuntime: no\n\nAllowed HTTP headers.\n\nhttp.cors.allow-credentials\nDefault: false\nRuntime: no\n\nAdd the Access-Control-Allow-Credentials header to responses.\n\nBlobs\nblobs.path\nRuntime: no\n\nPath to a filesystem directory where to store blob data allocated for this node.\n\nBy default blobs will be stored under the same path as normal data. A relative path value is interpreted as relative to CRATE_HOME.\n\nRepositories\n\nRepositories are used to backup a CrateDB cluster.\n\nrepositories.url.allowed_urls\nRuntime: no\n\nThis setting only applies to repositories of type url.\n\nWith this setting a list of urls can be specified which are allowed to be used if a repository of type url is created.\n\nWildcards are supported in the host, path, query and fragment parts.\n\nThis setting is a security measure to prevent access to arbitrary resources.\n\nIn addition, the supported protocols can be restricted using the repositories.url.supported_protocols setting.\n\nrepositories.url.supported_protocols\nDefault: http, https, ftp, file and jar\nRuntime: no\n\nA list of protocols that are supported by repositories of type url.\n\nThe jar protocol is used to access the contents of jar files. For more info, see the java JarURLConnection documentation.\n\nSee also the path.repo Setting.\n\nQueries\nindices.query.bool.max_clause_count\nDefault: 8192\nRuntime: no\n\nThis setting limits the number of boolean clauses that can be generated by != ANY(), LIKE ANY(), ILIKE ANY(), NOT LIKE ANY() and NOT ILIKE ANY() operators on arrays in order to prevent users from executing queries that may result in heavy memory consumption causing nodes to crash with OutOfMemory exceptions. Throws TooManyClauses errors when the limit is exceeded.\n\nNote\n\nYou can avoid TooManyClauses errors by increasing this setting. The number of boolean clauses used can be larger than the elements of the array .\n\nLegacy\nlegacy.table_function_column_naming\nDefault: false\nRuntime: no\n\nSince CrateDB 5.0.0, if the table function is not aliased and is returning a single base data typed column, the table function name is used as the column name. This setting can be set in order to use the naming convention prior to 5.0.0.\n\nThe following table functions are affected by this setting:\n\nunnest\n\nregexp_matches\n\ngenerate_series\n\nWhen the setting is set and a single column is expected to be returned, the returned column will be named col1, groups, or col1 respectively.\n\nNote\n\nBeware that if not all nodes in the cluster are consistently set or unset, the behaviour will depend on the node handling the query.\n\nJavaScript language\nlang.js.enabled\nDefault: true\nRuntime: no\n\nSetting to enable or disable JavaScript UDF support.\n\nCustom attributes\n\nThe node.attr namespace is a bag of custom attributes. Custom attributes can be used to control shard allocation.\n\nYou can create any attribute you want under this namespace, like node.attr.key: value. These attributes use the node.attr namespace to distinguish them from core node attribute like node.name.\n\nCustom attributes are not validated by CrateDB, unlike core node attributes."
  },
  {
    "title": "Storage and consistency — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/concepts/storage-consistency.html",
    "html": "5.6\nStorage and consistency\n\nThis document provides an overview on how CrateDB stores and distributes state across the cluster and what consistency and durability guarantees are provided.\n\nNote\n\nSince CrateDB heavily relies on Elasticsearch and Lucene for storage and cluster consensus, concepts shown here might look familiar to Elasticsearch users, since the implementation is actually reused from the Elasticsearch code.\n\nTable of contents\n\nData storage\n\nAtomicity at document level\n\nDurability\n\nAddressing documents\n\nConsistency\n\nCluster meta data\n\nData storage\n\nEvery table in CrateDB is sharded, which means that tables are divided and distributed across the nodes of a cluster. Each shard in CrateDB is a Lucene index broken down into segments getting stored on the filesystem. Physically the files reside under one of the configured data directories of a node.\n\nLucene only appends data to segment files, which means that data written to the disc will never be mutated. This makes it easy for replication and recovery, since syncing a shard is simply a matter of fetching data from a specific marker.\n\nAn arbitrary number of replica shards can be configured per table. Every operational replica holds a full synchronized copy of the primary shard.\n\nWith read operations, there is no difference between executing the operation on the primary shard or on any of the replicas. CrateDB randomly assigns a shard when routing an operation. It is possible to configure this behavior if required, see our best practice guide on multi zone setups for more details.\n\nWrite operations are handled differently than reads. Such operations are synchronous over all active replicas with the following flow:\n\nThe primary shard and the active replicas are looked up in the cluster state for the given operation. The primary shard and a quorum of the configured replicas need to be available for this step to succeed.\n\nThe operation is routed to the according primary shard for execution.\n\nThe operation gets executed on the primary shard\n\nIf the operation succeeds on the primary, the operation gets executed on all replicas in parallel.\n\nAfter all replica operations finish the operation result gets returned to the caller.\n\nShould any replica shard fail to write the data or times out in step 5, it’s immediately considered as unavailable.\n\nAtomicity at document level\n\nEach row of a table in CrateDB is a semi structured document which can be nested arbitrarily deep through the use of object and array types.\n\nOperations on documents are atomic. Meaning that a write operation on a document either succeeds as a whole or has no effect at all. This is always the case, regardless of the nesting depth or size of the document.\n\nCrateDB does not provide transactions. Since every document in CrateDB has a version number assigned, which gets increased every time a change occurs, patterns like Optimistic Concurrency Control can help to work around that limitation.\n\nDurability\n\nEach shard has a WAL also known as translog. It guarantees that operations on documents are persisted to disk without having to issue a Lucene-Commit for every write operation. When the translog gets flushed all data is written to the persistent index storage of Lucene and the translog gets cleared.\n\nIn case of an unclean shutdown of a shard, the transactions in the translog are getting replayed upon startup to ensure that all executed operations are permanent.\n\nThe translog is also directly transferred when a newly allocated replica initializes itself from the primary shard. There is no need to flush segments to disc just for replica recovery purposes.\n\nAddressing documents\n\nEvery document has an internal identifier. By default this identifier is derived from the primary key. Documents living in tables without a primary key are assigned a unique auto-generated ID automatically when created.\n\nEach document is routed to one specific shard according to the routing column. All rows that have the same routing column row value are stored in the same shard. The routing column can be specified with the CLUSTERED clause when creating the table. If a primary key has been defined, it will be used as the default routing column, otherwise the internal document ID is used.\n\nWhile transparent to the user, internally there are two ways how CrateDB accesses documents:\n\nget\n\nDirect access by identifier. Only applicable if the routing key and the identifier can be computed from the given query specification. (e.g: the full primary key is defined in the where clause).\n\nThis is the most efficient way to access a document, since only a single shard gets accessed and only a simple index lookup on the _id field has to be done.\n\nsearch\n\nQuery by matching against fields of documents across all candidate shards of the table.\n\nConsistency\n\nCrateDB is eventual consistent for search operations. Search operations are performed on shared IndexReaders which besides other functionality, provide caching and reverse lookup capabilities for shards. An IndexReader is always bound to the Lucene segment it was started from, which means it has to be refreshed in order to see new changes, this is done on a time based manner, but can also be done manually (see refresh). Therefore a search only sees a change if the according IndexReader was refreshed after that change occurred.\n\nIf a query specification results in a get operation, changes are visible immediately. This is achieved by looking up the document in the translog first, which will always have the most recent version of the document. The common update and fetch use-case is therefore possible. If a client updates a row and that row is looked up by its primary key after that update the changes will always be visible, since the information will be retrieved directly from the translog.\n\nNote\n\nDirty reads can occur if the primary shard becomes isolated. The primary will only realize it is isolated once it tries to communicate with its replicas or the master. At that point, a write operation is already committed into the primary and can be read by a concurrent read operation. In order to minimise the window of opportunity for this phenomena, the CrateDB nodes communicate with the master every second (by default) and once they realise no master is known, they will start rejecting write operations.\n\nEvery replica shard is updated synchronously with its primary and always carries the same information. Therefore it does not matter if the primary or a replica shard is accessed in terms of consistency. Only the refresh of the IndexReader affects consistency.\n\nCaution\n\nSome outage conditions can affect these consistency claims. See the resiliency documentation for details.\n\nCluster meta data\n\nCluster meta data is held in the so called “Cluster State”, which contains the following information:\n\nTables schemas.\n\nPrimary and replica shard locations. Basically just a mapping from shard number to the storage node.\n\nStatus of each shard, which tells if a shard is currently ready for use or has any other state like “initializing”, “recovering” or cannot be assigned at all.\n\nInformation about discovered nodes and their status.\n\nConfiguration information.\n\nEvery node has its own copy of the cluster state. However there is only one node allowed to change the cluster state at runtime. This node is called the “master” node and gets auto-elected. The “master” node has no special configuration at all, all nodes are master-eligible by default, and any master-eligible node can be elected as the master. There is also an automatic re-election if the current master node goes down for some reason.\n\nNote\n\nTo avoid a scenario where two masters could be elected due to network partitioning, CrateDB automatically defines a quorum of nodes with which it is possible to elect a master. For details on how this works and further information see Master Node Election.\n\nTo explain the flow of events for any cluster state change, here is an example flow for an ALTER TABLE statement which changes the schema of a table:\n\nA node in the cluster receives the ALTER TABLE request.\n\nThe node sends out a request to the current master node to change the table definition.\n\nThe master node applies the changes locally to the cluster state and sends out a notification to all affected nodes about the change.\n\nThe nodes apply the change, so that they are now in sync with the master.\n\nEvery node might take some local action depending on the type of cluster state change."
  },
  {
    "title": "CrateDB Reference — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/3.3/index.html",
    "html": "Note\n\nYou are not reading the most recent version of this documentation. 5.6 is the latest version available.\n\n3.3\nCrateDB Reference\n\nCrateDB is a distributed SQL database that makes it simple to store and analyze massive amounts of machine data in real-time.\n\nNote\n\nThis is a low-level reference manual.\n\nCheck out Getting Started With CrateDB for beginner documentation.\n\nCheck out the CrateDB Guide for high-level documentation, including overviews, best practices, and tutorials.\n\nSee Also\n\nCrateDB is an open source project and is hosted on GitHub.\n\nTable of Contents\n\nRunning CrateDB\nIntroduction\nCommand Line Options\nSignal Handling\nConfiguration\nNode Specific Settings\nCluster Wide Settings\nSession Settings\nLogging\nEnvironment Variables\nGeneral Use\nData Definition\nData Manipulation\nQuerying Crate\nBuilt-in Functions and Operators\nUser-Defined Functions\nBlobs\nOptimistic Concurrency Control\nInformation Schema\nAdministration\nSystem Information\nRuntime Configuration\nUser Management\nPrivileges\nAuthentication\nSecured Communications (SSL/TLS)\nIngestion Framework\nOptimization\nJobs Management\nJMX Monitoring\nSnapshots\nCloud Discovery\nUsage Data Collector\nSQL Syntax\nGeneral SQL\nSQL Statements\nClient Interfaces\nHTTP Endpoint\nPostgreSQL Wire Protocol\nCrateDB Editions\nEnterprise Features\nCommunity Edition\nAppendices\nRelease Notes\nCompatibility\nSQL Standard Compliance"
  },
  {
    "title": "Clustering — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/concepts/clustering.html",
    "html": "5.6\nClustering\n\nThe aim of this document is to describe, on a high level, how the distributed SQL database CrateDB uses a shared nothing architecture to form high- availability, resilient database clusters with minimal effort of configuration.\n\nIt will lay out the core concepts of the shared nothing architecture at the heart of CrateDB. The main difference to a primary-secondary architecture is that every node in the CrateDB cluster can perform every operation - hence all nodes are equal in terms of functionality (see Components of a CrateDB Node) and are configured the same.\n\nTable of contents\n\nComponents of a CrateDB Node\n\nSQL Handler\n\nJob Execution Service\n\nCluster State Service\n\nData storage\n\nMulti-node setup: Clusters\n\nCluster state management\n\nSettings, metadata, and routing\n\nMaster Node Election\n\nDiscovery\n\nNetworking\n\nCluster behavior\n\nApplication use case\n\nComponents of a CrateDB Node\n\nTo understand how a CrateDB cluster works it makes sense to first take a look at the components of an individual node of the cluster.\n\nFigure 1\n\nMultiple interconnected instances of CrateDB form a single database cluster. The components of each node are equal.\n\nFigure 1 shows that in CrateDB each node of a cluster contains the same components that (a) interface with each other, (b) with the same component from a different node and/or (c) with the outside world. These four major components are: SQL Handler, Job Execution Service, Cluster State Service, and Data Storage.\n\nSQL Handler\n\nThe SQL Handler part of a node is responsible for three aspects:\n\nhandling incoming client requests,\n\nparsing and analyzing the SQL statement from the request and\n\ncreating an execution plan based on the analyzed statement (abstract syntax tree)\n\nThe SQL Handler is the only of the four components that interfaces with the “outside world”. CrateDB supports three protocols to handle client requests:\n\nHTTP\n\na Binary Transport Protocol\n\nthe PostgreSQL Wire Protocol\n\nA typical request contains a SQL statement and its corresponding arguments.\n\nJob Execution Service\n\nThe Job Execution Service is responsible for the execution of a plan (“job”). The phases of the job and the resulting operations are already defined in the execution plan. A job usually consists of multiple operations that are distributed via the Transport Protocol to the involved nodes, be it the local node and/or one or multiple remote nodes. Jobs maintain IDs of their individual operations. This allows CrateDB to “track” (or for example “kill”) distributed queries.\n\nCluster State Service\n\nThe three main functions of the Cluster State Service are:\n\ncluster state management,\n\nelection of the master node and\n\nnode discovery, thus being the main component for cluster building (as described in section Multi-node setup: Clusters).\n\nIt communicates using the Binary Transport Protocol.\n\nData storage\n\nThe data storage component handles operations to store and retrieve data from disk based on the execution plan.\n\nIn CrateDB, the data stored in the tables is sharded, meaning that tables are divided and (usually) stored across multiple nodes. Each shard is a separate Lucene index that is stored physically on the filesystem. Reads and writes are operating on a shard level.\n\nMulti-node setup: Clusters\n\nA CrateDB cluster is a set of two or more CrateDB instances (referred to as nodes) running on different hosts which form a single, distributed database.\n\nFor inter-node communication, CrateDB uses a software specific transport protocol that utilizes byte-serialized Plain Old Java Objects (POJOs) and operates on a separate port. That so-called “transport port” must be open and reachable from all nodes in the cluster.\n\nCluster state management\n\nThe cluster state is versioned and all nodes in a cluster keep a copy of the latest cluster state. However, only a single node in the cluster – the master node – is allowed to change the state at runtime.\n\nSettings, metadata, and routing\n\nThe cluster state contains all necessary meta information to maintain the cluster and coordinate operations:\n\nGlobal cluster settings\n\nDiscovered nodes and their status\n\nSchemas of tables\n\nThe status and location of primary and replica shards\n\nWhen the master node updates the cluster state it will publish the new state to all nodes in the cluster and wait for all nodes to respond before processing the next update.\n\nMaster Node Election\n\nIn a CrateDB cluster there can only be one master node at any single time. The cluster only becomes available to serve requests once a master has been elected, and a new election takes place if the current master node becomes unavailable.\n\nBy default, all nodes are master-eligible, but a node setting is available to indicate, if desired, that a node must not take on the role of master.\n\nTo elect a master among the eligible nodes, a majority (floor(half)+1), also known as quorum, is required among a subset of all master-eligible nodes, this subset of nodes is known as the voting configuration. The voting configuration is a list which is persisted as part of the cluster state. It is maintained automatically in a way that makes so that split-brain scenarios are never possible.\n\nEvery time a node joins the cluster, or leaves the cluster, even if it is for a few seconds, CrateDB re-evaluates the voting configuration. If the new number of master-eligible nodes in the cluster is odd, CrateDB will put them all in the voting configuration. If the number is even, CrateDB will exclude one of the master-eligible nodes from the voting configuration.\n\nThe voting configuration is not shrunk below 3 nodes, meaning that if there were 3 nodes in the voting configuration and one of them becomes unavailable, they all stay in the voting configuration and a quorum of 2 nodes is still required. A master node rescinds its role if it cannot contact a quorum of nodes from the latest voting configuration.\n\nWarning\n\nIf you do infrastructure maintenance, please note that as nodes are shutdown or rebooted, they will temporarily leave the voting configuration, and for the cluster to elect a master a quorum is required among the nodes that were last in the voting configuration.\n\nFor instance, if you have a 5-nodes cluster, with all nodes master-eligible, and node 1 is currently the master, and you shutdown node 5, then node 4, then node 3, the cluster will stay available as the voting configuration will have adapted to only have nodes 1, 2, and 3 on it.\n\nIf you then shutdown one more node the cluster will become unavailable as a quorum of 2 nodes is now required and not available. To bring the cluster back online at this point you will require two nodes among 1, 2, and 3. Bringing back nodes 3, 4, and 5, will not be sufficient.\n\nNote\n\nSpecial settings and considerations applied prior to CrateDB version 4.0.0.\n\nDiscovery\n\nThe process of finding, adding and removing nodes is done in the discovery module.\n\nFigure 2\n\nPhases of the node discovery process. n1 and n2 already form a cluster where n1 is the elected master node, n3 joins the cluster. The cluster state update happens in parallel!\n\nNode discovery happens in multiple steps:\n\nCrateDB requires a list of potential host addresses for other CrateDB nodes when it is starting up. That list can either be provided by a static configuration or can be dynamically generated, for example by fetching DNS SRV records, querying the Amazon EC2 API, and so on.\n\nAll potential host addresses are pinged. Nodes which receive the request respond to it with information about the cluster it belongs to, the current master node, and its own node name.\n\nNow that the node knows the master node, it sends a join request. The Primary verifies the incoming request and adds the new node to the cluster state that now contains the complete list of all nodes in the cluster.\n\nThe cluster state is then published across the cluster. This guarantees the common knowledge of the node addition.\n\nCaution\n\nIf a node is started without any initial_master_nodes or a discovery_type set to single-node (e.g., the default configuration), it will never join a cluster even if the configuration is subsequently changed.\n\nIt is possible to force the node to forget its current cluster state by using the crate-node CLI tool. However, be aware that this may result in data loss.\n\nNetworking\n\nIn a CrateDB cluster all nodes have a direct link to all other nodes; this is known as full mesh topology. Due to simplicity reasons every node maintains a one-way connections to every other node in the network. The network topology of a 5 node cluster looks like this:\n\nFigure 3\n\nNetwork topology of a 5 node CrateDB cluster. Each line represents a one-way connection.\n\nThe advantages of a fully connected network are that it provides a high degree of reliability and the paths between nodes are the shortest possible. However, there are limitations in the size of such networked applications because the number of connections (c) grows quadratically with the number of nodes (n):\n\nc = n * (n - 1)\n\nCluster behavior\n\nThe fact that each CrateDB node in a cluster is equal allows applications and users to connect to any node and get the same response for the same operations. As already described in section Components of a CrateDB Node, the SQL handler is responsible for handling incoming client SQL requests, either using the HTTP transport protocol, or the PostgreSQL wire protocol.\n\nThe “handler node” that accepts the client request also returns the response to the client. It does neither redirect nor delegate the request to a different nodes. The handler node parses the incoming request into a syntax tree, analyzes it and creates an execution plan locally. Then the operations of the plan are executed in a distributed manner. The upstream of the final phase of the execution is always the handler which then returns the response to the client.\n\nApplication use case\n\nIn a conventional setup of an application using a primary-secondary database the deployed stack looks similar to this:\n\nFigure 4\n\nConventional deployment of an application-database stack.\n\nHowever, this given setup does not scale because all application servers use the same, single entry point to the database for writes (the application can still read from secondaries) and if that entry point is unavailable the complete stack is broken.\n\nChoosing a shared nothing architecture allows DevOps to deploy their applications in an “elastic” manner without SPoF. The idea is to extend the shared nothing architecture from the database to the application which in most cases is stateless already.\n\nFigure 5\n\nElastic deployment making use of the shared nothing architecture.\n\nIf you deploy an instance of CrateDB together with every application server you will be able to dynamically scale up and down your database backend depending on your needs. The application only needs to communicate to its “bound” CrateDB instance on localhost. The load balancer tracks the health of the hosts and if either the application or the database on a single host fails the complete host will taken out of the load balancing."
  },
  {
    "title": "Joins — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/concepts/joins.html",
    "html": "5.6\nJoins\n\nJoins are essential operations in relational databases. They create a link between rows based on common values and allow the meaningful combination of these rows. CrateDB supports joins and due to its distributed nature allows you to work with large amounts of data.\n\nIn this document we will present the following topics. First, an overview of the existing types of joins and algorithms provided. Then a description of how CrateDB implements them along with the necessary optimizations, which allows us to work with huge datasets.\n\nTable of contents\n\nJoin types\n\nCross join\n\nInner join\n\nEqui Join\n\nOuter join\n\nJoin algorithms\n\nNested loop join\n\nPrimitive nested loop\n\nDistributed nested loop\n\nHash join\n\nBasic algorithm\n\nBlock hash join\n\nSwitch tables optimization\n\nDistributed block hash join\n\nJoin optimizations\n\nQuery then fetch\n\nPush-down query optimization\n\nCross join elimination\n\nJoin types\n\nA join is a relational operation that merges two data sets based on certain properties. Join Types (Inspired by this article) shows which elements appear in which join.\n\nJoin Types\n\nFrom left to right, top to bottom: left join, right join, inner join, outer join, and cross join of a set L and R.\n\nCross join\n\nA cross join returns the Cartesian product of two or more relations. The result of the Cartesian product on the relation L and R consists of all possible permutations of each tuple of the relation L with every tuple of the relation R.\n\nInner join\n\nAn inner join is a join of two or more relations that returns only tuples that satisfy the join condition.\n\nEqui Join\n\nAn equi join is a subset of an inner join and a comparison-based join, that uses equality comparisons in the join condition. The equi join of the relation L and R combines tuple l of relation L with a tuple r of the relation R if the join attributes of both tuples are identical.\n\nOuter join\n\nAn outer join returns a relation consisting of tuples that satisfy the join condition and dangling tuples from both or one of the relations, respectively to the outer join type.\n\nAn outer join can be one of the following types:\n\nLeft outer join returns tuples of the relation L matching tuples of the relation R and dangling tuples of the relation R padded with null values.\n\nRight outer join returns tuples of the relation R matching tuples of the relation L and dangling tuples from the relation L padded with null values.\n\nFull outer join returns matching tuples of both relations and dangling tuples produced by left and right outer joins.\n\nJoin algorithms\n\nCrateDB supports (a) CROSS JOIN, (b) INNER JOIN, (c) EQUI JOIN, (d) LEFT JOIN, (e) RIGHT JOIN and (f) FULL JOIN. All of these join types are executed using the nested loop join algorithm except for the Equi Joins which are executed using the hash join algorithm. Special optimizations, according to the specific use cases, are applied to improve execution performance.\n\nNested loop join\n\nThe nested loop join is the simplest join algorithm. One of the relations is nominated as the inner relation and the other as the outer relation. Each tuple of the outer relation is compared with each tuple of the inner relation and if the join condition is satisfied, the tuples of the relation L and R are concatenated and added into the returned virtual relation:\n\nfor each tuple l ∈ L do\n    for each tuple r ∈ R do\n        if l.a Θ r.b\n            put tuple(l, r) in Q\n\n\nListing 1. Nested loop join algorithm.\n\nPrimitive nested loop\n\nFor joins on some relations, the nested loop operation can be executed directly on the handler node. Specifically for queries involving a CROSS JOIN or joins on system tables /information_schema each shard sends the data to the handler node. Afterwards, this node runs the nested loop, applies limits, etc. and ultimately returns the results. Similarly, joins can be nested, so instead of collecting data from shards the rows can be the result of a previous join or table function.\n\nDistributed nested loop\n\nRelations are usually distributed to different nodes which require the nested loop to acquire the data before being able to join. After finding the locations of the required shards (which is done in the planning stage), the smaller data set (based on the row count) is broadcast amongst all the nodes holding the shards they are joined with.\n\nAfter that, each of the receiving nodes can start running a nested loop on the subset it has just received. Finally, these intermediate results are pushed to the original (handler) node to merge and return the results to the requesting client (see Nodes that are holding the smaller shards broadcast the data to the processing nodes which then return the results to the requesting node.).\n\nNodes that are holding the smaller shards broadcast the data to the processing nodes which then return the results to the requesting node.\n\nQueries can be optimized if they contain (a) ORDER BY, (b) LIMIT, or (c) if INNER/EQUI JOIN. In any of these cases, the nested loop can be terminated earlier:\n\nOrdering allows determining whether there are records left\n\nLimit states the maximum number of rows that are returned\n\nConsequently, the number of rows is significantly reduced allowing the operation to complete much faster.\n\nHash join\n\nThe Hash Join algorithm is used to execute certain types of joins in a more efficient way than Nested Loop.\n\nBasic algorithm\n\nThe operation takes place in one node (the handler node to which the client is connected). The rows of the left relation of the join are read and a hashing algorithm is applied on the fields of the relation which participate in the join condition. The hashing algorithm generates a hash value which is used to store every row of the left relation in the proper position in a hash table.\n\nThen the rows of the right relation are read one-by-one and the same hashing algorithm is applied on the fields that participate in the join condition. The generated hash value is used to make a lookup in the hash table. If no entry is found, the row is skipped and the processing continues with the next row from the right relation. If an entry is found, the join condition is validated (handling hash collisions) and on successful validation the combined tuple of left and right relation is returned.\n\nBasic hash join algorithm\n\nBlock hash join\n\nThe Hash Join algorithm requires a hash table containing all the rows of the left relation to be stored in memory. Therefore, depending on the size of the relation (number of rows) and the size of each row, the size of this hash table might exceed the available memory of the node executing the hash join. To resolve this limitation the rows of the left relation are loaded into the hash table in blocks.\n\nOn every iteration the maximum available size of the hash table is calculated, based on the number of rows and size of each row of the table but also taking into account the available memory for query execution on the node. Once this block-size is calculated the rows of the left relation are processed and inserted into the hash table until the block-size is reached.\n\nThe operation then starts reading the rows of the right relation, process them one-by-one and performs the lookup and the join condition validation. Once all rows from the right relation are processed the hash table is re-initialized based on a new calculation of the block size and a new iteration starts until all rows of the left relation are processed.\n\nWith this algorithm the memory limitation is handled in expense of having to iterate over the rows of the right table multiple times, and it is the default algorithm used for Hash Join execution by CrateDB.\n\nSwitch tables optimization\n\nSince the right table can be processed multiple times (number of rows from left / block-size) the right table should be the smaller (in number of rows) of the two relations participating in the join. Therefore, if originally the right relation is larger than the left the query planner performs a switch to take advantage of this detail and execute the hash join with better performance.\n\nDistributed block hash join\n\nSince CrateDB is a distributed database and a standard deployment consists of at least three nodes and in most case of much more, the Hash Join algorithm execution can be further optimized (performance-wise) by executing it in a distributed manner across the CrateDB cluster.\n\nThe idea is to have the hash join operation executing in multiple nodes of the cluster in parallel and then merge the intermediate results before returning them to the client.\n\nA hashing algorithm is applied on every row of both the left and right relations. On the integer value generated by this hash, a modulo, by the number of nodes in the cluster, is applied and the resulting number defines the node to which this row should be sent. As a result each node of the cluster receives a subset of the whole data set which is ensured (by the hashing and modulo) to contain all candidate matching rows.\n\nEach node in turn performs a block hash join on this subset and sends its result tuples to the handler node (where the client issued the query). Finally, the handler node receives those intermediate results, merges them and applies any pending ORDER BY, LIMIT and OFFSET and sends the final result to the client.\n\nThis algorithm is used by CrateDB for most cases of hash join execution except for joins on complex subqueries that contain LIMIT and/or OFFSET.\n\nDistributed hash join algorithm\n\nJoin optimizations\nQuery then fetch\n\nJoin operations on large relation can be extremely slow especially if the join is executed with a Nested Loop. - which means that the runtime complexity grows quadratically (O(n*m)). Specifically for cross joins this results in large amounts of data sent over the network and loaded into memory at the handler node. CrateDB reduces the volume of data transferred by employing “Query Then Fetch”: First, filtering and ordering are applied (if possible where the data is located) to obtain the required document IDs. Next, as soon as the final data set is ready, CrateDB fetches the selected fields and returns the data to the client.\n\nPush-down query optimization\n\nComplex queries such as Listing 2 require the planner to decide when to filter, sort, and merge in order to efficiently execute the plan. In this case, the query would be split internally into subqueries before running the join. As shown in Figure 5, first filtering (and ordering) is applied to relations L and R on their shards, then the result is directly broadcast to the nodes running the join. Not only will this behavior reduce the number of rows to work with, it also distributes the workload among the nodes so that the (expensive) join operation can run faster.\n\nSELECT L.a, R.x\nFROM L, R\nWHERE L.id = R.id\n  AND L.b > 100\n  AND R.y < 10\nORDER BY L.a\n\n\nListing 2. An INNER JOIN on ids (effectively an EQUI JOIN) which can be optimized.\n\nFigure 5\n\nComplex queries are broken down into subqueries that are run on their shards before joining.\n\nCross join elimination\n\nThe optimizer will try to eliminate cross joins in the query plan by changing the join-order. Cross join elimination replaces a CROSS JOIN with an INNER JOIN if query conditions used in the WHERE clause or other join conditions allow for it. An example:\n\nSELECT *\nFROM t1 CROSS JOIN t2\nINNER JOIN t3\nON t3.z = t1.x AND t3.z = t2.y\n\n\nThe cross join elimination will change the order of the query from t1, t2, t3 to t2, t1, t3 so that each join has a join condition and the CROSS JOIN can be replaced by an INNER JOIN. When reordering, it will try to preserve the original join order as much as possible. If a CROSS JOIN cannot be eliminated, the original join order will be maintained. This optimizer rule can be disabled with the optimizer eliminate cross join session setting:\n\nSET optimizer_eliminate_cross_join = false\n\n\nNote that this setting is experimental, and may change in the future."
  },
  {
    "title": "CrateDB Reference — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/4.8/index.html",
    "html": "4.8\nCrateDB Reference\n\nCrateDB is a distributed SQL database that makes it simple to store and analyze massive amounts of machine data in real-time.\n\nNote\n\nThis resource assumes you know the basics. If not, check out the Tutorials section for beginner material.\n\nSee Also\n\nCrateDB is an open source project and is hosted on GitHub.\n\nTable of contents\n\nConcepts\nJoins\nClustering\nStorage and consistency\nResiliency\nCLI tools\ncrate\ncrate-node\nConfiguration\nNode-specific settings\nCluster-wide settings\nSession settings\nLogging\nEnvironment variables\nGeneral use\nData definition\nData manipulation\nQuerying\nBuilt-in functions and operators\nUser-defined functions\nBlobs\nOptimistic Concurrency Control\nInformation schema\nAdministration\nSystem information\nRuntime configuration\nUser management\nPrivileges\nAuthentication\nSecured communications (SSL/TLS)\nOptimization\nJobs management\nJMX monitoring\nSnapshots\nLogical replication\nCloud discovery\nUsage Data Collector\nSQL syntax\nGeneral SQL\nSQL Statements\nClient interfaces\nHTTP endpoint\nPostgreSQL wire protocol\nAppendices\nRelease Notes\nSQL compatibility\nSQL standard compliance\nResiliency Issues\nGlossary"
  },
  {
    "title": "CrateDB Reference — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.5/index.html",
    "html": "5.5\nCrateDB Reference\n\nCrateDB is a distributed SQL database that makes it simple to store and analyze massive amounts of machine data in real-time.\n\nNote\n\nThis resource assumes you know the basics. If not, check out the Tutorials section for beginner material.\n\nSee Also\n\nCrateDB is an open source project and is hosted on GitHub.\n\nTable of contents\n\nConcepts\nJoins\nClustering\nStorage and consistency\nResiliency\nCLI tools\ncrate\ncrate-node\nConfiguration\nNode-specific settings\nCluster-wide settings\nSession settings\nLogging\nEnvironment variables\nGeneral use\nData definition\nData manipulation\nQuerying\nBuilt-in functions and operators\nUser-defined functions\nBlobs\nOptimistic Concurrency Control\nInformation schema\nAdministration\nSystem information\nRuntime configuration\nUser management\nPrivileges\nAuthentication\nSecured communications (SSL/TLS)\nOptimization\nJobs management\nJMX monitoring\nSnapshots\nLogical replication\nCloud discovery\nUsage Data Collector\nSQL syntax\nGeneral SQL\nSQL Statements\nClient interfaces\nHTTP endpoint\nPostgreSQL wire protocol\nAppendices\nRelease Notes\nSQL compatibility\nSQL standard compliance\nResiliency Issues\nGlossary"
  },
  {
    "title": "CrateDB Reference — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/index.html",
    "html": "5.6\nCrateDB Reference\n\nCrateDB is a distributed SQL database that makes it simple to store and analyze massive amounts of machine data in real-time.\n\nNote\n\nThis resource assumes you know the basics. If not, check out the Tutorials section for beginner material.\n\nSee Also\n\nCrateDB is an open source project and is hosted on GitHub.\n\nTable of contents\n\nConcepts\nJoins\nClustering\nStorage and consistency\nResiliency\nCLI tools\ncrate\ncrate-node\nConfiguration\nNode-specific settings\nCluster-wide settings\nSession settings\nLogging\nEnvironment variables\nGeneral use\nData definition\nData manipulation\nQuerying\nBuilt-in functions and operators\nUser-defined functions\nBlobs\nOptimistic Concurrency Control\nInformation schema\nAdministration\nSystem information\nRuntime configuration\nUsers and roles management\nPrivileges\nAuthentication\nSecured communications (SSL/TLS)\nOptimization\nJobs management\nJMX monitoring\nSnapshots\nLogical replication\nCloud discovery\nUsage Data Collector\nSQL syntax\nGeneral SQL\nSQL Statements\nClient interfaces\nHTTP endpoint\nPostgreSQL wire protocol\nAppendices\nRelease Notes\nSQL compatibility\nSQL standard compliance\nResiliency Issues\nGlossary"
  },
  {
    "title": "CrateDB Reference — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/index.html",
    "html": "5.6\nCrateDB Reference\n\nCrateDB is a distributed SQL database that makes it simple to store and analyze massive amounts of machine data in real-time.\n\nNote\n\nThis resource assumes you know the basics. If not, check out the Tutorials section for beginner material.\n\nSee Also\n\nCrateDB is an open source project and is hosted on GitHub.\n\nTable of contents\n\nConcepts\nJoins\nClustering\nStorage and consistency\nResiliency\nCLI tools\ncrate\ncrate-node\nConfiguration\nNode-specific settings\nCluster-wide settings\nSession settings\nLogging\nEnvironment variables\nGeneral use\nData definition\nData manipulation\nQuerying\nBuilt-in functions and operators\nUser-defined functions\nBlobs\nOptimistic Concurrency Control\nInformation schema\nAdministration\nSystem information\nRuntime configuration\nUsers and roles management\nPrivileges\nAuthentication\nSecured communications (SSL/TLS)\nOptimization\nJobs management\nJMX monitoring\nSnapshots\nLogical replication\nCloud discovery\nUsage Data Collector\nSQL syntax\nGeneral SQL\nSQL Statements\nClient interfaces\nHTTP endpoint\nPostgreSQL wire protocol\nAppendices\nRelease Notes\nSQL compatibility\nSQL standard compliance\nResiliency Issues\nGlossary"
  },
  {
    "title": "CrateDB Reference — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/master/index.html",
    "html": "master\nCrateDB Reference\n\nCrateDB is a distributed SQL database that makes it simple to store and analyze massive amounts of data in real-time.\n\nNote\n\nThis resource assumes you know the basics. If not, check out the Tutorials section for beginner material.\n\nSee Also\n\nCrateDB is an open source project and is hosted on GitHub.\n\nTable of contents\n\nConcepts\nJoins\nClustering\nStorage and consistency\nResiliency\nCLI tools\ncrate\ncrate-node\nConfiguration\nNode-specific settings\nCluster-wide settings\nSession settings\nLogging\nEnvironment variables\nGeneral use\nData definition\nData manipulation\nQuerying\nBuilt-in functions and operators\nUser-defined functions\nBlobs\nOptimistic Concurrency Control\nInformation schema\nAdministration\nSystem information\nRuntime configuration\nUsers and roles management\nPrivileges\nAuthentication\nSecured communications (SSL/TLS)\nOptimization\nJobs management\nJMX monitoring\nSnapshots\nForeign Data Wrappers\nLogical replication\nCloud discovery\nUsage Data Collector\nSQL syntax\nGeneral SQL\nSQL Statements\nClient interfaces\nHTTP endpoint\nPostgreSQL wire protocol\nAppendices\nRelease Notes\nSQL compatibility\nSQL standard compliance\nResiliency Issues\nGlossary"
  },
  {
    "title": "CrateDB Ecosystem Catalog — CrateDB: Clients and Tools",
    "url": "https://cratedb.com/docs/crate/clients-tools/en/latest/",
    "html": "CrateDB Ecosystem Catalog\n\nDatabase drivers, libraries, frameworks, and applications for CrateDB.\n\nAbout CrateDB\n\nCrateDB is a distributed and scalable open-source SQL database based on Lucene, with PostgreSQL compatibility. CrateDB clusters store information in the range of billions of records, and terabytes of data, and run analytics in near real time, even with complex queries. CrateDB can be used for enterprise data warehouse workloads, it works across clouds and scales with your data.\n\nConnectivity\n\nThe canonical set of database drivers, client- and developer-applications, and how to configure them to connect to CrateDB.\n\nJust to name a few, the sections below are about the CrateDB Admin UI, the Crash CLI terminal program, connecting with PostgreSQL’s psql client, the DataGrip, and DBeaver IDE applications, the Java/JDBC/Python drivers, the SQLAlchemy and Flink dialects, and more.\n\n IDE\n\nConnect to CrateDB using a database IDE like DataGrip or DBeaver.\n\n CLI\n\nConnect to CrateDB using command-line based terminal programs.\n\n Drivers\n\nList of HTTP and PostgreSQL client drivers, and tutorials.\n\n DataFrame Libraries\n\nConnectivity with DataFrame libraries like pandas and Dask.\n\n ORM Libraries\n\nConnectivity with ORM libraries like SQLAlchemy.\n\nIntegrations\n\nCrateDB integrates with a diverse set of applications and tools concerned with analytics, visualization, and data wrangling, in the areas of data loading and export (ETL), business intelligence (BI), metrics aggregation and monitoring, machine learning, and more.\n\n Overview\n\nLearn how to use CrateDB with popular applications, frameworks, and tools. All on one page.\n\n ETL\n\nETL applications and frameworks for transferring data in and out of CrateDB.\n\n System Metrics\n\nIntegrations and long-term storage for systems monitoring tools like Prometheus and Telegraf.\n\n Data Visualization\n\nVisualize information in your CrateDB cluster.\n\n Business Intelligence\n\nAnalyze information in your CrateDB cluster.\n\n Machine Learning\n\nAdapters and integrations with machine learning frameworks.\n\nNote\n\nContributions to the pages in this section and subsections are much welcome. If you would like to add items about integrations with other tools to this documentation section, please get in touch, or directly edit this page on GitHub. You will find corresponding links within the topmost right navigation element.\n\nSee Also\n\nLooking for the previous content on this page? Visit [Legacy] CrateDB Clients and Tools."
  },
  {
    "title": "The CrateDB Shell — CrateDB: Crash CLI",
    "url": "https://cratedb.com/docs/crate/crash/en/latest/",
    "html": "latest\nThe CrateDB Shell\n\nThe CrateDB Shell (aka Crash) is an interactive command-line interface (CLI) tool for working with CrateDB.\n\nSee Also\n\nCrash is an open source project and is hosted on GitHub.\n\nScreenshots\n\n  \n\nTable of contents\n\nGetting started\nInstallation\nRun\nQuery\nRunning Crash\nCommand-line options\nUser configuration directory\nEnvironment variables\nStatus messages\nUsing a pager program\nCommands\nTroubleshooting\nDebugging connection errors\nSSL connection errors\nAppendices\nResponse formats\nCompatibility"
  },
  {
    "title": "The CrateDB Admin UI — CrateDB: Admin UI",
    "url": "https://cratedb.com/docs/crate/admin-ui/en/latest/",
    "html": "latest\nThe CrateDB Admin UI\n\nCrateDB ships with a web administration user interface (or Admin UI).\n\nThe CrateDB Admin UI runs on every CrateDB node. You can use it to inspect and interact with the whole CrateDB cluster in a number of ways.\n\nSee Also\n\nThe CrateDB Admin UI is an open source project and is hosted on GitHub.\n\nTable of contents\n\nConnecting\n\nNavigating\n\nStatus bar\n\nTabs\n\nConnecting\n\nYou can access the Admin UI via HTTP on port 4200:\n\nhttp://HOSTNAME:4200/\n\n\nReplace HOSTNAME with the hostname of the CrateDB node. If CrateDB is running locally, this will be localhost.\n\nNavigate to this URL in a web browser.\n\nTip\n\nIf you access port 4200 via a client library or command-line tool like curl or wget, the request will be handled by the CrateDB Rest API, and the response will be in JSON.\n\nNavigating\n\nThis is what the Admin UI looks like when it first loads:\n\nTake note of the status bar (at the top) and the tabs (down the left side).\n\nStatus bar\n\nAlong the top of the screen, from left to right, the status bar shows:\n\nCluster name\n\nCrateDB version\n\nNumber of nodes in the cluster\n\nHealth checks\n\nData status\n\nGreen – All data is replicated and available\n\nYellow – Some records are unreplicated\n\nRed – Some data is unavailable\n\nCluster status:\n\nGreen – Good configuration\n\nYellow – Some configuration warnings\n\nRed – Some configuration errors\n\nAverage cluster load (for the past 1 minute, 5 minutes, and 15 minutes)\n\nSettings and notifications menu\n\nTabs\n\nOn the left-hand side, from top to bottom, the tabs are:\n\nOverview screen\n\nSQL console\n\nTables browser\n\nViews browser\n\nShards browser\n\nCluster browser\n\nMonitoring overview\n\nPrivileges browser\n\nHelp screen"
  },
  {
    "title": "Appendices — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/appendices/index.html",
    "html": "5.6\nAppendices\n\nSupplementary information for the CrateDB reference manual.\n\nTable of contents\n\nRelease Notes\nVersions\nOlder Versions\nSQL compatibility\nImplementation notes\nUnsupported features and functions\nSQL standard compliance\nResiliency Issues\nKnown issues\nFixed issues\nGlossary\nTerms"
  },
  {
    "title": "Client interfaces — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/interfaces/index.html",
    "html": "5.6\nClient interfaces\n\nCrateDB has two primary client interfaces:\n\nHTTP endpoint\nPostgreSQL wire protocol\n\nSee Also\n\nConnecting to CrateDB — Includes an introduction to the web Admin UI, the CrateDB shell, and the CrateDB HTTP endpoint\n\nClient Libraries — Officially supported clients and community supported clients"
  },
  {
    "title": "SQL syntax — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/sql/index.html",
    "html": "5.6\nSQL syntax\n\nYou can use Structured Query Language (SQL) to query your data.\n\nThis section of the documentation provides a complete SQL syntax reference for CrateDB.\n\nNote\n\nFor introductions to CrateDB functionality, we recommend you consult the appropriate top-level section of the documentation. The SQL syntax reference assumes a basic familiarity with the relevant parts of CrateDB.\n\nSee Also\n\nGeneral use: Data definition\n\nGeneral use: Data manipulation\n\nGeneral use: Querying\n\nGeneral use: Built-in functions and operators\n\nGeneral SQL\nConstraints\nValue expressions\nLexical structure\nSQL Statements\nALTER CLUSTER\nALTER PUBLICATION\nALTER TABLE\nALTER ROLE\nALTER USER\nANALYZE\nBEGIN\nCLOSE\nCOMMIT\nCOPY FROM\nCOPY TO\nCREATE ANALYZER\nCREATE BLOB TABLE\nCREATE FUNCTION\nCREATE PUBLICATION\nCREATE REPOSITORY\nCREATE SNAPSHOT\nCREATE SUBSCRIPTION\nCREATE TABLE\nCREATE TABLE AS\nCREATE ROLE\nCREATE USER\nCREATE VIEW\nDEALLOCATE\nDECLARE\nDELETE\nDENY\nDISCARD\nDROP ANALYZER\nDROP FUNCTION\nDROP PUBLICATION\nDROP REPOSITORY\nDROP SNAPSHOT\nDROP SUBSCRIPTION\nDROP TABLE\nDROP ROLE\nDROP USER\nDROP VIEW\nEND\nEXPLAIN\nFETCH\nGRANT\nINSERT\nKILL\nOPTIMIZE\nREFRESH\nRESTORE SNAPSHOT\nREVOKE\nSELECT\nSET and RESET\nSET LICENSE\nSET AND RESET SESSION AUTHORIZATION\nSET TRANSACTION\nSHOW (session settings)\nSHOW COLUMNS\nSHOW CREATE TABLE\nSHOW SCHEMAS\nSHOW TABLES\nSTART TRANSACTION\nUPDATE\nVALUES\nWITH"
  },
  {
    "title": "Administration — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/admin/index.html",
    "html": "5.6\nAdministration\n\nThis section of the documentation covers any feature primarily of interest to a database administrator.\n\nTable of contents\n\nSystem information\nRuntime configuration\nUsers and roles management\nPrivileges\nAuthentication\nAuthentication Methods\nHost-Based Authentication (HBA)\nSecured communications (SSL/TLS)\nOptimization\nJobs management\nJMX monitoring\nSnapshots\nLogical replication\nCloud discovery\nUsage Data Collector"
  },
  {
    "title": "General use — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/general/index.html",
    "html": "5.6\nGeneral use\n\nThis section of the documentation covers any feature primarily of interest to a general user.\n\nTable of contents\n\nData definition\nCreating tables\nData types\nSystem columns\nGenerated columns\nConstraints\nStorage\nPartitioned tables\nSharding\nReplication\nShard allocation filtering\nColumn policy\nFulltext indices\nFulltext analyzers\nShow Create Table\nViews\nAltering tables\nData manipulation\nInserting data\nUpdating data\nDeleting data\nImport and export\nQuerying\nSelecting data\nJoins\nUnion\nRefresh\nFulltext search\nGeo search\nBuilt-in functions and operators\nScalar functions\nAggregation\nArithmetic operators\nBit operators\nTable functions\nComparison operators\nArray comparisons\nSubquery expressions\nWindow functions\nUser-defined functions\nCREATE OR REPLACE\nSupported types\nOverloading\nDeterminism\nDROP FUNCTION\nSupported languages\nBlobs\nCreating a table for blobs\nCustom location for storing blob data\nList\nAltering a blob table\nDeleting a blob table\nUsing blob tables\nOptimistic Concurrency Control\nIntroduction\nOptimistic update\nOptimistic delete\nKnown limitations\nInformation schema\nAccess\nVirtual tables"
  },
  {
    "title": "Configuration — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/config/index.html",
    "html": "5.6\nConfiguration\n\nCrateDB ships with sensible defaults, so configuration is typically not needed for basic, single node use.\n\nCrateDB can be configured via configuration files. These files are located in the config directory inside the CRATE_HOME directory.\n\nThe configuration directory can changed via the path.conf setting, like so:\n\nsh$ ./bin/crate -Cpath.conf=<CUSTOM_CONFIG_DIR>\n\n\nHere, replace <CUSTOM_CONFIG_DIR> with the path to your custom configuration directory.\n\nThe primary configuration file is named crate.yml. The default version of this file has a commented out listing of every available setting. (Some features, such as logging, use feature-specific files.)\n\nSettings can be configured via the configuration file or via the -C option at startup. So, for example, you can set the cluster name at startup, like so:\n\nsh$ ./bin/crate -Ccluster.name=cluster\n\n\nSettings passed at startup use the same name as the settings in the configuration file. So the equivalent setting in the configuration file would be:\n\ncluster.name = cluster\n\n\nSettings are applied in the following order:\n\nDefault values\n\nConfiguration file\n\nCommand-line options\n\nEach setting value overwrites any previous value. So, for example, command line settings will override configuration file settings.\n\nTip\n\nCluster settings can be changed at runtime.\n\nNote\n\nIf you’re just getting started with a particular part of CrateDB, we recommend you consult the appropriate top-level section of the documentation. The rest of this configuration documentation assumes a basic familiarity with the relevant parts of CrateDB.\n\nTable of contents\n\nNode-specific settings\nBasics\nNode types\nGeneral\nNetworking\nPaths\nPlug-ins\nCPU\nMemory\nGarbage collection\nAuthentication\nSecured communications (SSL/TLS)\nCross-origin resource sharing (CORS)\nBlobs\nRepositories\nQueries\nLegacy\nJavaScript language\nCustom attributes\nCluster-wide settings\nNon-runtime cluster-wide settings\nCollecting stats\nShard limits\nUsage data collector\nGraceful stop\nBulk operations\nDiscovery\nRouting allocation\nRecovery\nMemory management\nQuery circuit breaker\nRequest circuit breaker\nAccounting circuit breaker\nStats circuit breakers\nTotal circuit breaker\nThread pools\nOverload Protection\nMetadata\nLogical Replication\nSession settings\nUsage\nSupported session settings\nLogging\nApplication logging\nJVM logging\nEnvironment variables\nApplication variables\nJVM variables"
  },
  {
    "title": "CLI tools — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/cli-tools.html",
    "html": "5.6\nCLI tools\n\nCrateDB ships with command-line interface (CLI) tools (also referred to as executables) in the bin directory.\n\nIf your working directory is CRATE_HOME, you can run an executable like this:\n\nsh$ bin/crate\n\n\nOtherwise, you can run:\n\nsh$ <PATH_TO_CRATE_HOME>/bin/crate\n\n\nHere, replace <PATH_TO_CRATE_HOME> with a path to CRATE_HOME.\n\nAlternatively, if the CrateDB bin directory is on your PATH, you can run an executable directly:\n\nsh$ crate\n\n\nTable of contents\n\ncrate\n\nSynopsis\n\nOptions\n\nSignal handling\n\nExample\n\ncrate-node\n\nSynopsis\n\nCommands\n\nOptions\n\ncrate\n\nThe crate executable runs the CrateDB daemon.\n\nSee Also\n\nThis section is a low-level command reference. For help installing CrateDB for the first time, check out the CrateDB installation tutorial. Alternatively, consult the deployment guide for help running CrateDB in production.\n\nSynopsis\nsh$ bin/crate [-dhvCDX] [-p <PID_FILE>]\n\nOptions\n\nOption\n\n\t\n\nDescription\n\n\n\n\n-h\n\n\t\n\nPrint usage information\n\n\n\n\n-v\n\n\t\n\nPrint version information\n\n\n\n\n-C\n\n\t\n\nSet a CrateDB configuration value (overrides configuration file)\n\n\n\n\n-D\n\n\t\n\nSet a Java system property value\n\n\n\n\n-X\n\n\t\n\nSet a nonstandard java option\n\nSignal handling\n\nThe CrateDB process can handle the following signals.\n\nSignal\n\n\t\n\nDescription\n\n\n\n\nTERM\n\n\t\n\nTerminates the process\n\n\n\n\nINT\n\n\t\n\nTerminates the process\n\nTip\n\nThe TERM signal stops CrateDB immediately. As a result, pending requests may fail. To ensure that CrateDB finishes handling pending requests before the node is stopped, you can, instead, perform a graceful stop with the DECOMMISSION statement.\n\nExample\n\nThe simplest way to start a CrateDB instance is to invoke crate without parameters:\n\nsh$ bin/crate\n\n\nThis command starts the process in the foreground.\n\nIt’s helpful to write the process ID (PID) to a PID file with the use of echo $!. So you execute the following:\n\nsh$ bin/crate & echo $! > \"/tmp/crate.pid\"\n\n\nTo stop the process, send a TERM signal using the PID file, like so:\n\nsh$ kill -TERM `cat /tmp/crate.pid`\n\ncrate-node\n\nThe crate-node executable is a tool that can help you:\n\nRepurpose a node\n\nPerform an unsafe cluster bootstrap\n\nDetach a node from its cluster\n\nSee Also\n\nThis section is a low-level command reference. For help using crate-node, consult the troubleshooting guide.\n\nSynopsis\nsh$ bin/crate-node repurpose|unsafe-bootstrap|detach-cluster\n[--ordinal <INT>] [-C<key>=<value>]\n[-h, --help] ([-s, --silent] | [-v, --verbose])\n\nCommands\n\nCommand\n\n\t\n\nDescription\n\n\n\n\nrepurpose\n\n\t\n\nClean up any unnecessary data on disk after changing the role of a node.\n\n\n\n\nunsafe-bootstrap\n\n\t\n\nForce the election of a master and create a new cluster in the event of losing the majority of master-eligible nodes.\n\n\n\n\ndetach-cluster\n\n\t\n\nDetach a node from a cluster so that it can join a new one.\n\n\n\n\nremove-settings\n\n\t\n\nRemove persistent settings from the cluster state in case where it contains incompatible settings that prevent the cluster from forming.\n\n\n\n\noverride-version\n\n\t\n\nOverride the version number stored in the data path to be able to force a node to startup even when the node version is not compatible with the meta data.\n\n\n\n\nfix-metadata\n\n\t\n\nFix corrupted metadata after running table swap like: ALTER CLUSTER SWAP TABLE “schema”.”table” TO “schema.table”;\n\nOptions\n\nOption\n\n\t\n\nDescription\n\n\n\n\n--ordinal <INT>\n\n\t\n\nSpecify which node to target if there is more than one node sharing a data path\n\n\n\n\n-C\n\n\t\n\nSet a CrateDB configuration value (overrides configuration file)\n\n\n\n\n-h, --help\n\n\t\n\nReturn all of the command parameters\n\n\n\n\n-s, --silent\n\n\t\n\nShow minimal output\n\n\n\n\n-v, --verbose\n\n\t\n\nShows verbose output"
  },
  {
    "title": "Concepts — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/concepts/index.html",
    "html": "5.6\nConcepts\n\nThis section of the documentation covers important CrateDB concepts.\n\nTable of contents\n\nJoins\nClustering\nStorage and consistency\nResiliency"
  },
  {
    "title": "The CrateDB Guide — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/",
    "html": "The CrateDB Guide\n\nGuides and tutorials about how to use CrateDB and CrateDB Cloud in practice.\n\nCrateDB is a distributed and scalable SQL database for storing and analyzing massive amounts of data in near real-time, even with complex queries. It is PostgreSQL-compatible, and based on Lucene.\n\nInstallation\n\nInstalling CrateDB\nGetting Started\n\nGetting started with CrateDB\nAdministration\n\nCrateDB Administration\nPerformance Guides\n\nCrateDB Performance Guides\nApplication Domains\n\nLearn how to apply CrateDB’s features to optimally cover use-cases in different application and topic domains.\n\nDocument Store\n\nStoring JSON documents using CrateDB’s `OBJECT` data type\nFull-Text Search\n\nAbout CrateDB’s full-text search capabilities\nIndustrial Data\n\nCrateDB in industrial / IIoT / Industry 4.0 scenarios\nTime Series Data\n\nManaging Time Series Data with CrateDB\nMachine Learning\n\nMachine Learning with CrateDB\nIntegrations\n\nLearn how to use CrateDB with 3rd-party software applications, libraries, and frameworks.\n\nETL\n\nLoad and export data into/from CrateDB\nMetrics\n\nCrateDB with metrics collection agents, brokers, and stores\nData Visualization\n\nData visualization with CrateDB\nBusiness Intelligence\n\nAnalyse information with CrateDB\nMachine Learning\n\nMachine Learning with CrateDB\nSoftware Testing\n\nSoftware testing with CrateDB\nReference Architectures\n\nReference architectures illustrating how CrateDB can be used in a variety of use-cases.\n\nReference Architectures\n\nReference Architectures with CrateDB\n\nTip\n\nPlease also visit the Overview of CrateDB integration tutorials.\n\nSee Also\n\nCrateDB and its documentation are open source projects. Contributions to the pages in this section and subsections are much appreciated. If you can spot a flaw, or would like to contribute additional content, you are most welcome.\n\nYou will find corresponding links within the topmost right navigation element on each page, linking to the relevant page where this project is hosted on GitHub."
  },
  {
    "title": "Installation — CrateDB: Guide",
    "url": "https://cratedb.com/docs/guide/install/",
    "html": "Installation\n\nThis section of the documentation covers the installation of CrateDB on different kinds of operating systems and environments, both suitable for on-premises and development sandbox operations.\n\nThe first step to using any software package is getting it properly installed. Please read this section carefully.\n\nDebian, Ubuntu\n\n \n\nDebian and Ubuntu Linux\nRed Hat, SUSE\n\n \n\nRPM Linux: Red Hat, SUSE\nmacOS\n\nmacOS\nWindows\n\nWindows\nTarball Archive\n\nInstallation from Tarball\nContainer Setup\n\nContainer Setup\nCloud Hosting\n\nCloud Hosting\nConfig Settings\n\nConfiguration Settings\n\nWe recommend to use the package-based installation methods for CrateDB on Debian, Ubuntu, and Derivates and CrateDB on Red Hat, SUSE, and Derivates, by subscribing to the corresponding package release channels.\n\nAlternatively, you can also download release archives and run CrateDB manually, by using the Ad Hoc method.\n\nNotes\n\nAfter the installation is finished, the CrateDB service should be up and running, and will run a HTTP server on localhost:4200. To access the Admin UI from your local machine, navigate to:\n\nhttp://localhost:4200/\n\n\nNote\n\nCrateDB requires a Java virtual machine to run.\n\nStarting with CrateDB 4.2, Java is bundled with CrateDB, and no extra installation is necessary.\n\nCrateDB versions before 4.2 required a separate Java installation. For CrateDB 3.0 to 4.1, Java 11 is the minimum requirement. CrateDB versions before 3.0 require Java 8. We recommend to use OpenJDK on Linux Systems."
  },
  {
    "title": "CrateDB Cloud — CrateDB Cloud",
    "url": "https://cratedb.com/docs/cloud/en/latest/",
    "html": "CrateDB Cloud\n\nCrateDB Cloud is a fully managed, terabyte-scale, and cost-effective analytics database that lets you run analytics over vast amounts of data in near real time, even with complex queries.\n\nIt is an SQL database service for enterprise data warehouse workloads, that works across clouds and scales with your data.\n\nDatabase Features\n\nCrateDB Cloud helps you manage and analyze your data with procedures like machine learning, geospatial analysis, and business intelligence.\n\nCrateDB Cloud’s scalable, distributed analysis engine lets you query terabytes worth of data efficiently.\n\nCrateDB provides a rich data model including container-, geospatial-, and vector-data types, and capabilities for full-text search.\n\nOperational Benefits\n\nCrateDB Cloud is a fully managed enterprise service, allowing you to deploy, monitor, back up, and scale your CrateDB clusters in the cloud without the need to do database management yourself.\n\nWith CrateDB Cloud, there’s no infrastructure to set up or manage, letting you focus on finding meaningful insights using plain SQL, and taking advantage of flexible pricing models across on-demand and flat-rate options.\n\nLearn\n\nUsers around the world rely on CrateDB Cloud clusters to store billions of records and terabytes of data, all accessible without delays. If you want to start using CrateDB Cloud, or make the most of your existing subscription, we are maintaining resources and tutorials to support you correspondingly.\n\n Quick Start\n\nLearn how to sign up and get started with a free cluster.\n\n Tutorials\n\nLearn how to use key features of CrateDB.\n\n Manage\n\nLearn how to manage your cluster.\n\n Web Console\n\nThe web-based management console for all your managed and on-premise clusters.\n\n Croud CLI\n\nA command-line based terminal program to operate your managed clusters.\n\n CrateDB Cloud on Kubernetes\n\nRun your own CrateDB Cloud region using Kubernetes.\n\nDo you want to learn about key database drivers and client applications for CrateDB, such as CrateDB Admin UI, crash, psql, DataGrip, and DBeaver? Discover how to configure these tools and explore CrateDB’s compatibility with analytics, ETL, BI, and monitoring solutions.\n\n Admin UI\n\nEach CrateDB Cloud cluster offers a dedicated Admin UI, which can be used to explore data, schema metadata, and cluster status information.\n\n Clients, Tools, and Integrations\n\nLearn about compatible client applications and tools, and how to configure your favorite client library to connect to a CrateDB cluster.\n\n Import data\n\nLearn how to import data into your CrateDB Cloud clusters.\n\nNote\n\nLike CrateDB itself, this is an open source documentation project. Suggestions for improvements, and source code contributions, are always welcome. "
  },
  {
    "title": "Search — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/search.html",
    "html": "5.6\nSearch\n\nFrom here you can search these documents. Enter your search words into the box below and click \"search\". Note that the search function will automatically search for all of the words. Pages containing fewer words won't appear in the result list.\n\nSearch"
  },
  {
    "title": "CrateDB Reference — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/#",
    "html": "5.6\nCrateDB Reference\n\nCrateDB is a distributed SQL database that makes it simple to store and analyze massive amounts of machine data in real-time.\n\nNote\n\nThis resource assumes you know the basics. If not, check out the Tutorials section for beginner material.\n\nSee Also\n\nCrateDB is an open source project and is hosted on GitHub.\n\nTable of contents\n\nConcepts\nJoins\nClustering\nStorage and consistency\nResiliency\nCLI tools\ncrate\ncrate-node\nConfiguration\nNode-specific settings\nCluster-wide settings\nSession settings\nLogging\nEnvironment variables\nGeneral use\nData definition\nData manipulation\nQuerying\nBuilt-in functions and operators\nUser-defined functions\nBlobs\nOptimistic Concurrency Control\nInformation schema\nAdministration\nSystem information\nRuntime configuration\nUsers and roles management\nPrivileges\nAuthentication\nSecured communications (SSL/TLS)\nOptimization\nJobs management\nJMX monitoring\nSnapshots\nLogical replication\nCloud discovery\nUsage Data Collector\nSQL syntax\nGeneral SQL\nSQL Statements\nClient interfaces\nHTTP endpoint\nPostgreSQL wire protocol\nAppendices\nRelease Notes\nSQL compatibility\nSQL standard compliance\nResiliency Issues\nGlossary"
  },
  {
    "title": "CrateDB Reference — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/",
    "html": "5.6\nCrateDB Reference\n\nCrateDB is a distributed SQL database that makes it simple to store and analyze massive amounts of machine data in real-time.\n\nNote\n\nThis resource assumes you know the basics. If not, check out the Tutorials section for beginner material.\n\nSee Also\n\nCrateDB is an open source project and is hosted on GitHub.\n\nTable of contents\n\nConcepts\nJoins\nClustering\nStorage and consistency\nResiliency\nCLI tools\ncrate\ncrate-node\nConfiguration\nNode-specific settings\nCluster-wide settings\nSession settings\nLogging\nEnvironment variables\nGeneral use\nData definition\nData manipulation\nQuerying\nBuilt-in functions and operators\nUser-defined functions\nBlobs\nOptimistic Concurrency Control\nInformation schema\nAdministration\nSystem information\nRuntime configuration\nUsers and roles management\nPrivileges\nAuthentication\nSecured communications (SSL/TLS)\nOptimization\nJobs management\nJMX monitoring\nSnapshots\nLogical replication\nCloud discovery\nUsage Data Collector\nSQL syntax\nGeneral SQL\nSQL Statements\nClient interfaces\nHTTP endpoint\nPostgreSQL wire protocol\nAppendices\nRelease Notes\nSQL compatibility\nSQL standard compliance\nResiliency Issues\nGlossary"
  },
  {
    "title": "CrateDB Reference — CrateDB: Reference",
    "url": "https://cratedb.com/docs/crate/reference/en/5.6/",
    "html": "5.6\nCrateDB Reference\n\nCrateDB is a distributed SQL database that makes it simple to store and analyze massive amounts of machine data in real-time.\n\nNote\n\nThis resource assumes you know the basics. If not, check out the Tutorials section for beginner material.\n\nSee Also\n\nCrateDB is an open source project and is hosted on GitHub.\n\nTable of contents\n\nConcepts\nJoins\nClustering\nStorage and consistency\nResiliency\nCLI tools\ncrate\ncrate-node\nConfiguration\nNode-specific settings\nCluster-wide settings\nSession settings\nLogging\nEnvironment variables\nGeneral use\nData definition\nData manipulation\nQuerying\nBuilt-in functions and operators\nUser-defined functions\nBlobs\nOptimistic Concurrency Control\nInformation schema\nAdministration\nSystem information\nRuntime configuration\nUsers and roles management\nPrivileges\nAuthentication\nSecured communications (SSL/TLS)\nOptimization\nJobs management\nJMX monitoring\nSnapshots\nLogical replication\nCloud discovery\nUsage Data Collector\nSQL syntax\nGeneral SQL\nSQL Statements\nClient interfaces\nHTTP endpoint\nPostgreSQL wire protocol\nAppendices\nRelease Notes\nSQL compatibility\nSQL standard compliance\nResiliency Issues\nGlossary"
  }
]